{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results from Hyperparameter Tuning For Current UPDRS\n",
    "Using the protein and peptide data as well as the visit month, predict the UPDRS value as either Mild, Moderate, or Severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from the csv file for xgboost hyperparameter tuning\n",
    "xgb_hyperparams_df = pd.read_csv('../data/processed/xgboost_extrafeats_future_cat_hyperparam_results.csv', index_col=0)\n",
    "lgb_hyperparams_df = pd.read_csv('../data/processed/lgboost_extrafeats_future_cat_hyperparam_results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.873455</td>\n",
       "      <td>0.885309</td>\n",
       "      <td>0.556294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_split_gain</th>\n",
       "      <td>1.025932</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.073092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.940496</td>\n",
       "      <td>0.574412</td>\n",
       "      <td>0.684648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>0.894024</td>\n",
       "      <td>2.040004</td>\n",
       "      <td>2.496106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>0.222292</td>\n",
       "      <td>2.131621</td>\n",
       "      <td>4.798424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>6.000214</td>\n",
       "      <td>5.622438</td>\n",
       "      <td>4.857030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.606967</td>\n",
       "      <td>0.895645</td>\n",
       "      <td>0.961862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   updrs_1   updrs_2   updrs_3\n",
       "colsample_bytree  0.873455  0.885309  0.556294\n",
       "min_split_gain    1.025932  0.000952  0.073092\n",
       "learning_rate     0.940496  0.574412  0.684648\n",
       "max_depth         1.000000  1.000000  2.000000\n",
       "min_child_weight  0.894024  2.040004  2.496106\n",
       "reg_alpha         0.222292  2.131621  4.798424\n",
       "reg_lambda        6.000214  5.622438  4.857030\n",
       "subsample         0.606967  0.895645  0.961862"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.796869</td>\n",
       "      <td>0.703083</td>\n",
       "      <td>0.888702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.005378</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.920227</td>\n",
       "      <td>0.756913</td>\n",
       "      <td>0.637066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>0.283017</td>\n",
       "      <td>2.460482</td>\n",
       "      <td>2.294683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>1.582413</td>\n",
       "      <td>5.804418</td>\n",
       "      <td>0.829745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>2.446018</td>\n",
       "      <td>7.883427</td>\n",
       "      <td>2.323503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.757228</td>\n",
       "      <td>0.691198</td>\n",
       "      <td>0.846702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   updrs_1   updrs_2   updrs_3\n",
       "colsample_bytree  0.796869  0.703083  0.888702\n",
       "gamma             0.020274  0.005378  0.000230\n",
       "learning_rate     0.920227  0.756913  0.637066\n",
       "max_depth         7.000000  4.000000  3.000000\n",
       "min_child_weight  0.283017  2.460482  2.294683\n",
       "reg_alpha         1.582413  5.804418  0.829745\n",
       "reg_lambda        2.446018  7.883427  2.323503\n",
       "subsample         0.757228  0.691198  0.846702"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the protein and updrs data\n",
    "updrs1_df = pd.read_csv('../data/processed/train_updrs_1_cat.csv')\n",
    "updrs2_df = pd.read_csv('../data/processed/train_updrs_2_cat.csv')\n",
    "updrs3_df = pd.read_csv('../data/processed/train_updrs_3_cat.csv')\n",
    "\n",
    "\n",
    "train_data_new_feats = pd.read_csv(\"../data/processed/train_data_new_feats.csv\")\n",
    "\n",
    "    updrs1_cols = [\n",
    "        \"visit_id\",\n",
    "        \"patient_id\",\n",
    "        \"visit_month\",\n",
    "        \"updrs_1\",\n",
    "        \"num_prot_pep\",\n",
    "        \"num_prot\",\n",
    "        \"num_pept\",\n",
    "        \"updrs_1_cat\",\n",
    "    ]\n",
    "    updrs2_cols = [\n",
    "        \"visit_id\",\n",
    "        \"patient_id\",\n",
    "        \"visit_month\",\n",
    "        \"updrs_2\",\n",
    "        \"num_prot_pep\",\n",
    "        \"num_prot\",\n",
    "        \"num_pept\",\n",
    "        \"updrs_2_cat\",\n",
    "    ]\n",
    "    updrs3_cols = [\n",
    "        \"visit_id\",\n",
    "        \"patient_id\",\n",
    "        \"visit_month\",\n",
    "        \"updrs_3\",\n",
    "        \"num_prot_pep\",\n",
    "        \"num_prot\",\n",
    "        \"num_pept\",\n",
    "        \"updrs_3_cat\",\n",
    "    ]\n",
    "\n",
    "    new_updrs1_df = updrs1_df[updrs1_cols].merge(\n",
    "        train_data_new_feats, how=\"left\", on=[\"visit_id\", \"patient_id\"]\n",
    "    )\n",
    "    new_updrs2_df = updrs2_df[updrs2_cols].merge(\n",
    "        train_data_new_feats, how=\"left\", on=[\"visit_id\", \"patient_id\"]\n",
    "    )\n",
    "    new_updrs3_df = updrs3_df[updrs3_cols].merge(\n",
    "        train_data_new_feats, how=\"left\", on=[\"visit_id\", \"patient_id\"]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mild        854\n",
       "moderate    199\n",
       "severe       15\n",
       "Name: updrs_1_cat, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_updrs1_df['updrs_1_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mild        910\n",
       "moderate    158\n",
       "Name: updrs_2_cat, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_updrs2_df['updrs_2_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mild        880\n",
       "moderate    168\n",
       "severe       10\n",
       "Name: updrs_3_cat, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_updrs3_df['updrs_3_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the categorical updrs scores with numerical for mild, moderate and severe\n",
    "## combine the moderate and severe categories since there are very few severe observations\n",
    "new_updrs1_df['updrs_1_cat'] = new_updrs1_df['updrs_1_cat'].map({'mild': 0, 'moderate': 1, 'severe': 1})\n",
    "new_updrs2_df['updrs_2_cat'] = new_updrs2_df['updrs_2_cat'].map({'mild': 0, 'moderate': 1, 'severe': 1})\n",
    "new_updrs3_df['updrs_3_cat'] = new_updrs3_df['updrs_3_cat'].map({'mild': 0, 'moderate': 1, 'severe': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    880\n",
       "1    178\n",
       "Name: updrs_3_cat, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_updrs3_df['updrs_3_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visit_id', 'patient_id', 'visit_month', 'updrs_3', 'O00391', 'O00533',\n",
       "       'O00584', 'O14498', 'O14773', 'O14791',\n",
       "       ...\n",
       "       'YVNKEIQNAVNGVK_P10909', 'YWGVASFLQK_P02753',\n",
       "       'YYC(UniMod_4)FQGNQFLR_P02790', 'YYTYLIMNK_P01024',\n",
       "       'YYWGGQYTWDMAK_P02675', 'kfold', 'num_prot_pep', 'num_prot', 'num_pept',\n",
       "       'updrs_3_cat'],\n",
       "      dtype='object', length=1204)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_updrs3_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_fold_validation(df, model, target):\n",
    "\n",
    "    updrs_results = dict()\n",
    "    \n",
    "    for fold in range(0, 5):\n",
    "        # get the train and test data for the current fold\n",
    "        train = df[df['kfold'] != fold].reset_index(drop=True)\n",
    "        test = df[df['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # get the train and test data for the current fold\n",
    "        drop_cols = ['visit_id', 'patient_id', f'{target}', 'kfold', f'{target}_cat']\n",
    "        X_train = train.drop(columns=drop_cols)\n",
    "        y_train = train[f'{target}_cat']\n",
    "        X_test = test.drop(columns=drop_cols)\n",
    "        y_test = test[f'{target}_cat']\n",
    "\n",
    "        # train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # make predictions\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "\n",
    "        # save the results\n",
    "        updrs_results[f'{target}_fold_{fold}'] = {\n",
    "            'auc_score': roc_auc_score(y_test, preds),\n",
    "            'acc_score': accuracy_score(y_test, preds),\n",
    "            'precision_score': precision_score(y_test, preds),\n",
    "            'recall_score': recall_score(y_test, preds),\n",
    "        }\n",
    "        \n",
    "    mean_auc = np.mean([updrs_results[f'{target}_fold_{fold}']['auc_score'] for fold in range(0, 5)])\n",
    "    mean_acc = np.mean([updrs_results[f'{target}_fold_{fold}']['acc_score'] for fold in range(0, 5)])\n",
    "    mean_precision = np.mean([updrs_results[f'{target}_fold_{fold}']['precision_score'] for fold in range(0, 5)])\n",
    "    mean_recall = np.mean([updrs_results[f'{target}_fold_{fold}']['recall_score'] for fold in range(0, 5)])\n",
    "    \n",
    "    return mean_auc, mean_acc, mean_precision, mean_recall\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_xgboost_model(xgb_hyperparams_df, target):\n",
    "    # train the model using the hyperparameters from the hyperparameter tuning\n",
    "    updrs_hp = xgb_hyperparams_df[target].to_dict()\n",
    "    updrs_hp['max_depth'] = int(updrs_hp['max_depth'])\n",
    "    model = XGBClassifier(**updrs_hp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': 0.7968694289238077,\n",
       " 'gamma': 0.0202737874859375,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': 0.9202265298452442,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 0.2830172877142298,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 1.5824128102949098,\n",
       " 'reg_lambda': 2.4460175307567287,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 0.7572283003963074,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model function\n",
    "# model = prepare_xgboost_model(xgb_hyperparams_df, 'updrs_1')\n",
    "# model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [new_updrs1_df, new_updrs2_df, new_updrs3_df]):\n",
    "    model = prepare_xgboost_model(xgb_hyperparams_df, updrs)\n",
    "    auc, acc, prec, recall = cross_fold_validation(df, model, updrs)\n",
    "    xgb_results[updrs] = {\"auc\":auc,\n",
    "                        \"acc\":acc,\n",
    "                        \"prec\":prec,\n",
    "                        \"recall\":recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updrs_1': {'auc': 0.6093154487852749,\n",
       "  'acc': 0.8052125839147031,\n",
       "  'prec': 0.5327690700104493,\n",
       "  'recall': 0.2820071412099511},\n",
       " 'updrs_2': {'auc': 0.5974217666205179,\n",
       "  'acc': 0.8557851783598789,\n",
       "  'prec': 0.536969696969697,\n",
       "  'recall': 0.22995267072346176},\n",
       " 'updrs_3': {'auc': 0.6031350592526609,\n",
       "  'acc': 0.8393186086023429,\n",
       "  'prec': 0.557085020242915,\n",
       "  'recall': 0.2470530802449308}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Classifier Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lgboost_model(lgb_hyperparams_df, target):\n",
    "    # train the model using the hyperparameters from the hyperparameter tuning\n",
    "    updrs_hp = lgb_hyperparams_df[target].to_dict()\n",
    "    updrs_hp['max_depth'] = int(updrs_hp['max_depth'])\n",
    "    model = LGBMClassifier(**updrs_hp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.873455</td>\n",
       "      <td>0.885309</td>\n",
       "      <td>0.556294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_split_gain</th>\n",
       "      <td>1.025932</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.073092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.940496</td>\n",
       "      <td>0.574412</td>\n",
       "      <td>0.684648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>0.894024</td>\n",
       "      <td>2.040004</td>\n",
       "      <td>2.496106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   updrs_1   updrs_2   updrs_3\n",
       "colsample_bytree  0.873455  0.885309  0.556294\n",
       "min_split_gain    1.025932  0.000952  0.073092\n",
       "learning_rate     0.940496  0.574412  0.684648\n",
       "max_depth         1.000000  1.000000  2.000000\n",
       "min_child_weight  0.894024  2.040004  2.496106"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_hyperparams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [new_updrs1_df, new_updrs2_df, new_updrs3_df]):\n",
    "    model = prepare_lgboost_model(lgb_hyperparams_df, updrs)\n",
    "    auc, acc, prec, recall = cross_fold_validation(df, model, updrs)\n",
    "    lgb_results[updrs] = {\"auc\":auc,\n",
    "                        \"acc\":acc,\n",
    "                        \"prec\":prec,\n",
    "                        \"recall\":recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updrs_1': {'auc': 0.6151252296757459,\n",
       "  'acc': 0.8033390373393006,\n",
       "  'prec': 0.520641309155241,\n",
       "  'recall': 0.30190212639789565},\n",
       " 'updrs_2': {'auc': 0.5859883723833501,\n",
       "  'acc': 0.8661182045544293,\n",
       "  'prec': 0.6645021645021645,\n",
       "  'recall': 0.1884504271928207},\n",
       " 'updrs_3': {'auc': 0.6041906052528299,\n",
       "  'acc': 0.8506706608244656,\n",
       "  'prec': 0.6663461538461538,\n",
       "  'recall': 0.23215966390839057}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = new_updrs1_df[new_updrs1_df['kfold'] != 4].reset_index(drop=True)\n",
    "test_df = new_updrs1_df[new_updrs1_df['kfold'] == 4].reset_index(drop=True)\n",
    "X_train = train_df.drop(columns=['visit_id', 'patient_id', 'updrs_1', 'kfold', 'updrs_1_cat'])\n",
    "y_train = train_df['updrs_1_cat']\n",
    "X_test = test_df.drop(columns=['visit_id', 'patient_id', 'updrs_1', 'kfold', 'updrs_1_cat'])\n",
    "y_test = test_df['updrs_1_cat']\n",
    "\n",
    "model = prepare_lgboost_model(lgb_hyperparams_df, 'updrs_1')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "test_df['preds'] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 0.5562941718857943,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.6846478046658827,\n",
       " 'max_depth': 2,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 2.496105959484359,\n",
       " 'min_split_gain': 0.0730918428869304,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 4.798424399916732,\n",
       " 'reg_lambda': 4.85702963468057,\n",
       " 'silent': 'warn',\n",
       " 'subsample': 0.9618618264069032,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_lgboost_model(lgb_hyperparams_df, 'updrs_1')\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    193\n",
       "1     20\n",
       "Name: preds, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['preds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 1205)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_preds = test_df[test_df['updrs_1_cat'] != test_df['preds']]\n",
    "wrong_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    32\n",
       "0    10\n",
       "Name: updrs_1_cat, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_preds['updrs_1_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View XGBoost Results with Forecasting the UPDRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max category for each patient\n",
    "max_df = new_updrs1_df.groupby(['patient_id'])['updrs_1_cat'].max().reset_index()\n",
    "max_df = max_df.rename(columns={'updrs_1_cat': 'updrs_1_max_cat'})\n",
    "# merge the max category with the original dataframe\n",
    "new_updrs1_df = new_updrs1_df.merge(max_df, on=['patient_id'], how='left')\n",
    "# take only the visit months that are 12 or less\n",
    "updrs1_yr_df = new_updrs1_df[new_updrs1_df['visit_month'] <= 12]\n",
    "updrs1_yr_df = updrs1_yr_df.drop(columns=['updrs_1_cat'])\n",
    "updrs1_yr_df.rename(columns={'updrs_1_max_cat': 'updrs_1_cat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max category for each patient\n",
    "max_df = new_updrs2_df.groupby(['patient_id'])['updrs_2_cat'].max().reset_index()\n",
    "max_df = max_df.rename(columns={'updrs_2_cat': 'updrs_2_max_cat'})\n",
    "# merge the max category with the original dataframe\n",
    "new_updrs2_df = new_updrs2_df.merge(max_df, on=['patient_id'], how='left')\n",
    "# take only the visit months that are 12 or less\n",
    "updrs2_yr_df = new_updrs2_df[new_updrs2_df['visit_month'] <= 12]\n",
    "updrs2_yr_df = updrs2_yr_df.drop(columns=['updrs_2_cat'])\n",
    "updrs2_yr_df.rename(columns={'updrs_2_max_cat': 'updrs_2_cat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max category for each patient\n",
    "max_df = new_updrs3_df.groupby(['patient_id'])['updrs_3_cat'].max().reset_index()\n",
    "max_df = max_df.rename(columns={'updrs_3_cat': 'updrs_3_max_cat'})\n",
    "# merge the max category with the original dataframe\n",
    "new_updrs3_df = new_updrs3_df.merge(max_df, on=['patient_id'], how='left')\n",
    "# take only the visit months that are 12 or less\n",
    "updrs3_yr_df = new_updrs3_df[new_updrs3_df['visit_month'] <= 12]\n",
    "updrs3_yr_df = updrs3_yr_df.drop(columns=['updrs_3_cat'])\n",
    "updrs3_yr_df.rename(columns={'updrs_3_max_cat': 'updrs_3_cat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_forecast_hyperparams_df = pd.read_csv('../data/processed/xgboost_extrafeats_future_cat_hyperparam_results.csv', index_col=0)\n",
    "lgb_forecast_hyperparams_df = pd.read_csv('../data/processed/lgboost_extrafeats_future_cat_hyperparam_results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.811327</td>\n",
       "      <td>0.984160</td>\n",
       "      <td>0.895658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_split_gain</th>\n",
       "      <td>0.000096</td>\n",
       "      <td>1.272034</td>\n",
       "      <td>0.001123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.993501</td>\n",
       "      <td>0.515028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>2.174744</td>\n",
       "      <td>2.421318</td>\n",
       "      <td>0.218445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>1.434613</td>\n",
       "      <td>0.133531</td>\n",
       "      <td>0.750725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>1.360761</td>\n",
       "      <td>5.750833</td>\n",
       "      <td>2.372409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.650306</td>\n",
       "      <td>0.857437</td>\n",
       "      <td>0.637569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   updrs_1   updrs_2   updrs_3\n",
       "colsample_bytree  0.811327  0.984160  0.895658\n",
       "min_split_gain    0.000096  1.272034  0.001123\n",
       "learning_rate     0.999408  0.993501  0.515028\n",
       "max_depth         8.000000  8.000000  2.000000\n",
       "min_child_weight  2.174744  2.421318  0.218445\n",
       "reg_alpha         1.434613  0.133531  0.750725\n",
       "reg_lambda        1.360761  5.750833  2.372409\n",
       "subsample         0.650306  0.857437  0.637569"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_forecast_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.654963</td>\n",
       "      <td>0.537749</td>\n",
       "      <td>0.671407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.019979</td>\n",
       "      <td>2.102313</td>\n",
       "      <td>0.274657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.955370</td>\n",
       "      <td>0.381512</td>\n",
       "      <td>0.984645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>0.954945</td>\n",
       "      <td>0.922874</td>\n",
       "      <td>0.381240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>2.555897</td>\n",
       "      <td>2.143399</td>\n",
       "      <td>0.016335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>2.678180</td>\n",
       "      <td>2.991090</td>\n",
       "      <td>2.369731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.857239</td>\n",
       "      <td>0.960498</td>\n",
       "      <td>0.841739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   updrs_1   updrs_2   updrs_3\n",
       "colsample_bytree  0.654963  0.537749  0.671407\n",
       "gamma             0.019979  2.102313  0.274657\n",
       "learning_rate     0.955370  0.381512  0.984645\n",
       "max_depth         7.000000  1.000000  8.000000\n",
       "min_child_weight  0.954945  0.922874  0.381240\n",
       "reg_alpha         2.555897  2.143399  0.016335\n",
       "reg_lambda        2.678180  2.991090  2.369731\n",
       "subsample         0.857239  0.960498  0.841739"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS: updrs_1\n",
      "Hyperparameters: {'objective': 'binary:logistic', 'use_label_encoder': True, 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6549634006719862, 'gamma': 0.0199785023643822, 'gpu_id': None, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.9553704552877034, 'max_delta_step': None, 'max_depth': 7, 'min_child_weight': 0.9549453357994346, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': 2.555897216231412, 'reg_lambda': 2.6781795457566164, 'scale_pos_weight': None, 'subsample': 0.8572385518822795, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "\n",
      "\n",
      "[22:27:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "UPDRS: updrs_2\n",
      "Hyperparameters: {'objective': 'binary:logistic', 'use_label_encoder': True, 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.537749460013336, 'gamma': 2.1023127616083404, 'gpu_id': None, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.3815123008515123, 'max_delta_step': None, 'max_depth': 1, 'min_child_weight': 0.9228735177115128, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': 2.143398816537053, 'reg_lambda': 2.991089677288993, 'scale_pos_weight': None, 'subsample': 0.960498227037516, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "\n",
      "\n",
      "[22:27:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "UPDRS: updrs_3\n",
      "Hyperparameters: {'objective': 'binary:logistic', 'use_label_encoder': True, 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6714068957480999, 'gamma': 0.274656686524417, 'gpu_id': None, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.984644679185605, 'max_delta_step': None, 'max_depth': 8, 'min_child_weight': 0.3812400018567903, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': 0.0163347231969157, 'reg_lambda': 2.369731383945717, 'scale_pos_weight': None, 'subsample': 0.8417386965614311, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "\n",
      "\n",
      "[22:27:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_forecast_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    model = prepare_xgboost_model(xgb_forecast_hyperparams_df, updrs)\n",
    "    print(f'UPDRS: {updrs}')\n",
    "    print(f'Hyperparameters: {model.get_params()}')\n",
    "    print('\\n')\n",
    "    auc, acc, prec, recall = cross_fold_validation(df, model, updrs)\n",
    "    xgb_forecast_results[updrs] = {\"auc\":auc,\n",
    "                        \"acc\":acc,\n",
    "                        \"prec\":prec,\n",
    "                        \"recall\":recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updrs_1': {'auc': 0.627263605237623,\n",
       "  'acc': 0.6597958631565353,\n",
       "  'prec': 0.6092886111709641,\n",
       "  'recall': 0.44451518234007975},\n",
       " 'updrs_2': {'auc': 0.6038171736597545,\n",
       "  'acc': 0.7180423742923743,\n",
       "  'prec': 0.5844444444444444,\n",
       "  'recall': 0.3060383963862225},\n",
       " 'updrs_3': {'auc': 0.6472226542235492,\n",
       "  'acc': 0.6761404407023932,\n",
       "  'prec': 0.5981565275682923,\n",
       "  'recall': 0.518812769010043}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBoost Future Categorical Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS: updrs_1\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8113265579673227, 'importance_type': 'split', 'learning_rate': 0.9994080717510486, 'max_depth': 8, 'min_child_samples': 20, 'min_child_weight': 2.174744365518736, 'min_split_gain': 9.642306134042292e-05, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 1.4346132341028175, 'reg_lambda': 1.3607611957920254, 'silent': 'warn', 'subsample': 0.6503063443960371, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "\n",
      "UPDRS: updrs_2\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.9841604959860072, 'importance_type': 'split', 'learning_rate': 0.993501395286756, 'max_depth': 8, 'min_child_samples': 20, 'min_child_weight': 2.4213180697793946, 'min_split_gain': 1.2720340722131132, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.1335312624938516, 'reg_lambda': 5.75083331569941, 'silent': 'warn', 'subsample': 0.8574371766837283, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "\n",
      "UPDRS: updrs_3\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8956582328541773, 'importance_type': 'split', 'learning_rate': 0.5150279095625026, 'max_depth': 2, 'min_child_samples': 20, 'min_child_weight': 0.2184445175550065, 'min_split_gain': 0.001123048721636, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.750724759580743, 'reg_lambda': 2.3724092233371863, 'silent': 'warn', 'subsample': 0.6375686496970143, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgb_forecast_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    model = prepare_lgboost_model(lgb_forecast_hyperparams_df, updrs)\n",
    "    print(f'UPDRS: {updrs}')\n",
    "    print(model.get_params())\n",
    "    print('\\n')\n",
    "    auc, acc, prec, recall = cross_fold_validation(df, model, updrs)\n",
    "    lgb_forecast_results[updrs] = {\"auc\":auc,\n",
    "                        \"acc\":acc,\n",
    "                        \"prec\":prec,\n",
    "                        \"recall\":recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updrs_1': {'auc': 0.639971567890737,\n",
       "  'acc': 0.672581877009455,\n",
       "  'prec': 0.6115300127713921,\n",
       "  'recall': 0.47299858524208727},\n",
       " 'updrs_2': {'auc': 0.6270216415015286,\n",
       "  'acc': 0.7222527472527472,\n",
       "  'prec': 0.5772689075630252,\n",
       "  'recall': 0.3796517974778844},\n",
       " 'updrs_3': {'auc': 0.6714000151485306,\n",
       "  'acc': 0.7035588045663139,\n",
       "  'prec': 0.6505525846702318,\n",
       "  'recall': 0.5241950194712032}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_forecast_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# run cross fold validation on the updrs 1 data\n",
    "updrs1_results = cross_fold_validation(updrs1_df, model, 'updrs_1')\n",
    "\n",
    "model = LogisticRegression()\n",
    "updrs2_results = cross_fold_validation(updrs2_df, model, 'updrs_2')\n",
    "\n",
    "model = LogisticRegression()\n",
    "updrs3_results = cross_fold_validation(updrs3_df, model, 'updrs_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.511721383287093,\n",
       " 0.780847703040674,\n",
       " 0.32282051282051283,\n",
       " 0.062211088895139766)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5258909849605227,\n",
       " 0.8464349962704578,\n",
       " 0.42333333333333323,\n",
       " 0.07048988874546684)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5240749211448716,\n",
       " 0.8194894035589735,\n",
       " 0.3485714285714286,\n",
       " 0.07900396151669495)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs3_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# run cross fold validation on the updrs 1 data\n",
    "updrs1_forecast_results = cross_fold_validation(updrs1_yr_df, model, 'updrs_1')\n",
    "\n",
    "model = LogisticRegression()\n",
    "updrs2_forecast_results = cross_fold_validation(updrs2_yr_df, model, 'updrs_2')\n",
    "\n",
    "model = LogisticRegression()\n",
    "updrs3_forecast_results = cross_fold_validation(updrs3_yr_df, model, 'updrs_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5634874999789138,\n",
       " 0.5994172665293609,\n",
       " 0.4912421630094044,\n",
       " 0.3920253253358589)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs1_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.582742150911908, 0.6805340492840493, 0.4779128959276018, 0.3289821193299454)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs2_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5758538282007636,\n",
       " 0.6218648310387985,\n",
       " 0.5047414576826341,\n",
       " 0.3744481451117032)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs3_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easypy37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
