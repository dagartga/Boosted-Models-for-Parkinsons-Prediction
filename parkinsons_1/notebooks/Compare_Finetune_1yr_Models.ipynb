{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from yellowbrick.model_selection import validation_curve\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results from Hyperparameter Tuning For Current UPDRS\n",
    "Using the protein and peptide data as well as the visit month, predict the UPDRS value as either Mild, Moderate, or Severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from the csv file for xgboost hyperparameter tuning\n",
    "xgb_hyperparams_df = pd.read_csv('../data/processed/xgboost_cat_hyperparam_results.csv', index_col=0)\n",
    "lgb_hyperparams_df = pd.read_csv('../data/processed/lgboost_cat_hyperparam_results.csv', index_col=0)\n",
    "cboost_hyperparams_df = pd.read_csv('../data/processed/catboost_future_cat_hyperparam_results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.629116</td>\n",
       "      <td>0.856101</td>\n",
       "      <td>0.731011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.628567</td>\n",
       "      <td>0.750618</td>\n",
       "      <td>0.556761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>3.578848</td>\n",
       "      <td>16.437261</td>\n",
       "      <td>5.512251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_split_gain</th>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>7.735830</td>\n",
       "      <td>6.684875</td>\n",
       "      <td>1.484969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>3.493592</td>\n",
       "      <td>3.002659</td>\n",
       "      <td>3.025202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.916682</td>\n",
       "      <td>0.882994</td>\n",
       "      <td>0.556561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   updrs_1    updrs_2   updrs_3\n",
       "colsample_bytree  0.629116   0.856101  0.731011\n",
       "learning_rate     0.628567   0.750618  0.556761\n",
       "max_depth         5.000000   8.000000  3.000000\n",
       "min_child_weight  3.578848  16.437261  5.512251\n",
       "min_split_gain    0.005489   0.000046  0.000197\n",
       "reg_alpha         7.735830   6.684875  1.484969\n",
       "reg_lambda        3.493592   3.002659  3.025202\n",
       "subsample         0.916682   0.882994  0.556561"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.708264</td>\n",
       "      <td>0.694449</td>\n",
       "      <td>0.643855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>1.070175</td>\n",
       "      <td>0.032390</td>\n",
       "      <td>1.949759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.969325</td>\n",
       "      <td>0.993978</td>\n",
       "      <td>0.502350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.926601</td>\n",
       "      <td>2.980635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>1.641284</td>\n",
       "      <td>5.318046</td>\n",
       "      <td>3.232656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>6.011928</td>\n",
       "      <td>6.999249</td>\n",
       "      <td>7.487927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.900018</td>\n",
       "      <td>0.710181</td>\n",
       "      <td>0.885401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   updrs_1   updrs_2   updrs_3\n",
       "colsample_bytree  0.708264  0.694449  0.643855\n",
       "gamma             1.070175  0.032390  1.949759\n",
       "learning_rate     0.969325  0.993978  0.502350\n",
       "max_depth         2.000000  3.000000  2.000000\n",
       "min_child_weight  0.301500  0.926601  2.980635\n",
       "reg_alpha         1.641284  5.318046  3.232656\n",
       "reg_lambda        6.011928  6.999249  7.487927\n",
       "subsample         0.900018  0.710181  0.885401"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bagging_temperature</th>\n",
       "      <td>4.429427</td>\n",
       "      <td>4.095758</td>\n",
       "      <td>1.162359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <td>5.641367</td>\n",
       "      <td>5.757078</td>\n",
       "      <td>7.356710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.366781</td>\n",
       "      <td>0.954836</td>\n",
       "      <td>0.973876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_data_in_leaf</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      updrs_1   updrs_2   updrs_3\n",
       "bagging_temperature  4.429427  4.095758  1.162359\n",
       "depth                7.000000  6.000000  4.000000\n",
       "l2_leaf_reg          5.641367  5.757078  7.356710\n",
       "learning_rate        0.366781  0.954836  0.973876\n",
       "min_data_in_leaf     8.000000  8.000000  4.000000"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cboost_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the protein and updrs data\n",
    "updrs1_df = pd.read_csv('../data/processed/train_updrs_1_cat.csv')\n",
    "updrs2_df = pd.read_csv('../data/processed/train_updrs_2_cat.csv')\n",
    "updrs3_df = pd.read_csv('../data/processed/train_updrs_3_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mild        854\n",
       "moderate    199\n",
       "severe       15\n",
       "Name: updrs_1_cat, dtype: int64"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs1_df['updrs_1_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mild        910\n",
       "moderate    158\n",
       "Name: updrs_2_cat, dtype: int64"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs2_df['updrs_2_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mild        880\n",
       "moderate    168\n",
       "severe       10\n",
       "Name: updrs_3_cat, dtype: int64"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs3_df['updrs_3_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the categorical updrs scores with numerical for mild, moderate and severe\n",
    "## combine the moderate and severe categories since there are very few severe observations\n",
    "updrs1_df['updrs_1_cat'] = updrs1_df['updrs_1_cat'].map({'mild': 0, 'moderate': 1, 'severe': 1})\n",
    "updrs2_df['updrs_2_cat'] = updrs2_df['updrs_2_cat'].map({'mild': 0, 'moderate': 1, 'severe': 1})\n",
    "updrs3_df['updrs_3_cat'] = updrs3_df['updrs_3_cat'].map({'mild': 0, 'moderate': 1, 'severe': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    880\n",
       "1    178\n",
       "Name: updrs_3_cat, dtype: int64"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs3_df['updrs_3_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visit_id', 'patient_id', 'visit_month', 'updrs_3', 'O00391', 'O00533',\n",
       "       'O00584', 'O14498', 'O14773', 'O14791',\n",
       "       ...\n",
       "       'YVNKEIQNAVNGVK_P10909', 'YWGVASFLQK_P02753',\n",
       "       'YYC(UniMod_4)FQGNQFLR_P02790', 'YYTYLIMNK_P01024',\n",
       "       'YYWGGQYTWDMAK_P02675', 'kfold', 'num_prot_pep', 'num_prot', 'num_pept',\n",
       "       'updrs_3_cat'],\n",
       "      dtype='object', length=1204)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs3_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Validation Curve on XGBoost Fine Tune Parameters\n",
    "- max-depth\n",
    "- subsampling\n",
    "- gamma\n",
    "- reg_alpha\n",
    "- reg_lambda\n",
    "- scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prep the dataframe for validation curve\n",
    "train1_df = updrs1_df.drop(columns = ['visit_id', 'patient_id', 'kfold', 'updrs_1'])\n",
    "X_train_df = train1_df.drop(columns = ['updrs_1_cat'])\n",
    "y_train_df = train1_df['updrs_1_cat']\n",
    "updrs1_xgb_params = xgb_hyperparams_df['updrs_1'].to_dict()\n",
    "# plot the validation curve for the xgboost model\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "viz = validation_curve(XGBClassifier(**updrs1_xgb_params), X_train_df, y_train_df, param_name='max_depth', param_range=range(1, 14), cv=4, scoring='roc_auc', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.71445514, 0.7017529 , 0.69126154, 0.69241194, 0.668643  ,\n",
       "        0.66203679, 0.70479067, 0.71355748, 0.68777748, 0.68578406,\n",
       "        0.68184798, 0.69873032, 0.68945365]),\n",
       " array([0.05381775, 0.04903765, 0.05380173, 0.03209121, 0.05376155,\n",
       "        0.05328391, 0.05450618, 0.04014622, 0.01840309, 0.03561568,\n",
       "        0.04056826, 0.01941138, 0.03711483]))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz.test_scores_mean_, viz.test_scores_std_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Depth = 1 is the best AUC for UPDRS_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prep the dataframe for validation curve\n",
    "train2_df = updrs2_df.drop(columns = ['visit_id', 'patient_id', 'kfold', 'updrs_2'])\n",
    "X_train_df = train2_df.drop(columns = ['updrs_2_cat'])\n",
    "y_train_df = train2_df['updrs_2_cat']\n",
    "updrs1_xgb_params = xgb_hyperparams_df['updrs_2'].to_dict()\n",
    "# plot the validation curve for the xgboost model\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "viz = validation_curve(XGBClassifier(**updrs1_xgb_params), X_train_df, y_train_df, param_name='max_depth', param_range=range(1, 14), cv=4, scoring='roc_auc', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.76662579, 0.81235422, 0.75016367, 0.76002083, 0.75595844,\n",
       "        0.75723516, 0.76252662, 0.77417235, 0.74057096, 0.74590423,\n",
       "        0.75851441, 0.75403512, 0.74404572]),\n",
       " array([0.05105557, 0.02567155, 0.02191648, 0.03534794, 0.0287824 ,\n",
       "        0.04120492, 0.02101713, 0.03786163, 0.04046029, 0.04153199,\n",
       "        0.01393639, 0.04541293, 0.03770327]))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the results for updrs 2\n",
    "viz.test_scores_mean_, viz.test_scores_std_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDRS 2 Max Depth = 2 has the best AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import ValidationCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Validation Curve for XGBClassifier'}, xlabel='max_depth', ylabel='score'>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prep the dataframe for validation curve\n",
    "train3_df = updrs3_df.drop(columns = ['visit_id', 'patient_id', 'kfold', 'updrs_3'])\n",
    "X_train_df = train3_df.drop(columns = ['updrs_3_cat'])\n",
    "y_train_df = train3_df['updrs_3_cat']\n",
    "updrs1_xgb_params = xgb_hyperparams_df['updrs_3'].to_dict()\n",
    "# plot the validation curve for the xgboost model\n",
    "# Create an instance of ValidationCurve\n",
    "viz = ValidationCurve(\n",
    "    XGBClassifier(**updrs1_xgb_params, verbosity=0, silent=True),  #  Classifier object\n",
    "    param_name=\"max_depth\",  # The parameter to validate\n",
    "    param_range=range(1, 14),  # The parameter values to evaluate\n",
    "    cv=4,  # Number of cross-validation folds\n",
    "    scoring=\"roc_auc\",  # Scoring metric for evaluation\n",
    "    n_jobs=-1,  # Number of CPU cores to use (set to -1 to use all available cores)\n",
    ")\n",
    "\n",
    "# Fit the validation curve on your training data\n",
    "viz.fit(X_train_df, y_train_df)  # Replace X and y with your training data\n",
    "\n",
    "# Visualize the validation curve\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.74509412, 0.74688935, 0.71476125, 0.69985365, 0.72011421,\n",
       "        0.73757633, 0.74066001, 0.7318727 , 0.73691977, 0.7500396 ,\n",
       "        0.74814566, 0.74814566, 0.74814566]),\n",
       " array([0.03059773, 0.02631448, 0.03663231, 0.01413316, 0.02111557,\n",
       "        0.02983815, 0.02227891, 0.02067911, 0.01794835, 0.00363372,\n",
       "        0.00195867, 0.00195867, 0.00195867]))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the results for updrs 3\n",
    "viz.test_scores_mean_, viz.test_scores_std_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest Decent Model with good AUC is Max Depth = 2, but the best with low std is Max Depth = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_fold_validation(df, model, target):\n",
    "\n",
    "    updrs_results = dict()\n",
    "    \n",
    "    for fold in range(0, 5):\n",
    "        # get the train and test data for the current fold\n",
    "        train = df[df['kfold'] != fold].reset_index(drop=True)\n",
    "        test = df[df['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # get the train and test data for the current fold\n",
    "        drop_cols = ['visit_id', 'patient_id', f'{target}', 'kfold', f'{target}_cat']\n",
    "        X_train = train.drop(columns=drop_cols)\n",
    "        y_train = train[f'{target}_cat']\n",
    "        X_test = test.drop(columns=drop_cols)\n",
    "        y_test = test[f'{target}_cat']\n",
    "\n",
    "        # train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # make predictions\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "\n",
    "        # save the results\n",
    "        updrs_results[f'{target}_fold_{fold}'] = {\n",
    "            'auc_score': roc_auc_score(y_test, preds),\n",
    "            'acc_score': accuracy_score(y_test, preds),\n",
    "            'precision_score': precision_score(y_test, preds),\n",
    "            'recall_score': recall_score(y_test, preds),\n",
    "        }\n",
    "        \n",
    "    mean_auc = np.mean([updrs_results[f'{target}_fold_{fold}']['auc_score'] for fold in range(0, 5)])\n",
    "    mean_acc = np.mean([updrs_results[f'{target}_fold_{fold}']['acc_score'] for fold in range(0, 5)])\n",
    "    mean_precision = np.mean([updrs_results[f'{target}_fold_{fold}']['precision_score'] for fold in range(0, 5)])\n",
    "    mean_recall = np.mean([updrs_results[f'{target}_fold_{fold}']['recall_score'] for fold in range(0, 5)])\n",
    "    \n",
    "    return mean_auc, mean_acc, mean_precision, mean_recall\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_xgboost_model(xgb_hyperparams_df, target):\n",
    "    # train the model using the hyperparameters from the hyperparameter tuning\n",
    "    updrs_hp = xgb_hyperparams_df[target].to_dict()\n",
    "    if target == 'updrs_1':\n",
    "        updrs_hp['max_depth'] = 1\n",
    "    else:\n",
    "        updrs_hp['max_depth'] = 2\n",
    "    \n",
    "    updrs_hp['verbosity'] = 0\n",
    "    updrs_hp['silent'] = True\n",
    "    updrs_hp['random_state'] = 42\n",
    "    \n",
    "    model = XGBClassifier(**updrs_hp)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model function\n",
    "# model = prepare_xgboost_model(xgb_hyperparams_df, 'updrs_1')\n",
    "# model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_df, updrs2_df, updrs3_df]):\n",
    "    model = prepare_xgboost_model(xgb_hyperparams_df, updrs)\n",
    "    auc, acc, prec, recall = cross_fold_validation(df, model, updrs)\n",
    "    xgb_results[updrs] = {\"auc\":auc,\n",
    "                        \"acc\":acc,\n",
    "                        \"prec\":prec,\n",
    "                        \"recall\":recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updrs_1': {'auc': 0.6139061690442671,\n",
       "  'acc': 0.7958711772190777,\n",
       "  'prec': 0.4831489426817569,\n",
       "  'recall': 0.31090643697210674},\n",
       " 'updrs_2': {'auc': 0.6208551123468933,\n",
       "  'acc': 0.8623535606160326,\n",
       "  'prec': 0.5805932629462041,\n",
       "  'recall': 0.27808101296945115},\n",
       " 'updrs_3': {'auc': 0.6185961118239277,\n",
       "  'acc': 0.8591969954395065,\n",
       "  'prec': 0.7281562881562882,\n",
       "  'recall': 0.25652528589710255}}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View XGBoost Results with Forecasting the UPDRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max category for each patient\n",
    "max_df = updrs1_df.groupby(['patient_id'])['updrs_1_cat'].max().reset_index()\n",
    "max_df = max_df.rename(columns={'updrs_1_cat': 'updrs_1_max_cat'})\n",
    "# merge the max category with the original dataframe\n",
    "updrs1_df = updrs1_df.merge(max_df, on=['patient_id'], how='left')\n",
    "# take only the visit months that are 12 or less\n",
    "updrs1_yr_df = updrs1_df[updrs1_df['visit_month'] <= 12]\n",
    "updrs1_yr_df = updrs1_yr_df.drop(columns=['updrs_1_cat'])\n",
    "updrs1_yr_df.rename(columns={'updrs_1_max_cat': 'updrs_1_cat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max category for each patient\n",
    "max_df = updrs2_df.groupby(['patient_id'])['updrs_2_cat'].max().reset_index()\n",
    "max_df = max_df.rename(columns={'updrs_2_cat': 'updrs_2_max_cat'})\n",
    "# merge the max category with the original dataframe\n",
    "updrs2_df = updrs2_df.merge(max_df, on=['patient_id'], how='left')\n",
    "# take only the visit months that are 12 or less\n",
    "updrs2_yr_df = updrs2_df[updrs2_df['visit_month'] <= 12]\n",
    "updrs2_yr_df = updrs2_yr_df.drop(columns=['updrs_2_cat'])\n",
    "updrs2_yr_df.rename(columns={'updrs_2_max_cat': 'updrs_2_cat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max category for each patient\n",
    "max_df = updrs3_df.groupby(['patient_id'])['updrs_3_cat'].max().reset_index()\n",
    "max_df = max_df.rename(columns={'updrs_3_cat': 'updrs_3_max_cat'})\n",
    "# merge the max category with the original dataframe\n",
    "updrs3_df = updrs3_df.merge(max_df, on=['patient_id'], how='left')\n",
    "# take only the visit months that are 12 or less\n",
    "updrs3_yr_df = updrs3_df[updrs3_df['visit_month'] <= 12]\n",
    "updrs3_yr_df = updrs3_yr_df.drop(columns=['updrs_3_cat'])\n",
    "updrs3_yr_df.rename(columns={'updrs_3_max_cat': 'updrs_3_cat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_forecast_hyperparams_df = pd.read_csv('../data/processed/xgboost_future_cat_hyperparam_results.csv', index_col=0)\n",
    "lgb_forecast_hyperparams_df = pd.read_csv('../data/processed/lgboost_future_cat_hyperparam_results.csv', index_col=0)\n",
    "cboost_forecast_hyperparams_df = pd.read_csv('../data/processed/catboost_future_cat_hyperparam_results.csv', index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.970757</td>\n",
       "      <td>0.903629</td>\n",
       "      <td>0.686326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.983567</td>\n",
       "      <td>0.174551</td>\n",
       "      <td>0.910126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>1.232586</td>\n",
       "      <td>2.797518</td>\n",
       "      <td>13.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_split_gain</th>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.297876</td>\n",
       "      <td>0.004011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>7.437594</td>\n",
       "      <td>1.543678</td>\n",
       "      <td>4.159710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>7.499285</td>\n",
       "      <td>3.401574</td>\n",
       "      <td>1.926977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.728666</td>\n",
       "      <td>0.948910</td>\n",
       "      <td>0.839655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   updrs_1   updrs_2    updrs_3\n",
       "colsample_bytree  0.970757  0.903629   0.686326\n",
       "learning_rate     0.983567  0.174551   0.910126\n",
       "max_depth         8.000000  2.000000   6.000000\n",
       "min_child_weight  1.232586  2.797518  13.387500\n",
       "min_split_gain    0.007964  0.297876   0.004011\n",
       "reg_alpha         7.437594  1.543678   4.159710\n",
       "reg_lambda        7.499285  3.401574   1.926977\n",
       "subsample         0.728666  0.948910   0.839655"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_forecast_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.999434</td>\n",
       "      <td>0.972336</td>\n",
       "      <td>0.579427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.218284</td>\n",
       "      <td>0.108280</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.876356</td>\n",
       "      <td>0.650330</td>\n",
       "      <td>0.759742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>0.292568</td>\n",
       "      <td>7.076801</td>\n",
       "      <td>0.642850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>6.803953</td>\n",
       "      <td>2.521598</td>\n",
       "      <td>2.426225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>1.268909</td>\n",
       "      <td>1.677160</td>\n",
       "      <td>3.504071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.682804</td>\n",
       "      <td>0.693480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_state</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    updrs_1    updrs_2    updrs_3\n",
       "colsample_bytree   0.999434   0.972336   0.579427\n",
       "gamma              0.218284   0.108280   0.000070\n",
       "learning_rate      0.876356   0.650330   0.759742\n",
       "max_depth          1.000000   2.000000   2.000000\n",
       "min_child_weight   0.292568   7.076801   0.642850\n",
       "reg_alpha          6.803953   2.521598   2.426225\n",
       "reg_lambda         1.268909   1.677160   3.504071\n",
       "subsample          0.535242   0.682804   0.693480\n",
       "random_state      42.000000  42.000000  42.000000"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_hyperparams_df.loc['max_depth', 'updrs_1'] = 1\n",
    "xgb_forecast_hyperparams_df.loc['max_depth', 'updrs_2'] = 2\n",
    "xgb_forecast_hyperparams_df.loc['max_depth', 'updrs_3'] = 2\n",
    "xgb_forecast_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bagging_temperature</th>\n",
       "      <td>4.429427</td>\n",
       "      <td>4.095758</td>\n",
       "      <td>1.162359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <td>5.641367</td>\n",
       "      <td>5.757078</td>\n",
       "      <td>7.356710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.366781</td>\n",
       "      <td>0.954836</td>\n",
       "      <td>0.973876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_data_in_leaf</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      updrs_1   updrs_2   updrs_3\n",
       "bagging_temperature  4.429427  4.095758  1.162359\n",
       "depth                7.000000  6.000000  4.000000\n",
       "l2_leaf_reg          5.641367  5.757078  7.356710\n",
       "learning_rate        0.366781  0.954836  0.973876\n",
       "min_data_in_leaf     8.000000  8.000000  4.000000"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cboost_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS: updrs_1\n",
      "Hyperparameters: {'objective': 'binary:logistic', 'use_label_encoder': True, 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.9994339895225436, 'gamma': 0.2182840730386759, 'gpu_id': None, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.8763564203413519, 'max_delta_step': None, 'max_depth': 1, 'min_child_weight': 0.2925675445213144, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 6.803952572268468, 'reg_lambda': 1.2689093632909028, 'scale_pos_weight': None, 'subsample': 0.5352418739052822, 'tree_method': None, 'validate_parameters': None, 'verbosity': 0, 'silent': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS: updrs_2\n",
      "Hyperparameters: {'objective': 'binary:logistic', 'use_label_encoder': True, 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.9723358597087076, 'gamma': 0.1082797129959026, 'gpu_id': None, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.6503304038389104, 'max_delta_step': None, 'max_depth': 2, 'min_child_weight': 7.076800505115973, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 2.5215977214254304, 'reg_lambda': 1.677159748913182, 'scale_pos_weight': None, 'subsample': 0.6828035084965419, 'tree_method': None, 'validate_parameters': None, 'verbosity': 0, 'silent': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS: updrs_3\n",
      "Hyperparameters: {'objective': 'binary:logistic', 'use_label_encoder': True, 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.5794273693429098, 'gamma': 6.958373374993177e-05, 'gpu_id': None, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.7597417382901697, 'max_delta_step': None, 'max_depth': 2, 'min_child_weight': 0.6428495541542038, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 2.426225167246236, 'reg_lambda': 3.5040708421969304, 'scale_pos_weight': None, 'subsample': 0.6934802250073431, 'tree_method': None, 'validate_parameters': None, 'verbosity': 0, 'silent': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb_forecast_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    model = prepare_xgboost_model(xgb_forecast_hyperparams_df, updrs)\n",
    "    print(f'UPDRS: {updrs}')\n",
    "    print(f'Hyperparameters: {model.get_params()}')\n",
    "    print('\\n')\n",
    "    auc, acc, prec, recall = cross_fold_validation(df, model, updrs)\n",
    "    xgb_forecast_results[updrs] = {\"auc\":auc,\n",
    "                        \"acc\":acc,\n",
    "                        \"prec\":prec,\n",
    "                        \"recall\":recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updrs_1': {'auc': 0.6134458268310631,\n",
       "  'acc': 0.6391945322430447,\n",
       "  'prec': 0.5657722007722008,\n",
       "  'recall': 0.4540922749541081},\n",
       " 'updrs_2': {'auc': 0.6758429459236683,\n",
       "  'acc': 0.7423097735597736,\n",
       "  'prec': 0.6126916221033868,\n",
       "  'recall': 0.5000278562017693},\n",
       " 'updrs_3': {'auc': 0.6393999920247535,\n",
       "  'acc': 0.6718242500094815,\n",
       "  'prec': 0.5971260033996407,\n",
       "  'recall': 0.4918466898954704}}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_folds(df, target):\n",
    "    # calculate the number of bins by Sturge's rule\n",
    "    num_bins = int(np.floor(1 + np.log2(len(df))))\n",
    "    df.loc[:, \"bins\"] = pd.cut(df[f'{updrs}_cat'], bins=num_bins, labels=False)\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "        \n",
    "    # initiate the kfold class from sklearn\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "        \n",
    "    # create a kfold column\n",
    "    df['kfold'] = -1\n",
    "\n",
    "    # fill the kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=df['bins'].values)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "            \n",
    "    # drop the bins column\n",
    "    df = df.drop('bins', axis=1)\n",
    "    max_kfold = df['kfold'].max()\n",
    "    \n",
    "    print(f'{max_kfold + 1} Kfolds created for {target}_cat')\n",
    "    return df, max_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(train_df, test_df, updrs, model):\n",
    "    features = train_df.drop([f'{updrs}_cat', 'kfold'], axis=1)\n",
    "    target = train_df[f'{updrs}_cat']\n",
    "    X_test = test_df.drop([f'{updrs}_cat', 'kfold'], axis=1)\n",
    "    y_test = test_df[f'{updrs}_cat']\n",
    "\n",
    "\n",
    "    model.fit(features, target)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Evaluate AUC on the test data\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    print(\"AUC on Test Data:\", auc)\n",
    "\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    filename =f'../models/xgboost_{updrs}_model_max_depth.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6933660933660935\n",
      "updrs_1 Results: \n",
      "Test AUC = 0.6933660933660935 \n",
      "Train AUC = 0.9232719962888511\n",
      "\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.752599890530925\n",
      "updrs_2 Results: \n",
      "Test AUC = 0.752599890530925 \n",
      "Train AUC = 1.0\n",
      "\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8505102040816327\n",
      "updrs_3 Results: \n",
      "Test AUC = 0.8505102040816327 \n",
      "Train AUC = 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    model = prepare_xgboost_model(xgb_forecast_hyperparams_df, updrs)\n",
    "    temp_df, test_kfold = create_folds(df, updrs)\n",
    "    temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "    X_train = temp_df[(temp_df['kfold'] != test_kfold)].reset_index(drop=True)\n",
    "    y_train = X_train[f'{updrs}_cat']\n",
    "    X_test = temp_df[temp_df['kfold'] == test_kfold].reset_index(drop=True)\n",
    "    y_test = X_test[f'{updrs}_cat']\n",
    "    \n",
    "    best_model = train_xgboost(X_train, X_test, updrs, model)\n",
    "    # compare the results of the model\n",
    "    X_test = X_test.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "    test_preds = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    test_auc = roc_auc_score(y_test, test_preds)\n",
    "    X_train = X_train.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "    train_preds = best_model.predict(X_train)\n",
    "    train_auc = roc_auc_score(y_train, train_preds)\n",
    "    print(f'{updrs} Results: \\nTest AUC = {test_auc} \\nTrain AUC = {train_auc}\\n')\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the scale_pos_weight hyperparameter\n",
    "Use # of negative / # of positive for the value of scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_scale_xgboost_model(xgb_hyperparams_df, target):\n",
    "    # train the model using the hyperparameters from the hyperparameter tuning\n",
    "    updrs_hp = xgb_hyperparams_df[target].to_dict()\n",
    "    if target == 'updrs_1':\n",
    "        updrs_hp['max_depth'] = 1\n",
    "        updrs_hp['scale_pos_weight'] = 1.5\n",
    "        updrs_hp['random_state'] = 42\n",
    "    elif target == 'updrs_2':\n",
    "        updrs_hp['max_depth'] = 2\n",
    "        updrs_hp['scale_pos_weight'] = 2.2\n",
    "        updrs_hp['random_state'] = 42\n",
    "    else:\n",
    "        updrs_hp['max_depth'] = 2\n",
    "        updrs_hp['scale_pos_weight'] = 1.6\n",
    "        updrs_hp['random_state'] = 42\n",
    "    \n",
    "    model = XGBClassifier(**updrs_hp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6923832923832924\n",
      "updrs_1 Results: \n",
      "Test AUC = 0.6923832923832924 \n",
      "Train AUC = 0.9967527447038813\n",
      "\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7318007662835249\n",
      "updrs_2 Results: \n",
      "Test AUC = 0.7318007662835249 \n",
      "Train AUC = 1.0\n",
      "\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7005102040816327\n",
      "updrs_3 Results: \n",
      "Test AUC = 0.7005102040816327 \n",
      "Train AUC = 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    model = prepare_scale_xgboost_model(xgb_forecast_hyperparams_df, updrs)\n",
    "    temp_df, test_kfold = create_folds(df, updrs)\n",
    "    temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "    X_train = temp_df[(temp_df['kfold'] != test_kfold)].reset_index(drop=True)\n",
    "    y_train = X_train[f'{updrs}_cat']\n",
    "    X_test = temp_df[temp_df['kfold'] == test_kfold].reset_index(drop=True)\n",
    "    y_test = X_test[f'{updrs}_cat']\n",
    "    \n",
    "    best_model = train_xgboost(X_train, X_test, updrs, model)\n",
    "    # compare the results of the model\n",
    "    X_test = X_test.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "    test_preds = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    test_auc = roc_auc_score(y_test, test_preds)\n",
    "    X_train = X_train.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "    train_preds = best_model.predict_proba(X_train)[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, train_preds)\n",
    "    print(f'{updrs} Results: \\nTest AUC = {test_auc} \\nTrain AUC = {train_auc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the hyperparameters for the xgboost model best performers\n",
    "xgb_forecast_hyperparams_df.loc['max_depth', 'updrs_1'] = 1\n",
    "xgb_forecast_hyperparams_df.loc['max_depth', 'updrs_2'] = 2\n",
    "xgb_forecast_hyperparams_df.loc['max_depth', 'updrs_3'] = 2\n",
    "xgb_forecast_hyperparams_df.loc['scale_pos_weight', 'updrs_1'] = 1.5\n",
    "xgb_forecast_hyperparams_df.loc['scale_pos_weight', 'updrs_2'] = 2.2\n",
    "xgb_forecast_hyperparams_df.loc['scale_pos_weight', 'updrs_3'] = 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.999434</td>\n",
       "      <td>0.972336</td>\n",
       "      <td>0.579427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.876356</td>\n",
       "      <td>0.650330</td>\n",
       "      <td>0.759742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>6.803953</td>\n",
       "      <td>2.521598</td>\n",
       "      <td>2.426225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>1.268909</td>\n",
       "      <td>1.677160</td>\n",
       "      <td>3.504071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    updrs_1    updrs_2    updrs_3\n",
       "colsample_bytree   0.999434   0.972336   0.579427\n",
       "gamma              1.000000   1.000000   1.000000\n",
       "learning_rate      0.876356   0.650330   0.759742\n",
       "max_depth          1.000000   2.000000   2.000000\n",
       "min_child_weight  31.000000  31.000000  31.000000\n",
       "reg_alpha          6.803953   2.521598   2.426225\n",
       "reg_lambda         1.268909   1.677160   3.504071\n",
       "subsample          0.600000   0.900000   0.700000\n",
       "scale_pos_weight   1.500000   2.200000   1.600000"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6775793650793651\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7103174603174603\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6264880952380952\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6486486486486487\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6771498771498772\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6498015873015873\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.652281746031746\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6393849206349206\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7361179361179361\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6958230958230959\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6626984126984127\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7251984126984128\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6319444444444445\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6201474201474201\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6918918918918919\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.714781746031746\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6612103174603174\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6711309523809523\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6987714987714988\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6437346437346437\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7579365079365079\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6934523809523809\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6929563492063492\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6108108108108108\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6953316953316954\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8180803571428572\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6746651785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7455357142857142\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6787082649151616\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7285166940339355\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7829241071428572\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7209821428571428\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7578125\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7591680350301041\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7427476737821566\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7862723214285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6958705357142857\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7338169642857143\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.668856048166393\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7296113847837986\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8113839285714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6685267857142857\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7684151785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7690202517788725\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7208538587848933\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8203125\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6819196428571428\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7851562500000001\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7701149425287357\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7640941434044882\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6096230158730158\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.5823412698412698\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6499999999999999\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7459183673469388\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7086734693877551\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6726190476190477\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6651785714285714\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7882653061224489\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7811224489795918\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.701530612244898\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6547619047619048\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7108134920634921\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7209183673469389\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6683673469387754\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8229591836734694\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6899801587301587\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6716269841269841\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7122448979591837\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7709183673469389\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7602040816326531\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6666666666666667\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6502976190476191\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7045918367346938\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7683673469387756\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7852040816326531\n"
     ]
    }
   ],
   "source": [
    "updrs_results = pd.DataFrame(columns = ['updrs', 'subsample', 'test_auc', 'train_auc'])\n",
    "i = 0\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    for subsample in np.arange(0.5, 1.0, 0.1): \n",
    "        test_auc = []\n",
    "        train_auc = []\n",
    "        for fold in range(0, 5):\n",
    "            xgb_forecast_hyperparams_df.loc['subsample', f'{updrs}'] = subsample\n",
    "            model = prepare_scale_xgboost_model(xgb_forecast_hyperparams_df, updrs)\n",
    "            temp_df, test_kfold = create_folds(df, updrs)\n",
    "            temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "            X_train = temp_df[(temp_df['kfold'] != fold)].reset_index(drop=True)\n",
    "            y_train = X_train[f'{updrs}_cat']\n",
    "            X_test = temp_df[temp_df['kfold'] == fold].reset_index(drop=True)\n",
    "            y_test = X_test[f'{updrs}_cat']\n",
    "    \n",
    "            best_model = train_xgboost(X_train, X_test, updrs, model)\n",
    "            # compare the results of the model\n",
    "            X_test = X_test.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "            test_preds = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            fold_test_auc = roc_auc_score(y_test, test_preds)\n",
    "            X_train = X_train.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "            train_preds = best_model.predict_proba(X_train)[:, 1]\n",
    "            fold_train_auc = roc_auc_score(y_train, train_preds)\n",
    "            test_auc.append(fold_test_auc)\n",
    "            train_auc.append(fold_train_auc)\n",
    "        updrs_results.loc[i, 'updrs'] = updrs\n",
    "        updrs_results.loc[i, 'subsample'] = subsample\n",
    "        updrs_results.loc[i, 'test_auc'] = np.mean(test_auc)\n",
    "        updrs_results.loc[i, 'train_auc'] = np.mean(train_auc)\n",
    "        i += 1\n",
    "            \n",
    "        #print(f'{updrs} Results: \\nSubsample: {subsample}\\nTest AUC = {test_auc} \\nTrain AUC = {train_auc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs</th>\n",
       "      <th>subsample</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.668037</td>\n",
       "      <td>0.991898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.674682</td>\n",
       "      <td>0.99548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.666376</td>\n",
       "      <td>0.996368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.677926</td>\n",
       "      <td>0.996875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.690098</td>\n",
       "      <td>0.996824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.729101</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.752727</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.722885</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.74764</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.764319</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.659311</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.721743</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.715564</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.720995</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      updrs subsample  test_auc train_auc\n",
       "0   updrs_1       0.5  0.668037  0.991898\n",
       "1   updrs_1       0.6  0.674682   0.99548\n",
       "2   updrs_1       0.7  0.666376  0.996368\n",
       "3   updrs_1       0.8  0.677926  0.996875\n",
       "4   updrs_1       0.9  0.690098  0.996824\n",
       "5   updrs_2       0.5  0.729101       1.0\n",
       "6   updrs_2       0.6  0.752727       1.0\n",
       "7   updrs_2       0.7  0.722885       1.0\n",
       "8   updrs_2       0.8   0.74764       1.0\n",
       "9   updrs_2       0.9  0.764319       1.0\n",
       "10  updrs_3       0.5  0.659311       1.0\n",
       "11  updrs_3       0.6  0.721743       1.0\n",
       "12  updrs_3       0.7  0.715564       1.0\n",
       "13  updrs_3       0.8  0.720995       1.0\n",
       "14  updrs_3       0.9  0.715026       1.0"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS 1 Best Subsample: 0.8999999999999999\n",
      " UPDRS 1 Max AUC: 0.6900975488475488\n",
      "UPDRS 2 Best Subsample: 0.8999999999999999\n",
      " UPDRS 2 Max AUC: 0.7643194957580735\n",
      "UPDRS 3 Best Subsample: 0.6\n",
      " UPDRS 3 Max AUC: 0.7217431972789116\n"
     ]
    }
   ],
   "source": [
    "updrs1_max = updrs_results[updrs_results['updrs'] == 'updrs_1']['test_auc'].max()\n",
    "updrs1_best_subsample = updrs_results[(updrs_results['updrs'] == 'updrs_1') & (updrs_results['test_auc'] == updrs1_max)]['subsample'].values[0]\n",
    "updrs2_max = updrs_results[updrs_results['updrs'] == 'updrs_2']['test_auc'].max()\n",
    "updrs2_best_subsample = updrs_results[(updrs_results['updrs'] == 'updrs_2') & (updrs_results['test_auc'] == updrs2_max)]['subsample'].values[0]\n",
    "updrs3_max = updrs_results[updrs_results['updrs'] == 'updrs_3']['test_auc'].max()\n",
    "updrs3_best_subsample = updrs_results[(updrs_results['updrs'] == 'updrs_3') & (updrs_results['test_auc'] == updrs3_max)]['subsample'].values[0]\n",
    "\n",
    "print(f'UPDRS 1 Best Subsample: {updrs1_best_subsample}\\n UPDRS 1 Max AUC: {updrs1_max}')\n",
    "print(f'UPDRS 2 Best Subsample: {updrs2_best_subsample}\\n UPDRS 2 Max AUC: {updrs2_max}')\n",
    "print(f'UPDRS 3 Best Subsample: {updrs3_best_subsample}\\n UPDRS 3 Max AUC: {updrs3_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFJCAYAAAChG+XKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAncklEQVR4nO3de3BUZZ7/8U/fE9IhwHALauIajSsJmE0c19IiChgvg9ZSZdUYRMfa4qLzx/Kr2ohsiTMikwpYW/zxQ0GXWaFG9qfGlar5Kc783Iq7lWwBaw0tcTYRwg54mxkhSgxJOvec8/ujk05359IYE/rJ4f2qsnLOeZ7n9POlc/zkOd3puGzbtgUAAIzhTvUEAABAPMIZAADDEM4AABiGcAYAwDCEMwAAhvGmegKSZFmWwuGwfD6fXC5XqqcDAMCUsm1bfX19ysjIkNs9cp1sRDiHw2GdPn061dMAAOCyys/PV2Zm5ojjRoSzz+eTFJmk3++flHM2NDSosLBwUs6Vak6pxSl1SNRiKqfU4pQ6JGoZS29vr06fPh3Nv0RGhPPQrWy/369AIDBp553Mc6WaU2pxSh0StZjKKbU4pQ6JWsYz1ku5vCEMAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwDOEMAIBhLimcP/74Yz322GMjjv/7v/+7HnroIT388MN66623JEnd3d36u7/7Oz3yyCPasGGDWlpaJnfGAAA4XNJw/uUvf6lnn31WPT09ccf7+vq0Y8cO7d+/XwcPHlR1dbW++eYbvfHGG8rPz9frr7+u1atXa+/evVM2eQAAnChpOOfk5OjFF18ccfzMmTPKyclRVlaW/H6/SkpK9Lvf/U6hUEjLli2TJJWWlurYsWOTP2sAABws6SeE3XvvvfrjH/844nhHR0fc54FmZGSoo6Mj7nhGRoba29sveTINDQ2X3PdShEKhST1fKjmlFqfUIVGLqaayFtu2ZcuSLVsa/GrLkm0n7Cc5rsG2+GOxY2z9v/9qkuRS5AOkXHLJFfN19GORT5tKOBa3n3DMlfycccdc8eNHe5yR8+H7ayIm/PGdwWBQ4XA4uh8Oh5WZmRl3PBwOa+bMmZd8zsLCwkn7aLRQKKSSkpJJOVeqOaUWp9QhUctYLNuSbVuy7AFZ9kBk27JkDR4bbrNkD34d3k9os8ZpG2Pc+fPnNHfeD2RZ321ctM2yZCvxsYfGRQIU30UknN0ut1yuSGi7XG5JLrkHv7pcrrg219AxuSWXS+6hHyAS2mLHRc8X26aE88a0aZy2xHEa3Pa4vepuDui2W5ZNyr9MT0/PuAvSCYdzXl6ePv/8c7W2tmrGjBk6fvy41q1bpz//+c+qra3V0qVLVVdX55j/geH7sW1btm1pwOqPrDrsyNoj8tWOORZZNdgJ7cPHhrej5x1xbGhlM1b70EpF0VWKPe45Yx870t7a/6XOfu0Z8zFHO2f8sZivCe3Rf5OhsbHH4v7tLNkJ7ZHzjnYsYWzMsZaeC2r9pGEwrIZDaThcB2RrrJAdiBlnRni1nDtzSf3cLs9gaES+ul0euQa/+jw+ud2jtbljxg1tJ7S53XLJLbd7lLaYca6YY4nj/vCHM7r++rzodTP0PFr20HNoxbUNfb/YMW2R59iKaYvZTmjTOG1x/WRHn+fRzzl4bdnD/do72pWRkRE3t3FrkiXLutSahr/HL4eF3qWX5XGkCYTzu+++q87OTj388MP6h3/4B61bt062beuhhx7SggULtGbNGm3ZskVr1qyRz+fTrl27pmLemESWPaD+gT71W30asHrVP9CnASuyHzneG9kf3O63+jQQsx3p3xvTf/g8/dbg+IE+2bLVcPRQqsudNF82/VeqpzBp2lr+lHDENRgaQwE1GCLuwSDx+Ea2xQVWfLBF29zjtI0ZlqOPGxl6Hp06eUqFBUvig2+MkDVZs6dT18y5KdXTmBShUEglN0/tIm30HzgSjyWGuvWdfuBwya0vTp+f0jpiXVI4X3311dFflXrwwQejx1esWKEVK1bE9U1PT9fu3bsncYpXLtu21G/1q9/uUUf3t3FBFwnCXg1Y/eOE5FihOrg/GMKWPTCp8/a4ffK6ffJ6fPJ70zXDPVNej0/hji5lzczS0OtoI173cg29UuUefp0t5rZV5NDwbafE18ziXpsb6idJg7fDNPiYia+zRR8j9rFc47d/+eWXys3JjZ5z5Jj4xxztnCPqiz7OaGNiax7tNUMN3i5Uwr/J8JgRxwYf679/36Cior8aEbLTUZr7K2XNmJfqaeAyG752pvb79kvX11N6/lhG/MnI6ca2bQ3Y/aMGXX/MinFgYDAoE4I0+Sp0KFj7o4958vg733vebpdXXk8kOH3ugNJ8wUiIun2RQPX4o6HqcQ9vR9r90W2v2y9PzHak/+Bxt3fMP4EWCoVUUuiMlzk6v/Lrxmxn1OJ1BRTwpqd6GgBiODKcLWtAfXaX2roujAzM2NutA70xt28HgzLmdu7Yq9A+TeZrHC65B4PPP7jaTItue90+tV0Ma97c+cNBOhiinmh4Du6PEZhD26bfygMARDgunG3b1v898b91sbtZp0KHv+fZXDErR58C3gxlBJKtJCMrUE/MdmKQJoaq2+0ZdxahUEgl+c5YpQEAknNcOLtcLl2/oFh/+LJR8+cuiFs9jgzV4VVoXKgOBmfkzSOj36IFAGCqOC6cJWnJ1Xep93ymSm5gtQkAmH54ERIAAMMQzgAAGIZwBgDAMIQzAACGIZwBADAM4QwAgGEIZwAADEM4AwBgGMIZAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwDOEMAIBhCGcAAAxDOAMAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMAzhDACAYQhnAAAMQzgDAGAYwhkAAMMQzgAAGMabrINlWdq2bZuamprk9/tVWVmp3NzcaPu+ffv03nvvKRgMav369Vq+fLlaW1t17733Kj8/X5J099136/HHH5+6KgAAcJCk4VxTU6Pe3l5VV1ervr5eO3fu1MsvvyxJampq0uHDh/Wv//qvkqTy8nLddttt+uSTT/TAAw/oZz/72dTOHgAAB0p6WzsUCmnZsmWSpKKiIjU0NETbzpw5o1tvvVWBQECBQEC5ublqampSQ0ODGhsb9eijj2rTpk1qbm6eugoAAHAYl23b9ngdtm7dqnvuuUd33nmnJOmuu+5STU2NvF6vzpw5o4qKCv3Lv/yL+vr6tHr1au3cuVPhcFgzZszQ7bffrnfeeUc1NTXavXv3mI/R09MTF/oAAFwJCgsLFQgERhxPels7GAwqHA5H9y3LktcbGZaXl6e1a9dq/fr1WrRokW6++WbNnj1bS5YsUXp6uiSprKxs3GC+lElORCgUUklJyaScK9WcUotT6pCoxVROqcUpdUjUMpZki9Kkt7WLi4tVV1cnSaqvr4++yUuSWlpaFA6H9eabb+r555/XV199pRtuuEHPPvus3n//fUnSsWPHVFBQ8H3rAADgipF05VxWVqYjR46ovLxctm2rqqpKBw4cUE5OjlasWKGzZ8/qoYceks/n09NPPy2Px6OKigo988wzeuONN5Senq7KysrLUQsAAI6QNJzdbre2b98edywvLy+6ndgmSddcc40OHjw4CdMDAODKw4eQAABgGMIZAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwDOEMAIBhCGcAAAxDOAMAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMAzhDACAYQhnAAAMQzgDAGAYwhkAAMMQzgAAGIZwBgDAMIQzAACGIZwBADAM4QwAgGEIZwAADEM4AwBgGMIZAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwDOEMAIBhCGcAAAzjTdbBsixt27ZNTU1N8vv9qqysVG5ubrR93759eu+99xQMBrV+/XotX75cLS0teuqpp9Td3a358+drx44dSk9Pn9JCAABwiqQr55qaGvX29qq6uloVFRXauXNntK2pqUmHDx/WW2+9pf3792v37t3q6urS3r179cADD+j111/X4sWLVV1dPaVFAADgJEnDORQKadmyZZKkoqIiNTQ0RNvOnDmjW2+9VYFAQIFAQLm5uWpqaoobU1paqqNHj07R9AEAcJ6kt7U7OjoUDAaj+x6PR/39/fJ6vbrxxhu1b98+dXR0qK+vTydOnNDDDz+sjo4OZWZmSpIyMjLU3t5+SZOJDf7JEAqFJvV8qeSUWpxSh0QtpnJKLU6pQ6KWiUgazsFgUOFwOLpvWZa83siwvLw8rV27VuvXr9eiRYt08803a/bs2dExaWlpCofDmjlz5iVNprCwUIFAYIKlxAuFQiopKZmUc6WaU2pxSh0StZjKKbU4pQ6JWsbS09Mz7oI06W3t4uJi1dXVSZLq6+uVn58fbWtpaVE4HNabb76p559/Xl999ZVuuOEGFRcXq7a2VpJUV1fnmCcGAIDLIenKuaysTEeOHFF5ebls21ZVVZUOHDignJwcrVixQmfPntVDDz0kn8+np59+Wh6PRz/96U+1ZcsWvfXWW5o9e7Z27dp1OWoBAMARkoaz2+3W9u3b447l5eVFtxPbJGnu3Ll69dVXJ2F6AABcefgQEgAADEM4AwBgGMIZAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwDOEMAIBhCGcAAAxDOAMAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMAzhDACAYQhnAAAMQzgDAGAYwhkAAMMQzgAAGIZwBgDAMIQzAACGIZwBADAM4QwAgGEIZwAADEM4AwBgGMIZAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwjDdZB8uytG3bNjU1Ncnv96uyslK5ubnR9v379+vw4cNyuVx68sknVVZWJtu2VVpaqmuvvVaSVFRUpIqKiikrAgAAJ0kazjU1Nert7VV1dbXq6+u1c+dOvfzyy5KktrY2vfbaa/q3f/s3dXV1afXq1SorK9MXX3yhgoICvfLKK1NeAAAATpP0tnYoFNKyZcskRVbADQ0N0bb09HQtWrRIXV1d6urqksvlkiQ1Njbq/Pnzeuyxx7RhwwadPXt2iqYPAIDzJF05d3R0KBgMRvc9Ho/6+/vl9UaGZmdna9WqVRoYGNATTzwhSZo3b542btyo+++/X8ePH9fmzZt16NChKSoBAABnSRrOwWBQ4XA4um9ZVjSY6+rq1NzcrA8++ECStG7dOhUXF6uwsFAej0eSdMstt6i5uVm2bUdX1mOJXZVPhlAoNKnnSyWn1OKUOiRqMZVTanFKHRK1TETScC4uLtZ//Md/6Ec/+pHq6+uVn58fbcvKylJaWpr8fr9cLpcyMzPV1taml156SbNmzdKGDRt06tQpZWdnJw1mSSosLFQgEPh+FQ0KhUIqKSmZlHOlmlNqcUodErWYyim1OKUOiVrG0tPTM+6CNGk4l5WV6ciRIyovL5dt26qqqtKBAweUk5OjlStX6ujRo/rxj38st9ut4uJi3XHHHVqyZIk2b96s2tpaeTwe7dixY1KKAQDgSpA0nN1ut7Zv3x53LC8vL7q9adMmbdq0Ka49KytL+/btm6QpAgBwZeFDSAAAMAzhDACAYQhnAAAMQzgDAGAYwhkAAMMQzgAAGIZwBgDAMIQzAACGIZwBADAM4QwAgGEIZwAADEM4AwBgGMIZAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwDOEMAIBhCGcAAAxDOAMAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMAzhDACAYQhnAAAMQzgDAGAYwhkAAMMQzgAAGIZwBgDAMIQzAACG8SbrYFmWtm3bpqamJvn9flVWVio3Nzfavn//fh0+fFgul0tPPvmkysrK1N3drc2bN+vChQvKyMjQCy+8oDlz5kxpIQAAOEXSlXNNTY16e3tVXV2tiooK7dy5M9rW1tam1157TW+++ab279+vqqoqSdIbb7yh/Px8vf7661q9erX27t07dRUAAOAwScM5FApp2bJlkqSioiI1NDRE29LT07Vo0SJ1dXWpq6tLLpdrxJjS0lIdO3ZsKuYOAIAjJb2t3dHRoWAwGN33eDzq7++X1xsZmp2drVWrVmlgYEBPPPFEdExmZqYkKSMjQ+3t7Zc0mdjgnwyhUGhSz5dKTqnFKXVI1GIqp9TilDokapmIpOEcDAYVDoej+5ZlRYO5rq5Ozc3N+uCDDyRJ69atU3FxcdyYcDismTNnXtJkCgsLFQgEvnMRowmFQiopKZmUc6WaU2pxSh0StZjKKbU4pQ6JWsbS09Mz7oI06W3t4uJi1dXVSZLq6+uVn58fbcvKylJaWpr8fr8CgYAyMzPV1tam4uJi1dbWSooEuFOeGAAALoekK+eysjIdOXJE5eXlsm1bVVVVOnDggHJycrRy5UodPXpUP/7xj+V2u1VcXKw77rhDJSUl2rJli9asWSOfz6ddu3ZdjloAAHCEpOHsdru1ffv2uGN5eXnR7U2bNmnTpk1x7enp6dq9e/ckTREAgCsLH0ICAIBhCGcAAAxDOAMAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMAzhDACAYQhnAAAMQzgDAGAYwhkAAMMQzgAAGIZwBgDAMIQzAACGIZwBADAM4QwAgGEIZwAADEM4AwBgGMIZAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwDOEMAIBhCGcAAAxDOAMAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMIw3WQfLsrRt2zY1NTXJ7/ersrJSubm5kqSTJ0+qqqoq2re+vl579uzR0qVLde+99yo/P1+SdPfdd+vxxx+fohIAAHCWpOFcU1Oj3t5eVVdXq76+Xjt37tTLL78sSbrpppt08OBBSdJvf/tbzZ8/X6WlpTp69KgeeOAB/exnP5va2QMA4EBJb2uHQiEtW7ZMklRUVKSGhoYRfTo7O/Xiiy9q69atkqSGhgY1Njbq0Ucf1aZNm9Tc3DzJ0wYAwLlctm3b43XYunWr7rnnHt15552SpLvuuks1NTXyeocX3a+99ppaW1u1adMmSZHV9owZM3T77bfrnXfeUU1NjXbv3j3mY/T09Iwa+gAAOFlhYaECgcCI40lvaweDQYXD4ei+ZVlxwSxJ7777blz43nbbbUpPT5cklZWVjRvMlzLJiQiFQiopKZmUc6WaU2pxSh0StZjKKbU4pQ6JWsaSbFGa9LZ2cXGx6urqJEXe8DX0Jq8h7e3t6u3tVXZ2dvTYs88+q/fff1+SdOzYMRUUFExo8gAAXImSrpzLysp05MgRlZeXy7ZtVVVV6cCBA8rJydHKlSv16aef6qqrroobU1FRoWeeeUZvvPGG0tPTVVlZOWUFAADgNEnD2e12a/v27XHH8vLyottLly7V3r1749qvueaa6Lu4AQDAd8OHkAAAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMAzhDACAYQhnAAAMQzgDAGAYwhkAAMMQzgAAGIZwBgDAMIQzAACGIZwBADAM4QwAgGEIZwAADEM4AwBgGMIZAADDEM4AABiGcAYAwDCEMwAAhiGcAQAwDOEMAIBhCGcAAAxDOAMAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMAzhDACAYbzJOliWpW3btqmpqUl+v1+VlZXKzc2VJJ08eVJVVVXRvvX19dqzZ48KCwv11FNPqbu7W/Pnz9eOHTuUnp4+dVUAAOAgSVfONTU16u3tVXV1tSoqKrRz585o20033aSDBw/q4MGDeuSRR3TPPfeotLRUe/fu1QMPPKDXX39dixcvVnV19ZQWAQCAkyQN51AopGXLlkmSioqK1NDQMKJPZ2enXnzxRW3dunXEmNLSUh09enQy5wwAgKMlva3d0dGhYDAY3fd4POrv75fXOzz07bff1n333ac5c+ZEx2RmZkqSMjIy1N7efkmTGS34v49QKDSp50slp9TilDokajGVU2pxSh0StUxE0nAOBoMKh8PRfcuy4oJZkt59913t3r17xJi0tDSFw2HNnDnzkiZTWFioQCBwqXMfVygUUklJyaScK9WcUotT6pCoxVROqcUpdUjUMpaenp5xF6RJb2sXFxerrq5OUuQNX/n5+XHt7e3t6u3tVXZ2dtyY2tpaSVJdXZ1jnhgAAC6HpCvnsrIyHTlyROXl5bJtW1VVVTpw4IBycnK0cuVKffrpp7rqqqvixvz0pz/Vli1b9NZbb2n27NnatWvXlBUAAIDTJA1nt9ut7du3xx3Ly8uLbi9dulR79+6Na587d65effXVSZoiAABXFj6EBAAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMAzhDACAYQhnAAAMQzgDAGAYwhkAAMMQzgAAGIZwBgDAMIQzAACGIZwBADBM0r9KBUyEZdn6ojWsxnOt+uTcRTWeb1XjF+c088MLcrtccrkkt8slt9sV+Tq073LJM8qxEfvu0duGx44xfoxzR8YNH3ONNj7mMT//7KLOeD5L/njfqcbY+sY+39iPGVvfcJvL5Ur1twOA74hwxvdi27b+2NqpxvODIXyuVZ+cb9Un5y+qo6d/5IDmzss/yaly9E+pnsElG+0HkKEAl20p7d2z8rnd8nnc8rpd8nlGbvvcbnnGaItuJzmH1+2SN1k/t3uwz1jncMvncY16DsApCGdcEtu29VVb12D4XlTDV8Mh3NbdF9fX53HrxnkztXhhlgoWzor+9+1np/XDkhJZtj34n2K2Y/at4f2BuP3EMQnjrZHnG4gbO8Z4a+z5jDX+s88/1zVX54xz7nHGj/N4ifUMxO0nqT+hHntwf2DUscPz6+jslNfvV9+ApX7LUlefrb4BS31WZL9vILI/Hbhdku+tU2OE+OB+Qpt3rPAfpZ8vSZvX4xo8ntgv8sNG7DmG+yXsu11q7enXNx3dsqXoc2lr6Gvkeozdtmx7cHv0fsPto/Qb3Lbs2PaYfjHbI8+T8Fhx840c+58/tevPaV9Gj41fT+J8R85z9PlGjimh36h1J4xJnG9iv6F2n8etkrT4/9dNJcIZcWzb1vn27ugKuPHcRX1yrlWN5y+qtas3rq/X7VL+vJlafOMsFSzI0uLBEL5+buaoq5jQF4O3azX9b7OG/O0qKbkx1dOYFKFQSCUlJeP2GQqAoaCOhLalPit2P6FtwB6jX3xbf1y/hHEJbf0D9mCfxH6WBixb37a1KZA+Q/2Djxfbr6uvX209I89h2fZl+pf+jg6dTvUMJk/tl6mewaT4X3+1QD+6TI9FOF/Bvu7ojntN+JNzkTC+0NkT18/jdun6H2Rq+fULVbAwS4sXzFLBwizlz5spv9eTotnjcnK5XPK4XPK4pTSfuc/5pfygkciyYn5QsOwxfhCw4gI/rl/cDwLxbQMx5xivX39Cv28utGj27NlyuyL/9i5JLpfkkmvwqwbfFxG/P6Lf0PsONEa/we3h84zdzx3XntBvcHvkeVz605/+qGuuvnr4HKPWE/mq6HzH75e87sT5TqzuofdsuCT5vW65mj//vt+il4xwvgK0dPao8VzMKvhcqxrPt+rrjvgQdrmkvB9k6o6/mKfC7FmDITxLN86fqQAhDIdyu13yuz1G/aA5kR8yTBUKdaukpCDV05gUoW++uGyPRTg7SGtX72AIR14LHloJn2vvGtH3uh8E9dc58yIr4YWzVLBglv5ywUyl+/iWAIBU4//E01Bbd68+OX9x+Jb0YBj/6eLId0Lnzs7Q/TddFfea8E3zZyoj4EvBzAEAl4JwNlhHT59Onr+o35xt1Zt/CkVfF/6ydWQIX501Q/fcuEgFMe+Qvml+ljLTCGEAmG4IZwN09vbr5PmLI35X+LOWcEyvP0uSsmem6+787Jg3Zs3S4gVZykr3p2byAIBJRzhfRt19AzrVPHwbeui29NmWdiX+NseCzDStuH6hFi/MUrCnTfffulQFC7M0e0YgNZMHAFw2hPMU6Okf0Omv26Lh23Aucjv6zIWOEb9TOTcjoNLrFmjxgvgP7PhBxnAIh0IhlVw3/3KXAQBIEcL5e+gbsOJCeOg14f/5pl0DVnwIz073646/mBf9HeHIO6SzND8zPUWzBwCYinC+BP0Dlv7wTfuI14RPf90+4mMNs9J8+uucuZGPrlwwa/Ad0llamJnOHyAAAFwSwjnGgGXp7IWO6O8KN567qE/Ot6qpuU29CSGcGfCp+Ko50c+PXrxglgqzZ2nRTEIYAPD9XJHhbFm2Pm3pGPH50aea29TdPxDXd4bfo6WLZkdeC475XeFrZs0ghAEAU8LR4Tza3xT+ZPCd0l198SGc7vNoccznRg+9Jpw7Oyi3mxAGAFw+jgznnR/8t/7Pf53V52+fVrg3/m8KB7xu/eX8rOi7oxcvnKXChbN07ZwMedz8PVgAQOo5Mpxf+91ZnWnt1l/On6XFC7NUuHBW9Hb0dXOC8vJH2QEABksazpZladu2bWpqapLf71dlZaVyc3Oj7bW1tdqzZ49s21ZBQYGee+45SVJpaamuvfZaSVJRUZEqKiqmpoJR/PfTD+p46CP99Q9vuWyPCQDAZEkazjU1Nert7VV1dbXq6+u1c+dOvfzyy5Kkjo4O/eM//qNee+01zZkzR7/85S/17bffqr29XQUFBXrllVemvIDReNxueXmdGAAwTSW9vxsKhbRs2TJJkRVwQ0NDtO3EiRPKz8/XCy+8oEceeURz587VnDlz1NjYqPPnz+uxxx7Thg0bdPbs2amrAAAAh0m6cu7o6FAwGIzuezwe9ff3y+v16ttvv9WHH36oX//615oxY4bWrl2roqIizZs3Txs3btT999+v48ePa/PmzTp06FDSycQG/2QIhUKTer5UckotTqlDohZTOaUWp9QhUctEJA3nYDCocHj4ryNZliWvNzJs1qxZWrJkiebNmydJuuWWW3Ty5EktX75cHo8neqy5uVm2bSf9veDCwkIFApPzhx1CoZBKSkom5Vyp5pRanFKHRC2mckotTqlDopax9PT0jLsgTXpbu7i4WHV1dZKk+vp65efnR9sKCgp0+vRptbS0qL+/Xx9//LGuv/56vfTSS/rVr34lSTp16pSys7P5wA4AAC5R0pVzWVmZjhw5ovLyctm2raqqKh04cEA5OTlauXKlKioqtH79eknSfffdp/z8fG3cuFGbN29WbW2tPB6PduzYMeWFAADgFEnD2e12a/v27XHH8vLyoturVq3SqlWr4tqzsrK0b9++SZoiAABXFj6NAwAAwxDOAAAYhnAGAMAwhDMAAIYx4g9f2LYtSert7Z3U8/b09Ezq+VLJKbU4pQ6JWkzllFqcUodELaMZyruh/EvkssdquYza29t1+vTpVE8DAIDLKj8/X5mZmSOOGxHOlmUpHA7L5/PxYSUAAMezbVt9fX3KyMiQ2z3yFWYjwhkAAAzjDWEAABiGcAYAwDCEMwAAhiGcAQAwjBG/5/xdWZalbdu2qampSX6/X5WVlcrNzY22V1ZW6qOPPlJGRoYkae/everr69NTTz2l7u5uzZ8/Xzt27FB6enqqSpA0sToGBgZ07733Rv905913363HH388JfOPlayW2tpa7dmzR7Ztq6CgQM8995x6enq0efNmXbhwQRkZGXrhhRc0Z86cFFYRMZFaJKm0tFTXXnutJKmoqEgVFRWpmH6c8Wo5efKkqqqqon3r6+u1Z88eFRYWTqtrZaw6li5dOi2vlf379+vw4cNyuVx68sknVVZWpu7u7ml5rYxWi23bxl0ryerYt2+f3nvvPQWDQa1fv17Lly9XS0vL1F4n9jT0/vvv21u2bLFt27ZPnDhhP/nkk3Ht5eXl9oULF+KO/eIXv7APHTpk27Zt/9M//ZN94MCByzLX8UykjiNHjtjbt2+/bHO8VOPV0t7ebq9atSpay759++wLFy7Y+/fvt3fv3m3btm0fPnzY/sUvfnH5Jz6KidTy2Wef2U888URK5jueZN9jQ37zm9/Yf//3f2/b9vS8VobE1jEdr5WLFy/ad955p93T02O3trbad911l23b9rS8VsaqxcRrZbw6Tp06ZT/44IN2d3e33d3dba9evdru7Oyc8utkWt7WDoVCWrZsmaTIT10NDQ3RNsuy9Pnnn+vnP/+5ysvL9fbbb48YU1paqqNHj17+iSeYSB0NDQ1qbGzUo48+qk2bNqm5uTklc080Xi0nTpxQfn6+XnjhBT3yyCOaO3eu5syZM+I5OXbsWErmnmgitTQ2Nur8+fN67LHHtGHDBp09ezZV048zXi1DOjs79eKLL2rr1q0jxkyHa2VIYh3T8VpJT0/XokWL1NXVpa6urujnPkzHa2WsWky8Vsar48yZM7r11lsVCAQUCASUm5urpqamKb9OpuVt7Y6ODgWDwei+x+NRf3+/vF6vOjs79eijj+pv//ZvNTAwoJ/85CcqLCxUR0dH9FNYMjIy1N7enqrpR02kjuuuu06FhYW6/fbb9c4776iyslK7d+9OYRUR49Xy7bff6sMPP9Svf/1rzZgxQ2vXrlVRUZGRz4k0sVrmzZunjRs36v7779fx48e1efNmHTp0KIVVRIxXy5C3335b9913X/Q2qYnPy0TqmI7XiiRlZ2dr1apVGhgY0BNPPBEdY9pzIk2sFhOvlfHquPHGG7Vv3z51dHSor69PJ06c0MMPPzzlz8m0DOdgMKhwOBzdtywr+s2Qnp6un/zkJ9F7/7fddptOnToVHZOWlqZwOKyZM2emZO6xJlLH3XffHT1WVlZmxP9spPFrmTVrlpYsWaJ58+ZJkm655RadPHkybowpz4k0sVqWL18uj8cTPdbc3CzbtlP+iXfj1TLk3Xffjfs+mm7XypDEOm677bZpd63U1dWpublZH3zwgSRp3bp1Ki4unpbXyli1FBYWGnetjFdHXl6e1q5dq/Xr12vRokW6+eabNXv27Cm/Tqblbe3i4mLV1dVJirz5Y+gNH5L02Wefac2aNRoYGFBfX58++ugjFRQUqLi4WLW1tZIi3zQlJSUpmXusidTx7LPP6v3335ckHTt2TAUFBSmZe6LxaikoKNDp06fV0tKi/v5+ffzxx7r++uuNfE6kidXy0ksv6Ve/+pUk6dSpU8rOzk55MEvj1yJFPte+t7dX2dnZcWNMe14mUsd0vFaysrKUlpYmv9+vQCCgzMxMtbW1GfmcSBOrxcRrZbw6WlpaFA6H9eabb+r555/XV199pRtuuGHKn5Np+fGdQ++sO336tGzbVlVVlerq6pSTk6OVK1fqn//5n/Xb3/5WPp9Pf/M3f6M1a9bom2++0ZYtWxQOhzV79mzt2rVLM2bMmHZ1fPnll3rmmWckRVbXlZWVmj9/fkrrkJLX8t577+nVV1+VJN13333auHGjurq6tGXLFn399dfy+XzatWtXdEWaShOp5eLFi9q8ebM6Ozvl8Xj085//XHl5eSmuJHktv//97/XKK69o79690THT8VoZrY7peq3s3r1b//mf/ym3263i4mI9/fTT6u7unpbXymi1tLW1GXetjFfHihUr9Nxzz6mxsVE+n08VFRX64Q9/OOXXybQMZwAAnGxa3tYGAMDJCGcAAAxDOAMAYBjCGQAAwxDOAAAYhnAGAMAwhDMAAIYhnAEAMMz/B2XTzh5l/QakAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(updrs_results[updrs_results['updrs'] == 'updrs_1']['subsample'], updrs_results[updrs_results['updrs'] == 'updrs_1']['test_auc'], label='test_auc')\n",
    "plt.plot(updrs_results[updrs_results['updrs'] == 'updrs_1']['subsample'], updrs_results[updrs_results['updrs'] == 'updrs_1']['train_auc'], label='train_auc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    updrs_1    updrs_2    updrs_3\n",
      "colsample_bytree   0.999434   0.972336   0.579427\n",
      "gamma              0.218284   0.108280   0.000070\n",
      "learning_rate      0.876356   0.650330   0.759742\n",
      "max_depth          1.000000   2.000000   2.000000\n",
      "min_child_weight   0.292568   7.076801   0.642850\n",
      "reg_alpha          6.803953   2.521598   2.426225\n",
      "reg_lambda         1.268909   1.677160   3.504071\n",
      "subsample          0.900000   0.900000   0.900000\n",
      "random_state      42.000000  42.000000  42.000000\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6953316953316954\n",
      "updrs_1 Results: \n",
      "Test AUC = 0.6953316953316954 \n",
      "Train AUC = 0.9975568269676821\n",
      "\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7640941434044882\n",
      "updrs_2 Results: \n",
      "Test AUC = 0.7640941434044882 \n",
      "Train AUC = 1.0\n",
      "\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7852040816326531\n",
      "updrs_3 Results: \n",
      "Test AUC = 0.7852040816326531 \n",
      "Train AUC = 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_forecast_hyperparams_df.loc['subsample', 'updrs_1'] = 0.9\n",
    "xgb_forecast_hyperparams_df.loc['subsample', 'updrs_2'] = 0.9\n",
    "xgb_forecast_hyperparams_df.loc['subsample', 'updrs_3'] = 0.9\n",
    "\n",
    "print(xgb_forecast_hyperparams_df)\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    model = prepare_scale_xgboost_model(xgb_forecast_hyperparams_df, updrs)\n",
    "    temp_df, test_kfold = create_folds(df, updrs)\n",
    "    temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "    X_train = temp_df[(temp_df['kfold'] != test_kfold)].reset_index(drop=True)\n",
    "    y_train = X_train[f'{updrs}_cat']\n",
    "    X_test = temp_df[temp_df['kfold'] == test_kfold].reset_index(drop=True)\n",
    "    y_test = X_test[f'{updrs}_cat']\n",
    "    \n",
    "    best_model = train_xgboost(X_train, X_test, updrs, model)\n",
    "    # compare the results of the model\n",
    "    X_test = X_test.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "    test_preds = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    test_auc = roc_auc_score(y_test, test_preds)\n",
    "    X_train = X_train.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "    train_preds = best_model.predict_proba(X_train)[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, train_preds)\n",
    "    print(f'{updrs} Results: \\nTest AUC = {test_auc} \\nTrain AUC = {train_auc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.999434</td>\n",
       "      <td>0.972336</td>\n",
       "      <td>0.579427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.218284</td>\n",
       "      <td>0.108280</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.876356</td>\n",
       "      <td>0.650330</td>\n",
       "      <td>0.759742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>0.292568</td>\n",
       "      <td>7.076801</td>\n",
       "      <td>0.642850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>6.803953</td>\n",
       "      <td>2.521598</td>\n",
       "      <td>2.426225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>1.268909</td>\n",
       "      <td>1.677160</td>\n",
       "      <td>3.504071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_state</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    updrs_1    updrs_2    updrs_3\n",
       "colsample_bytree   0.999434   0.972336   0.579427\n",
       "gamma              0.218284   0.108280   0.000070\n",
       "learning_rate      0.876356   0.650330   0.759742\n",
       "max_depth          1.000000   2.000000   2.000000\n",
       "min_child_weight   0.292568   7.076801   0.642850\n",
       "reg_alpha          6.803953   2.521598   2.426225\n",
       "reg_lambda         1.268909   1.677160   3.504071\n",
       "subsample          0.900000   0.900000   0.900000\n",
       "random_state      42.000000  42.000000  42.000000"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.763888888888889\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6964285714285715\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6954365079365079\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6186732186732187\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7002457002457002\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7524801587301587\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6919642857142857\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6929563492063492\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6054054054054054\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6992628992628993\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7594246031746031\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6889880952380952\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6929563492063492\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6201474201474202\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6938574938574938\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7455357142857143\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6934523809523809\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6919642857142857\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6083538083538084\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7051597051597053\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7395833333333333\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.697420634920635\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6984126984126984\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6117936117936119\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.716953316953317\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7450396825396826\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7068452380952381\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6944444444444445\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6186732186732188\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.717936117936118\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.746031746031746\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.71875\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.662202380952381\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6142506142506142\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6751842751842753\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7351190476190476\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7088293650793651\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6964285714285715\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6167076167076168\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6909090909090909\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7420634920634921\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7013888888888888\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6656746031746031\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6402948402948403\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.686977886977887\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7400793650793651\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6984126984126984\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6770833333333334\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.631941031941032\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6972972972972973\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8203125\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6819196428571428\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7851562500000001\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7728516694033936\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7635467980295567\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8175223214285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6930803571428572\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7873883928571428\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7619047619047619\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7575259989053093\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8147321428571429\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6813616071428572\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7795758928571429\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7646414887794198\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7597153804050356\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8119419642857143\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6964285714285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7773437499999999\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7679255610290093\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7733990147783252\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8186383928571428\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6796875\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7745535714285715\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7695675971538041\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7717569786535303\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8046875\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6902901785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7885044642857143\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7580733442802409\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.756431308155446\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.80078125\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6802455357142858\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7762276785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7575259989053093\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7608100711548987\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7957589285714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7059151785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.765625\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7471264367816092\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7717569786535303\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7868303571428572\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.69140625\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.76953125\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7432950191570881\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7914614121510674\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8024553571428572\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6869419642857142\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7689732142857143\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7443897099069513\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7777777777777778\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6512896825396826\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6403769841269842\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7040816326530612\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7739795918367347\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.773469387755102\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6607142857142857\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6378968253968255\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7107142857142857\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7673469387755102\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.779591836734694\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6537698412698413\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6344246031746031\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6994897959183674\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.785204081632653\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7826530612244897\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6537698412698413\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6264880952380953\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6908163265306122\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7811224489795918\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7688775510204083\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6532738095238095\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6121031746031746\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6903061224489796\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7673469387755102\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7887755102040817\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6349206349206349\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6140873015873015\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6591836734693878\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7882653061224489\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7831632653061225\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6378968253968254\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6121031746031745\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6693877551020407\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7892857142857143\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.798469387755102\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6408730158730158\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6011904761904762\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6739795918367347\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7775510204081633\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7658163265306123\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6319444444444444\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6091269841269841\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6826530612244898\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7821428571428571\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7760204081632652\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6339285714285715\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.621031746031746\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6816326530612244\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7612244897959184\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7566326530612244\n"
     ]
    }
   ],
   "source": [
    "# check the gamma parameter\n",
    "updrs_results = pd.DataFrame(columns = ['updrs', 'gamma', 'test_auc', 'train_auc'])\n",
    "i = 0\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    for gamma in np.arange(0.1, 1.1, 0.1): \n",
    "        test_auc = []\n",
    "        train_auc = []\n",
    "        for fold in range(0, 5):\n",
    "            xgb_forecast_hyperparams_df.loc['gamma', f'{updrs}'] = gamma\n",
    "            model = prepare_scale_xgboost_model(xgb_forecast_hyperparams_df, updrs)\n",
    "            temp_df, test_kfold = create_folds(df, updrs)\n",
    "            temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "            X_train = temp_df[(temp_df['kfold'] != fold)].reset_index(drop=True)\n",
    "            y_train = X_train[f'{updrs}_cat']\n",
    "            X_test = temp_df[temp_df['kfold'] == fold].reset_index(drop=True)\n",
    "            y_test = X_test[f'{updrs}_cat']\n",
    "    \n",
    "            best_model = train_xgboost(X_train, X_test, updrs, model)\n",
    "            # compare the results of the model\n",
    "            X_test = X_test.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "            test_preds = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            fold_test_auc = roc_auc_score(y_test, test_preds)\n",
    "            X_train = X_train.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "            train_preds = best_model.predict_proba(X_train)[:, 1]\n",
    "            fold_train_auc = roc_auc_score(y_train, train_preds)\n",
    "            test_auc.append(fold_test_auc)\n",
    "            train_auc.append(fold_train_auc)\n",
    "        updrs_results.loc[i, 'updrs'] = updrs\n",
    "        updrs_results.loc[i, 'gamma'] = gamma\n",
    "        updrs_results.loc[i, 'test_auc'] = np.mean(test_auc)\n",
    "        updrs_results.loc[i, 'train_auc'] = np.mean(train_auc)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs</th>\n",
       "      <th>gamma</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.694935</td>\n",
       "      <td>0.997831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.688414</td>\n",
       "      <td>0.996917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.691075</td>\n",
       "      <td>0.995929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.688893</td>\n",
       "      <td>0.994984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.692833</td>\n",
       "      <td>0.993668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.696588</td>\n",
       "      <td>0.993236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.683284</td>\n",
       "      <td>0.992489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.689599</td>\n",
       "      <td>0.991822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.68728</td>\n",
       "      <td>0.98989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.688963</td>\n",
       "      <td>0.988215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.764757</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.763484</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.760005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.765408</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.762841</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.759597</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.755118</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.757237</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.756505</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.756108</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.708639</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.711253</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.711108</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.704215</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.702361</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.695924</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.701429</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.691882</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.696378</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.69089</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      updrs gamma  test_auc train_auc\n",
       "0   updrs_1   0.1  0.694935  0.997831\n",
       "1   updrs_1   0.2  0.688414  0.996917\n",
       "2   updrs_1   0.3  0.691075  0.995929\n",
       "3   updrs_1   0.4  0.688893  0.994984\n",
       "4   updrs_1   0.5  0.692833  0.993668\n",
       "5   updrs_1   0.6  0.696588  0.993236\n",
       "6   updrs_1   0.7  0.683284  0.992489\n",
       "7   updrs_1   0.8  0.689599  0.991822\n",
       "8   updrs_1   0.9   0.68728   0.98989\n",
       "9   updrs_1   1.0  0.688963  0.988215\n",
       "10  updrs_2   0.1  0.764757       1.0\n",
       "11  updrs_2   0.2  0.763484       1.0\n",
       "12  updrs_2   0.3  0.760005       1.0\n",
       "13  updrs_2   0.4  0.765408       1.0\n",
       "14  updrs_2   0.5  0.762841       1.0\n",
       "15  updrs_2   0.6  0.759597       1.0\n",
       "16  updrs_2   0.7  0.755118       1.0\n",
       "17  updrs_2   0.8  0.757237       1.0\n",
       "18  updrs_2   0.9  0.756505       1.0\n",
       "19  updrs_2   1.0  0.756108       1.0\n",
       "20  updrs_3   0.1  0.708639       1.0\n",
       "21  updrs_3   0.2  0.711253       1.0\n",
       "22  updrs_3   0.3  0.711108       1.0\n",
       "23  updrs_3   0.4  0.704215       1.0\n",
       "24  updrs_3   0.5  0.702361       1.0\n",
       "25  updrs_3   0.6  0.695924       1.0\n",
       "26  updrs_3   0.7  0.701429       1.0\n",
       "27  updrs_3   0.8  0.691882       1.0\n",
       "28  updrs_3   0.9  0.696378       1.0\n",
       "29  updrs_3   1.0   0.69089       1.0"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS 1 Best gamma: 0.6\n",
      " UPDRS 1 Max AUC: 0.6965877403377404\n",
      "UPDRS 2 Best gamma: 0.4\n",
      " UPDRS 2 Max AUC: 0.765407772304324\n",
      "UPDRS 3 Best gamma: 0.2\n",
      " UPDRS 3 Max AUC: 0.7112528344671202\n"
     ]
    }
   ],
   "source": [
    "updrs1_max = updrs_results[updrs_results['updrs'] == 'updrs_1']['test_auc'].max()\n",
    "updrs1_best_gamma = updrs_results[(updrs_results['updrs'] == 'updrs_1') & (updrs_results['test_auc'] == updrs1_max)]['gamma'].values[0]\n",
    "updrs2_max = updrs_results[updrs_results['updrs'] == 'updrs_2']['test_auc'].max()\n",
    "updrs2_best_gamma = updrs_results[(updrs_results['updrs'] == 'updrs_2') & (updrs_results['test_auc'] == updrs2_max)]['gamma'].values[0]\n",
    "updrs3_max = updrs_results[updrs_results['updrs'] == 'updrs_3']['test_auc'].max()\n",
    "updrs3_best_gamma = updrs_results[(updrs_results['updrs'] == 'updrs_3') & (updrs_results['test_auc'] == updrs3_max)]['gamma'].values[0]\n",
    "\n",
    "print(f'UPDRS 1 Best gamma: {updrs1_best_gamma}\\n UPDRS 1 Max AUC: {updrs1_max}')\n",
    "print(f'UPDRS 2 Best gamma: {updrs2_best_gamma}\\n UPDRS 2 Max AUC: {updrs2_max}')\n",
    "print(f'UPDRS 3 Best gamma: {updrs3_best_gamma}\\n UPDRS 3 Max AUC: {updrs3_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_forecast_hyperparams_df.loc['gamma', 'updrs_1'] = 0.6\n",
    "xgb_forecast_hyperparams_df.loc['gamma', 'updrs_2'] = 0.4\n",
    "xgb_forecast_hyperparams_df.loc['gamma', 'updrs_3'] = 0.0007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.999434</td>\n",
       "      <td>0.972336</td>\n",
       "      <td>0.579427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.876356</td>\n",
       "      <td>0.650330</td>\n",
       "      <td>0.759742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>0.292568</td>\n",
       "      <td>7.076801</td>\n",
       "      <td>0.642850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>6.803953</td>\n",
       "      <td>2.521598</td>\n",
       "      <td>2.426225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>1.268909</td>\n",
       "      <td>1.677160</td>\n",
       "      <td>3.504071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_state</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    updrs_1    updrs_2    updrs_3\n",
       "colsample_bytree   0.999434   0.972336   0.579427\n",
       "gamma              0.600000   0.400000   1.000000\n",
       "learning_rate      0.876356   0.650330   0.759742\n",
       "max_depth          1.000000   2.000000   2.000000\n",
       "min_child_weight   0.292568   7.076801   0.642850\n",
       "reg_alpha          6.803953   2.521598   2.426225\n",
       "reg_lambda         1.268909   1.677160   3.504071\n",
       "subsample          0.900000   0.900000   0.900000\n",
       "random_state      42.000000  42.000000  42.000000"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6805555555555556\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6245039682539683\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6532738095238095\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.5945945945945946\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7523341523341524\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6805555555555556\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6245039682539683\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6532738095238095\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.5945945945945946\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7523341523341524\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6805555555555556\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6245039682539683\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6532738095238095\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.5945945945945946\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7523341523341524\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6805555555555556\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6245039682539683\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6532738095238095\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.5945945945945946\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7523341523341524\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6865079365079365\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6245039682539683\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6532738095238095\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.5990171990171991\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7140049140049141\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7063492063492064\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7083333333333334\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6626984126984128\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6014742014742015\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7621621621621621\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6840277777777778\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6572420634920635\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7018849206349207\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6245700245700245\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6835380835380835\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6507936507936508\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6711309523809523\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6939484126984127\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6029484029484028\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7233415233415234\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.668154761904762\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6999007936507937\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6964285714285714\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.62997542997543\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6874692874692875\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.689484126984127\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.662202380952381\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.670138888888889\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6486486486486487\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7238329238329239\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6651785714285714\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6453373015873015\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6517857142857143\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6127764127764128\n",
      "5 Kfolds created for updrs_1_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7695331695331695\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8376116071428572\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7393973214285715\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.75390625\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7980295566502463\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7476737821565408\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8878348214285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7114955357142856\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7427455357142858\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7066228790366722\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6694033935413246\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8895089285714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7522321428571428\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7734375\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7361795292829776\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7624521072796935\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8973214285714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7896205357142857\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7494419642857144\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7394636015325671\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8232074438970991\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8543526785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6964285714285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7204241071428571\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7602627257799671\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.782703886152162\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.80078125\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7377232142857143\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7862723214285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7640941434044882\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7449370552818828\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8091517857142857\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7840401785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7684151785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7848932676518884\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7728516694033936\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7979910714285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6428571428571428\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7410714285714285\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7931034482758621\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7657361795292829\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8426339285714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6969866071428571\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7728794642857142\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7859879584017515\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7870826491516145\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8777901785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6886160714285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7862723214285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7799671592775042\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.735632183908046\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7996651785714286\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6891741071428572\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7745535714285714\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7378215654077723\n",
      "5 Kfolds created for updrs_2_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7307060755336617\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6309523809523809\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6765873015873016\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6040816326530613\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.789795918367347\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6979591836734694\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.619047619047619\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6755952380952381\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7132653061224489\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7724489795918368\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6979591836734694\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6086309523809524\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.621031746031746\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.5892857142857142\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7698979591836734\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8397959183673469\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7619047619047619\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6398809523809523\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.698469387755102\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7566326530612244\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7551020408163266\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6850198412698413\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6423611111111112\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6403061224489796\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.764795918367347\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.753061224489796\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6830357142857143\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6324404761904762\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6673469387755102\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6561224489795918\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7423469387755102\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6433531746031746\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.5843253968253969\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6229591836734695\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7255102040816327\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7811224489795918\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6825396825396826\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6438492063492063\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6653061224489796\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7198979591836735\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8061224489795918\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6825396825396826\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6031746031746031\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6520408163265307\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.698469387755102\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.8244897959183672\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6755952380952381\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6304563492063492\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6326530612244898\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7158163265306122\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7642857142857143\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6438492063492064\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6155753968253969\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.6704081632653062\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7443877551020409\n",
      "5 Kfolds created for updrs_3_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test Data: 0.7479591836734695\n"
     ]
    }
   ],
   "source": [
    "# try min_child_weight\n",
    "# check the gamma parameter\n",
    "updrs_results = pd.DataFrame(columns = ['updrs', 'min_child_weight', 'test_auc', 'train_auc'])\n",
    "i = 0\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    for min_child_weight in range(1, 32, 3): \n",
    "        test_auc = []\n",
    "        train_auc = []\n",
    "        for fold in range(0, 5):\n",
    "            xgb_forecast_hyperparams_df.loc['min_child_weight', f'{updrs}'] = min_child_weight\n",
    "            model = prepare_scale_xgboost_model(xgb_forecast_hyperparams_df, updrs)\n",
    "            temp_df, test_kfold = create_folds(df, updrs)\n",
    "            temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "            X_train = temp_df[(temp_df['kfold'] != fold)].reset_index(drop=True)\n",
    "            y_train = X_train[f'{updrs}_cat']\n",
    "            X_test = temp_df[temp_df['kfold'] == fold].reset_index(drop=True)\n",
    "            y_test = X_test[f'{updrs}_cat']\n",
    "    \n",
    "            best_model = train_xgboost(X_train, X_test, updrs, model)\n",
    "            # compare the results of the model\n",
    "            X_test = X_test.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "            test_preds = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            fold_test_auc = roc_auc_score(y_test, test_preds)\n",
    "            X_train = X_train.drop(columns=[f'{updrs}_cat', 'kfold'])\n",
    "            train_preds = best_model.predict_proba(X_train)[:, 1]\n",
    "            fold_train_auc = roc_auc_score(y_train, train_preds)\n",
    "            test_auc.append(fold_test_auc)\n",
    "            train_auc.append(fold_train_auc)\n",
    "        updrs_results.loc[i, 'updrs'] = updrs\n",
    "        updrs_results.loc[i, 'min_child_weight'] = min_child_weight\n",
    "        updrs_results.loc[i, 'test_auc'] = np.mean(test_auc)\n",
    "        updrs_results.loc[i, 'train_auc'] = np.mean(train_auc)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661052</td>\n",
       "      <td>0.986124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.661052</td>\n",
       "      <td>0.986124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.661052</td>\n",
       "      <td>0.986124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.661052</td>\n",
       "      <td>0.986124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.655462</td>\n",
       "      <td>0.984603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.688203</td>\n",
       "      <td>0.984787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.670253</td>\n",
       "      <td>0.984386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.668433</td>\n",
       "      <td>0.984651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.676386</td>\n",
       "      <td>0.980798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.678861</td>\n",
       "      <td>0.976167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>updrs_1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.668922</td>\n",
       "      <td>0.974588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.775324</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.74362</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.782762</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.799811</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>13</td>\n",
       "      <td>0.762834</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.766762</td>\n",
       "      <td>0.999917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.78387</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>22</td>\n",
       "      <td>0.748152</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.777114</td>\n",
       "      <td>0.999751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>28</td>\n",
       "      <td>0.773656</td>\n",
       "      <td>0.998562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>updrs_2</td>\n",
       "      <td>31</td>\n",
       "      <td>0.746384</td>\n",
       "      <td>0.994383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.679875</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.695663</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.685728</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.722398</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.697109</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.999759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>19</td>\n",
       "      <td>0.671454</td>\n",
       "      <td>0.999798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>22</td>\n",
       "      <td>0.703543</td>\n",
       "      <td>0.998694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>25</td>\n",
       "      <td>0.692143</td>\n",
       "      <td>0.998745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>28</td>\n",
       "      <td>0.683761</td>\n",
       "      <td>0.996104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>updrs_3</td>\n",
       "      <td>31</td>\n",
       "      <td>0.684436</td>\n",
       "      <td>0.990114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      updrs min_child_weight  test_auc train_auc\n",
       "0   updrs_1                1  0.661052  0.986124\n",
       "1   updrs_1                4  0.661052  0.986124\n",
       "2   updrs_1                7  0.661052  0.986124\n",
       "3   updrs_1               10  0.661052  0.986124\n",
       "4   updrs_1               13  0.655462  0.984603\n",
       "5   updrs_1               16  0.688203  0.984787\n",
       "6   updrs_1               19  0.670253  0.984386\n",
       "7   updrs_1               22  0.668433  0.984651\n",
       "8   updrs_1               25  0.676386  0.980798\n",
       "9   updrs_1               28  0.678861  0.976167\n",
       "10  updrs_1               31  0.668922  0.974588\n",
       "11  updrs_2                1  0.775324       1.0\n",
       "12  updrs_2                4   0.74362       1.0\n",
       "13  updrs_2                7  0.782762       1.0\n",
       "14  updrs_2               10  0.799811       1.0\n",
       "15  updrs_2               13  0.762834  0.999979\n",
       "16  updrs_2               16  0.766762  0.999917\n",
       "17  updrs_2               19   0.78387  0.999993\n",
       "18  updrs_2               22  0.748152  0.999972\n",
       "19  updrs_2               25  0.777114  0.999751\n",
       "20  updrs_2               28  0.773656  0.998562\n",
       "21  updrs_2               31  0.746384  0.994383\n",
       "22  updrs_3                1  0.679875       1.0\n",
       "23  updrs_3                4  0.695663       1.0\n",
       "24  updrs_3                7  0.685728       1.0\n",
       "25  updrs_3               10  0.722398       1.0\n",
       "26  updrs_3               13  0.697109  0.999994\n",
       "27  updrs_3               16  0.676259  0.999759\n",
       "28  updrs_3               19  0.671454  0.999798\n",
       "29  updrs_3               22  0.703543  0.998694\n",
       "30  updrs_3               25  0.692143  0.998745\n",
       "31  updrs_3               28  0.683761  0.996104\n",
       "32  updrs_3               31  0.684436  0.990114"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS 1 Best min_child_weight: 16\n",
      " UPDRS 1 Max AUC: 0.6882034632034632\n",
      "UPDRS 2 Best min_child_weight: 10\n",
      " UPDRS 2 Max AUC: 0.799810994800219\n",
      "UPDRS 3 Best min_child_weight: 10\n",
      " UPDRS 3 Max AUC: 0.7223979591836736\n"
     ]
    }
   ],
   "source": [
    "updrs1_max = updrs_results[updrs_results['updrs'] == 'updrs_1']['test_auc'].max()\n",
    "updrs1_best_min_child_weight = updrs_results[(updrs_results['updrs'] == 'updrs_1') & (updrs_results['test_auc'] == updrs1_max)]['min_child_weight'].values[0]\n",
    "updrs2_max = updrs_results[updrs_results['updrs'] == 'updrs_2']['test_auc'].max()\n",
    "updrs2_best_min_child_weight = updrs_results[(updrs_results['updrs'] == 'updrs_2') & (updrs_results['test_auc'] == updrs2_max)]['min_child_weight'].values[0]\n",
    "updrs3_max = updrs_results[updrs_results['updrs'] == 'updrs_3']['test_auc'].max()\n",
    "updrs3_best_min_child_weight = updrs_results[(updrs_results['updrs'] == 'updrs_3') & (updrs_results['test_auc'] == updrs3_max)]['min_child_weight'].values[0]\n",
    "\n",
    "print(f'UPDRS 1 Best min_child_weight: {updrs1_best_min_child_weight}\\n UPDRS 1 Max AUC: {updrs1_max}')\n",
    "print(f'UPDRS 2 Best min_child_weight: {updrs2_best_min_child_weight}\\n UPDRS 2 Max AUC: {updrs2_max}')\n",
    "print(f'UPDRS 3 Best min_child_weight: {updrs3_best_min_child_weight}\\n UPDRS 3 Max AUC: {updrs3_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.999434</td>\n",
       "      <td>0.972336</td>\n",
       "      <td>0.579427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.876356</td>\n",
       "      <td>0.650330</td>\n",
       "      <td>0.759742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>6.803953</td>\n",
       "      <td>2.521598</td>\n",
       "      <td>2.426225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>1.268909</td>\n",
       "      <td>1.677160</td>\n",
       "      <td>3.504071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    updrs_1    updrs_2    updrs_3\n",
       "colsample_bytree   0.999434   0.972336   0.579427\n",
       "gamma              1.000000   0.900000   1.000000\n",
       "learning_rate      0.876356   0.650330   0.759742\n",
       "max_depth          1.000000   2.000000   2.000000\n",
       "min_child_weight  31.000000  31.000000  31.000000\n",
       "reg_alpha          6.803953   2.521598   2.426225\n",
       "reg_lambda         1.268909   1.677160   3.504071\n",
       "subsample          0.900000   0.900000   0.900000\n",
       "scale_pos_weight   1.500000   2.200000   1.600000"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBoost Future Categorical Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS: updrs_1\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.970757221693757, 'importance_type': 'split', 'learning_rate': 0.9835668264286576, 'max_depth': 8, 'min_child_samples': 20, 'min_child_weight': 1.2325864748203452, 'min_split_gain': 0.007964040585614, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 7.437593735288674, 'reg_lambda': 7.499284502976287, 'silent': 'warn', 'subsample': 0.7286661893753408, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "\n",
      "UPDRS: updrs_2\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.9036286590547646, 'importance_type': 'split', 'learning_rate': 0.1745511363640152, 'max_depth': 2, 'min_child_samples': 20, 'min_child_weight': 2.7975180017133723, 'min_split_gain': 0.2978757291694001, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 1.5436778945279928, 'reg_lambda': 3.401574051688944, 'silent': 'warn', 'subsample': 0.948910131036216, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "\n",
      "UPDRS: updrs_3\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.6863256427614786, 'importance_type': 'split', 'learning_rate': 0.9101256658806474, 'max_depth': 6, 'min_child_samples': 20, 'min_child_weight': 13.38750018696772, 'min_split_gain': 0.0040114152530728, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 4.159710062052169, 'reg_lambda': 1.926977284964978, 'silent': 'warn', 'subsample': 0.8396550111985219, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgb_forecast_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    model = prepare_lgboost_model(lgb_forecast_hyperparams_df, updrs)\n",
    "    print(f'UPDRS: {updrs}')\n",
    "    print(model.get_params())\n",
    "    print('\\n')\n",
    "    auc, acc, prec, recall = cross_fold_validation(df, model, updrs)\n",
    "    lgb_forecast_results[updrs] = {\"auc\":auc,\n",
    "                        \"acc\":acc,\n",
    "                        \"prec\":prec,\n",
    "                        \"recall\":recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updrs_1': {'auc': 0.59664793015473,\n",
       "  'acc': 0.6300638403447596,\n",
       "  'prec': 0.5485747458161251,\n",
       "  'recall': 0.41962947373344095},\n",
       " 'updrs_2': {'auc': 0.6251290954000639,\n",
       "  'acc': 0.7347631535131536,\n",
       "  'prec': 0.6323177476118652,\n",
       "  'recall': 0.33857255787690566},\n",
       " 'updrs_3': {'auc': 0.5969516130707487,\n",
       "  'acc': 0.6313568096484241,\n",
       "  'prec': 0.531607724043764,\n",
       "  'recall': 0.4460611805697889}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_forecast_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Catboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDRS: updrs_1\n",
      "Hyperparameters: {'learning_rate': 0.3667810794764569, 'depth': 7, 'l2_leaf_reg': 5.641366638454, 'bagging_temperature': 4.429427339938372, 'min_data_in_leaf': 8}\n",
      "\n",
      "\n",
      "0:\tlearn: 0.5964092\ttotal: 360ms\tremaining: 5m 59s\n",
      "1:\tlearn: 0.5284874\ttotal: 692ms\tremaining: 5m 45s\n",
      "2:\tlearn: 0.4746385\ttotal: 1.02s\tremaining: 5m 37s\n",
      "3:\tlearn: 0.4301078\ttotal: 1.34s\tremaining: 5m 34s\n",
      "4:\tlearn: 0.3818352\ttotal: 1.67s\tremaining: 5m 32s\n",
      "5:\tlearn: 0.3581313\ttotal: 2s\tremaining: 5m 31s\n",
      "6:\tlearn: 0.3207303\ttotal: 2.32s\tremaining: 5m 29s\n",
      "7:\tlearn: 0.2842559\ttotal: 2.64s\tremaining: 5m 27s\n",
      "8:\tlearn: 0.2643040\ttotal: 2.96s\tremaining: 5m 26s\n",
      "9:\tlearn: 0.2496839\ttotal: 3.28s\tremaining: 5m 25s\n",
      "10:\tlearn: 0.2323576\ttotal: 3.61s\tremaining: 5m 24s\n",
      "11:\tlearn: 0.2093736\ttotal: 3.94s\tremaining: 5m 24s\n",
      "12:\tlearn: 0.1869665\ttotal: 4.27s\tremaining: 5m 24s\n",
      "13:\tlearn: 0.1679286\ttotal: 4.6s\tremaining: 5m 24s\n",
      "14:\tlearn: 0.1571655\ttotal: 4.93s\tremaining: 5m 23s\n",
      "15:\tlearn: 0.1523514\ttotal: 5.26s\tremaining: 5m 23s\n",
      "16:\tlearn: 0.1381481\ttotal: 5.58s\tremaining: 5m 22s\n",
      "17:\tlearn: 0.1260279\ttotal: 5.91s\tremaining: 5m 22s\n",
      "18:\tlearn: 0.1119219\ttotal: 6.23s\tremaining: 5m 21s\n",
      "19:\tlearn: 0.1012453\ttotal: 6.56s\tremaining: 5m 21s\n",
      "20:\tlearn: 0.0894669\ttotal: 6.88s\tremaining: 5m 20s\n",
      "21:\tlearn: 0.0815609\ttotal: 7.21s\tremaining: 5m 20s\n",
      "22:\tlearn: 0.0730983\ttotal: 7.53s\tremaining: 5m 19s\n",
      "23:\tlearn: 0.0664611\ttotal: 7.85s\tremaining: 5m 19s\n",
      "24:\tlearn: 0.0606534\ttotal: 8.18s\tremaining: 5m 19s\n",
      "25:\tlearn: 0.0560918\ttotal: 8.51s\tremaining: 5m 18s\n",
      "26:\tlearn: 0.0510370\ttotal: 8.83s\tremaining: 5m 18s\n",
      "27:\tlearn: 0.0464288\ttotal: 9.18s\tremaining: 5m 18s\n",
      "28:\tlearn: 0.0433834\ttotal: 9.54s\tremaining: 5m 19s\n",
      "29:\tlearn: 0.0405692\ttotal: 9.9s\tremaining: 5m 20s\n",
      "30:\tlearn: 0.0375873\ttotal: 10.2s\tremaining: 5m 19s\n",
      "31:\tlearn: 0.0351544\ttotal: 10.6s\tremaining: 5m 19s\n",
      "32:\tlearn: 0.0329861\ttotal: 10.9s\tremaining: 5m 18s\n",
      "33:\tlearn: 0.0306632\ttotal: 11.2s\tremaining: 5m 18s\n",
      "34:\tlearn: 0.0286226\ttotal: 11.5s\tremaining: 5m 17s\n",
      "35:\tlearn: 0.0272922\ttotal: 11.9s\tremaining: 5m 17s\n",
      "36:\tlearn: 0.0253287\ttotal: 12.2s\tremaining: 5m 17s\n",
      "37:\tlearn: 0.0240633\ttotal: 12.5s\tremaining: 5m 16s\n",
      "38:\tlearn: 0.0232455\ttotal: 12.8s\tremaining: 5m 16s\n",
      "39:\tlearn: 0.0222399\ttotal: 13.2s\tremaining: 5m 15s\n",
      "40:\tlearn: 0.0212921\ttotal: 13.5s\tremaining: 5m 15s\n",
      "41:\tlearn: 0.0204743\ttotal: 13.8s\tremaining: 5m 15s\n",
      "42:\tlearn: 0.0195784\ttotal: 14.1s\tremaining: 5m 14s\n",
      "43:\tlearn: 0.0183412\ttotal: 14.5s\tremaining: 5m 14s\n",
      "44:\tlearn: 0.0177336\ttotal: 14.8s\tremaining: 5m 13s\n",
      "45:\tlearn: 0.0169403\ttotal: 15.1s\tremaining: 5m 13s\n",
      "46:\tlearn: 0.0162425\ttotal: 15.4s\tremaining: 5m 13s\n",
      "47:\tlearn: 0.0157316\ttotal: 15.8s\tremaining: 5m 12s\n",
      "48:\tlearn: 0.0152532\ttotal: 16.1s\tremaining: 5m 12s\n",
      "49:\tlearn: 0.0146847\ttotal: 16.4s\tremaining: 5m 12s\n",
      "50:\tlearn: 0.0141718\ttotal: 16.7s\tremaining: 5m 11s\n",
      "51:\tlearn: 0.0136276\ttotal: 17.1s\tremaining: 5m 11s\n",
      "52:\tlearn: 0.0134061\ttotal: 17.4s\tremaining: 5m 11s\n",
      "53:\tlearn: 0.0130169\ttotal: 17.7s\tremaining: 5m 10s\n",
      "54:\tlearn: 0.0127132\ttotal: 18.1s\tremaining: 5m 10s\n",
      "55:\tlearn: 0.0123290\ttotal: 18.4s\tremaining: 5m 10s\n",
      "56:\tlearn: 0.0119145\ttotal: 18.7s\tremaining: 5m 9s\n",
      "57:\tlearn: 0.0115305\ttotal: 19s\tremaining: 5m 9s\n",
      "58:\tlearn: 0.0111486\ttotal: 19.4s\tremaining: 5m 8s\n",
      "59:\tlearn: 0.0107781\ttotal: 19.7s\tremaining: 5m 8s\n",
      "60:\tlearn: 0.0104740\ttotal: 20s\tremaining: 5m 8s\n",
      "61:\tlearn: 0.0102334\ttotal: 20.3s\tremaining: 5m 7s\n",
      "62:\tlearn: 0.0100110\ttotal: 20.7s\tremaining: 5m 7s\n",
      "63:\tlearn: 0.0098272\ttotal: 21s\tremaining: 5m 7s\n",
      "64:\tlearn: 0.0096172\ttotal: 21.3s\tremaining: 5m 6s\n",
      "65:\tlearn: 0.0094184\ttotal: 21.6s\tremaining: 5m 6s\n",
      "66:\tlearn: 0.0092688\ttotal: 22s\tremaining: 5m 5s\n",
      "67:\tlearn: 0.0090235\ttotal: 22.3s\tremaining: 5m 5s\n",
      "68:\tlearn: 0.0088300\ttotal: 22.6s\tremaining: 5m 5s\n",
      "69:\tlearn: 0.0086840\ttotal: 23s\tremaining: 5m 5s\n",
      "70:\tlearn: 0.0085472\ttotal: 23.3s\tremaining: 5m 5s\n",
      "71:\tlearn: 0.0082705\ttotal: 23.6s\tremaining: 5m 4s\n",
      "72:\tlearn: 0.0081016\ttotal: 24s\tremaining: 5m 4s\n",
      "73:\tlearn: 0.0079666\ttotal: 24.3s\tremaining: 5m 3s\n",
      "74:\tlearn: 0.0078284\ttotal: 24.6s\tremaining: 5m 3s\n",
      "75:\tlearn: 0.0077096\ttotal: 24.9s\tremaining: 5m 3s\n",
      "76:\tlearn: 0.0075843\ttotal: 25.3s\tremaining: 5m 3s\n",
      "77:\tlearn: 0.0073539\ttotal: 25.6s\tremaining: 5m 2s\n",
      "78:\tlearn: 0.0071974\ttotal: 26s\tremaining: 5m 2s\n",
      "79:\tlearn: 0.0071137\ttotal: 26.3s\tremaining: 5m 2s\n",
      "80:\tlearn: 0.0069893\ttotal: 26.6s\tremaining: 5m 1s\n",
      "81:\tlearn: 0.0068402\ttotal: 26.9s\tremaining: 5m 1s\n",
      "82:\tlearn: 0.0067442\ttotal: 27.3s\tremaining: 5m 1s\n",
      "83:\tlearn: 0.0065614\ttotal: 27.6s\tremaining: 5m 1s\n",
      "84:\tlearn: 0.0063455\ttotal: 27.9s\tremaining: 5m\n",
      "85:\tlearn: 0.0062511\ttotal: 28.3s\tremaining: 5m\n",
      "86:\tlearn: 0.0060821\ttotal: 28.6s\tremaining: 5m\n",
      "87:\tlearn: 0.0060259\ttotal: 29s\tremaining: 5m\n",
      "88:\tlearn: 0.0059543\ttotal: 29.3s\tremaining: 4m 59s\n",
      "89:\tlearn: 0.0058844\ttotal: 29.6s\tremaining: 4m 59s\n",
      "90:\tlearn: 0.0057648\ttotal: 29.9s\tremaining: 4m 59s\n",
      "91:\tlearn: 0.0056968\ttotal: 30.3s\tremaining: 4m 58s\n",
      "92:\tlearn: 0.0055901\ttotal: 30.6s\tremaining: 4m 58s\n",
      "93:\tlearn: 0.0054788\ttotal: 30.9s\tremaining: 4m 58s\n",
      "94:\tlearn: 0.0054131\ttotal: 31.3s\tremaining: 4m 57s\n",
      "95:\tlearn: 0.0053518\ttotal: 31.6s\tremaining: 4m 57s\n",
      "96:\tlearn: 0.0052790\ttotal: 31.9s\tremaining: 4m 57s\n",
      "97:\tlearn: 0.0051864\ttotal: 32.3s\tremaining: 4m 56s\n",
      "98:\tlearn: 0.0051118\ttotal: 32.6s\tremaining: 4m 56s\n",
      "99:\tlearn: 0.0050602\ttotal: 32.9s\tremaining: 4m 56s\n",
      "100:\tlearn: 0.0050123\ttotal: 33.3s\tremaining: 4m 56s\n",
      "101:\tlearn: 0.0049631\ttotal: 33.6s\tremaining: 4m 55s\n",
      "102:\tlearn: 0.0049220\ttotal: 33.9s\tremaining: 4m 55s\n",
      "103:\tlearn: 0.0048679\ttotal: 34.3s\tremaining: 4m 55s\n",
      "104:\tlearn: 0.0047921\ttotal: 34.6s\tremaining: 4m 54s\n",
      "105:\tlearn: 0.0047593\ttotal: 34.9s\tremaining: 4m 54s\n",
      "106:\tlearn: 0.0047143\ttotal: 35.2s\tremaining: 4m 54s\n",
      "107:\tlearn: 0.0046661\ttotal: 35.6s\tremaining: 4m 53s\n",
      "108:\tlearn: 0.0045802\ttotal: 35.9s\tremaining: 4m 53s\n",
      "109:\tlearn: 0.0044943\ttotal: 36.2s\tremaining: 4m 53s\n",
      "110:\tlearn: 0.0044360\ttotal: 36.5s\tremaining: 4m 52s\n",
      "111:\tlearn: 0.0043707\ttotal: 36.9s\tremaining: 4m 52s\n",
      "112:\tlearn: 0.0043105\ttotal: 37.2s\tremaining: 4m 52s\n",
      "113:\tlearn: 0.0042793\ttotal: 37.5s\tremaining: 4m 51s\n",
      "114:\tlearn: 0.0042418\ttotal: 37.9s\tremaining: 4m 51s\n",
      "115:\tlearn: 0.0041755\ttotal: 38.2s\tremaining: 4m 51s\n",
      "116:\tlearn: 0.0041361\ttotal: 38.5s\tremaining: 4m 50s\n",
      "117:\tlearn: 0.0040815\ttotal: 38.8s\tremaining: 4m 50s\n",
      "118:\tlearn: 0.0040464\ttotal: 39.2s\tremaining: 4m 49s\n",
      "119:\tlearn: 0.0039972\ttotal: 39.5s\tremaining: 4m 49s\n",
      "120:\tlearn: 0.0039537\ttotal: 39.8s\tremaining: 4m 49s\n",
      "121:\tlearn: 0.0039232\ttotal: 40.1s\tremaining: 4m 48s\n",
      "122:\tlearn: 0.0038896\ttotal: 40.5s\tremaining: 4m 48s\n",
      "123:\tlearn: 0.0038494\ttotal: 40.8s\tremaining: 4m 48s\n",
      "124:\tlearn: 0.0038108\ttotal: 41.1s\tremaining: 4m 47s\n",
      "125:\tlearn: 0.0037524\ttotal: 41.4s\tremaining: 4m 47s\n",
      "126:\tlearn: 0.0037217\ttotal: 41.8s\tremaining: 4m 47s\n",
      "127:\tlearn: 0.0036912\ttotal: 42.1s\tremaining: 4m 46s\n",
      "128:\tlearn: 0.0036641\ttotal: 42.4s\tremaining: 4m 46s\n",
      "129:\tlearn: 0.0036173\ttotal: 42.7s\tremaining: 4m 45s\n",
      "130:\tlearn: 0.0035918\ttotal: 43.1s\tremaining: 4m 45s\n",
      "131:\tlearn: 0.0035424\ttotal: 43.4s\tremaining: 4m 45s\n",
      "132:\tlearn: 0.0035126\ttotal: 43.7s\tremaining: 4m 44s\n",
      "133:\tlearn: 0.0034893\ttotal: 44s\tremaining: 4m 44s\n",
      "134:\tlearn: 0.0034587\ttotal: 44.4s\tremaining: 4m 44s\n",
      "135:\tlearn: 0.0034399\ttotal: 44.7s\tremaining: 4m 43s\n",
      "136:\tlearn: 0.0034136\ttotal: 45s\tremaining: 4m 43s\n",
      "137:\tlearn: 0.0033929\ttotal: 45.3s\tremaining: 4m 43s\n",
      "138:\tlearn: 0.0033739\ttotal: 45.7s\tremaining: 4m 42s\n",
      "139:\tlearn: 0.0033495\ttotal: 46s\tremaining: 4m 42s\n",
      "140:\tlearn: 0.0033257\ttotal: 46.3s\tremaining: 4m 42s\n",
      "141:\tlearn: 0.0032870\ttotal: 46.7s\tremaining: 4m 41s\n",
      "142:\tlearn: 0.0032660\ttotal: 47s\tremaining: 4m 41s\n",
      "143:\tlearn: 0.0032222\ttotal: 47.3s\tremaining: 4m 41s\n",
      "144:\tlearn: 0.0032032\ttotal: 47.6s\tremaining: 4m 40s\n",
      "145:\tlearn: 0.0031782\ttotal: 48s\tremaining: 4m 40s\n",
      "146:\tlearn: 0.0031537\ttotal: 48.3s\tremaining: 4m 40s\n",
      "147:\tlearn: 0.0031236\ttotal: 48.6s\tremaining: 4m 39s\n",
      "148:\tlearn: 0.0030986\ttotal: 48.9s\tremaining: 4m 39s\n",
      "149:\tlearn: 0.0030785\ttotal: 49.3s\tremaining: 4m 39s\n",
      "150:\tlearn: 0.0030554\ttotal: 49.6s\tremaining: 4m 38s\n",
      "151:\tlearn: 0.0030373\ttotal: 49.9s\tremaining: 4m 38s\n",
      "152:\tlearn: 0.0030109\ttotal: 50.3s\tremaining: 4m 38s\n",
      "153:\tlearn: 0.0029834\ttotal: 50.6s\tremaining: 4m 37s\n",
      "154:\tlearn: 0.0029673\ttotal: 50.9s\tremaining: 4m 37s\n",
      "155:\tlearn: 0.0029375\ttotal: 51.2s\tremaining: 4m 37s\n",
      "156:\tlearn: 0.0029187\ttotal: 51.6s\tremaining: 4m 36s\n",
      "157:\tlearn: 0.0028986\ttotal: 51.9s\tremaining: 4m 36s\n",
      "158:\tlearn: 0.0028781\ttotal: 52.2s\tremaining: 4m 36s\n",
      "159:\tlearn: 0.0028534\ttotal: 52.5s\tremaining: 4m 35s\n",
      "160:\tlearn: 0.0028085\ttotal: 52.9s\tremaining: 4m 35s\n",
      "161:\tlearn: 0.0027914\ttotal: 53.2s\tremaining: 4m 35s\n",
      "162:\tlearn: 0.0027755\ttotal: 53.5s\tremaining: 4m 34s\n",
      "163:\tlearn: 0.0027616\ttotal: 53.9s\tremaining: 4m 34s\n",
      "164:\tlearn: 0.0027468\ttotal: 54.2s\tremaining: 4m 34s\n",
      "165:\tlearn: 0.0027196\ttotal: 54.5s\tremaining: 4m 33s\n",
      "166:\tlearn: 0.0026990\ttotal: 54.8s\tremaining: 4m 33s\n",
      "167:\tlearn: 0.0026848\ttotal: 55.2s\tremaining: 4m 33s\n",
      "168:\tlearn: 0.0026729\ttotal: 55.5s\tremaining: 4m 32s\n",
      "169:\tlearn: 0.0026542\ttotal: 55.8s\tremaining: 4m 32s\n",
      "170:\tlearn: 0.0026403\ttotal: 56.1s\tremaining: 4m 32s\n",
      "171:\tlearn: 0.0026120\ttotal: 56.5s\tremaining: 4m 31s\n",
      "172:\tlearn: 0.0026009\ttotal: 56.8s\tremaining: 4m 31s\n",
      "173:\tlearn: 0.0025895\ttotal: 57.1s\tremaining: 4m 31s\n",
      "174:\tlearn: 0.0025732\ttotal: 57.4s\tremaining: 4m 30s\n",
      "175:\tlearn: 0.0025515\ttotal: 57.8s\tremaining: 4m 30s\n",
      "176:\tlearn: 0.0025404\ttotal: 58.1s\tremaining: 4m 30s\n",
      "177:\tlearn: 0.0025152\ttotal: 58.4s\tremaining: 4m 29s\n",
      "178:\tlearn: 0.0024999\ttotal: 58.7s\tremaining: 4m 29s\n",
      "179:\tlearn: 0.0024860\ttotal: 59.1s\tremaining: 4m 29s\n",
      "180:\tlearn: 0.0024748\ttotal: 59.4s\tremaining: 4m 28s\n",
      "181:\tlearn: 0.0024540\ttotal: 59.7s\tremaining: 4m 28s\n",
      "182:\tlearn: 0.0024307\ttotal: 1m\tremaining: 4m 28s\n",
      "183:\tlearn: 0.0024304\ttotal: 1m\tremaining: 4m 27s\n",
      "184:\tlearn: 0.0024128\ttotal: 1m\tremaining: 4m 27s\n",
      "185:\tlearn: 0.0023921\ttotal: 1m 1s\tremaining: 4m 27s\n",
      "186:\tlearn: 0.0023789\ttotal: 1m 1s\tremaining: 4m 26s\n",
      "187:\tlearn: 0.0023680\ttotal: 1m 1s\tremaining: 4m 26s\n",
      "188:\tlearn: 0.0023522\ttotal: 1m 2s\tremaining: 4m 26s\n",
      "189:\tlearn: 0.0023398\ttotal: 1m 2s\tremaining: 4m 25s\n",
      "190:\tlearn: 0.0023293\ttotal: 1m 2s\tremaining: 4m 25s\n",
      "191:\tlearn: 0.0023134\ttotal: 1m 2s\tremaining: 4m 25s\n",
      "192:\tlearn: 0.0022955\ttotal: 1m 3s\tremaining: 4m 24s\n",
      "193:\tlearn: 0.0022866\ttotal: 1m 3s\tremaining: 4m 24s\n",
      "194:\tlearn: 0.0022753\ttotal: 1m 3s\tremaining: 4m 24s\n",
      "195:\tlearn: 0.0022609\ttotal: 1m 4s\tremaining: 4m 23s\n",
      "196:\tlearn: 0.0022507\ttotal: 1m 4s\tremaining: 4m 23s\n",
      "197:\tlearn: 0.0022317\ttotal: 1m 4s\tremaining: 4m 23s\n",
      "198:\tlearn: 0.0022233\ttotal: 1m 5s\tremaining: 4m 22s\n",
      "199:\tlearn: 0.0022102\ttotal: 1m 5s\tremaining: 4m 22s\n",
      "200:\tlearn: 0.0022061\ttotal: 1m 5s\tremaining: 4m 21s\n",
      "201:\tlearn: 0.0021974\ttotal: 1m 6s\tremaining: 4m 21s\n",
      "202:\tlearn: 0.0021847\ttotal: 1m 6s\tremaining: 4m 21s\n",
      "203:\tlearn: 0.0021774\ttotal: 1m 6s\tremaining: 4m 20s\n",
      "204:\tlearn: 0.0021646\ttotal: 1m 7s\tremaining: 4m 20s\n",
      "205:\tlearn: 0.0021531\ttotal: 1m 7s\tremaining: 4m 20s\n",
      "206:\tlearn: 0.0021459\ttotal: 1m 7s\tremaining: 4m 19s\n",
      "207:\tlearn: 0.0021283\ttotal: 1m 8s\tremaining: 4m 19s\n",
      "208:\tlearn: 0.0021187\ttotal: 1m 8s\tremaining: 4m 19s\n",
      "209:\tlearn: 0.0020969\ttotal: 1m 8s\tremaining: 4m 18s\n",
      "210:\tlearn: 0.0020893\ttotal: 1m 9s\tremaining: 4m 18s\n",
      "211:\tlearn: 0.0020797\ttotal: 1m 9s\tremaining: 4m 18s\n",
      "212:\tlearn: 0.0020673\ttotal: 1m 9s\tremaining: 4m 17s\n",
      "213:\tlearn: 0.0020564\ttotal: 1m 10s\tremaining: 4m 17s\n",
      "214:\tlearn: 0.0020437\ttotal: 1m 10s\tremaining: 4m 17s\n",
      "215:\tlearn: 0.0020347\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "216:\tlearn: 0.0020241\ttotal: 1m 11s\tremaining: 4m 16s\n",
      "217:\tlearn: 0.0020123\ttotal: 1m 11s\tremaining: 4m 16s\n",
      "218:\tlearn: 0.0019892\ttotal: 1m 11s\tremaining: 4m 16s\n",
      "219:\tlearn: 0.0019784\ttotal: 1m 12s\tremaining: 4m 15s\n",
      "220:\tlearn: 0.0019682\ttotal: 1m 12s\tremaining: 4m 15s\n",
      "221:\tlearn: 0.0019580\ttotal: 1m 12s\tremaining: 4m 15s\n",
      "222:\tlearn: 0.0019579\ttotal: 1m 13s\tremaining: 4m 14s\n",
      "223:\tlearn: 0.0019487\ttotal: 1m 13s\tremaining: 4m 14s\n",
      "224:\tlearn: 0.0019321\ttotal: 1m 13s\tremaining: 4m 14s\n",
      "225:\tlearn: 0.0019174\ttotal: 1m 14s\tremaining: 4m 13s\n",
      "226:\tlearn: 0.0019102\ttotal: 1m 14s\tremaining: 4m 13s\n",
      "227:\tlearn: 0.0018992\ttotal: 1m 14s\tremaining: 4m 13s\n",
      "228:\tlearn: 0.0018884\ttotal: 1m 15s\tremaining: 4m 12s\n",
      "229:\tlearn: 0.0018818\ttotal: 1m 15s\tremaining: 4m 12s\n",
      "230:\tlearn: 0.0018676\ttotal: 1m 15s\tremaining: 4m 12s\n",
      "231:\tlearn: 0.0018650\ttotal: 1m 16s\tremaining: 4m 11s\n",
      "232:\tlearn: 0.0018650\ttotal: 1m 16s\tremaining: 4m 11s\n",
      "233:\tlearn: 0.0018592\ttotal: 1m 16s\tremaining: 4m 10s\n",
      "234:\tlearn: 0.0018491\ttotal: 1m 17s\tremaining: 4m 10s\n",
      "235:\tlearn: 0.0018491\ttotal: 1m 17s\tremaining: 4m 10s\n",
      "236:\tlearn: 0.0018365\ttotal: 1m 17s\tremaining: 4m 10s\n",
      "237:\tlearn: 0.0018301\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "238:\tlearn: 0.0018232\ttotal: 1m 18s\tremaining: 4m 9s\n",
      "239:\tlearn: 0.0018174\ttotal: 1m 18s\tremaining: 4m 9s\n",
      "240:\tlearn: 0.0018069\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "241:\tlearn: 0.0017947\ttotal: 1m 19s\tremaining: 4m 8s\n",
      "242:\tlearn: 0.0017881\ttotal: 1m 19s\tremaining: 4m 8s\n",
      "243:\tlearn: 0.0017827\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "244:\tlearn: 0.0017770\ttotal: 1m 20s\tremaining: 4m 7s\n",
      "245:\tlearn: 0.0017635\ttotal: 1m 20s\tremaining: 4m 7s\n",
      "246:\tlearn: 0.0017573\ttotal: 1m 20s\tremaining: 4m 6s\n",
      "247:\tlearn: 0.0017496\ttotal: 1m 21s\tremaining: 4m 6s\n",
      "248:\tlearn: 0.0017364\ttotal: 1m 21s\tremaining: 4m 6s\n",
      "249:\tlearn: 0.0017237\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "250:\tlearn: 0.0017199\ttotal: 1m 22s\tremaining: 4m 5s\n",
      "251:\tlearn: 0.0017120\ttotal: 1m 22s\tremaining: 4m 5s\n",
      "252:\tlearn: 0.0017090\ttotal: 1m 22s\tremaining: 4m 4s\n",
      "253:\tlearn: 0.0016888\ttotal: 1m 23s\tremaining: 4m 4s\n",
      "254:\tlearn: 0.0016888\ttotal: 1m 23s\tremaining: 4m 4s\n",
      "255:\tlearn: 0.0016819\ttotal: 1m 23s\tremaining: 4m 3s\n",
      "256:\tlearn: 0.0016765\ttotal: 1m 24s\tremaining: 4m 3s\n",
      "257:\tlearn: 0.0016765\ttotal: 1m 24s\tremaining: 4m 3s\n",
      "258:\tlearn: 0.0016690\ttotal: 1m 24s\tremaining: 4m 2s\n",
      "259:\tlearn: 0.0016690\ttotal: 1m 25s\tremaining: 4m 2s\n",
      "260:\tlearn: 0.0016627\ttotal: 1m 25s\tremaining: 4m 2s\n",
      "261:\tlearn: 0.0016527\ttotal: 1m 25s\tremaining: 4m 1s\n",
      "262:\tlearn: 0.0016526\ttotal: 1m 26s\tremaining: 4m 1s\n",
      "263:\tlearn: 0.0016525\ttotal: 1m 26s\tremaining: 4m 1s\n",
      "264:\tlearn: 0.0016456\ttotal: 1m 26s\tremaining: 4m\n",
      "265:\tlearn: 0.0016455\ttotal: 1m 27s\tremaining: 4m\n",
      "266:\tlearn: 0.0016339\ttotal: 1m 27s\tremaining: 4m\n",
      "267:\tlearn: 0.0016339\ttotal: 1m 27s\tremaining: 3m 59s\n",
      "268:\tlearn: 0.0016335\ttotal: 1m 28s\tremaining: 3m 59s\n",
      "269:\tlearn: 0.0016333\ttotal: 1m 28s\tremaining: 3m 59s\n",
      "270:\tlearn: 0.0016288\ttotal: 1m 28s\tremaining: 3m 58s\n",
      "271:\tlearn: 0.0016258\ttotal: 1m 29s\tremaining: 3m 58s\n",
      "272:\tlearn: 0.0016183\ttotal: 1m 29s\tremaining: 3m 58s\n",
      "273:\tlearn: 0.0016140\ttotal: 1m 29s\tremaining: 3m 57s\n",
      "274:\tlearn: 0.0016055\ttotal: 1m 30s\tremaining: 3m 57s\n",
      "275:\tlearn: 0.0016025\ttotal: 1m 30s\tremaining: 3m 57s\n",
      "276:\tlearn: 0.0015964\ttotal: 1m 30s\tremaining: 3m 56s\n",
      "277:\tlearn: 0.0015964\ttotal: 1m 31s\tremaining: 3m 56s\n",
      "278:\tlearn: 0.0015964\ttotal: 1m 31s\tremaining: 3m 56s\n",
      "279:\tlearn: 0.0015903\ttotal: 1m 31s\tremaining: 3m 55s\n",
      "280:\tlearn: 0.0015832\ttotal: 1m 32s\tremaining: 3m 55s\n",
      "281:\tlearn: 0.0015832\ttotal: 1m 32s\tremaining: 3m 55s\n",
      "282:\tlearn: 0.0015765\ttotal: 1m 32s\tremaining: 3m 54s\n",
      "283:\tlearn: 0.0015763\ttotal: 1m 33s\tremaining: 3m 54s\n",
      "284:\tlearn: 0.0015762\ttotal: 1m 33s\tremaining: 3m 54s\n",
      "285:\tlearn: 0.0015762\ttotal: 1m 33s\tremaining: 3m 53s\n",
      "286:\tlearn: 0.0015694\ttotal: 1m 34s\tremaining: 3m 53s\n",
      "287:\tlearn: 0.0015694\ttotal: 1m 34s\tremaining: 3m 53s\n",
      "288:\tlearn: 0.0015620\ttotal: 1m 34s\tremaining: 3m 52s\n",
      "289:\tlearn: 0.0015576\ttotal: 1m 34s\tremaining: 3m 52s\n",
      "290:\tlearn: 0.0015511\ttotal: 1m 35s\tremaining: 3m 52s\n",
      "291:\tlearn: 0.0015507\ttotal: 1m 35s\tremaining: 3m 51s\n",
      "292:\tlearn: 0.0015506\ttotal: 1m 35s\tremaining: 3m 51s\n",
      "293:\tlearn: 0.0015455\ttotal: 1m 36s\tremaining: 3m 51s\n",
      "294:\tlearn: 0.0015410\ttotal: 1m 36s\tremaining: 3m 50s\n",
      "295:\tlearn: 0.0015340\ttotal: 1m 36s\tremaining: 3m 50s\n",
      "296:\tlearn: 0.0015339\ttotal: 1m 37s\tremaining: 3m 50s\n",
      "297:\tlearn: 0.0015337\ttotal: 1m 37s\tremaining: 3m 49s\n",
      "298:\tlearn: 0.0015336\ttotal: 1m 37s\tremaining: 3m 49s\n",
      "299:\tlearn: 0.0015231\ttotal: 1m 38s\tremaining: 3m 49s\n",
      "300:\tlearn: 0.0015178\ttotal: 1m 38s\tremaining: 3m 48s\n",
      "301:\tlearn: 0.0015125\ttotal: 1m 38s\tremaining: 3m 48s\n",
      "302:\tlearn: 0.0015123\ttotal: 1m 39s\tremaining: 3m 48s\n",
      "303:\tlearn: 0.0015123\ttotal: 1m 39s\tremaining: 3m 47s\n",
      "304:\tlearn: 0.0015079\ttotal: 1m 39s\tremaining: 3m 47s\n",
      "305:\tlearn: 0.0015079\ttotal: 1m 40s\tremaining: 3m 47s\n",
      "306:\tlearn: 0.0015021\ttotal: 1m 40s\tremaining: 3m 46s\n",
      "307:\tlearn: 0.0014928\ttotal: 1m 40s\tremaining: 3m 46s\n",
      "308:\tlearn: 0.0014806\ttotal: 1m 41s\tremaining: 3m 46s\n",
      "309:\tlearn: 0.0014806\ttotal: 1m 41s\tremaining: 3m 45s\n",
      "310:\tlearn: 0.0014804\ttotal: 1m 41s\tremaining: 3m 45s\n",
      "311:\tlearn: 0.0014713\ttotal: 1m 42s\tremaining: 3m 45s\n",
      "312:\tlearn: 0.0014712\ttotal: 1m 42s\tremaining: 3m 44s\n",
      "313:\tlearn: 0.0014659\ttotal: 1m 42s\tremaining: 3m 44s\n",
      "314:\tlearn: 0.0014621\ttotal: 1m 43s\tremaining: 3m 44s\n",
      "315:\tlearn: 0.0014550\ttotal: 1m 43s\tremaining: 3m 44s\n",
      "316:\tlearn: 0.0014502\ttotal: 1m 43s\tremaining: 3m 43s\n",
      "317:\tlearn: 0.0014466\ttotal: 1m 44s\tremaining: 3m 43s\n",
      "318:\tlearn: 0.0014395\ttotal: 1m 44s\tremaining: 3m 43s\n",
      "319:\tlearn: 0.0014394\ttotal: 1m 44s\tremaining: 3m 42s\n",
      "320:\tlearn: 0.0014350\ttotal: 1m 45s\tremaining: 3m 42s\n",
      "321:\tlearn: 0.0014270\ttotal: 1m 45s\tremaining: 3m 42s\n",
      "322:\tlearn: 0.0014241\ttotal: 1m 45s\tremaining: 3m 41s\n",
      "323:\tlearn: 0.0014191\ttotal: 1m 46s\tremaining: 3m 41s\n",
      "324:\tlearn: 0.0014190\ttotal: 1m 46s\tremaining: 3m 41s\n",
      "325:\tlearn: 0.0014190\ttotal: 1m 46s\tremaining: 3m 40s\n",
      "326:\tlearn: 0.0014120\ttotal: 1m 47s\tremaining: 3m 40s\n",
      "327:\tlearn: 0.0014088\ttotal: 1m 47s\tremaining: 3m 40s\n",
      "328:\tlearn: 0.0014088\ttotal: 1m 47s\tremaining: 3m 39s\n",
      "329:\tlearn: 0.0014086\ttotal: 1m 48s\tremaining: 3m 39s\n",
      "330:\tlearn: 0.0014085\ttotal: 1m 48s\tremaining: 3m 39s\n",
      "331:\tlearn: 0.0014067\ttotal: 1m 48s\tremaining: 3m 38s\n",
      "332:\tlearn: 0.0014029\ttotal: 1m 49s\tremaining: 3m 38s\n",
      "333:\tlearn: 0.0014029\ttotal: 1m 49s\tremaining: 3m 38s\n",
      "334:\tlearn: 0.0014029\ttotal: 1m 49s\tremaining: 3m 37s\n",
      "335:\tlearn: 0.0013971\ttotal: 1m 49s\tremaining: 3m 37s\n",
      "336:\tlearn: 0.0013941\ttotal: 1m 50s\tremaining: 3m 37s\n",
      "337:\tlearn: 0.0013941\ttotal: 1m 50s\tremaining: 3m 36s\n",
      "338:\tlearn: 0.0013941\ttotal: 1m 50s\tremaining: 3m 36s\n",
      "339:\tlearn: 0.0013941\ttotal: 1m 51s\tremaining: 3m 36s\n",
      "340:\tlearn: 0.0013900\ttotal: 1m 51s\tremaining: 3m 35s\n",
      "341:\tlearn: 0.0013900\ttotal: 1m 51s\tremaining: 3m 35s\n",
      "342:\tlearn: 0.0013900\ttotal: 1m 52s\tremaining: 3m 35s\n",
      "343:\tlearn: 0.0013899\ttotal: 1m 52s\tremaining: 3m 34s\n",
      "344:\tlearn: 0.0013864\ttotal: 1m 52s\tremaining: 3m 34s\n",
      "345:\tlearn: 0.0013833\ttotal: 1m 53s\tremaining: 3m 34s\n",
      "346:\tlearn: 0.0013794\ttotal: 1m 53s\tremaining: 3m 33s\n",
      "347:\tlearn: 0.0013794\ttotal: 1m 53s\tremaining: 3m 33s\n",
      "348:\tlearn: 0.0013793\ttotal: 1m 54s\tremaining: 3m 33s\n",
      "349:\tlearn: 0.0013793\ttotal: 1m 54s\tremaining: 3m 32s\n",
      "350:\tlearn: 0.0013793\ttotal: 1m 54s\tremaining: 3m 32s\n",
      "351:\tlearn: 0.0013792\ttotal: 1m 55s\tremaining: 3m 32s\n",
      "352:\tlearn: 0.0013736\ttotal: 1m 55s\tremaining: 3m 31s\n",
      "353:\tlearn: 0.0013735\ttotal: 1m 55s\tremaining: 3m 31s\n",
      "354:\tlearn: 0.0013735\ttotal: 1m 56s\tremaining: 3m 31s\n",
      "355:\tlearn: 0.0013689\ttotal: 1m 56s\tremaining: 3m 30s\n",
      "356:\tlearn: 0.0013689\ttotal: 1m 56s\tremaining: 3m 30s\n",
      "357:\tlearn: 0.0013689\ttotal: 1m 57s\tremaining: 3m 30s\n",
      "358:\tlearn: 0.0013687\ttotal: 1m 57s\tremaining: 3m 29s\n",
      "359:\tlearn: 0.0013664\ttotal: 1m 57s\tremaining: 3m 29s\n",
      "360:\tlearn: 0.0013663\ttotal: 1m 58s\tremaining: 3m 29s\n",
      "361:\tlearn: 0.0013663\ttotal: 1m 58s\tremaining: 3m 28s\n",
      "362:\tlearn: 0.0013636\ttotal: 1m 58s\tremaining: 3m 28s\n",
      "363:\tlearn: 0.0013586\ttotal: 1m 59s\tremaining: 3m 28s\n",
      "364:\tlearn: 0.0013585\ttotal: 1m 59s\tremaining: 3m 27s\n",
      "365:\tlearn: 0.0013585\ttotal: 1m 59s\tremaining: 3m 27s\n",
      "366:\tlearn: 0.0013566\ttotal: 2m\tremaining: 3m 27s\n",
      "367:\tlearn: 0.0013564\ttotal: 2m\tremaining: 3m 26s\n",
      "368:\tlearn: 0.0013564\ttotal: 2m\tremaining: 3m 26s\n",
      "369:\tlearn: 0.0013564\ttotal: 2m 1s\tremaining: 3m 26s\n",
      "370:\tlearn: 0.0013564\ttotal: 2m 1s\tremaining: 3m 25s\n",
      "371:\tlearn: 0.0013564\ttotal: 2m 1s\tremaining: 3m 25s\n",
      "372:\tlearn: 0.0013563\ttotal: 2m 2s\tremaining: 3m 25s\n",
      "373:\tlearn: 0.0013562\ttotal: 2m 2s\tremaining: 3m 24s\n",
      "374:\tlearn: 0.0013523\ttotal: 2m 2s\tremaining: 3m 24s\n",
      "375:\tlearn: 0.0013523\ttotal: 2m 2s\tremaining: 3m 24s\n",
      "376:\tlearn: 0.0013523\ttotal: 2m 3s\tremaining: 3m 23s\n",
      "377:\tlearn: 0.0013490\ttotal: 2m 3s\tremaining: 3m 23s\n",
      "378:\tlearn: 0.0013489\ttotal: 2m 4s\tremaining: 3m 23s\n",
      "379:\tlearn: 0.0013488\ttotal: 2m 4s\tremaining: 3m 22s\n",
      "380:\tlearn: 0.0013486\ttotal: 2m 4s\tremaining: 3m 22s\n",
      "381:\tlearn: 0.0013486\ttotal: 2m 5s\tremaining: 3m 22s\n",
      "382:\tlearn: 0.0013485\ttotal: 2m 5s\tremaining: 3m 21s\n",
      "383:\tlearn: 0.0013484\ttotal: 2m 5s\tremaining: 3m 21s\n",
      "384:\tlearn: 0.0013370\ttotal: 2m 5s\tremaining: 3m 21s\n",
      "385:\tlearn: 0.0013369\ttotal: 2m 6s\tremaining: 3m 20s\n",
      "386:\tlearn: 0.0013315\ttotal: 2m 6s\tremaining: 3m 20s\n",
      "387:\tlearn: 0.0013315\ttotal: 2m 6s\tremaining: 3m 20s\n",
      "388:\tlearn: 0.0013314\ttotal: 2m 7s\tremaining: 3m 19s\n",
      "389:\tlearn: 0.0013286\ttotal: 2m 7s\tremaining: 3m 19s\n",
      "390:\tlearn: 0.0013246\ttotal: 2m 7s\tremaining: 3m 19s\n",
      "391:\tlearn: 0.0013212\ttotal: 2m 8s\tremaining: 3m 18s\n",
      "392:\tlearn: 0.0013210\ttotal: 2m 8s\tremaining: 3m 18s\n",
      "393:\tlearn: 0.0013210\ttotal: 2m 8s\tremaining: 3m 18s\n",
      "394:\tlearn: 0.0013189\ttotal: 2m 9s\tremaining: 3m 17s\n",
      "395:\tlearn: 0.0013189\ttotal: 2m 9s\tremaining: 3m 17s\n",
      "396:\tlearn: 0.0013189\ttotal: 2m 9s\tremaining: 3m 17s\n",
      "397:\tlearn: 0.0013153\ttotal: 2m 10s\tremaining: 3m 17s\n",
      "398:\tlearn: 0.0013107\ttotal: 2m 10s\tremaining: 3m 16s\n",
      "399:\tlearn: 0.0013105\ttotal: 2m 10s\tremaining: 3m 16s\n",
      "400:\tlearn: 0.0013104\ttotal: 2m 11s\tremaining: 3m 16s\n",
      "401:\tlearn: 0.0013104\ttotal: 2m 11s\tremaining: 3m 15s\n",
      "402:\tlearn: 0.0013053\ttotal: 2m 11s\tremaining: 3m 15s\n",
      "403:\tlearn: 0.0013053\ttotal: 2m 12s\tremaining: 3m 15s\n",
      "404:\tlearn: 0.0013052\ttotal: 2m 12s\tremaining: 3m 14s\n",
      "405:\tlearn: 0.0013052\ttotal: 2m 12s\tremaining: 3m 14s\n",
      "406:\tlearn: 0.0013052\ttotal: 2m 13s\tremaining: 3m 14s\n",
      "407:\tlearn: 0.0013052\ttotal: 2m 13s\tremaining: 3m 13s\n",
      "408:\tlearn: 0.0013052\ttotal: 2m 13s\tremaining: 3m 13s\n",
      "409:\tlearn: 0.0013010\ttotal: 2m 14s\tremaining: 3m 13s\n",
      "410:\tlearn: 0.0013010\ttotal: 2m 14s\tremaining: 3m 12s\n",
      "411:\tlearn: 0.0013009\ttotal: 2m 14s\tremaining: 3m 12s\n",
      "412:\tlearn: 0.0013009\ttotal: 2m 15s\tremaining: 3m 12s\n",
      "413:\tlearn: 0.0012986\ttotal: 2m 15s\tremaining: 3m 11s\n",
      "414:\tlearn: 0.0012963\ttotal: 2m 15s\tremaining: 3m 11s\n",
      "415:\tlearn: 0.0012962\ttotal: 2m 16s\tremaining: 3m 11s\n",
      "416:\tlearn: 0.0012902\ttotal: 2m 16s\tremaining: 3m 10s\n",
      "417:\tlearn: 0.0012902\ttotal: 2m 16s\tremaining: 3m 10s\n",
      "418:\tlearn: 0.0012902\ttotal: 2m 17s\tremaining: 3m 10s\n",
      "419:\tlearn: 0.0012902\ttotal: 2m 17s\tremaining: 3m 9s\n",
      "420:\tlearn: 0.0012902\ttotal: 2m 17s\tremaining: 3m 9s\n",
      "421:\tlearn: 0.0012900\ttotal: 2m 18s\tremaining: 3m 9s\n",
      "422:\tlearn: 0.0012882\ttotal: 2m 18s\tremaining: 3m 8s\n",
      "423:\tlearn: 0.0012864\ttotal: 2m 18s\tremaining: 3m 8s\n",
      "424:\tlearn: 0.0012863\ttotal: 2m 19s\tremaining: 3m 8s\n",
      "425:\tlearn: 0.0012863\ttotal: 2m 19s\tremaining: 3m 7s\n",
      "426:\tlearn: 0.0012830\ttotal: 2m 19s\tremaining: 3m 7s\n",
      "427:\tlearn: 0.0012732\ttotal: 2m 20s\tremaining: 3m 7s\n",
      "428:\tlearn: 0.0012729\ttotal: 2m 20s\tremaining: 3m 6s\n",
      "429:\tlearn: 0.0012729\ttotal: 2m 20s\tremaining: 3m 6s\n",
      "430:\tlearn: 0.0012698\ttotal: 2m 20s\tremaining: 3m 6s\n",
      "431:\tlearn: 0.0012693\ttotal: 2m 21s\tremaining: 3m 5s\n",
      "432:\tlearn: 0.0012693\ttotal: 2m 21s\tremaining: 3m 5s\n",
      "433:\tlearn: 0.0012693\ttotal: 2m 21s\tremaining: 3m 5s\n",
      "434:\tlearn: 0.0012692\ttotal: 2m 22s\tremaining: 3m 4s\n",
      "435:\tlearn: 0.0012692\ttotal: 2m 22s\tremaining: 3m 4s\n",
      "436:\tlearn: 0.0012689\ttotal: 2m 22s\tremaining: 3m 4s\n",
      "437:\tlearn: 0.0012688\ttotal: 2m 23s\tremaining: 3m 3s\n",
      "438:\tlearn: 0.0012637\ttotal: 2m 23s\tremaining: 3m 3s\n",
      "439:\tlearn: 0.0012603\ttotal: 2m 23s\tremaining: 3m 3s\n",
      "440:\tlearn: 0.0012571\ttotal: 2m 24s\tremaining: 3m 2s\n",
      "441:\tlearn: 0.0012571\ttotal: 2m 24s\tremaining: 3m 2s\n",
      "442:\tlearn: 0.0012570\ttotal: 2m 24s\tremaining: 3m 2s\n",
      "443:\tlearn: 0.0012570\ttotal: 2m 25s\tremaining: 3m 1s\n",
      "444:\tlearn: 0.0012563\ttotal: 2m 25s\tremaining: 3m 1s\n",
      "445:\tlearn: 0.0012540\ttotal: 2m 25s\tremaining: 3m 1s\n",
      "446:\tlearn: 0.0012540\ttotal: 2m 26s\tremaining: 3m\n",
      "447:\tlearn: 0.0012539\ttotal: 2m 26s\tremaining: 3m\n",
      "448:\tlearn: 0.0012538\ttotal: 2m 26s\tremaining: 3m\n",
      "449:\tlearn: 0.0012538\ttotal: 2m 27s\tremaining: 2m 59s\n",
      "450:\tlearn: 0.0012538\ttotal: 2m 27s\tremaining: 2m 59s\n",
      "451:\tlearn: 0.0012538\ttotal: 2m 27s\tremaining: 2m 59s\n",
      "452:\tlearn: 0.0012538\ttotal: 2m 28s\tremaining: 2m 58s\n",
      "453:\tlearn: 0.0012494\ttotal: 2m 28s\tremaining: 2m 58s\n",
      "454:\tlearn: 0.0012464\ttotal: 2m 28s\tremaining: 2m 58s\n",
      "455:\tlearn: 0.0012463\ttotal: 2m 29s\tremaining: 2m 57s\n",
      "456:\tlearn: 0.0012463\ttotal: 2m 29s\tremaining: 2m 57s\n",
      "457:\tlearn: 0.0012462\ttotal: 2m 29s\tremaining: 2m 57s\n",
      "458:\tlearn: 0.0012421\ttotal: 2m 30s\tremaining: 2m 56s\n",
      "459:\tlearn: 0.0012383\ttotal: 2m 30s\tremaining: 2m 56s\n",
      "460:\tlearn: 0.0012346\ttotal: 2m 30s\tremaining: 2m 56s\n",
      "461:\tlearn: 0.0012323\ttotal: 2m 31s\tremaining: 2m 55s\n",
      "462:\tlearn: 0.0012313\ttotal: 2m 31s\tremaining: 2m 55s\n",
      "463:\tlearn: 0.0012282\ttotal: 2m 31s\tremaining: 2m 55s\n",
      "464:\tlearn: 0.0012255\ttotal: 2m 32s\tremaining: 2m 55s\n",
      "465:\tlearn: 0.0012255\ttotal: 2m 32s\tremaining: 2m 54s\n",
      "466:\tlearn: 0.0012237\ttotal: 2m 32s\tremaining: 2m 54s\n",
      "467:\tlearn: 0.0012195\ttotal: 2m 33s\tremaining: 2m 54s\n",
      "468:\tlearn: 0.0012172\ttotal: 2m 33s\tremaining: 2m 53s\n",
      "469:\tlearn: 0.0012172\ttotal: 2m 33s\tremaining: 2m 53s\n",
      "470:\tlearn: 0.0012171\ttotal: 2m 34s\tremaining: 2m 53s\n",
      "471:\tlearn: 0.0012123\ttotal: 2m 34s\tremaining: 2m 52s\n",
      "472:\tlearn: 0.0012123\ttotal: 2m 34s\tremaining: 2m 52s\n",
      "473:\tlearn: 0.0012123\ttotal: 2m 35s\tremaining: 2m 52s\n",
      "474:\tlearn: 0.0012123\ttotal: 2m 35s\tremaining: 2m 51s\n",
      "475:\tlearn: 0.0012123\ttotal: 2m 35s\tremaining: 2m 51s\n",
      "476:\tlearn: 0.0012119\ttotal: 2m 36s\tremaining: 2m 51s\n",
      "477:\tlearn: 0.0012118\ttotal: 2m 36s\tremaining: 2m 50s\n",
      "478:\tlearn: 0.0012096\ttotal: 2m 36s\tremaining: 2m 50s\n",
      "479:\tlearn: 0.0012080\ttotal: 2m 37s\tremaining: 2m 50s\n",
      "480:\tlearn: 0.0012057\ttotal: 2m 37s\tremaining: 2m 49s\n",
      "481:\tlearn: 0.0012057\ttotal: 2m 37s\tremaining: 2m 49s\n",
      "482:\tlearn: 0.0012057\ttotal: 2m 37s\tremaining: 2m 49s\n",
      "483:\tlearn: 0.0012057\ttotal: 2m 38s\tremaining: 2m 48s\n",
      "484:\tlearn: 0.0012056\ttotal: 2m 38s\tremaining: 2m 48s\n",
      "485:\tlearn: 0.0012018\ttotal: 2m 38s\tremaining: 2m 48s\n",
      "486:\tlearn: 0.0012018\ttotal: 2m 39s\tremaining: 2m 47s\n",
      "487:\tlearn: 0.0011971\ttotal: 2m 39s\tremaining: 2m 47s\n",
      "488:\tlearn: 0.0011971\ttotal: 2m 39s\tremaining: 2m 47s\n",
      "489:\tlearn: 0.0011970\ttotal: 2m 40s\tremaining: 2m 46s\n",
      "490:\tlearn: 0.0011970\ttotal: 2m 40s\tremaining: 2m 46s\n",
      "491:\tlearn: 0.0011970\ttotal: 2m 40s\tremaining: 2m 46s\n",
      "492:\tlearn: 0.0011970\ttotal: 2m 41s\tremaining: 2m 45s\n",
      "493:\tlearn: 0.0011939\ttotal: 2m 41s\tremaining: 2m 45s\n",
      "494:\tlearn: 0.0011926\ttotal: 2m 41s\tremaining: 2m 45s\n",
      "495:\tlearn: 0.0011882\ttotal: 2m 42s\tremaining: 2m 44s\n",
      "496:\tlearn: 0.0011882\ttotal: 2m 42s\tremaining: 2m 44s\n",
      "497:\tlearn: 0.0011882\ttotal: 2m 42s\tremaining: 2m 44s\n",
      "498:\tlearn: 0.0011882\ttotal: 2m 43s\tremaining: 2m 43s\n",
      "499:\tlearn: 0.0011882\ttotal: 2m 43s\tremaining: 2m 43s\n",
      "500:\tlearn: 0.0011882\ttotal: 2m 43s\tremaining: 2m 43s\n",
      "501:\tlearn: 0.0011882\ttotal: 2m 44s\tremaining: 2m 42s\n",
      "502:\tlearn: 0.0011882\ttotal: 2m 44s\tremaining: 2m 42s\n",
      "503:\tlearn: 0.0011882\ttotal: 2m 44s\tremaining: 2m 42s\n",
      "504:\tlearn: 0.0011882\ttotal: 2m 45s\tremaining: 2m 41s\n",
      "505:\tlearn: 0.0011882\ttotal: 2m 45s\tremaining: 2m 41s\n",
      "506:\tlearn: 0.0011882\ttotal: 2m 45s\tremaining: 2m 41s\n",
      "507:\tlearn: 0.0011868\ttotal: 2m 46s\tremaining: 2m 40s\n",
      "508:\tlearn: 0.0011844\ttotal: 2m 46s\tremaining: 2m 40s\n",
      "509:\tlearn: 0.0011822\ttotal: 2m 46s\tremaining: 2m 40s\n",
      "510:\tlearn: 0.0011776\ttotal: 2m 47s\tremaining: 2m 39s\n",
      "511:\tlearn: 0.0011773\ttotal: 2m 47s\tremaining: 2m 39s\n",
      "512:\tlearn: 0.0011751\ttotal: 2m 47s\tremaining: 2m 39s\n",
      "513:\tlearn: 0.0011719\ttotal: 2m 48s\tremaining: 2m 38s\n",
      "514:\tlearn: 0.0011719\ttotal: 2m 48s\tremaining: 2m 38s\n",
      "515:\tlearn: 0.0011697\ttotal: 2m 48s\tremaining: 2m 38s\n",
      "516:\tlearn: 0.0011697\ttotal: 2m 49s\tremaining: 2m 37s\n",
      "517:\tlearn: 0.0011697\ttotal: 2m 49s\tremaining: 2m 37s\n",
      "518:\tlearn: 0.0011697\ttotal: 2m 49s\tremaining: 2m 37s\n",
      "519:\tlearn: 0.0011664\ttotal: 2m 50s\tremaining: 2m 37s\n",
      "520:\tlearn: 0.0011664\ttotal: 2m 50s\tremaining: 2m 36s\n",
      "521:\tlearn: 0.0011664\ttotal: 2m 50s\tremaining: 2m 36s\n",
      "522:\tlearn: 0.0011664\ttotal: 2m 51s\tremaining: 2m 36s\n",
      "523:\tlearn: 0.0011664\ttotal: 2m 51s\tremaining: 2m 35s\n",
      "524:\tlearn: 0.0011664\ttotal: 2m 51s\tremaining: 2m 35s\n",
      "525:\tlearn: 0.0011663\ttotal: 2m 52s\tremaining: 2m 35s\n",
      "526:\tlearn: 0.0011663\ttotal: 2m 52s\tremaining: 2m 34s\n",
      "527:\tlearn: 0.0011663\ttotal: 2m 52s\tremaining: 2m 34s\n",
      "528:\tlearn: 0.0011663\ttotal: 2m 53s\tremaining: 2m 34s\n",
      "529:\tlearn: 0.0011662\ttotal: 2m 53s\tremaining: 2m 33s\n",
      "530:\tlearn: 0.0011662\ttotal: 2m 53s\tremaining: 2m 33s\n",
      "531:\tlearn: 0.0011655\ttotal: 2m 53s\tremaining: 2m 33s\n",
      "532:\tlearn: 0.0011654\ttotal: 2m 54s\tremaining: 2m 32s\n",
      "533:\tlearn: 0.0011653\ttotal: 2m 54s\tremaining: 2m 32s\n",
      "534:\tlearn: 0.0011651\ttotal: 2m 54s\tremaining: 2m 32s\n",
      "535:\tlearn: 0.0011626\ttotal: 2m 55s\tremaining: 2m 31s\n",
      "536:\tlearn: 0.0011624\ttotal: 2m 55s\tremaining: 2m 31s\n",
      "537:\tlearn: 0.0011624\ttotal: 2m 55s\tremaining: 2m 31s\n",
      "538:\tlearn: 0.0011590\ttotal: 2m 56s\tremaining: 2m 30s\n",
      "539:\tlearn: 0.0011573\ttotal: 2m 56s\tremaining: 2m 30s\n",
      "540:\tlearn: 0.0011571\ttotal: 2m 56s\tremaining: 2m 30s\n",
      "541:\tlearn: 0.0011570\ttotal: 2m 57s\tremaining: 2m 29s\n",
      "542:\tlearn: 0.0011570\ttotal: 2m 57s\tremaining: 2m 29s\n",
      "543:\tlearn: 0.0011533\ttotal: 2m 57s\tremaining: 2m 29s\n",
      "544:\tlearn: 0.0011532\ttotal: 2m 58s\tremaining: 2m 28s\n",
      "545:\tlearn: 0.0011496\ttotal: 2m 58s\tremaining: 2m 28s\n",
      "546:\tlearn: 0.0011496\ttotal: 2m 58s\tremaining: 2m 28s\n",
      "547:\tlearn: 0.0011467\ttotal: 2m 59s\tremaining: 2m 27s\n",
      "548:\tlearn: 0.0011447\ttotal: 2m 59s\tremaining: 2m 27s\n",
      "549:\tlearn: 0.0011445\ttotal: 2m 59s\tremaining: 2m 27s\n",
      "550:\tlearn: 0.0011422\ttotal: 3m\tremaining: 2m 26s\n",
      "551:\tlearn: 0.0011421\ttotal: 3m\tremaining: 2m 26s\n",
      "552:\tlearn: 0.0011402\ttotal: 3m\tremaining: 2m 26s\n",
      "553:\tlearn: 0.0011384\ttotal: 3m 1s\tremaining: 2m 25s\n",
      "554:\tlearn: 0.0011384\ttotal: 3m 1s\tremaining: 2m 25s\n",
      "555:\tlearn: 0.0011384\ttotal: 3m 1s\tremaining: 2m 25s\n",
      "556:\tlearn: 0.0011384\ttotal: 3m 2s\tremaining: 2m 24s\n",
      "557:\tlearn: 0.0011384\ttotal: 3m 2s\tremaining: 2m 24s\n",
      "558:\tlearn: 0.0011384\ttotal: 3m 2s\tremaining: 2m 24s\n",
      "559:\tlearn: 0.0011378\ttotal: 3m 3s\tremaining: 2m 23s\n",
      "560:\tlearn: 0.0011351\ttotal: 3m 3s\tremaining: 2m 23s\n",
      "561:\tlearn: 0.0011326\ttotal: 3m 3s\tremaining: 2m 23s\n",
      "562:\tlearn: 0.0011326\ttotal: 3m 4s\tremaining: 2m 22s\n",
      "563:\tlearn: 0.0011326\ttotal: 3m 4s\tremaining: 2m 22s\n",
      "564:\tlearn: 0.0011326\ttotal: 3m 4s\tremaining: 2m 22s\n",
      "565:\tlearn: 0.0011326\ttotal: 3m 5s\tremaining: 2m 21s\n",
      "566:\tlearn: 0.0011317\ttotal: 3m 5s\tremaining: 2m 21s\n",
      "567:\tlearn: 0.0011316\ttotal: 3m 5s\tremaining: 2m 21s\n",
      "568:\tlearn: 0.0011307\ttotal: 3m 6s\tremaining: 2m 20s\n",
      "569:\tlearn: 0.0011277\ttotal: 3m 6s\tremaining: 2m 20s\n",
      "570:\tlearn: 0.0011277\ttotal: 3m 6s\tremaining: 2m 20s\n",
      "571:\tlearn: 0.0011276\ttotal: 3m 7s\tremaining: 2m 19s\n",
      "572:\tlearn: 0.0011276\ttotal: 3m 7s\tremaining: 2m 19s\n",
      "573:\tlearn: 0.0011276\ttotal: 3m 7s\tremaining: 2m 19s\n",
      "574:\tlearn: 0.0011276\ttotal: 3m 8s\tremaining: 2m 18s\n",
      "575:\tlearn: 0.0011263\ttotal: 3m 8s\tremaining: 2m 18s\n",
      "576:\tlearn: 0.0011260\ttotal: 3m 8s\tremaining: 2m 18s\n",
      "577:\tlearn: 0.0011260\ttotal: 3m 8s\tremaining: 2m 17s\n",
      "578:\tlearn: 0.0011259\ttotal: 3m 9s\tremaining: 2m 17s\n",
      "579:\tlearn: 0.0011259\ttotal: 3m 9s\tremaining: 2m 17s\n",
      "580:\tlearn: 0.0011258\ttotal: 3m 9s\tremaining: 2m 16s\n",
      "581:\tlearn: 0.0011258\ttotal: 3m 10s\tremaining: 2m 16s\n",
      "582:\tlearn: 0.0011234\ttotal: 3m 10s\tremaining: 2m 16s\n",
      "583:\tlearn: 0.0011206\ttotal: 3m 10s\tremaining: 2m 16s\n",
      "584:\tlearn: 0.0011206\ttotal: 3m 11s\tremaining: 2m 15s\n",
      "585:\tlearn: 0.0011206\ttotal: 3m 11s\tremaining: 2m 15s\n",
      "586:\tlearn: 0.0011205\ttotal: 3m 11s\tremaining: 2m 15s\n",
      "587:\tlearn: 0.0011172\ttotal: 3m 12s\tremaining: 2m 14s\n",
      "588:\tlearn: 0.0011131\ttotal: 3m 12s\tremaining: 2m 14s\n",
      "589:\tlearn: 0.0011130\ttotal: 3m 12s\tremaining: 2m 14s\n",
      "590:\tlearn: 0.0011130\ttotal: 3m 13s\tremaining: 2m 13s\n",
      "591:\tlearn: 0.0011130\ttotal: 3m 13s\tremaining: 2m 13s\n",
      "592:\tlearn: 0.0011092\ttotal: 3m 13s\tremaining: 2m 13s\n",
      "593:\tlearn: 0.0011086\ttotal: 3m 14s\tremaining: 2m 12s\n",
      "594:\tlearn: 0.0011086\ttotal: 3m 14s\tremaining: 2m 12s\n",
      "595:\tlearn: 0.0011086\ttotal: 3m 14s\tremaining: 2m 12s\n",
      "596:\tlearn: 0.0011073\ttotal: 3m 15s\tremaining: 2m 11s\n",
      "597:\tlearn: 0.0011073\ttotal: 3m 15s\tremaining: 2m 11s\n",
      "598:\tlearn: 0.0011072\ttotal: 3m 15s\tremaining: 2m 11s\n",
      "599:\tlearn: 0.0011048\ttotal: 3m 16s\tremaining: 2m 10s\n",
      "600:\tlearn: 0.0011006\ttotal: 3m 16s\tremaining: 2m 10s\n",
      "601:\tlearn: 0.0010986\ttotal: 3m 16s\tremaining: 2m 10s\n",
      "602:\tlearn: 0.0010954\ttotal: 3m 17s\tremaining: 2m 9s\n",
      "603:\tlearn: 0.0010954\ttotal: 3m 17s\tremaining: 2m 9s\n",
      "604:\tlearn: 0.0010954\ttotal: 3m 17s\tremaining: 2m 9s\n",
      "605:\tlearn: 0.0010952\ttotal: 3m 18s\tremaining: 2m 8s\n",
      "606:\tlearn: 0.0010952\ttotal: 3m 18s\tremaining: 2m 8s\n",
      "607:\tlearn: 0.0010952\ttotal: 3m 18s\tremaining: 2m 8s\n",
      "608:\tlearn: 0.0010952\ttotal: 3m 19s\tremaining: 2m 7s\n",
      "609:\tlearn: 0.0010951\ttotal: 3m 19s\tremaining: 2m 7s\n",
      "610:\tlearn: 0.0010950\ttotal: 3m 19s\tremaining: 2m 7s\n",
      "611:\tlearn: 0.0010950\ttotal: 3m 20s\tremaining: 2m 6s\n",
      "612:\tlearn: 0.0010950\ttotal: 3m 20s\tremaining: 2m 6s\n",
      "613:\tlearn: 0.0010949\ttotal: 3m 20s\tremaining: 2m 6s\n",
      "614:\tlearn: 0.0010949\ttotal: 3m 21s\tremaining: 2m 5s\n",
      "615:\tlearn: 0.0010948\ttotal: 3m 21s\tremaining: 2m 5s\n",
      "616:\tlearn: 0.0010947\ttotal: 3m 21s\tremaining: 2m 5s\n",
      "617:\tlearn: 0.0010947\ttotal: 3m 21s\tremaining: 2m 4s\n",
      "618:\tlearn: 0.0010915\ttotal: 3m 22s\tremaining: 2m 4s\n",
      "619:\tlearn: 0.0010898\ttotal: 3m 22s\tremaining: 2m 4s\n",
      "620:\tlearn: 0.0010898\ttotal: 3m 22s\tremaining: 2m 3s\n",
      "621:\tlearn: 0.0010867\ttotal: 3m 23s\tremaining: 2m 3s\n",
      "622:\tlearn: 0.0010867\ttotal: 3m 23s\tremaining: 2m 3s\n",
      "623:\tlearn: 0.0010867\ttotal: 3m 23s\tremaining: 2m 2s\n",
      "624:\tlearn: 0.0010867\ttotal: 3m 24s\tremaining: 2m 2s\n",
      "625:\tlearn: 0.0010867\ttotal: 3m 24s\tremaining: 2m 2s\n",
      "626:\tlearn: 0.0010866\ttotal: 3m 24s\tremaining: 2m 1s\n",
      "627:\tlearn: 0.0010866\ttotal: 3m 25s\tremaining: 2m 1s\n",
      "628:\tlearn: 0.0010866\ttotal: 3m 25s\tremaining: 2m 1s\n",
      "629:\tlearn: 0.0010850\ttotal: 3m 25s\tremaining: 2m\n",
      "630:\tlearn: 0.0010849\ttotal: 3m 26s\tremaining: 2m\n",
      "631:\tlearn: 0.0010838\ttotal: 3m 26s\tremaining: 2m\n",
      "632:\tlearn: 0.0010838\ttotal: 3m 26s\tremaining: 1m 59s\n",
      "633:\tlearn: 0.0010821\ttotal: 3m 27s\tremaining: 1m 59s\n",
      "634:\tlearn: 0.0010821\ttotal: 3m 27s\tremaining: 1m 59s\n",
      "635:\tlearn: 0.0010821\ttotal: 3m 27s\tremaining: 1m 58s\n",
      "636:\tlearn: 0.0010820\ttotal: 3m 28s\tremaining: 1m 58s\n",
      "637:\tlearn: 0.0010820\ttotal: 3m 28s\tremaining: 1m 58s\n",
      "638:\tlearn: 0.0010820\ttotal: 3m 28s\tremaining: 1m 57s\n",
      "639:\tlearn: 0.0010817\ttotal: 3m 29s\tremaining: 1m 57s\n",
      "640:\tlearn: 0.0010795\ttotal: 3m 29s\tremaining: 1m 57s\n",
      "641:\tlearn: 0.0010795\ttotal: 3m 29s\tremaining: 1m 57s\n",
      "642:\tlearn: 0.0010795\ttotal: 3m 30s\tremaining: 1m 56s\n",
      "643:\tlearn: 0.0010794\ttotal: 3m 30s\tremaining: 1m 56s\n",
      "644:\tlearn: 0.0010794\ttotal: 3m 30s\tremaining: 1m 56s\n",
      "645:\tlearn: 0.0010794\ttotal: 3m 31s\tremaining: 1m 55s\n",
      "646:\tlearn: 0.0010792\ttotal: 3m 31s\tremaining: 1m 55s\n",
      "647:\tlearn: 0.0010792\ttotal: 3m 31s\tremaining: 1m 55s\n",
      "648:\tlearn: 0.0010792\ttotal: 3m 32s\tremaining: 1m 54s\n",
      "649:\tlearn: 0.0010791\ttotal: 3m 32s\tremaining: 1m 54s\n",
      "650:\tlearn: 0.0010790\ttotal: 3m 32s\tremaining: 1m 54s\n",
      "651:\tlearn: 0.0010790\ttotal: 3m 33s\tremaining: 1m 53s\n",
      "652:\tlearn: 0.0010790\ttotal: 3m 33s\tremaining: 1m 53s\n",
      "653:\tlearn: 0.0010790\ttotal: 3m 33s\tremaining: 1m 53s\n",
      "654:\tlearn: 0.0010789\ttotal: 3m 34s\tremaining: 1m 52s\n",
      "655:\tlearn: 0.0010789\ttotal: 3m 34s\tremaining: 1m 52s\n",
      "656:\tlearn: 0.0010789\ttotal: 3m 34s\tremaining: 1m 52s\n",
      "657:\tlearn: 0.0010789\ttotal: 3m 35s\tremaining: 1m 51s\n",
      "658:\tlearn: 0.0010789\ttotal: 3m 35s\tremaining: 1m 51s\n",
      "659:\tlearn: 0.0010789\ttotal: 3m 35s\tremaining: 1m 51s\n",
      "660:\tlearn: 0.0010789\ttotal: 3m 36s\tremaining: 1m 50s\n",
      "661:\tlearn: 0.0010750\ttotal: 3m 36s\tremaining: 1m 50s\n",
      "662:\tlearn: 0.0010750\ttotal: 3m 36s\tremaining: 1m 50s\n",
      "663:\tlearn: 0.0010711\ttotal: 3m 37s\tremaining: 1m 49s\n",
      "664:\tlearn: 0.0010711\ttotal: 3m 37s\tremaining: 1m 49s\n",
      "665:\tlearn: 0.0010710\ttotal: 3m 37s\tremaining: 1m 49s\n",
      "666:\tlearn: 0.0010709\ttotal: 3m 38s\tremaining: 1m 48s\n",
      "667:\tlearn: 0.0010709\ttotal: 3m 38s\tremaining: 1m 48s\n",
      "668:\tlearn: 0.0010709\ttotal: 3m 38s\tremaining: 1m 48s\n",
      "669:\tlearn: 0.0010709\ttotal: 3m 39s\tremaining: 1m 47s\n",
      "670:\tlearn: 0.0010709\ttotal: 3m 39s\tremaining: 1m 47s\n",
      "671:\tlearn: 0.0010707\ttotal: 3m 39s\tremaining: 1m 47s\n",
      "672:\tlearn: 0.0010707\ttotal: 3m 40s\tremaining: 1m 46s\n",
      "673:\tlearn: 0.0010707\ttotal: 3m 40s\tremaining: 1m 46s\n",
      "674:\tlearn: 0.0010707\ttotal: 3m 40s\tremaining: 1m 46s\n",
      "675:\tlearn: 0.0010706\ttotal: 3m 41s\tremaining: 1m 45s\n",
      "676:\tlearn: 0.0010706\ttotal: 3m 41s\tremaining: 1m 45s\n",
      "677:\tlearn: 0.0010705\ttotal: 3m 41s\tremaining: 1m 45s\n",
      "678:\tlearn: 0.0010705\ttotal: 3m 42s\tremaining: 1m 44s\n",
      "679:\tlearn: 0.0010703\ttotal: 3m 42s\tremaining: 1m 44s\n",
      "680:\tlearn: 0.0010703\ttotal: 3m 42s\tremaining: 1m 44s\n",
      "681:\tlearn: 0.0010703\ttotal: 3m 42s\tremaining: 1m 43s\n",
      "682:\tlearn: 0.0010675\ttotal: 3m 43s\tremaining: 1m 43s\n",
      "683:\tlearn: 0.0010675\ttotal: 3m 43s\tremaining: 1m 43s\n",
      "684:\tlearn: 0.0010674\ttotal: 3m 43s\tremaining: 1m 42s\n",
      "685:\tlearn: 0.0010646\ttotal: 3m 44s\tremaining: 1m 42s\n",
      "686:\tlearn: 0.0010645\ttotal: 3m 44s\tremaining: 1m 42s\n",
      "687:\tlearn: 0.0010645\ttotal: 3m 44s\tremaining: 1m 42s\n",
      "688:\tlearn: 0.0010625\ttotal: 3m 45s\tremaining: 1m 41s\n",
      "689:\tlearn: 0.0010624\ttotal: 3m 45s\tremaining: 1m 41s\n",
      "690:\tlearn: 0.0010624\ttotal: 3m 45s\tremaining: 1m 41s\n",
      "691:\tlearn: 0.0010624\ttotal: 3m 46s\tremaining: 1m 40s\n",
      "692:\tlearn: 0.0010624\ttotal: 3m 46s\tremaining: 1m 40s\n",
      "693:\tlearn: 0.0010624\ttotal: 3m 46s\tremaining: 1m 40s\n",
      "694:\tlearn: 0.0010624\ttotal: 3m 47s\tremaining: 1m 39s\n",
      "695:\tlearn: 0.0010624\ttotal: 3m 47s\tremaining: 1m 39s\n",
      "696:\tlearn: 0.0010624\ttotal: 3m 47s\tremaining: 1m 39s\n",
      "697:\tlearn: 0.0010605\ttotal: 3m 48s\tremaining: 1m 38s\n",
      "698:\tlearn: 0.0010605\ttotal: 3m 48s\tremaining: 1m 38s\n",
      "699:\tlearn: 0.0010605\ttotal: 3m 48s\tremaining: 1m 38s\n",
      "700:\tlearn: 0.0010604\ttotal: 3m 49s\tremaining: 1m 37s\n",
      "701:\tlearn: 0.0010601\ttotal: 3m 49s\tremaining: 1m 37s\n",
      "702:\tlearn: 0.0010601\ttotal: 3m 49s\tremaining: 1m 37s\n",
      "703:\tlearn: 0.0010601\ttotal: 3m 50s\tremaining: 1m 36s\n",
      "704:\tlearn: 0.0010601\ttotal: 3m 50s\tremaining: 1m 36s\n",
      "705:\tlearn: 0.0010598\ttotal: 3m 50s\tremaining: 1m 36s\n",
      "706:\tlearn: 0.0010598\ttotal: 3m 51s\tremaining: 1m 35s\n",
      "707:\tlearn: 0.0010598\ttotal: 3m 51s\tremaining: 1m 35s\n",
      "708:\tlearn: 0.0010598\ttotal: 3m 51s\tremaining: 1m 35s\n",
      "709:\tlearn: 0.0010598\ttotal: 3m 52s\tremaining: 1m 34s\n",
      "710:\tlearn: 0.0010597\ttotal: 3m 52s\tremaining: 1m 34s\n",
      "711:\tlearn: 0.0010596\ttotal: 3m 52s\tremaining: 1m 34s\n",
      "712:\tlearn: 0.0010595\ttotal: 3m 53s\tremaining: 1m 33s\n",
      "713:\tlearn: 0.0010594\ttotal: 3m 53s\tremaining: 1m 33s\n",
      "714:\tlearn: 0.0010594\ttotal: 3m 53s\tremaining: 1m 33s\n",
      "715:\tlearn: 0.0010594\ttotal: 3m 54s\tremaining: 1m 32s\n",
      "716:\tlearn: 0.0010594\ttotal: 3m 54s\tremaining: 1m 32s\n",
      "717:\tlearn: 0.0010594\ttotal: 3m 54s\tremaining: 1m 32s\n",
      "718:\tlearn: 0.0010594\ttotal: 3m 55s\tremaining: 1m 31s\n",
      "719:\tlearn: 0.0010594\ttotal: 3m 55s\tremaining: 1m 31s\n",
      "720:\tlearn: 0.0010592\ttotal: 3m 55s\tremaining: 1m 31s\n",
      "721:\tlearn: 0.0010592\ttotal: 3m 56s\tremaining: 1m 30s\n",
      "722:\tlearn: 0.0010591\ttotal: 3m 56s\tremaining: 1m 30s\n",
      "723:\tlearn: 0.0010591\ttotal: 3m 56s\tremaining: 1m 30s\n",
      "724:\tlearn: 0.0010591\ttotal: 3m 57s\tremaining: 1m 29s\n",
      "725:\tlearn: 0.0010590\ttotal: 3m 57s\tremaining: 1m 29s\n",
      "726:\tlearn: 0.0010589\ttotal: 3m 57s\tremaining: 1m 29s\n",
      "727:\tlearn: 0.0010588\ttotal: 3m 58s\tremaining: 1m 28s\n",
      "728:\tlearn: 0.0010585\ttotal: 3m 58s\tremaining: 1m 28s\n",
      "729:\tlearn: 0.0010585\ttotal: 3m 58s\tremaining: 1m 28s\n",
      "730:\tlearn: 0.0010568\ttotal: 3m 59s\tremaining: 1m 27s\n",
      "731:\tlearn: 0.0010568\ttotal: 3m 59s\tremaining: 1m 27s\n",
      "732:\tlearn: 0.0010568\ttotal: 3m 59s\tremaining: 1m 27s\n",
      "733:\tlearn: 0.0010568\ttotal: 3m 59s\tremaining: 1m 26s\n",
      "734:\tlearn: 0.0010568\ttotal: 4m\tremaining: 1m 26s\n",
      "735:\tlearn: 0.0010568\ttotal: 4m\tremaining: 1m 26s\n",
      "736:\tlearn: 0.0010568\ttotal: 4m\tremaining: 1m 25s\n",
      "737:\tlearn: 0.0010568\ttotal: 4m 1s\tremaining: 1m 25s\n",
      "738:\tlearn: 0.0010568\ttotal: 4m 1s\tremaining: 1m 25s\n",
      "739:\tlearn: 0.0010568\ttotal: 4m 1s\tremaining: 1m 25s\n",
      "740:\tlearn: 0.0010568\ttotal: 4m 2s\tremaining: 1m 24s\n",
      "741:\tlearn: 0.0010568\ttotal: 4m 2s\tremaining: 1m 24s\n",
      "742:\tlearn: 0.0010568\ttotal: 4m 2s\tremaining: 1m 24s\n",
      "743:\tlearn: 0.0010566\ttotal: 4m 3s\tremaining: 1m 23s\n",
      "744:\tlearn: 0.0010566\ttotal: 4m 3s\tremaining: 1m 23s\n",
      "745:\tlearn: 0.0010565\ttotal: 4m 3s\tremaining: 1m 23s\n",
      "746:\tlearn: 0.0010565\ttotal: 4m 4s\tremaining: 1m 22s\n",
      "747:\tlearn: 0.0010565\ttotal: 4m 4s\tremaining: 1m 22s\n",
      "748:\tlearn: 0.0010564\ttotal: 4m 4s\tremaining: 1m 22s\n",
      "749:\tlearn: 0.0010564\ttotal: 4m 5s\tremaining: 1m 21s\n",
      "750:\tlearn: 0.0010564\ttotal: 4m 5s\tremaining: 1m 21s\n",
      "751:\tlearn: 0.0010564\ttotal: 4m 5s\tremaining: 1m 21s\n",
      "752:\tlearn: 0.0010547\ttotal: 4m 6s\tremaining: 1m 20s\n",
      "753:\tlearn: 0.0010547\ttotal: 4m 6s\tremaining: 1m 20s\n",
      "754:\tlearn: 0.0010537\ttotal: 4m 6s\tremaining: 1m 20s\n",
      "755:\tlearn: 0.0010537\ttotal: 4m 7s\tremaining: 1m 19s\n",
      "756:\tlearn: 0.0010513\ttotal: 4m 7s\tremaining: 1m 19s\n",
      "757:\tlearn: 0.0010513\ttotal: 4m 7s\tremaining: 1m 19s\n",
      "758:\tlearn: 0.0010512\ttotal: 4m 8s\tremaining: 1m 18s\n",
      "759:\tlearn: 0.0010512\ttotal: 4m 8s\tremaining: 1m 18s\n",
      "760:\tlearn: 0.0010489\ttotal: 4m 8s\tremaining: 1m 18s\n",
      "761:\tlearn: 0.0010488\ttotal: 4m 9s\tremaining: 1m 17s\n",
      "762:\tlearn: 0.0010487\ttotal: 4m 9s\tremaining: 1m 17s\n",
      "763:\tlearn: 0.0010487\ttotal: 4m 9s\tremaining: 1m 17s\n",
      "764:\tlearn: 0.0010487\ttotal: 4m 10s\tremaining: 1m 16s\n",
      "765:\tlearn: 0.0010487\ttotal: 4m 10s\tremaining: 1m 16s\n",
      "766:\tlearn: 0.0010487\ttotal: 4m 10s\tremaining: 1m 16s\n",
      "767:\tlearn: 0.0010472\ttotal: 4m 11s\tremaining: 1m 15s\n",
      "768:\tlearn: 0.0010452\ttotal: 4m 11s\tremaining: 1m 15s\n",
      "769:\tlearn: 0.0010452\ttotal: 4m 11s\tremaining: 1m 15s\n",
      "770:\tlearn: 0.0010452\ttotal: 4m 12s\tremaining: 1m 14s\n",
      "771:\tlearn: 0.0010443\ttotal: 4m 12s\tremaining: 1m 14s\n",
      "772:\tlearn: 0.0010442\ttotal: 4m 12s\tremaining: 1m 14s\n",
      "773:\tlearn: 0.0010442\ttotal: 4m 13s\tremaining: 1m 13s\n",
      "774:\tlearn: 0.0010442\ttotal: 4m 13s\tremaining: 1m 13s\n",
      "775:\tlearn: 0.0010442\ttotal: 4m 13s\tremaining: 1m 13s\n",
      "776:\tlearn: 0.0010440\ttotal: 4m 13s\tremaining: 1m 12s\n",
      "777:\tlearn: 0.0010440\ttotal: 4m 14s\tremaining: 1m 12s\n",
      "778:\tlearn: 0.0010423\ttotal: 4m 14s\tremaining: 1m 12s\n",
      "779:\tlearn: 0.0010423\ttotal: 4m 14s\tremaining: 1m 11s\n",
      "780:\tlearn: 0.0010423\ttotal: 4m 15s\tremaining: 1m 11s\n",
      "781:\tlearn: 0.0010423\ttotal: 4m 15s\tremaining: 1m 11s\n",
      "782:\tlearn: 0.0010423\ttotal: 4m 15s\tremaining: 1m 10s\n",
      "783:\tlearn: 0.0010423\ttotal: 4m 16s\tremaining: 1m 10s\n",
      "784:\tlearn: 0.0010422\ttotal: 4m 16s\tremaining: 1m 10s\n",
      "785:\tlearn: 0.0010422\ttotal: 4m 16s\tremaining: 1m 9s\n",
      "786:\tlearn: 0.0010421\ttotal: 4m 17s\tremaining: 1m 9s\n",
      "787:\tlearn: 0.0010420\ttotal: 4m 17s\tremaining: 1m 9s\n",
      "788:\tlearn: 0.0010399\ttotal: 4m 17s\tremaining: 1m 8s\n",
      "789:\tlearn: 0.0010399\ttotal: 4m 18s\tremaining: 1m 8s\n",
      "790:\tlearn: 0.0010399\ttotal: 4m 18s\tremaining: 1m 8s\n",
      "791:\tlearn: 0.0010399\ttotal: 4m 18s\tremaining: 1m 7s\n",
      "792:\tlearn: 0.0010398\ttotal: 4m 19s\tremaining: 1m 7s\n",
      "793:\tlearn: 0.0010398\ttotal: 4m 19s\tremaining: 1m 7s\n",
      "794:\tlearn: 0.0010398\ttotal: 4m 19s\tremaining: 1m 7s\n",
      "795:\tlearn: 0.0010364\ttotal: 4m 20s\tremaining: 1m 6s\n",
      "796:\tlearn: 0.0010333\ttotal: 4m 20s\tremaining: 1m 6s\n",
      "797:\tlearn: 0.0010316\ttotal: 4m 20s\tremaining: 1m 6s\n",
      "798:\tlearn: 0.0010313\ttotal: 4m 21s\tremaining: 1m 5s\n",
      "799:\tlearn: 0.0010310\ttotal: 4m 21s\tremaining: 1m 5s\n",
      "800:\tlearn: 0.0010271\ttotal: 4m 21s\tremaining: 1m 5s\n",
      "801:\tlearn: 0.0010259\ttotal: 4m 22s\tremaining: 1m 4s\n",
      "802:\tlearn: 0.0010253\ttotal: 4m 22s\tremaining: 1m 4s\n",
      "803:\tlearn: 0.0010253\ttotal: 4m 22s\tremaining: 1m 4s\n",
      "804:\tlearn: 0.0010253\ttotal: 4m 23s\tremaining: 1m 3s\n",
      "805:\tlearn: 0.0010253\ttotal: 4m 23s\tremaining: 1m 3s\n",
      "806:\tlearn: 0.0010253\ttotal: 4m 23s\tremaining: 1m 3s\n",
      "807:\tlearn: 0.0010253\ttotal: 4m 24s\tremaining: 1m 2s\n",
      "808:\tlearn: 0.0010249\ttotal: 4m 24s\tremaining: 1m 2s\n",
      "809:\tlearn: 0.0010248\ttotal: 4m 24s\tremaining: 1m 2s\n",
      "810:\tlearn: 0.0010248\ttotal: 4m 25s\tremaining: 1m 1s\n",
      "811:\tlearn: 0.0010247\ttotal: 4m 25s\tremaining: 1m 1s\n",
      "812:\tlearn: 0.0010246\ttotal: 4m 25s\tremaining: 1m 1s\n",
      "813:\tlearn: 0.0010246\ttotal: 4m 26s\tremaining: 1m\n",
      "814:\tlearn: 0.0010244\ttotal: 4m 26s\tremaining: 1m\n",
      "815:\tlearn: 0.0010244\ttotal: 4m 26s\tremaining: 1m\n",
      "816:\tlearn: 0.0010243\ttotal: 4m 27s\tremaining: 59.8s\n",
      "817:\tlearn: 0.0010243\ttotal: 4m 27s\tremaining: 59.5s\n",
      "818:\tlearn: 0.0010243\ttotal: 4m 27s\tremaining: 59.2s\n",
      "819:\tlearn: 0.0010243\ttotal: 4m 28s\tremaining: 58.8s\n",
      "820:\tlearn: 0.0010243\ttotal: 4m 28s\tremaining: 58.5s\n",
      "821:\tlearn: 0.0010243\ttotal: 4m 28s\tremaining: 58.2s\n",
      "822:\tlearn: 0.0010243\ttotal: 4m 29s\tremaining: 57.9s\n",
      "823:\tlearn: 0.0010242\ttotal: 4m 29s\tremaining: 57.5s\n",
      "824:\tlearn: 0.0010218\ttotal: 4m 29s\tremaining: 57.2s\n",
      "825:\tlearn: 0.0010217\ttotal: 4m 29s\tremaining: 56.9s\n",
      "826:\tlearn: 0.0010217\ttotal: 4m 30s\tremaining: 56.5s\n",
      "827:\tlearn: 0.0010217\ttotal: 4m 30s\tremaining: 56.2s\n",
      "828:\tlearn: 0.0010208\ttotal: 4m 30s\tremaining: 55.9s\n",
      "829:\tlearn: 0.0010207\ttotal: 4m 31s\tremaining: 55.6s\n",
      "830:\tlearn: 0.0010207\ttotal: 4m 31s\tremaining: 55.2s\n",
      "831:\tlearn: 0.0010207\ttotal: 4m 31s\tremaining: 54.9s\n",
      "832:\tlearn: 0.0010207\ttotal: 4m 32s\tremaining: 54.6s\n",
      "833:\tlearn: 0.0010207\ttotal: 4m 32s\tremaining: 54.3s\n",
      "834:\tlearn: 0.0010204\ttotal: 4m 32s\tremaining: 53.9s\n",
      "835:\tlearn: 0.0010204\ttotal: 4m 33s\tremaining: 53.6s\n",
      "836:\tlearn: 0.0010204\ttotal: 4m 33s\tremaining: 53.3s\n",
      "837:\tlearn: 0.0010197\ttotal: 4m 33s\tremaining: 53s\n",
      "838:\tlearn: 0.0010196\ttotal: 4m 34s\tremaining: 52.6s\n",
      "839:\tlearn: 0.0010196\ttotal: 4m 34s\tremaining: 52.3s\n",
      "840:\tlearn: 0.0010195\ttotal: 4m 34s\tremaining: 52s\n",
      "841:\tlearn: 0.0010195\ttotal: 4m 35s\tremaining: 51.6s\n",
      "842:\tlearn: 0.0010194\ttotal: 4m 35s\tremaining: 51.3s\n",
      "843:\tlearn: 0.0010194\ttotal: 4m 35s\tremaining: 51s\n",
      "844:\tlearn: 0.0010194\ttotal: 4m 36s\tremaining: 50.7s\n",
      "845:\tlearn: 0.0010194\ttotal: 4m 36s\tremaining: 50.3s\n",
      "846:\tlearn: 0.0010193\ttotal: 4m 36s\tremaining: 50s\n",
      "847:\tlearn: 0.0010192\ttotal: 4m 37s\tremaining: 49.7s\n",
      "848:\tlearn: 0.0010192\ttotal: 4m 37s\tremaining: 49.4s\n",
      "849:\tlearn: 0.0010189\ttotal: 4m 37s\tremaining: 49s\n",
      "850:\tlearn: 0.0010189\ttotal: 4m 38s\tremaining: 48.7s\n",
      "851:\tlearn: 0.0010187\ttotal: 4m 38s\tremaining: 48.4s\n",
      "852:\tlearn: 0.0010186\ttotal: 4m 38s\tremaining: 48.1s\n",
      "853:\tlearn: 0.0010185\ttotal: 4m 39s\tremaining: 47.7s\n",
      "854:\tlearn: 0.0010185\ttotal: 4m 39s\tremaining: 47.4s\n",
      "855:\tlearn: 0.0010185\ttotal: 4m 39s\tremaining: 47.1s\n",
      "856:\tlearn: 0.0010185\ttotal: 4m 40s\tremaining: 46.7s\n",
      "857:\tlearn: 0.0010185\ttotal: 4m 40s\tremaining: 46.4s\n",
      "858:\tlearn: 0.0010185\ttotal: 4m 40s\tremaining: 46.1s\n",
      "859:\tlearn: 0.0010185\ttotal: 4m 41s\tremaining: 45.8s\n",
      "860:\tlearn: 0.0010157\ttotal: 4m 41s\tremaining: 45.4s\n",
      "861:\tlearn: 0.0010157\ttotal: 4m 41s\tremaining: 45.1s\n",
      "862:\tlearn: 0.0010148\ttotal: 4m 42s\tremaining: 44.8s\n",
      "863:\tlearn: 0.0010147\ttotal: 4m 42s\tremaining: 44.5s\n",
      "864:\tlearn: 0.0010147\ttotal: 4m 42s\tremaining: 44.1s\n",
      "865:\tlearn: 0.0010134\ttotal: 4m 43s\tremaining: 43.8s\n",
      "866:\tlearn: 0.0010134\ttotal: 4m 43s\tremaining: 43.5s\n",
      "867:\tlearn: 0.0010134\ttotal: 4m 43s\tremaining: 43.1s\n",
      "868:\tlearn: 0.0010134\ttotal: 4m 44s\tremaining: 42.8s\n",
      "869:\tlearn: 0.0010134\ttotal: 4m 44s\tremaining: 42.5s\n",
      "870:\tlearn: 0.0010114\ttotal: 4m 44s\tremaining: 42.2s\n",
      "871:\tlearn: 0.0010113\ttotal: 4m 45s\tremaining: 41.8s\n",
      "872:\tlearn: 0.0010113\ttotal: 4m 45s\tremaining: 41.5s\n",
      "873:\tlearn: 0.0010113\ttotal: 4m 45s\tremaining: 41.2s\n",
      "874:\tlearn: 0.0010109\ttotal: 4m 46s\tremaining: 40.9s\n",
      "875:\tlearn: 0.0010105\ttotal: 4m 46s\tremaining: 40.5s\n",
      "876:\tlearn: 0.0010103\ttotal: 4m 46s\tremaining: 40.2s\n",
      "877:\tlearn: 0.0010103\ttotal: 4m 46s\tremaining: 39.9s\n",
      "878:\tlearn: 0.0010102\ttotal: 4m 47s\tremaining: 39.6s\n",
      "879:\tlearn: 0.0010102\ttotal: 4m 47s\tremaining: 39.2s\n",
      "880:\tlearn: 0.0010102\ttotal: 4m 48s\tremaining: 38.9s\n",
      "881:\tlearn: 0.0010102\ttotal: 4m 48s\tremaining: 38.6s\n",
      "882:\tlearn: 0.0010102\ttotal: 4m 48s\tremaining: 38.3s\n",
      "883:\tlearn: 0.0010101\ttotal: 4m 49s\tremaining: 37.9s\n",
      "884:\tlearn: 0.0010101\ttotal: 4m 49s\tremaining: 37.6s\n",
      "885:\tlearn: 0.0010098\ttotal: 4m 49s\tremaining: 37.3s\n",
      "886:\tlearn: 0.0010098\ttotal: 4m 50s\tremaining: 36.9s\n",
      "887:\tlearn: 0.0010098\ttotal: 4m 50s\tremaining: 36.6s\n",
      "888:\tlearn: 0.0010098\ttotal: 4m 50s\tremaining: 36.3s\n",
      "889:\tlearn: 0.0010098\ttotal: 4m 50s\tremaining: 36s\n",
      "890:\tlearn: 0.0010098\ttotal: 4m 51s\tremaining: 35.6s\n",
      "891:\tlearn: 0.0010097\ttotal: 4m 51s\tremaining: 35.3s\n",
      "892:\tlearn: 0.0010097\ttotal: 4m 51s\tremaining: 35s\n",
      "893:\tlearn: 0.0010097\ttotal: 4m 52s\tremaining: 34.7s\n",
      "894:\tlearn: 0.0010097\ttotal: 4m 52s\tremaining: 34.3s\n",
      "895:\tlearn: 0.0010097\ttotal: 4m 52s\tremaining: 34s\n",
      "896:\tlearn: 0.0010096\ttotal: 4m 53s\tremaining: 33.7s\n",
      "897:\tlearn: 0.0010096\ttotal: 4m 53s\tremaining: 33.3s\n",
      "898:\tlearn: 0.0010096\ttotal: 4m 53s\tremaining: 33s\n",
      "899:\tlearn: 0.0010096\ttotal: 4m 54s\tremaining: 32.7s\n",
      "900:\tlearn: 0.0010096\ttotal: 4m 54s\tremaining: 32.4s\n",
      "901:\tlearn: 0.0010095\ttotal: 4m 54s\tremaining: 32s\n",
      "902:\tlearn: 0.0010095\ttotal: 4m 55s\tremaining: 31.7s\n",
      "903:\tlearn: 0.0010095\ttotal: 4m 55s\tremaining: 31.4s\n",
      "904:\tlearn: 0.0010095\ttotal: 4m 55s\tremaining: 31.1s\n",
      "905:\tlearn: 0.0010094\ttotal: 4m 56s\tremaining: 30.7s\n",
      "906:\tlearn: 0.0010094\ttotal: 4m 56s\tremaining: 30.4s\n",
      "907:\tlearn: 0.0010093\ttotal: 4m 56s\tremaining: 30.1s\n",
      "908:\tlearn: 0.0010093\ttotal: 4m 57s\tremaining: 29.8s\n",
      "909:\tlearn: 0.0010093\ttotal: 4m 57s\tremaining: 29.4s\n",
      "910:\tlearn: 0.0010088\ttotal: 4m 57s\tremaining: 29.1s\n",
      "911:\tlearn: 0.0010085\ttotal: 4m 58s\tremaining: 28.8s\n",
      "912:\tlearn: 0.0010085\ttotal: 4m 58s\tremaining: 28.4s\n",
      "913:\tlearn: 0.0010085\ttotal: 4m 58s\tremaining: 28.1s\n",
      "914:\tlearn: 0.0010084\ttotal: 4m 59s\tremaining: 27.8s\n",
      "915:\tlearn: 0.0010082\ttotal: 4m 59s\tremaining: 27.5s\n",
      "916:\tlearn: 0.0010080\ttotal: 4m 59s\tremaining: 27.1s\n",
      "917:\tlearn: 0.0010080\ttotal: 5m\tremaining: 26.8s\n",
      "918:\tlearn: 0.0010078\ttotal: 5m\tremaining: 26.5s\n",
      "919:\tlearn: 0.0010078\ttotal: 5m\tremaining: 26.2s\n",
      "920:\tlearn: 0.0010078\ttotal: 5m 1s\tremaining: 25.8s\n",
      "921:\tlearn: 0.0010076\ttotal: 5m 1s\tremaining: 25.5s\n",
      "922:\tlearn: 0.0010076\ttotal: 5m 1s\tremaining: 25.2s\n",
      "923:\tlearn: 0.0010075\ttotal: 5m 2s\tremaining: 24.8s\n",
      "924:\tlearn: 0.0010074\ttotal: 5m 2s\tremaining: 24.5s\n",
      "925:\tlearn: 0.0010072\ttotal: 5m 2s\tremaining: 24.2s\n",
      "926:\tlearn: 0.0010059\ttotal: 5m 3s\tremaining: 23.9s\n",
      "927:\tlearn: 0.0010059\ttotal: 5m 3s\tremaining: 23.5s\n",
      "928:\tlearn: 0.0010056\ttotal: 5m 3s\tremaining: 23.2s\n",
      "929:\tlearn: 0.0010054\ttotal: 5m 4s\tremaining: 22.9s\n",
      "930:\tlearn: 0.0010036\ttotal: 5m 4s\tremaining: 22.6s\n",
      "931:\tlearn: 0.0010036\ttotal: 5m 4s\tremaining: 22.2s\n",
      "932:\tlearn: 0.0010036\ttotal: 5m 5s\tremaining: 21.9s\n",
      "933:\tlearn: 0.0010034\ttotal: 5m 5s\tremaining: 21.6s\n",
      "934:\tlearn: 0.0010010\ttotal: 5m 5s\tremaining: 21.3s\n",
      "935:\tlearn: 0.0010010\ttotal: 5m 6s\tremaining: 20.9s\n",
      "936:\tlearn: 0.0010010\ttotal: 5m 6s\tremaining: 20.6s\n",
      "937:\tlearn: 0.0010009\ttotal: 5m 6s\tremaining: 20.3s\n",
      "938:\tlearn: 0.0010008\ttotal: 5m 7s\tremaining: 19.9s\n",
      "939:\tlearn: 0.0010004\ttotal: 5m 7s\tremaining: 19.6s\n",
      "940:\tlearn: 0.0010004\ttotal: 5m 7s\tremaining: 19.3s\n",
      "941:\tlearn: 0.0010004\ttotal: 5m 7s\tremaining: 19s\n",
      "942:\tlearn: 0.0010003\ttotal: 5m 8s\tremaining: 18.6s\n",
      "943:\tlearn: 0.0010003\ttotal: 5m 8s\tremaining: 18.3s\n",
      "944:\tlearn: 0.0010003\ttotal: 5m 8s\tremaining: 18s\n",
      "945:\tlearn: 0.0010003\ttotal: 5m 9s\tremaining: 17.7s\n",
      "946:\tlearn: 0.0010003\ttotal: 5m 9s\tremaining: 17.3s\n",
      "947:\tlearn: 0.0010003\ttotal: 5m 10s\tremaining: 17s\n",
      "948:\tlearn: 0.0010003\ttotal: 5m 10s\tremaining: 16.7s\n",
      "949:\tlearn: 0.0010002\ttotal: 5m 10s\tremaining: 16.4s\n",
      "950:\tlearn: 0.0009977\ttotal: 5m 11s\tremaining: 16s\n",
      "951:\tlearn: 0.0009976\ttotal: 5m 11s\tremaining: 15.7s\n",
      "952:\tlearn: 0.0009975\ttotal: 5m 11s\tremaining: 15.4s\n",
      "953:\tlearn: 0.0009975\ttotal: 5m 11s\tremaining: 15s\n",
      "954:\tlearn: 0.0009958\ttotal: 5m 12s\tremaining: 14.7s\n",
      "955:\tlearn: 0.0009957\ttotal: 5m 12s\tremaining: 14.4s\n",
      "956:\tlearn: 0.0009957\ttotal: 5m 12s\tremaining: 14.1s\n",
      "957:\tlearn: 0.0009946\ttotal: 5m 13s\tremaining: 13.7s\n",
      "958:\tlearn: 0.0009946\ttotal: 5m 13s\tremaining: 13.4s\n",
      "959:\tlearn: 0.0009946\ttotal: 5m 13s\tremaining: 13.1s\n",
      "960:\tlearn: 0.0009946\ttotal: 5m 14s\tremaining: 12.8s\n",
      "961:\tlearn: 0.0009942\ttotal: 5m 14s\tremaining: 12.4s\n",
      "962:\tlearn: 0.0009938\ttotal: 5m 14s\tremaining: 12.1s\n",
      "963:\tlearn: 0.0009912\ttotal: 5m 15s\tremaining: 11.8s\n",
      "964:\tlearn: 0.0009912\ttotal: 5m 15s\tremaining: 11.4s\n",
      "965:\tlearn: 0.0009912\ttotal: 5m 15s\tremaining: 11.1s\n",
      "966:\tlearn: 0.0009911\ttotal: 5m 16s\tremaining: 10.8s\n",
      "967:\tlearn: 0.0009911\ttotal: 5m 16s\tremaining: 10.5s\n",
      "968:\tlearn: 0.0009911\ttotal: 5m 16s\tremaining: 10.1s\n",
      "969:\tlearn: 0.0009911\ttotal: 5m 17s\tremaining: 9.81s\n",
      "970:\tlearn: 0.0009910\ttotal: 5m 17s\tremaining: 9.48s\n",
      "971:\tlearn: 0.0009910\ttotal: 5m 17s\tremaining: 9.16s\n",
      "972:\tlearn: 0.0009909\ttotal: 5m 18s\tremaining: 8.83s\n",
      "973:\tlearn: 0.0009909\ttotal: 5m 18s\tremaining: 8.5s\n",
      "974:\tlearn: 0.0009909\ttotal: 5m 18s\tremaining: 8.18s\n",
      "975:\tlearn: 0.0009909\ttotal: 5m 19s\tremaining: 7.85s\n",
      "976:\tlearn: 0.0009909\ttotal: 5m 19s\tremaining: 7.52s\n",
      "977:\tlearn: 0.0009909\ttotal: 5m 19s\tremaining: 7.19s\n",
      "978:\tlearn: 0.0009908\ttotal: 5m 20s\tremaining: 6.87s\n",
      "979:\tlearn: 0.0009908\ttotal: 5m 20s\tremaining: 6.54s\n",
      "980:\tlearn: 0.0009908\ttotal: 5m 20s\tremaining: 6.21s\n",
      "981:\tlearn: 0.0009908\ttotal: 5m 21s\tremaining: 5.89s\n",
      "982:\tlearn: 0.0009907\ttotal: 5m 21s\tremaining: 5.56s\n",
      "983:\tlearn: 0.0009907\ttotal: 5m 21s\tremaining: 5.23s\n",
      "984:\tlearn: 0.0009907\ttotal: 5m 22s\tremaining: 4.91s\n",
      "985:\tlearn: 0.0009907\ttotal: 5m 22s\tremaining: 4.58s\n",
      "986:\tlearn: 0.0009899\ttotal: 5m 22s\tremaining: 4.25s\n",
      "987:\tlearn: 0.0009899\ttotal: 5m 23s\tremaining: 3.92s\n",
      "988:\tlearn: 0.0009896\ttotal: 5m 23s\tremaining: 3.6s\n",
      "989:\tlearn: 0.0009892\ttotal: 5m 23s\tremaining: 3.27s\n",
      "990:\tlearn: 0.0009892\ttotal: 5m 24s\tremaining: 2.94s\n",
      "991:\tlearn: 0.0009892\ttotal: 5m 24s\tremaining: 2.62s\n",
      "992:\tlearn: 0.0009880\ttotal: 5m 24s\tremaining: 2.29s\n",
      "993:\tlearn: 0.0009859\ttotal: 5m 25s\tremaining: 1.96s\n",
      "994:\tlearn: 0.0009858\ttotal: 5m 25s\tremaining: 1.64s\n",
      "995:\tlearn: 0.0009858\ttotal: 5m 25s\tremaining: 1.31s\n",
      "996:\tlearn: 0.0009858\ttotal: 5m 26s\tremaining: 981ms\n",
      "997:\tlearn: 0.0009858\ttotal: 5m 26s\tremaining: 654ms\n",
      "998:\tlearn: 0.0009858\ttotal: 5m 26s\tremaining: 327ms\n",
      "999:\tlearn: 0.0009858\ttotal: 5m 27s\tremaining: 0us\n",
      "0:\tlearn: 0.6078572\ttotal: 376ms\tremaining: 6m 15s\n",
      "1:\tlearn: 0.5428918\ttotal: 703ms\tremaining: 5m 50s\n",
      "2:\tlearn: 0.4831155\ttotal: 1.03s\tremaining: 5m 41s\n",
      "3:\tlearn: 0.4357943\ttotal: 1.36s\tremaining: 5m 37s\n",
      "4:\tlearn: 0.3961711\ttotal: 1.69s\tremaining: 5m 35s\n",
      "5:\tlearn: 0.3571964\ttotal: 2.01s\tremaining: 5m 33s\n",
      "6:\tlearn: 0.3309616\ttotal: 2.34s\tremaining: 5m 32s\n",
      "7:\tlearn: 0.2999133\ttotal: 2.67s\tremaining: 5m 30s\n",
      "8:\tlearn: 0.2682381\ttotal: 3s\tremaining: 5m 29s\n",
      "9:\tlearn: 0.2415573\ttotal: 3.32s\tremaining: 5m 29s\n",
      "10:\tlearn: 0.2232377\ttotal: 3.65s\tremaining: 5m 28s\n",
      "11:\tlearn: 0.2095748\ttotal: 3.98s\tremaining: 5m 27s\n",
      "12:\tlearn: 0.1917014\ttotal: 4.3s\tremaining: 5m 26s\n",
      "13:\tlearn: 0.1760955\ttotal: 4.63s\tremaining: 5m 26s\n",
      "14:\tlearn: 0.1643635\ttotal: 4.96s\tremaining: 5m 25s\n",
      "15:\tlearn: 0.1508460\ttotal: 5.29s\tremaining: 5m 25s\n",
      "16:\tlearn: 0.1486374\ttotal: 5.63s\tremaining: 5m 25s\n",
      "17:\tlearn: 0.1335517\ttotal: 5.99s\tremaining: 5m 27s\n",
      "18:\tlearn: 0.1247472\ttotal: 6.36s\tremaining: 5m 28s\n",
      "19:\tlearn: 0.1162594\ttotal: 6.7s\tremaining: 5m 28s\n",
      "20:\tlearn: 0.1054708\ttotal: 7.04s\tremaining: 5m 28s\n",
      "21:\tlearn: 0.0930670\ttotal: 7.37s\tremaining: 5m 27s\n",
      "22:\tlearn: 0.0845998\ttotal: 7.7s\tremaining: 5m 26s\n",
      "23:\tlearn: 0.0743432\ttotal: 8.03s\tremaining: 5m 26s\n",
      "24:\tlearn: 0.0688814\ttotal: 8.36s\tremaining: 5m 25s\n",
      "25:\tlearn: 0.0652458\ttotal: 8.68s\tremaining: 5m 25s\n",
      "26:\tlearn: 0.0585447\ttotal: 9s\tremaining: 5m 24s\n",
      "27:\tlearn: 0.0545036\ttotal: 9.33s\tremaining: 5m 23s\n",
      "28:\tlearn: 0.0508624\ttotal: 9.66s\tremaining: 5m 23s\n",
      "29:\tlearn: 0.0464259\ttotal: 10s\tremaining: 5m 23s\n",
      "30:\tlearn: 0.0420986\ttotal: 10.3s\tremaining: 5m 23s\n",
      "31:\tlearn: 0.0392335\ttotal: 10.7s\tremaining: 5m 23s\n",
      "32:\tlearn: 0.0362718\ttotal: 11s\tremaining: 5m 22s\n",
      "33:\tlearn: 0.0342244\ttotal: 11.3s\tremaining: 5m 22s\n",
      "34:\tlearn: 0.0314266\ttotal: 11.7s\tremaining: 5m 21s\n",
      "35:\tlearn: 0.0297274\ttotal: 12s\tremaining: 5m 21s\n",
      "36:\tlearn: 0.0283417\ttotal: 12.4s\tremaining: 5m 21s\n",
      "37:\tlearn: 0.0267228\ttotal: 12.7s\tremaining: 5m 21s\n",
      "38:\tlearn: 0.0254601\ttotal: 13s\tremaining: 5m 20s\n",
      "39:\tlearn: 0.0243277\ttotal: 13.4s\tremaining: 5m 20s\n",
      "40:\tlearn: 0.0228994\ttotal: 13.7s\tremaining: 5m 20s\n",
      "41:\tlearn: 0.0217950\ttotal: 14s\tremaining: 5m 19s\n",
      "42:\tlearn: 0.0206735\ttotal: 14.4s\tremaining: 5m 19s\n",
      "43:\tlearn: 0.0194246\ttotal: 14.7s\tremaining: 5m 19s\n",
      "44:\tlearn: 0.0185060\ttotal: 15s\tremaining: 5m 18s\n",
      "45:\tlearn: 0.0179605\ttotal: 15.4s\tremaining: 5m 18s\n",
      "46:\tlearn: 0.0172973\ttotal: 15.7s\tremaining: 5m 18s\n",
      "47:\tlearn: 0.0166537\ttotal: 16s\tremaining: 5m 18s\n",
      "48:\tlearn: 0.0160281\ttotal: 16.4s\tremaining: 5m 18s\n",
      "49:\tlearn: 0.0154356\ttotal: 16.8s\tremaining: 5m 18s\n",
      "50:\tlearn: 0.0149200\ttotal: 17.1s\tremaining: 5m 18s\n",
      "51:\tlearn: 0.0145531\ttotal: 17.5s\tremaining: 5m 18s\n",
      "52:\tlearn: 0.0139843\ttotal: 17.9s\tremaining: 5m 19s\n",
      "53:\tlearn: 0.0134481\ttotal: 18.2s\tremaining: 5m 18s\n",
      "54:\tlearn: 0.0131196\ttotal: 18.5s\tremaining: 5m 18s\n",
      "55:\tlearn: 0.0125747\ttotal: 18.9s\tremaining: 5m 18s\n",
      "56:\tlearn: 0.0122966\ttotal: 19.2s\tremaining: 5m 17s\n",
      "57:\tlearn: 0.0118511\ttotal: 19.5s\tremaining: 5m 17s\n",
      "58:\tlearn: 0.0114920\ttotal: 19.9s\tremaining: 5m 17s\n",
      "59:\tlearn: 0.0112287\ttotal: 20.2s\tremaining: 5m 16s\n",
      "60:\tlearn: 0.0108999\ttotal: 20.6s\tremaining: 5m 16s\n",
      "61:\tlearn: 0.0106684\ttotal: 20.9s\tremaining: 5m 16s\n",
      "62:\tlearn: 0.0101239\ttotal: 21.2s\tremaining: 5m 15s\n",
      "63:\tlearn: 0.0099124\ttotal: 21.6s\tremaining: 5m 15s\n",
      "64:\tlearn: 0.0097203\ttotal: 21.9s\tremaining: 5m 15s\n",
      "65:\tlearn: 0.0094309\ttotal: 22.3s\tremaining: 5m 15s\n",
      "66:\tlearn: 0.0092049\ttotal: 22.6s\tremaining: 5m 14s\n",
      "67:\tlearn: 0.0090058\ttotal: 22.9s\tremaining: 5m 14s\n",
      "68:\tlearn: 0.0087988\ttotal: 23.3s\tremaining: 5m 14s\n",
      "69:\tlearn: 0.0085958\ttotal: 23.6s\tremaining: 5m 13s\n",
      "70:\tlearn: 0.0083883\ttotal: 24s\tremaining: 5m 13s\n",
      "71:\tlearn: 0.0080954\ttotal: 24.3s\tremaining: 5m 13s\n",
      "72:\tlearn: 0.0079880\ttotal: 24.7s\tremaining: 5m 13s\n",
      "73:\tlearn: 0.0078666\ttotal: 25s\tremaining: 5m 13s\n",
      "74:\tlearn: 0.0077128\ttotal: 25.4s\tremaining: 5m 12s\n",
      "75:\tlearn: 0.0075947\ttotal: 25.7s\tremaining: 5m 12s\n",
      "76:\tlearn: 0.0074100\ttotal: 26s\tremaining: 5m 12s\n",
      "77:\tlearn: 0.0073068\ttotal: 26.4s\tremaining: 5m 11s\n",
      "78:\tlearn: 0.0071260\ttotal: 26.8s\tremaining: 5m 11s\n",
      "79:\tlearn: 0.0070151\ttotal: 27.1s\tremaining: 5m 11s\n",
      "80:\tlearn: 0.0069190\ttotal: 27.5s\tremaining: 5m 11s\n",
      "81:\tlearn: 0.0067919\ttotal: 27.8s\tremaining: 5m 11s\n",
      "82:\tlearn: 0.0067252\ttotal: 28.2s\tremaining: 5m 11s\n",
      "83:\tlearn: 0.0066327\ttotal: 28.5s\tremaining: 5m 11s\n",
      "84:\tlearn: 0.0064948\ttotal: 28.9s\tremaining: 5m 11s\n",
      "85:\tlearn: 0.0064041\ttotal: 29.2s\tremaining: 5m 10s\n",
      "86:\tlearn: 0.0062751\ttotal: 29.6s\tremaining: 5m 10s\n",
      "87:\tlearn: 0.0061556\ttotal: 29.9s\tremaining: 5m 9s\n",
      "88:\tlearn: 0.0061038\ttotal: 30.2s\tremaining: 5m 9s\n",
      "89:\tlearn: 0.0060384\ttotal: 30.6s\tremaining: 5m 9s\n",
      "90:\tlearn: 0.0059098\ttotal: 30.9s\tremaining: 5m 8s\n",
      "91:\tlearn: 0.0058102\ttotal: 31.2s\tremaining: 5m 8s\n",
      "92:\tlearn: 0.0057246\ttotal: 31.6s\tremaining: 5m 7s\n",
      "93:\tlearn: 0.0056406\ttotal: 31.9s\tremaining: 5m 7s\n",
      "94:\tlearn: 0.0055782\ttotal: 32.2s\tremaining: 5m 6s\n",
      "95:\tlearn: 0.0054336\ttotal: 32.6s\tremaining: 5m 6s\n",
      "96:\tlearn: 0.0053434\ttotal: 32.9s\tremaining: 5m 6s\n",
      "97:\tlearn: 0.0052848\ttotal: 33.2s\tremaining: 5m 5s\n",
      "98:\tlearn: 0.0052133\ttotal: 33.5s\tremaining: 5m 5s\n",
      "99:\tlearn: 0.0051656\ttotal: 33.9s\tremaining: 5m 4s\n",
      "100:\tlearn: 0.0050325\ttotal: 34.2s\tremaining: 5m 4s\n",
      "101:\tlearn: 0.0049931\ttotal: 34.5s\tremaining: 5m 3s\n",
      "102:\tlearn: 0.0049443\ttotal: 34.8s\tremaining: 5m 3s\n",
      "103:\tlearn: 0.0048943\ttotal: 35.1s\tremaining: 5m 2s\n",
      "104:\tlearn: 0.0048273\ttotal: 35.5s\tremaining: 5m 2s\n",
      "105:\tlearn: 0.0047407\ttotal: 35.8s\tremaining: 5m 2s\n",
      "106:\tlearn: 0.0047061\ttotal: 36.1s\tremaining: 5m 1s\n",
      "107:\tlearn: 0.0046130\ttotal: 36.5s\tremaining: 5m 1s\n",
      "108:\tlearn: 0.0045644\ttotal: 36.8s\tremaining: 5m\n",
      "109:\tlearn: 0.0045038\ttotal: 37.1s\tremaining: 5m\n",
      "110:\tlearn: 0.0044380\ttotal: 37.4s\tremaining: 4m 59s\n",
      "111:\tlearn: 0.0043654\ttotal: 37.8s\tremaining: 4m 59s\n",
      "112:\tlearn: 0.0043246\ttotal: 38.1s\tremaining: 4m 59s\n",
      "113:\tlearn: 0.0042750\ttotal: 38.4s\tremaining: 4m 58s\n",
      "114:\tlearn: 0.0042343\ttotal: 38.8s\tremaining: 4m 58s\n",
      "115:\tlearn: 0.0041900\ttotal: 39.1s\tremaining: 4m 57s\n",
      "116:\tlearn: 0.0041539\ttotal: 39.4s\tremaining: 4m 57s\n",
      "117:\tlearn: 0.0041011\ttotal: 39.8s\tremaining: 4m 57s\n",
      "118:\tlearn: 0.0040504\ttotal: 40.1s\tremaining: 4m 56s\n",
      "119:\tlearn: 0.0039978\ttotal: 40.4s\tremaining: 4m 56s\n",
      "120:\tlearn: 0.0039626\ttotal: 40.7s\tremaining: 4m 55s\n",
      "121:\tlearn: 0.0039344\ttotal: 41.1s\tremaining: 4m 55s\n",
      "122:\tlearn: 0.0038764\ttotal: 41.4s\tremaining: 4m 55s\n",
      "123:\tlearn: 0.0038413\ttotal: 41.7s\tremaining: 4m 54s\n",
      "124:\tlearn: 0.0038091\ttotal: 42s\tremaining: 4m 54s\n",
      "125:\tlearn: 0.0037752\ttotal: 42.4s\tremaining: 4m 53s\n",
      "126:\tlearn: 0.0037362\ttotal: 42.7s\tremaining: 4m 53s\n",
      "127:\tlearn: 0.0037054\ttotal: 43s\tremaining: 4m 53s\n",
      "128:\tlearn: 0.0036806\ttotal: 43.3s\tremaining: 4m 52s\n",
      "129:\tlearn: 0.0036413\ttotal: 43.7s\tremaining: 4m 52s\n",
      "130:\tlearn: 0.0036164\ttotal: 44s\tremaining: 4m 51s\n",
      "131:\tlearn: 0.0035834\ttotal: 44.3s\tremaining: 4m 51s\n",
      "132:\tlearn: 0.0035553\ttotal: 44.7s\tremaining: 4m 51s\n",
      "133:\tlearn: 0.0035195\ttotal: 45s\tremaining: 4m 50s\n",
      "134:\tlearn: 0.0034869\ttotal: 45.3s\tremaining: 4m 50s\n",
      "135:\tlearn: 0.0034560\ttotal: 45.6s\tremaining: 4m 49s\n",
      "136:\tlearn: 0.0034235\ttotal: 46s\tremaining: 4m 49s\n",
      "137:\tlearn: 0.0033927\ttotal: 46.3s\tremaining: 4m 49s\n",
      "138:\tlearn: 0.0033671\ttotal: 46.6s\tremaining: 4m 48s\n",
      "139:\tlearn: 0.0033366\ttotal: 46.9s\tremaining: 4m 48s\n",
      "140:\tlearn: 0.0033121\ttotal: 47.3s\tremaining: 4m 47s\n",
      "141:\tlearn: 0.0032813\ttotal: 47.6s\tremaining: 4m 47s\n",
      "142:\tlearn: 0.0032631\ttotal: 47.9s\tremaining: 4m 47s\n",
      "143:\tlearn: 0.0032398\ttotal: 48.2s\tremaining: 4m 46s\n",
      "144:\tlearn: 0.0032090\ttotal: 48.6s\tremaining: 4m 46s\n",
      "145:\tlearn: 0.0031742\ttotal: 48.9s\tremaining: 4m 46s\n",
      "146:\tlearn: 0.0031448\ttotal: 49.2s\tremaining: 4m 45s\n",
      "147:\tlearn: 0.0031226\ttotal: 49.6s\tremaining: 4m 45s\n",
      "148:\tlearn: 0.0030960\ttotal: 49.9s\tremaining: 4m 44s\n",
      "149:\tlearn: 0.0030757\ttotal: 50.2s\tremaining: 4m 44s\n",
      "150:\tlearn: 0.0030554\ttotal: 50.5s\tremaining: 4m 44s\n",
      "151:\tlearn: 0.0030299\ttotal: 50.8s\tremaining: 4m 43s\n",
      "152:\tlearn: 0.0030088\ttotal: 51.2s\tremaining: 4m 43s\n",
      "153:\tlearn: 0.0029897\ttotal: 51.5s\tremaining: 4m 42s\n",
      "154:\tlearn: 0.0029745\ttotal: 51.8s\tremaining: 4m 42s\n",
      "155:\tlearn: 0.0029612\ttotal: 52.2s\tremaining: 4m 42s\n",
      "156:\tlearn: 0.0029401\ttotal: 52.5s\tremaining: 4m 41s\n",
      "157:\tlearn: 0.0029272\ttotal: 52.8s\tremaining: 4m 41s\n",
      "158:\tlearn: 0.0029086\ttotal: 53.1s\tremaining: 4m 41s\n",
      "159:\tlearn: 0.0028790\ttotal: 53.5s\tremaining: 4m 40s\n",
      "160:\tlearn: 0.0028419\ttotal: 53.8s\tremaining: 4m 40s\n",
      "161:\tlearn: 0.0028078\ttotal: 54.1s\tremaining: 4m 39s\n",
      "162:\tlearn: 0.0027696\ttotal: 54.5s\tremaining: 4m 39s\n",
      "163:\tlearn: 0.0027470\ttotal: 54.8s\tremaining: 4m 39s\n",
      "164:\tlearn: 0.0027255\ttotal: 55.1s\tremaining: 4m 38s\n",
      "165:\tlearn: 0.0026974\ttotal: 55.4s\tremaining: 4m 38s\n",
      "166:\tlearn: 0.0026877\ttotal: 55.8s\tremaining: 4m 38s\n",
      "167:\tlearn: 0.0026730\ttotal: 56.1s\tremaining: 4m 37s\n",
      "168:\tlearn: 0.0026620\ttotal: 56.4s\tremaining: 4m 37s\n",
      "169:\tlearn: 0.0026472\ttotal: 56.8s\tremaining: 4m 37s\n",
      "170:\tlearn: 0.0026291\ttotal: 57.1s\tremaining: 4m 36s\n",
      "171:\tlearn: 0.0026168\ttotal: 57.4s\tremaining: 4m 36s\n",
      "172:\tlearn: 0.0026005\ttotal: 57.7s\tremaining: 4m 36s\n",
      "173:\tlearn: 0.0025783\ttotal: 58.1s\tremaining: 4m 35s\n",
      "174:\tlearn: 0.0025546\ttotal: 58.4s\tremaining: 4m 35s\n",
      "175:\tlearn: 0.0025309\ttotal: 58.7s\tremaining: 4m 34s\n",
      "176:\tlearn: 0.0025175\ttotal: 59s\tremaining: 4m 34s\n",
      "177:\tlearn: 0.0024962\ttotal: 59.4s\tremaining: 4m 34s\n",
      "178:\tlearn: 0.0024759\ttotal: 59.7s\tremaining: 4m 33s\n",
      "179:\tlearn: 0.0024626\ttotal: 1m\tremaining: 4m 33s\n",
      "180:\tlearn: 0.0024510\ttotal: 1m\tremaining: 4m 33s\n",
      "181:\tlearn: 0.0024307\ttotal: 1m\tremaining: 4m 32s\n",
      "182:\tlearn: 0.0024171\ttotal: 1m 1s\tremaining: 4m 32s\n",
      "183:\tlearn: 0.0024001\ttotal: 1m 1s\tremaining: 4m 32s\n",
      "184:\tlearn: 0.0023904\ttotal: 1m 1s\tremaining: 4m 31s\n",
      "185:\tlearn: 0.0023763\ttotal: 1m 2s\tremaining: 4m 31s\n",
      "186:\tlearn: 0.0023649\ttotal: 1m 2s\tremaining: 4m 31s\n",
      "187:\tlearn: 0.0023495\ttotal: 1m 2s\tremaining: 4m 30s\n",
      "188:\tlearn: 0.0023326\ttotal: 1m 2s\tremaining: 4m 30s\n",
      "189:\tlearn: 0.0023236\ttotal: 1m 3s\tremaining: 4m 29s\n",
      "190:\tlearn: 0.0023099\ttotal: 1m 3s\tremaining: 4m 29s\n",
      "191:\tlearn: 0.0023001\ttotal: 1m 3s\tremaining: 4m 29s\n",
      "192:\tlearn: 0.0022875\ttotal: 1m 4s\tremaining: 4m 28s\n",
      "193:\tlearn: 0.0022782\ttotal: 1m 4s\tremaining: 4m 28s\n",
      "194:\tlearn: 0.0022655\ttotal: 1m 4s\tremaining: 4m 28s\n",
      "195:\tlearn: 0.0022518\ttotal: 1m 5s\tremaining: 4m 27s\n",
      "196:\tlearn: 0.0022326\ttotal: 1m 5s\tremaining: 4m 27s\n",
      "197:\tlearn: 0.0022163\ttotal: 1m 5s\tremaining: 4m 27s\n",
      "198:\tlearn: 0.0022036\ttotal: 1m 6s\tremaining: 4m 26s\n",
      "199:\tlearn: 0.0021943\ttotal: 1m 6s\tremaining: 4m 26s\n",
      "200:\tlearn: 0.0021787\ttotal: 1m 6s\tremaining: 4m 26s\n",
      "201:\tlearn: 0.0021706\ttotal: 1m 7s\tremaining: 4m 25s\n",
      "202:\tlearn: 0.0021598\ttotal: 1m 7s\tremaining: 4m 25s\n",
      "203:\tlearn: 0.0021498\ttotal: 1m 7s\tremaining: 4m 24s\n",
      "204:\tlearn: 0.0021407\ttotal: 1m 8s\tremaining: 4m 24s\n",
      "205:\tlearn: 0.0021304\ttotal: 1m 8s\tremaining: 4m 24s\n",
      "206:\tlearn: 0.0021187\ttotal: 1m 8s\tremaining: 4m 23s\n",
      "207:\tlearn: 0.0021060\ttotal: 1m 9s\tremaining: 4m 23s\n",
      "208:\tlearn: 0.0021060\ttotal: 1m 9s\tremaining: 4m 23s\n",
      "209:\tlearn: 0.0020792\ttotal: 1m 9s\tremaining: 4m 22s\n",
      "210:\tlearn: 0.0020741\ttotal: 1m 10s\tremaining: 4m 22s\n",
      "211:\tlearn: 0.0020630\ttotal: 1m 10s\tremaining: 4m 22s\n",
      "212:\tlearn: 0.0020452\ttotal: 1m 10s\tremaining: 4m 21s\n",
      "213:\tlearn: 0.0020303\ttotal: 1m 11s\tremaining: 4m 21s\n",
      "214:\tlearn: 0.0020154\ttotal: 1m 11s\tremaining: 4m 21s\n",
      "215:\tlearn: 0.0020078\ttotal: 1m 11s\tremaining: 4m 20s\n",
      "216:\tlearn: 0.0019948\ttotal: 1m 12s\tremaining: 4m 20s\n",
      "217:\tlearn: 0.0019848\ttotal: 1m 12s\tremaining: 4m 20s\n",
      "218:\tlearn: 0.0019771\ttotal: 1m 12s\tremaining: 4m 19s\n",
      "219:\tlearn: 0.0019681\ttotal: 1m 13s\tremaining: 4m 19s\n",
      "220:\tlearn: 0.0019587\ttotal: 1m 13s\tremaining: 4m 18s\n",
      "221:\tlearn: 0.0019466\ttotal: 1m 13s\tremaining: 4m 18s\n",
      "222:\tlearn: 0.0019370\ttotal: 1m 14s\tremaining: 4m 18s\n",
      "223:\tlearn: 0.0019269\ttotal: 1m 14s\tremaining: 4m 17s\n",
      "224:\tlearn: 0.0019268\ttotal: 1m 14s\tremaining: 4m 17s\n",
      "225:\tlearn: 0.0019174\ttotal: 1m 15s\tremaining: 4m 17s\n",
      "226:\tlearn: 0.0019122\ttotal: 1m 15s\tremaining: 4m 16s\n",
      "227:\tlearn: 0.0019043\ttotal: 1m 15s\tremaining: 4m 16s\n",
      "228:\tlearn: 0.0018950\ttotal: 1m 16s\tremaining: 4m 16s\n",
      "229:\tlearn: 0.0018949\ttotal: 1m 16s\tremaining: 4m 15s\n",
      "230:\tlearn: 0.0018948\ttotal: 1m 16s\tremaining: 4m 15s\n",
      "231:\tlearn: 0.0018807\ttotal: 1m 17s\tremaining: 4m 15s\n",
      "232:\tlearn: 0.0018734\ttotal: 1m 17s\tremaining: 4m 14s\n",
      "233:\tlearn: 0.0018671\ttotal: 1m 17s\tremaining: 4m 14s\n",
      "234:\tlearn: 0.0018668\ttotal: 1m 18s\tremaining: 4m 14s\n",
      "235:\tlearn: 0.0018588\ttotal: 1m 18s\tremaining: 4m 13s\n",
      "236:\tlearn: 0.0018468\ttotal: 1m 18s\tremaining: 4m 13s\n",
      "237:\tlearn: 0.0018411\ttotal: 1m 19s\tremaining: 4m 13s\n",
      "238:\tlearn: 0.0018308\ttotal: 1m 19s\tremaining: 4m 12s\n",
      "239:\tlearn: 0.0018260\ttotal: 1m 19s\tremaining: 4m 12s\n",
      "240:\tlearn: 0.0018260\ttotal: 1m 20s\tremaining: 4m 12s\n",
      "241:\tlearn: 0.0018230\ttotal: 1m 20s\tremaining: 4m 11s\n",
      "242:\tlearn: 0.0018068\ttotal: 1m 20s\tremaining: 4m 11s\n",
      "243:\tlearn: 0.0018068\ttotal: 1m 21s\tremaining: 4m 10s\n",
      "244:\tlearn: 0.0017950\ttotal: 1m 21s\tremaining: 4m 10s\n",
      "245:\tlearn: 0.0017888\ttotal: 1m 21s\tremaining: 4m 10s\n",
      "246:\tlearn: 0.0017845\ttotal: 1m 21s\tremaining: 4m 9s\n",
      "247:\tlearn: 0.0017779\ttotal: 1m 22s\tremaining: 4m 9s\n",
      "248:\tlearn: 0.0017703\ttotal: 1m 22s\tremaining: 4m 9s\n",
      "249:\tlearn: 0.0017631\ttotal: 1m 22s\tremaining: 4m 8s\n",
      "250:\tlearn: 0.0017551\ttotal: 1m 23s\tremaining: 4m 8s\n",
      "251:\tlearn: 0.0017493\ttotal: 1m 23s\tremaining: 4m 8s\n",
      "252:\tlearn: 0.0017427\ttotal: 1m 23s\tremaining: 4m 7s\n",
      "253:\tlearn: 0.0017357\ttotal: 1m 24s\tremaining: 4m 7s\n",
      "254:\tlearn: 0.0017293\ttotal: 1m 24s\tremaining: 4m 7s\n",
      "255:\tlearn: 0.0017212\ttotal: 1m 24s\tremaining: 4m 6s\n",
      "256:\tlearn: 0.0017211\ttotal: 1m 25s\tremaining: 4m 6s\n",
      "257:\tlearn: 0.0017210\ttotal: 1m 25s\tremaining: 4m 6s\n",
      "258:\tlearn: 0.0017177\ttotal: 1m 25s\tremaining: 4m 5s\n",
      "259:\tlearn: 0.0017062\ttotal: 1m 26s\tremaining: 4m 5s\n",
      "260:\tlearn: 0.0017027\ttotal: 1m 26s\tremaining: 4m 5s\n",
      "261:\tlearn: 0.0016959\ttotal: 1m 26s\tremaining: 4m 4s\n",
      "262:\tlearn: 0.0016917\ttotal: 1m 27s\tremaining: 4m 4s\n",
      "263:\tlearn: 0.0016861\ttotal: 1m 27s\tremaining: 4m 4s\n",
      "264:\tlearn: 0.0016789\ttotal: 1m 27s\tremaining: 4m 3s\n",
      "265:\tlearn: 0.0016767\ttotal: 1m 28s\tremaining: 4m 3s\n",
      "266:\tlearn: 0.0016701\ttotal: 1m 28s\tremaining: 4m 3s\n",
      "267:\tlearn: 0.0016608\ttotal: 1m 28s\tremaining: 4m 2s\n",
      "268:\tlearn: 0.0016566\ttotal: 1m 29s\tremaining: 4m 2s\n",
      "269:\tlearn: 0.0016565\ttotal: 1m 29s\tremaining: 4m 1s\n",
      "270:\tlearn: 0.0016565\ttotal: 1m 29s\tremaining: 4m 1s\n",
      "271:\tlearn: 0.0016526\ttotal: 1m 30s\tremaining: 4m 1s\n",
      "272:\tlearn: 0.0016405\ttotal: 1m 30s\tremaining: 4m\n",
      "273:\tlearn: 0.0016404\ttotal: 1m 30s\tremaining: 4m\n",
      "274:\tlearn: 0.0016356\ttotal: 1m 31s\tremaining: 4m\n",
      "275:\tlearn: 0.0016297\ttotal: 1m 31s\tremaining: 3m 59s\n",
      "276:\tlearn: 0.0016213\ttotal: 1m 31s\tremaining: 3m 59s\n",
      "277:\tlearn: 0.0016212\ttotal: 1m 32s\tremaining: 3m 59s\n",
      "278:\tlearn: 0.0016176\ttotal: 1m 32s\tremaining: 3m 58s\n",
      "279:\tlearn: 0.0016175\ttotal: 1m 32s\tremaining: 3m 58s\n",
      "280:\tlearn: 0.0016096\ttotal: 1m 33s\tremaining: 3m 58s\n",
      "281:\tlearn: 0.0016037\ttotal: 1m 33s\tremaining: 3m 58s\n",
      "282:\tlearn: 0.0015966\ttotal: 1m 33s\tremaining: 3m 57s\n",
      "283:\tlearn: 0.0015910\ttotal: 1m 34s\tremaining: 3m 57s\n",
      "284:\tlearn: 0.0015908\ttotal: 1m 34s\tremaining: 3m 57s\n",
      "285:\tlearn: 0.0015908\ttotal: 1m 34s\tremaining: 3m 56s\n",
      "286:\tlearn: 0.0015908\ttotal: 1m 35s\tremaining: 3m 56s\n",
      "287:\tlearn: 0.0015908\ttotal: 1m 35s\tremaining: 3m 56s\n",
      "288:\tlearn: 0.0015834\ttotal: 1m 35s\tremaining: 3m 55s\n",
      "289:\tlearn: 0.0015768\ttotal: 1m 36s\tremaining: 3m 55s\n",
      "290:\tlearn: 0.0015716\ttotal: 1m 36s\tremaining: 3m 55s\n",
      "291:\tlearn: 0.0015663\ttotal: 1m 36s\tremaining: 3m 54s\n",
      "292:\tlearn: 0.0015663\ttotal: 1m 37s\tremaining: 3m 54s\n",
      "293:\tlearn: 0.0015662\ttotal: 1m 37s\tremaining: 3m 54s\n",
      "294:\tlearn: 0.0015613\ttotal: 1m 37s\tremaining: 3m 53s\n",
      "295:\tlearn: 0.0015554\ttotal: 1m 38s\tremaining: 3m 53s\n",
      "296:\tlearn: 0.0015522\ttotal: 1m 38s\tremaining: 3m 52s\n",
      "297:\tlearn: 0.0015522\ttotal: 1m 38s\tremaining: 3m 52s\n",
      "298:\tlearn: 0.0015522\ttotal: 1m 39s\tremaining: 3m 52s\n",
      "299:\tlearn: 0.0015522\ttotal: 1m 39s\tremaining: 3m 51s\n",
      "300:\tlearn: 0.0015494\ttotal: 1m 39s\tremaining: 3m 51s\n",
      "301:\tlearn: 0.0015494\ttotal: 1m 40s\tremaining: 3m 51s\n",
      "302:\tlearn: 0.0015494\ttotal: 1m 40s\tremaining: 3m 50s\n",
      "303:\tlearn: 0.0015494\ttotal: 1m 40s\tremaining: 3m 50s\n",
      "304:\tlearn: 0.0015420\ttotal: 1m 41s\tremaining: 3m 50s\n",
      "305:\tlearn: 0.0015418\ttotal: 1m 41s\tremaining: 3m 49s\n",
      "306:\tlearn: 0.0015370\ttotal: 1m 41s\tremaining: 3m 49s\n",
      "307:\tlearn: 0.0015307\ttotal: 1m 42s\tremaining: 3m 49s\n",
      "308:\tlearn: 0.0015268\ttotal: 1m 42s\tremaining: 3m 48s\n",
      "309:\tlearn: 0.0015268\ttotal: 1m 42s\tremaining: 3m 48s\n",
      "310:\tlearn: 0.0015194\ttotal: 1m 43s\tremaining: 3m 48s\n",
      "311:\tlearn: 0.0015135\ttotal: 1m 43s\tremaining: 3m 47s\n",
      "312:\tlearn: 0.0015135\ttotal: 1m 43s\tremaining: 3m 47s\n",
      "313:\tlearn: 0.0015079\ttotal: 1m 44s\tremaining: 3m 47s\n",
      "314:\tlearn: 0.0015025\ttotal: 1m 44s\tremaining: 3m 46s\n",
      "315:\tlearn: 0.0014945\ttotal: 1m 44s\tremaining: 3m 46s\n",
      "316:\tlearn: 0.0014943\ttotal: 1m 44s\tremaining: 3m 46s\n",
      "317:\tlearn: 0.0014943\ttotal: 1m 45s\tremaining: 3m 45s\n",
      "318:\tlearn: 0.0014943\ttotal: 1m 45s\tremaining: 3m 45s\n",
      "319:\tlearn: 0.0014943\ttotal: 1m 45s\tremaining: 3m 45s\n",
      "320:\tlearn: 0.0014905\ttotal: 1m 46s\tremaining: 3m 44s\n",
      "321:\tlearn: 0.0014866\ttotal: 1m 46s\tremaining: 3m 44s\n",
      "322:\tlearn: 0.0014866\ttotal: 1m 46s\tremaining: 3m 44s\n",
      "323:\tlearn: 0.0014866\ttotal: 1m 47s\tremaining: 3m 43s\n",
      "324:\tlearn: 0.0014866\ttotal: 1m 47s\tremaining: 3m 43s\n",
      "325:\tlearn: 0.0014822\ttotal: 1m 47s\tremaining: 3m 43s\n",
      "326:\tlearn: 0.0014769\ttotal: 1m 48s\tremaining: 3m 42s\n",
      "327:\tlearn: 0.0014768\ttotal: 1m 48s\tremaining: 3m 42s\n",
      "328:\tlearn: 0.0014768\ttotal: 1m 48s\tremaining: 3m 42s\n",
      "329:\tlearn: 0.0014768\ttotal: 1m 49s\tremaining: 3m 41s\n",
      "330:\tlearn: 0.0014718\ttotal: 1m 49s\tremaining: 3m 41s\n",
      "331:\tlearn: 0.0014718\ttotal: 1m 49s\tremaining: 3m 41s\n",
      "332:\tlearn: 0.0014666\ttotal: 1m 50s\tremaining: 3m 40s\n",
      "333:\tlearn: 0.0014666\ttotal: 1m 50s\tremaining: 3m 40s\n",
      "334:\tlearn: 0.0014665\ttotal: 1m 50s\tremaining: 3m 40s\n",
      "335:\tlearn: 0.0014617\ttotal: 1m 51s\tremaining: 3m 39s\n",
      "336:\tlearn: 0.0014617\ttotal: 1m 51s\tremaining: 3m 39s\n",
      "337:\tlearn: 0.0014566\ttotal: 1m 51s\tremaining: 3m 39s\n",
      "338:\tlearn: 0.0014522\ttotal: 1m 52s\tremaining: 3m 38s\n",
      "339:\tlearn: 0.0014480\ttotal: 1m 52s\tremaining: 3m 38s\n",
      "340:\tlearn: 0.0014424\ttotal: 1m 52s\tremaining: 3m 37s\n",
      "341:\tlearn: 0.0014424\ttotal: 1m 53s\tremaining: 3m 37s\n",
      "342:\tlearn: 0.0014424\ttotal: 1m 53s\tremaining: 3m 37s\n",
      "343:\tlearn: 0.0014424\ttotal: 1m 53s\tremaining: 3m 36s\n",
      "344:\tlearn: 0.0014412\ttotal: 1m 54s\tremaining: 3m 36s\n",
      "345:\tlearn: 0.0014411\ttotal: 1m 54s\tremaining: 3m 36s\n",
      "346:\tlearn: 0.0014380\ttotal: 1m 54s\tremaining: 3m 35s\n",
      "347:\tlearn: 0.0014379\ttotal: 1m 55s\tremaining: 3m 35s\n",
      "348:\tlearn: 0.0014379\ttotal: 1m 55s\tremaining: 3m 35s\n",
      "349:\tlearn: 0.0014379\ttotal: 1m 55s\tremaining: 3m 34s\n",
      "350:\tlearn: 0.0014357\ttotal: 1m 56s\tremaining: 3m 34s\n",
      "351:\tlearn: 0.0014356\ttotal: 1m 56s\tremaining: 3m 34s\n",
      "352:\tlearn: 0.0014355\ttotal: 1m 56s\tremaining: 3m 33s\n",
      "353:\tlearn: 0.0014355\ttotal: 1m 57s\tremaining: 3m 33s\n",
      "354:\tlearn: 0.0014355\ttotal: 1m 57s\tremaining: 3m 33s\n",
      "355:\tlearn: 0.0014353\ttotal: 1m 57s\tremaining: 3m 32s\n",
      "356:\tlearn: 0.0014325\ttotal: 1m 57s\tremaining: 3m 32s\n",
      "357:\tlearn: 0.0014311\ttotal: 1m 58s\tremaining: 3m 32s\n",
      "358:\tlearn: 0.0014310\ttotal: 1m 58s\tremaining: 3m 31s\n",
      "359:\tlearn: 0.0014279\ttotal: 1m 58s\tremaining: 3m 31s\n",
      "360:\tlearn: 0.0014262\ttotal: 1m 59s\tremaining: 3m 31s\n",
      "361:\tlearn: 0.0014262\ttotal: 1m 59s\tremaining: 3m 30s\n",
      "362:\tlearn: 0.0014262\ttotal: 1m 59s\tremaining: 3m 30s\n",
      "363:\tlearn: 0.0014261\ttotal: 2m\tremaining: 3m 30s\n",
      "364:\tlearn: 0.0014261\ttotal: 2m\tremaining: 3m 29s\n",
      "365:\tlearn: 0.0014260\ttotal: 2m\tremaining: 3m 29s\n",
      "366:\tlearn: 0.0014215\ttotal: 2m 1s\tremaining: 3m 29s\n",
      "367:\tlearn: 0.0014168\ttotal: 2m 1s\tremaining: 3m 28s\n",
      "368:\tlearn: 0.0014110\ttotal: 2m 1s\tremaining: 3m 28s\n",
      "369:\tlearn: 0.0014076\ttotal: 2m 2s\tremaining: 3m 28s\n",
      "370:\tlearn: 0.0014035\ttotal: 2m 2s\tremaining: 3m 27s\n",
      "371:\tlearn: 0.0013994\ttotal: 2m 2s\tremaining: 3m 27s\n",
      "372:\tlearn: 0.0013950\ttotal: 2m 3s\tremaining: 3m 27s\n",
      "373:\tlearn: 0.0013950\ttotal: 2m 3s\tremaining: 3m 26s\n",
      "374:\tlearn: 0.0013898\ttotal: 2m 3s\tremaining: 3m 26s\n",
      "375:\tlearn: 0.0013898\ttotal: 2m 4s\tremaining: 3m 26s\n",
      "376:\tlearn: 0.0013897\ttotal: 2m 4s\tremaining: 3m 25s\n",
      "377:\tlearn: 0.0013822\ttotal: 2m 4s\tremaining: 3m 25s\n",
      "378:\tlearn: 0.0013822\ttotal: 2m 5s\tremaining: 3m 25s\n",
      "379:\tlearn: 0.0013779\ttotal: 2m 5s\tremaining: 3m 24s\n",
      "380:\tlearn: 0.0013777\ttotal: 2m 5s\tremaining: 3m 24s\n",
      "381:\tlearn: 0.0013777\ttotal: 2m 6s\tremaining: 3m 24s\n",
      "382:\tlearn: 0.0013777\ttotal: 2m 6s\tremaining: 3m 23s\n",
      "383:\tlearn: 0.0013777\ttotal: 2m 6s\tremaining: 3m 23s\n",
      "384:\tlearn: 0.0013776\ttotal: 2m 7s\tremaining: 3m 23s\n",
      "385:\tlearn: 0.0013775\ttotal: 2m 7s\tremaining: 3m 22s\n",
      "386:\tlearn: 0.0013774\ttotal: 2m 7s\tremaining: 3m 22s\n",
      "387:\tlearn: 0.0013730\ttotal: 2m 8s\tremaining: 3m 22s\n",
      "388:\tlearn: 0.0013680\ttotal: 2m 8s\tremaining: 3m 21s\n",
      "389:\tlearn: 0.0013655\ttotal: 2m 8s\tremaining: 3m 21s\n",
      "390:\tlearn: 0.0013644\ttotal: 2m 9s\tremaining: 3m 21s\n",
      "391:\tlearn: 0.0013632\ttotal: 2m 9s\tremaining: 3m 20s\n",
      "392:\tlearn: 0.0013632\ttotal: 2m 9s\tremaining: 3m 20s\n",
      "393:\tlearn: 0.0013632\ttotal: 2m 10s\tremaining: 3m 20s\n",
      "394:\tlearn: 0.0013631\ttotal: 2m 10s\tremaining: 3m 19s\n",
      "395:\tlearn: 0.0013631\ttotal: 2m 10s\tremaining: 3m 19s\n",
      "396:\tlearn: 0.0013630\ttotal: 2m 11s\tremaining: 3m 19s\n",
      "397:\tlearn: 0.0013627\ttotal: 2m 11s\tremaining: 3m 18s\n",
      "398:\tlearn: 0.0013578\ttotal: 2m 11s\tremaining: 3m 18s\n",
      "399:\tlearn: 0.0013568\ttotal: 2m 12s\tremaining: 3m 18s\n",
      "400:\tlearn: 0.0013560\ttotal: 2m 12s\tremaining: 3m 17s\n",
      "401:\tlearn: 0.0013517\ttotal: 2m 12s\tremaining: 3m 17s\n",
      "402:\tlearn: 0.0013517\ttotal: 2m 13s\tremaining: 3m 17s\n",
      "403:\tlearn: 0.0013483\ttotal: 2m 13s\tremaining: 3m 16s\n",
      "404:\tlearn: 0.0013446\ttotal: 2m 13s\tremaining: 3m 16s\n",
      "405:\tlearn: 0.0013445\ttotal: 2m 14s\tremaining: 3m 16s\n",
      "406:\tlearn: 0.0013444\ttotal: 2m 14s\tremaining: 3m 15s\n",
      "407:\tlearn: 0.0013392\ttotal: 2m 14s\tremaining: 3m 15s\n",
      "408:\tlearn: 0.0013392\ttotal: 2m 14s\tremaining: 3m 15s\n",
      "409:\tlearn: 0.0013390\ttotal: 2m 15s\tremaining: 3m 14s\n",
      "410:\tlearn: 0.0013388\ttotal: 2m 15s\tremaining: 3m 14s\n",
      "411:\tlearn: 0.0013387\ttotal: 2m 15s\tremaining: 3m 14s\n",
      "412:\tlearn: 0.0013387\ttotal: 2m 16s\tremaining: 3m 13s\n",
      "413:\tlearn: 0.0013386\ttotal: 2m 16s\tremaining: 3m 13s\n",
      "414:\tlearn: 0.0013386\ttotal: 2m 16s\tremaining: 3m 13s\n",
      "415:\tlearn: 0.0013379\ttotal: 2m 17s\tremaining: 3m 12s\n",
      "416:\tlearn: 0.0013378\ttotal: 2m 17s\tremaining: 3m 12s\n",
      "417:\tlearn: 0.0013343\ttotal: 2m 17s\tremaining: 3m 12s\n",
      "418:\tlearn: 0.0013297\ttotal: 2m 18s\tremaining: 3m 11s\n",
      "419:\tlearn: 0.0013296\ttotal: 2m 18s\tremaining: 3m 11s\n",
      "420:\tlearn: 0.0013295\ttotal: 2m 18s\tremaining: 3m 11s\n",
      "421:\tlearn: 0.0013293\ttotal: 2m 19s\tremaining: 3m 10s\n",
      "422:\tlearn: 0.0013291\ttotal: 2m 19s\tremaining: 3m 10s\n",
      "423:\tlearn: 0.0013260\ttotal: 2m 19s\tremaining: 3m 10s\n",
      "424:\tlearn: 0.0013226\ttotal: 2m 20s\tremaining: 3m 9s\n",
      "425:\tlearn: 0.0013226\ttotal: 2m 20s\tremaining: 3m 9s\n",
      "426:\tlearn: 0.0013226\ttotal: 2m 20s\tremaining: 3m 9s\n",
      "427:\tlearn: 0.0013226\ttotal: 2m 21s\tremaining: 3m 8s\n",
      "428:\tlearn: 0.0013226\ttotal: 2m 21s\tremaining: 3m 8s\n",
      "429:\tlearn: 0.0013226\ttotal: 2m 21s\tremaining: 3m 8s\n",
      "430:\tlearn: 0.0013226\ttotal: 2m 22s\tremaining: 3m 7s\n",
      "431:\tlearn: 0.0013226\ttotal: 2m 22s\tremaining: 3m 7s\n",
      "432:\tlearn: 0.0013225\ttotal: 2m 22s\tremaining: 3m 7s\n",
      "433:\tlearn: 0.0013225\ttotal: 2m 23s\tremaining: 3m 6s\n",
      "434:\tlearn: 0.0013175\ttotal: 2m 23s\tremaining: 3m 6s\n",
      "435:\tlearn: 0.0013142\ttotal: 2m 23s\tremaining: 3m 6s\n",
      "436:\tlearn: 0.0013142\ttotal: 2m 24s\tremaining: 3m 5s\n",
      "437:\tlearn: 0.0013098\ttotal: 2m 24s\tremaining: 3m 5s\n",
      "438:\tlearn: 0.0013098\ttotal: 2m 24s\tremaining: 3m 5s\n",
      "439:\tlearn: 0.0013097\ttotal: 2m 25s\tremaining: 3m 4s\n",
      "440:\tlearn: 0.0013097\ttotal: 2m 25s\tremaining: 3m 4s\n",
      "441:\tlearn: 0.0013097\ttotal: 2m 25s\tremaining: 3m 4s\n",
      "442:\tlearn: 0.0013079\ttotal: 2m 26s\tremaining: 3m 3s\n",
      "443:\tlearn: 0.0013079\ttotal: 2m 26s\tremaining: 3m 3s\n",
      "444:\tlearn: 0.0013048\ttotal: 2m 26s\tremaining: 3m 3s\n",
      "445:\tlearn: 0.0013009\ttotal: 2m 27s\tremaining: 3m 2s\n",
      "446:\tlearn: 0.0012990\ttotal: 2m 27s\tremaining: 3m 2s\n",
      "447:\tlearn: 0.0012979\ttotal: 2m 27s\tremaining: 3m 2s\n",
      "448:\tlearn: 0.0012979\ttotal: 2m 28s\tremaining: 3m 1s\n",
      "449:\tlearn: 0.0012978\ttotal: 2m 28s\tremaining: 3m 1s\n",
      "450:\tlearn: 0.0012978\ttotal: 2m 28s\tremaining: 3m 1s\n",
      "451:\tlearn: 0.0012978\ttotal: 2m 29s\tremaining: 3m\n",
      "452:\tlearn: 0.0012977\ttotal: 2m 29s\tremaining: 3m\n",
      "453:\tlearn: 0.0012977\ttotal: 2m 29s\tremaining: 3m\n",
      "454:\tlearn: 0.0012977\ttotal: 2m 30s\tremaining: 2m 59s\n",
      "455:\tlearn: 0.0012977\ttotal: 2m 30s\tremaining: 2m 59s\n",
      "456:\tlearn: 0.0012977\ttotal: 2m 30s\tremaining: 2m 59s\n",
      "457:\tlearn: 0.0012936\ttotal: 2m 31s\tremaining: 2m 58s\n",
      "458:\tlearn: 0.0012935\ttotal: 2m 31s\tremaining: 2m 58s\n",
      "459:\tlearn: 0.0012924\ttotal: 2m 31s\tremaining: 2m 58s\n",
      "460:\tlearn: 0.0012924\ttotal: 2m 31s\tremaining: 2m 57s\n",
      "461:\tlearn: 0.0012924\ttotal: 2m 32s\tremaining: 2m 57s\n",
      "462:\tlearn: 0.0012923\ttotal: 2m 32s\tremaining: 2m 57s\n",
      "463:\tlearn: 0.0012923\ttotal: 2m 32s\tremaining: 2m 56s\n",
      "464:\tlearn: 0.0012922\ttotal: 2m 33s\tremaining: 2m 56s\n",
      "465:\tlearn: 0.0012922\ttotal: 2m 33s\tremaining: 2m 56s\n",
      "466:\tlearn: 0.0012919\ttotal: 2m 33s\tremaining: 2m 55s\n",
      "467:\tlearn: 0.0012899\ttotal: 2m 34s\tremaining: 2m 55s\n",
      "468:\tlearn: 0.0012899\ttotal: 2m 34s\tremaining: 2m 55s\n",
      "469:\tlearn: 0.0012857\ttotal: 2m 34s\tremaining: 2m 54s\n",
      "470:\tlearn: 0.0012856\ttotal: 2m 35s\tremaining: 2m 54s\n",
      "471:\tlearn: 0.0012839\ttotal: 2m 35s\tremaining: 2m 54s\n",
      "472:\tlearn: 0.0012839\ttotal: 2m 35s\tremaining: 2m 53s\n",
      "473:\tlearn: 0.0012837\ttotal: 2m 36s\tremaining: 2m 53s\n",
      "474:\tlearn: 0.0012837\ttotal: 2m 36s\tremaining: 2m 53s\n",
      "475:\tlearn: 0.0012811\ttotal: 2m 36s\tremaining: 2m 52s\n",
      "476:\tlearn: 0.0012810\ttotal: 2m 37s\tremaining: 2m 52s\n",
      "477:\tlearn: 0.0012810\ttotal: 2m 37s\tremaining: 2m 52s\n",
      "478:\tlearn: 0.0012796\ttotal: 2m 37s\tremaining: 2m 51s\n",
      "479:\tlearn: 0.0012796\ttotal: 2m 38s\tremaining: 2m 51s\n",
      "480:\tlearn: 0.0012796\ttotal: 2m 38s\tremaining: 2m 51s\n",
      "481:\tlearn: 0.0012779\ttotal: 2m 38s\tremaining: 2m 50s\n",
      "482:\tlearn: 0.0012738\ttotal: 2m 39s\tremaining: 2m 50s\n",
      "483:\tlearn: 0.0012738\ttotal: 2m 39s\tremaining: 2m 50s\n",
      "484:\tlearn: 0.0012737\ttotal: 2m 39s\tremaining: 2m 49s\n",
      "485:\tlearn: 0.0012736\ttotal: 2m 40s\tremaining: 2m 49s\n",
      "486:\tlearn: 0.0012733\ttotal: 2m 40s\tremaining: 2m 49s\n",
      "487:\tlearn: 0.0012703\ttotal: 2m 40s\tremaining: 2m 48s\n",
      "488:\tlearn: 0.0012703\ttotal: 2m 41s\tremaining: 2m 48s\n",
      "489:\tlearn: 0.0012701\ttotal: 2m 41s\tremaining: 2m 48s\n",
      "490:\tlearn: 0.0012669\ttotal: 2m 41s\tremaining: 2m 47s\n",
      "491:\tlearn: 0.0012638\ttotal: 2m 42s\tremaining: 2m 47s\n",
      "492:\tlearn: 0.0012611\ttotal: 2m 42s\tremaining: 2m 47s\n",
      "493:\tlearn: 0.0012610\ttotal: 2m 42s\tremaining: 2m 46s\n",
      "494:\tlearn: 0.0012610\ttotal: 2m 43s\tremaining: 2m 46s\n",
      "495:\tlearn: 0.0012607\ttotal: 2m 43s\tremaining: 2m 46s\n",
      "496:\tlearn: 0.0012607\ttotal: 2m 43s\tremaining: 2m 45s\n",
      "497:\tlearn: 0.0012607\ttotal: 2m 44s\tremaining: 2m 45s\n",
      "498:\tlearn: 0.0012583\ttotal: 2m 44s\tremaining: 2m 45s\n",
      "499:\tlearn: 0.0012583\ttotal: 2m 44s\tremaining: 2m 44s\n",
      "500:\tlearn: 0.0012583\ttotal: 2m 45s\tremaining: 2m 44s\n",
      "501:\tlearn: 0.0012583\ttotal: 2m 45s\tremaining: 2m 44s\n",
      "502:\tlearn: 0.0012583\ttotal: 2m 45s\tremaining: 2m 43s\n",
      "503:\tlearn: 0.0012582\ttotal: 2m 46s\tremaining: 2m 43s\n",
      "504:\tlearn: 0.0012582\ttotal: 2m 46s\tremaining: 2m 43s\n",
      "505:\tlearn: 0.0012548\ttotal: 2m 46s\tremaining: 2m 42s\n",
      "506:\tlearn: 0.0012547\ttotal: 2m 47s\tremaining: 2m 42s\n",
      "507:\tlearn: 0.0012546\ttotal: 2m 47s\tremaining: 2m 42s\n",
      "508:\tlearn: 0.0012545\ttotal: 2m 47s\tremaining: 2m 41s\n",
      "509:\tlearn: 0.0012509\ttotal: 2m 48s\tremaining: 2m 41s\n",
      "510:\tlearn: 0.0012509\ttotal: 2m 48s\tremaining: 2m 41s\n",
      "511:\tlearn: 0.0012508\ttotal: 2m 48s\tremaining: 2m 40s\n",
      "512:\tlearn: 0.0012473\ttotal: 2m 48s\tremaining: 2m 40s\n",
      "513:\tlearn: 0.0012472\ttotal: 2m 49s\tremaining: 2m 40s\n",
      "514:\tlearn: 0.0012471\ttotal: 2m 49s\tremaining: 2m 39s\n",
      "515:\tlearn: 0.0012471\ttotal: 2m 49s\tremaining: 2m 39s\n",
      "516:\tlearn: 0.0012471\ttotal: 2m 50s\tremaining: 2m 39s\n",
      "517:\tlearn: 0.0012470\ttotal: 2m 50s\tremaining: 2m 38s\n",
      "518:\tlearn: 0.0012470\ttotal: 2m 50s\tremaining: 2m 38s\n",
      "519:\tlearn: 0.0012432\ttotal: 2m 51s\tremaining: 2m 38s\n",
      "520:\tlearn: 0.0012432\ttotal: 2m 51s\tremaining: 2m 37s\n",
      "521:\tlearn: 0.0012432\ttotal: 2m 51s\tremaining: 2m 37s\n",
      "522:\tlearn: 0.0012432\ttotal: 2m 52s\tremaining: 2m 37s\n",
      "523:\tlearn: 0.0012431\ttotal: 2m 52s\tremaining: 2m 36s\n",
      "524:\tlearn: 0.0012429\ttotal: 2m 52s\tremaining: 2m 36s\n",
      "525:\tlearn: 0.0012429\ttotal: 2m 53s\tremaining: 2m 36s\n",
      "526:\tlearn: 0.0012429\ttotal: 2m 53s\tremaining: 2m 35s\n",
      "527:\tlearn: 0.0012429\ttotal: 2m 53s\tremaining: 2m 35s\n",
      "528:\tlearn: 0.0012429\ttotal: 2m 54s\tremaining: 2m 35s\n",
      "529:\tlearn: 0.0012426\ttotal: 2m 54s\tremaining: 2m 34s\n",
      "530:\tlearn: 0.0012426\ttotal: 2m 54s\tremaining: 2m 34s\n",
      "531:\tlearn: 0.0012388\ttotal: 2m 55s\tremaining: 2m 34s\n",
      "532:\tlearn: 0.0012375\ttotal: 2m 55s\tremaining: 2m 33s\n",
      "533:\tlearn: 0.0012349\ttotal: 2m 55s\tremaining: 2m 33s\n",
      "534:\tlearn: 0.0012349\ttotal: 2m 56s\tremaining: 2m 33s\n",
      "535:\tlearn: 0.0012348\ttotal: 2m 56s\tremaining: 2m 32s\n",
      "536:\tlearn: 0.0012348\ttotal: 2m 56s\tremaining: 2m 32s\n",
      "537:\tlearn: 0.0012348\ttotal: 2m 57s\tremaining: 2m 32s\n",
      "538:\tlearn: 0.0012347\ttotal: 2m 57s\tremaining: 2m 31s\n",
      "539:\tlearn: 0.0012273\ttotal: 2m 57s\tremaining: 2m 31s\n",
      "540:\tlearn: 0.0012241\ttotal: 2m 58s\tremaining: 2m 31s\n",
      "541:\tlearn: 0.0012225\ttotal: 2m 58s\tremaining: 2m 30s\n",
      "542:\tlearn: 0.0012224\ttotal: 2m 58s\tremaining: 2m 30s\n",
      "543:\tlearn: 0.0012213\ttotal: 2m 59s\tremaining: 2m 30s\n",
      "544:\tlearn: 0.0012213\ttotal: 2m 59s\tremaining: 2m 29s\n",
      "545:\tlearn: 0.0012193\ttotal: 2m 59s\tremaining: 2m 29s\n",
      "546:\tlearn: 0.0012174\ttotal: 3m\tremaining: 2m 29s\n",
      "547:\tlearn: 0.0012174\ttotal: 3m\tremaining: 2m 28s\n",
      "548:\tlearn: 0.0012174\ttotal: 3m\tremaining: 2m 28s\n",
      "549:\tlearn: 0.0012174\ttotal: 3m 1s\tremaining: 2m 28s\n",
      "550:\tlearn: 0.0012174\ttotal: 3m 1s\tremaining: 2m 27s\n",
      "551:\tlearn: 0.0012174\ttotal: 3m 1s\tremaining: 2m 27s\n",
      "552:\tlearn: 0.0012174\ttotal: 3m 2s\tremaining: 2m 27s\n",
      "553:\tlearn: 0.0012172\ttotal: 3m 2s\tremaining: 2m 26s\n",
      "554:\tlearn: 0.0012172\ttotal: 3m 2s\tremaining: 2m 26s\n",
      "555:\tlearn: 0.0012171\ttotal: 3m 3s\tremaining: 2m 26s\n",
      "556:\tlearn: 0.0012127\ttotal: 3m 3s\tremaining: 2m 25s\n",
      "557:\tlearn: 0.0012079\ttotal: 3m 3s\tremaining: 2m 25s\n",
      "558:\tlearn: 0.0012078\ttotal: 3m 4s\tremaining: 2m 25s\n",
      "559:\tlearn: 0.0012078\ttotal: 3m 4s\tremaining: 2m 24s\n",
      "560:\tlearn: 0.0012078\ttotal: 3m 4s\tremaining: 2m 24s\n",
      "561:\tlearn: 0.0012075\ttotal: 3m 4s\tremaining: 2m 24s\n",
      "562:\tlearn: 0.0012074\ttotal: 3m 5s\tremaining: 2m 23s\n",
      "563:\tlearn: 0.0012074\ttotal: 3m 5s\tremaining: 2m 23s\n",
      "564:\tlearn: 0.0012074\ttotal: 3m 5s\tremaining: 2m 23s\n",
      "565:\tlearn: 0.0012021\ttotal: 3m 6s\tremaining: 2m 22s\n",
      "566:\tlearn: 0.0012019\ttotal: 3m 6s\tremaining: 2m 22s\n",
      "567:\tlearn: 0.0012018\ttotal: 3m 6s\tremaining: 2m 22s\n",
      "568:\tlearn: 0.0012018\ttotal: 3m 7s\tremaining: 2m 21s\n",
      "569:\tlearn: 0.0011998\ttotal: 3m 7s\tremaining: 2m 21s\n",
      "570:\tlearn: 0.0011998\ttotal: 3m 7s\tremaining: 2m 21s\n",
      "571:\tlearn: 0.0011996\ttotal: 3m 8s\tremaining: 2m 20s\n",
      "572:\tlearn: 0.0011994\ttotal: 3m 8s\tremaining: 2m 20s\n",
      "573:\tlearn: 0.0011994\ttotal: 3m 8s\tremaining: 2m 20s\n",
      "574:\tlearn: 0.0011991\ttotal: 3m 9s\tremaining: 2m 19s\n",
      "575:\tlearn: 0.0011988\ttotal: 3m 9s\tremaining: 2m 19s\n",
      "576:\tlearn: 0.0011987\ttotal: 3m 9s\tremaining: 2m 19s\n",
      "577:\tlearn: 0.0011986\ttotal: 3m 10s\tremaining: 2m 18s\n",
      "578:\tlearn: 0.0011986\ttotal: 3m 10s\tremaining: 2m 18s\n",
      "579:\tlearn: 0.0011983\ttotal: 3m 10s\tremaining: 2m 18s\n",
      "580:\tlearn: 0.0011957\ttotal: 3m 11s\tremaining: 2m 17s\n",
      "581:\tlearn: 0.0011956\ttotal: 3m 11s\tremaining: 2m 17s\n",
      "582:\tlearn: 0.0011916\ttotal: 3m 11s\tremaining: 2m 17s\n",
      "583:\tlearn: 0.0011916\ttotal: 3m 12s\tremaining: 2m 16s\n",
      "584:\tlearn: 0.0011915\ttotal: 3m 12s\tremaining: 2m 16s\n",
      "585:\tlearn: 0.0011883\ttotal: 3m 12s\tremaining: 2m 16s\n",
      "586:\tlearn: 0.0011882\ttotal: 3m 13s\tremaining: 2m 15s\n",
      "587:\tlearn: 0.0011811\ttotal: 3m 13s\tremaining: 2m 15s\n",
      "588:\tlearn: 0.0011811\ttotal: 3m 13s\tremaining: 2m 15s\n",
      "589:\tlearn: 0.0011809\ttotal: 3m 14s\tremaining: 2m 14s\n",
      "590:\tlearn: 0.0011809\ttotal: 3m 14s\tremaining: 2m 14s\n",
      "591:\tlearn: 0.0011809\ttotal: 3m 14s\tremaining: 2m 14s\n",
      "592:\tlearn: 0.0011802\ttotal: 3m 15s\tremaining: 2m 13s\n",
      "593:\tlearn: 0.0011802\ttotal: 3m 15s\tremaining: 2m 13s\n",
      "594:\tlearn: 0.0011801\ttotal: 3m 15s\tremaining: 2m 13s\n",
      "595:\tlearn: 0.0011801\ttotal: 3m 16s\tremaining: 2m 12s\n",
      "596:\tlearn: 0.0011772\ttotal: 3m 16s\tremaining: 2m 12s\n",
      "597:\tlearn: 0.0011754\ttotal: 3m 16s\tremaining: 2m 12s\n",
      "598:\tlearn: 0.0011754\ttotal: 3m 17s\tremaining: 2m 11s\n",
      "599:\tlearn: 0.0011754\ttotal: 3m 17s\tremaining: 2m 11s\n",
      "600:\tlearn: 0.0011753\ttotal: 3m 17s\tremaining: 2m 11s\n",
      "601:\tlearn: 0.0011753\ttotal: 3m 18s\tremaining: 2m 10s\n",
      "602:\tlearn: 0.0011753\ttotal: 3m 18s\tremaining: 2m 10s\n",
      "603:\tlearn: 0.0011751\ttotal: 3m 18s\tremaining: 2m 10s\n",
      "604:\tlearn: 0.0011751\ttotal: 3m 19s\tremaining: 2m 9s\n",
      "605:\tlearn: 0.0011751\ttotal: 3m 19s\tremaining: 2m 9s\n",
      "606:\tlearn: 0.0011751\ttotal: 3m 19s\tremaining: 2m 9s\n",
      "607:\tlearn: 0.0011750\ttotal: 3m 19s\tremaining: 2m 8s\n",
      "608:\tlearn: 0.0011750\ttotal: 3m 20s\tremaining: 2m 8s\n",
      "609:\tlearn: 0.0011750\ttotal: 3m 20s\tremaining: 2m 8s\n",
      "610:\tlearn: 0.0011748\ttotal: 3m 20s\tremaining: 2m 7s\n",
      "611:\tlearn: 0.0011748\ttotal: 3m 21s\tremaining: 2m 7s\n",
      "612:\tlearn: 0.0011748\ttotal: 3m 21s\tremaining: 2m 7s\n",
      "613:\tlearn: 0.0011748\ttotal: 3m 21s\tremaining: 2m 6s\n",
      "614:\tlearn: 0.0011748\ttotal: 3m 22s\tremaining: 2m 6s\n",
      "615:\tlearn: 0.0011747\ttotal: 3m 22s\tremaining: 2m 6s\n",
      "616:\tlearn: 0.0011746\ttotal: 3m 22s\tremaining: 2m 5s\n",
      "617:\tlearn: 0.0011746\ttotal: 3m 23s\tremaining: 2m 5s\n",
      "618:\tlearn: 0.0011728\ttotal: 3m 23s\tremaining: 2m 5s\n",
      "619:\tlearn: 0.0011728\ttotal: 3m 23s\tremaining: 2m 4s\n",
      "620:\tlearn: 0.0011710\ttotal: 3m 24s\tremaining: 2m 4s\n",
      "621:\tlearn: 0.0011710\ttotal: 3m 24s\tremaining: 2m 4s\n",
      "622:\tlearn: 0.0011709\ttotal: 3m 24s\tremaining: 2m 3s\n",
      "623:\tlearn: 0.0011709\ttotal: 3m 25s\tremaining: 2m 3s\n",
      "624:\tlearn: 0.0011709\ttotal: 3m 25s\tremaining: 2m 3s\n",
      "625:\tlearn: 0.0011708\ttotal: 3m 25s\tremaining: 2m 2s\n",
      "626:\tlearn: 0.0011708\ttotal: 3m 26s\tremaining: 2m 2s\n",
      "627:\tlearn: 0.0011683\ttotal: 3m 26s\tremaining: 2m 2s\n",
      "628:\tlearn: 0.0011681\ttotal: 3m 26s\tremaining: 2m 1s\n",
      "629:\tlearn: 0.0011680\ttotal: 3m 27s\tremaining: 2m 1s\n",
      "630:\tlearn: 0.0011680\ttotal: 3m 27s\tremaining: 2m 1s\n",
      "631:\tlearn: 0.0011680\ttotal: 3m 27s\tremaining: 2m\n",
      "632:\tlearn: 0.0011680\ttotal: 3m 28s\tremaining: 2m\n",
      "633:\tlearn: 0.0011680\ttotal: 3m 28s\tremaining: 2m\n",
      "634:\tlearn: 0.0011680\ttotal: 3m 28s\tremaining: 2m\n",
      "635:\tlearn: 0.0011679\ttotal: 3m 29s\tremaining: 1m 59s\n",
      "636:\tlearn: 0.0011678\ttotal: 3m 29s\tremaining: 1m 59s\n",
      "637:\tlearn: 0.0011678\ttotal: 3m 29s\tremaining: 1m 59s\n",
      "638:\tlearn: 0.0011678\ttotal: 3m 30s\tremaining: 1m 58s\n",
      "639:\tlearn: 0.0011678\ttotal: 3m 30s\tremaining: 1m 58s\n",
      "640:\tlearn: 0.0011678\ttotal: 3m 30s\tremaining: 1m 58s\n",
      "641:\tlearn: 0.0011664\ttotal: 3m 31s\tremaining: 1m 57s\n",
      "642:\tlearn: 0.0011663\ttotal: 3m 31s\tremaining: 1m 57s\n",
      "643:\tlearn: 0.0011663\ttotal: 3m 31s\tremaining: 1m 57s\n",
      "644:\tlearn: 0.0011662\ttotal: 3m 32s\tremaining: 1m 56s\n",
      "645:\tlearn: 0.0011644\ttotal: 3m 32s\tremaining: 1m 56s\n",
      "646:\tlearn: 0.0011644\ttotal: 3m 32s\tremaining: 1m 56s\n",
      "647:\tlearn: 0.0011642\ttotal: 3m 33s\tremaining: 1m 55s\n",
      "648:\tlearn: 0.0011642\ttotal: 3m 33s\tremaining: 1m 55s\n",
      "649:\tlearn: 0.0011639\ttotal: 3m 33s\tremaining: 1m 55s\n",
      "650:\tlearn: 0.0011606\ttotal: 3m 34s\tremaining: 1m 54s\n",
      "651:\tlearn: 0.0011570\ttotal: 3m 34s\tremaining: 1m 54s\n",
      "652:\tlearn: 0.0011570\ttotal: 3m 34s\tremaining: 1m 54s\n",
      "653:\tlearn: 0.0011547\ttotal: 3m 34s\tremaining: 1m 53s\n",
      "654:\tlearn: 0.0011546\ttotal: 3m 35s\tremaining: 1m 53s\n",
      "655:\tlearn: 0.0011546\ttotal: 3m 35s\tremaining: 1m 53s\n",
      "656:\tlearn: 0.0011546\ttotal: 3m 35s\tremaining: 1m 52s\n",
      "657:\tlearn: 0.0011546\ttotal: 3m 36s\tremaining: 1m 52s\n",
      "658:\tlearn: 0.0011545\ttotal: 3m 36s\tremaining: 1m 52s\n",
      "659:\tlearn: 0.0011545\ttotal: 3m 36s\tremaining: 1m 51s\n",
      "660:\tlearn: 0.0011533\ttotal: 3m 37s\tremaining: 1m 51s\n",
      "661:\tlearn: 0.0011518\ttotal: 3m 37s\tremaining: 1m 51s\n",
      "662:\tlearn: 0.0011516\ttotal: 3m 37s\tremaining: 1m 50s\n",
      "663:\tlearn: 0.0011514\ttotal: 3m 38s\tremaining: 1m 50s\n",
      "664:\tlearn: 0.0011514\ttotal: 3m 38s\tremaining: 1m 50s\n",
      "665:\tlearn: 0.0011514\ttotal: 3m 38s\tremaining: 1m 49s\n",
      "666:\tlearn: 0.0011514\ttotal: 3m 39s\tremaining: 1m 49s\n",
      "667:\tlearn: 0.0011514\ttotal: 3m 39s\tremaining: 1m 49s\n",
      "668:\tlearn: 0.0011512\ttotal: 3m 39s\tremaining: 1m 48s\n",
      "669:\tlearn: 0.0011510\ttotal: 3m 40s\tremaining: 1m 48s\n",
      "670:\tlearn: 0.0011510\ttotal: 3m 40s\tremaining: 1m 48s\n",
      "671:\tlearn: 0.0011509\ttotal: 3m 40s\tremaining: 1m 47s\n",
      "672:\tlearn: 0.0011509\ttotal: 3m 41s\tremaining: 1m 47s\n",
      "673:\tlearn: 0.0011508\ttotal: 3m 41s\tremaining: 1m 47s\n",
      "674:\tlearn: 0.0011508\ttotal: 3m 41s\tremaining: 1m 46s\n",
      "675:\tlearn: 0.0011508\ttotal: 3m 42s\tremaining: 1m 46s\n",
      "676:\tlearn: 0.0011508\ttotal: 3m 42s\tremaining: 1m 46s\n",
      "677:\tlearn: 0.0011508\ttotal: 3m 42s\tremaining: 1m 45s\n",
      "678:\tlearn: 0.0011507\ttotal: 3m 43s\tremaining: 1m 45s\n",
      "679:\tlearn: 0.0011506\ttotal: 3m 43s\tremaining: 1m 45s\n",
      "680:\tlearn: 0.0011505\ttotal: 3m 43s\tremaining: 1m 44s\n",
      "681:\tlearn: 0.0011504\ttotal: 3m 44s\tremaining: 1m 44s\n",
      "682:\tlearn: 0.0011504\ttotal: 3m 44s\tremaining: 1m 44s\n",
      "683:\tlearn: 0.0011504\ttotal: 3m 44s\tremaining: 1m 43s\n",
      "684:\tlearn: 0.0011504\ttotal: 3m 45s\tremaining: 1m 43s\n",
      "685:\tlearn: 0.0011504\ttotal: 3m 45s\tremaining: 1m 43s\n",
      "686:\tlearn: 0.0011502\ttotal: 3m 45s\tremaining: 1m 42s\n",
      "687:\tlearn: 0.0011502\ttotal: 3m 46s\tremaining: 1m 42s\n",
      "688:\tlearn: 0.0011502\ttotal: 3m 46s\tremaining: 1m 42s\n",
      "689:\tlearn: 0.0011502\ttotal: 3m 46s\tremaining: 1m 41s\n",
      "690:\tlearn: 0.0011502\ttotal: 3m 47s\tremaining: 1m 41s\n",
      "691:\tlearn: 0.0011502\ttotal: 3m 47s\tremaining: 1m 41s\n",
      "692:\tlearn: 0.0011502\ttotal: 3m 47s\tremaining: 1m 40s\n",
      "693:\tlearn: 0.0011502\ttotal: 3m 48s\tremaining: 1m 40s\n",
      "694:\tlearn: 0.0011501\ttotal: 3m 48s\tremaining: 1m 40s\n",
      "695:\tlearn: 0.0011483\ttotal: 3m 48s\tremaining: 1m 39s\n",
      "696:\tlearn: 0.0011478\ttotal: 3m 49s\tremaining: 1m 39s\n",
      "697:\tlearn: 0.0011458\ttotal: 3m 49s\tremaining: 1m 39s\n",
      "698:\tlearn: 0.0011457\ttotal: 3m 49s\tremaining: 1m 38s\n",
      "699:\tlearn: 0.0011421\ttotal: 3m 50s\tremaining: 1m 38s\n",
      "700:\tlearn: 0.0011420\ttotal: 3m 50s\tremaining: 1m 38s\n",
      "701:\tlearn: 0.0011417\ttotal: 3m 50s\tremaining: 1m 37s\n",
      "702:\tlearn: 0.0011417\ttotal: 3m 51s\tremaining: 1m 37s\n",
      "703:\tlearn: 0.0011411\ttotal: 3m 51s\tremaining: 1m 37s\n",
      "704:\tlearn: 0.0011410\ttotal: 3m 51s\tremaining: 1m 36s\n",
      "705:\tlearn: 0.0011410\ttotal: 3m 51s\tremaining: 1m 36s\n",
      "706:\tlearn: 0.0011409\ttotal: 3m 52s\tremaining: 1m 36s\n",
      "707:\tlearn: 0.0011409\ttotal: 3m 52s\tremaining: 1m 35s\n",
      "708:\tlearn: 0.0011409\ttotal: 3m 52s\tremaining: 1m 35s\n",
      "709:\tlearn: 0.0011407\ttotal: 3m 53s\tremaining: 1m 35s\n",
      "710:\tlearn: 0.0011407\ttotal: 3m 53s\tremaining: 1m 34s\n",
      "711:\tlearn: 0.0011406\ttotal: 3m 53s\tremaining: 1m 34s\n",
      "712:\tlearn: 0.0011406\ttotal: 3m 54s\tremaining: 1m 34s\n",
      "713:\tlearn: 0.0011405\ttotal: 3m 54s\tremaining: 1m 33s\n",
      "714:\tlearn: 0.0011403\ttotal: 3m 54s\tremaining: 1m 33s\n",
      "715:\tlearn: 0.0011403\ttotal: 3m 55s\tremaining: 1m 33s\n",
      "716:\tlearn: 0.0011402\ttotal: 3m 55s\tremaining: 1m 32s\n",
      "717:\tlearn: 0.0011400\ttotal: 3m 55s\tremaining: 1m 32s\n",
      "718:\tlearn: 0.0011399\ttotal: 3m 56s\tremaining: 1m 32s\n",
      "719:\tlearn: 0.0011399\ttotal: 3m 56s\tremaining: 1m 32s\n",
      "720:\tlearn: 0.0011399\ttotal: 3m 56s\tremaining: 1m 31s\n",
      "721:\tlearn: 0.0011398\ttotal: 3m 57s\tremaining: 1m 31s\n",
      "722:\tlearn: 0.0011398\ttotal: 3m 57s\tremaining: 1m 31s\n",
      "723:\tlearn: 0.0011398\ttotal: 3m 57s\tremaining: 1m 30s\n",
      "724:\tlearn: 0.0011398\ttotal: 3m 58s\tremaining: 1m 30s\n",
      "725:\tlearn: 0.0011398\ttotal: 3m 58s\tremaining: 1m 30s\n",
      "726:\tlearn: 0.0011398\ttotal: 3m 58s\tremaining: 1m 29s\n",
      "727:\tlearn: 0.0011398\ttotal: 3m 59s\tremaining: 1m 29s\n",
      "728:\tlearn: 0.0011398\ttotal: 3m 59s\tremaining: 1m 29s\n",
      "729:\tlearn: 0.0011397\ttotal: 3m 59s\tremaining: 1m 28s\n",
      "730:\tlearn: 0.0011397\ttotal: 4m\tremaining: 1m 28s\n",
      "731:\tlearn: 0.0011397\ttotal: 4m\tremaining: 1m 28s\n",
      "732:\tlearn: 0.0011396\ttotal: 4m\tremaining: 1m 27s\n",
      "733:\tlearn: 0.0011396\ttotal: 4m 1s\tremaining: 1m 27s\n",
      "734:\tlearn: 0.0011396\ttotal: 4m 1s\tremaining: 1m 27s\n",
      "735:\tlearn: 0.0011396\ttotal: 4m 1s\tremaining: 1m 26s\n",
      "736:\tlearn: 0.0011396\ttotal: 4m 2s\tremaining: 1m 26s\n",
      "737:\tlearn: 0.0011372\ttotal: 4m 2s\tremaining: 1m 26s\n",
      "738:\tlearn: 0.0011372\ttotal: 4m 2s\tremaining: 1m 25s\n",
      "739:\tlearn: 0.0011371\ttotal: 4m 3s\tremaining: 1m 25s\n",
      "740:\tlearn: 0.0011371\ttotal: 4m 3s\tremaining: 1m 25s\n",
      "741:\tlearn: 0.0011370\ttotal: 4m 3s\tremaining: 1m 24s\n",
      "742:\tlearn: 0.0011370\ttotal: 4m 4s\tremaining: 1m 24s\n",
      "743:\tlearn: 0.0011370\ttotal: 4m 4s\tremaining: 1m 24s\n",
      "744:\tlearn: 0.0011365\ttotal: 4m 4s\tremaining: 1m 23s\n",
      "745:\tlearn: 0.0011365\ttotal: 4m 5s\tremaining: 1m 23s\n",
      "746:\tlearn: 0.0011365\ttotal: 4m 5s\tremaining: 1m 23s\n",
      "747:\tlearn: 0.0011365\ttotal: 4m 5s\tremaining: 1m 22s\n",
      "748:\tlearn: 0.0011364\ttotal: 4m 6s\tremaining: 1m 22s\n",
      "749:\tlearn: 0.0011364\ttotal: 4m 6s\tremaining: 1m 22s\n",
      "750:\tlearn: 0.0011364\ttotal: 4m 6s\tremaining: 1m 21s\n",
      "751:\tlearn: 0.0011364\ttotal: 4m 7s\tremaining: 1m 21s\n",
      "752:\tlearn: 0.0011363\ttotal: 4m 7s\tremaining: 1m 21s\n",
      "753:\tlearn: 0.0011362\ttotal: 4m 7s\tremaining: 1m 20s\n",
      "754:\tlearn: 0.0011362\ttotal: 4m 8s\tremaining: 1m 20s\n",
      "755:\tlearn: 0.0011361\ttotal: 4m 8s\tremaining: 1m 20s\n",
      "756:\tlearn: 0.0011361\ttotal: 4m 8s\tremaining: 1m 19s\n",
      "757:\tlearn: 0.0011361\ttotal: 4m 9s\tremaining: 1m 19s\n",
      "758:\tlearn: 0.0011349\ttotal: 4m 9s\tremaining: 1m 19s\n",
      "759:\tlearn: 0.0011349\ttotal: 4m 9s\tremaining: 1m 18s\n",
      "760:\tlearn: 0.0011349\ttotal: 4m 10s\tremaining: 1m 18s\n",
      "761:\tlearn: 0.0011349\ttotal: 4m 10s\tremaining: 1m 18s\n",
      "762:\tlearn: 0.0011349\ttotal: 4m 10s\tremaining: 1m 17s\n",
      "763:\tlearn: 0.0011322\ttotal: 4m 11s\tremaining: 1m 17s\n",
      "764:\tlearn: 0.0011322\ttotal: 4m 11s\tremaining: 1m 17s\n",
      "765:\tlearn: 0.0011321\ttotal: 4m 11s\tremaining: 1m 16s\n",
      "766:\tlearn: 0.0011321\ttotal: 4m 12s\tremaining: 1m 16s\n",
      "767:\tlearn: 0.0011321\ttotal: 4m 12s\tremaining: 1m 16s\n",
      "768:\tlearn: 0.0011314\ttotal: 4m 12s\tremaining: 1m 15s\n",
      "769:\tlearn: 0.0011314\ttotal: 4m 12s\tremaining: 1m 15s\n",
      "770:\tlearn: 0.0011314\ttotal: 4m 13s\tremaining: 1m 15s\n",
      "771:\tlearn: 0.0011314\ttotal: 4m 13s\tremaining: 1m 14s\n",
      "772:\tlearn: 0.0011314\ttotal: 4m 13s\tremaining: 1m 14s\n",
      "773:\tlearn: 0.0011314\ttotal: 4m 14s\tremaining: 1m 14s\n",
      "774:\tlearn: 0.0011287\ttotal: 4m 14s\tremaining: 1m 13s\n",
      "775:\tlearn: 0.0011283\ttotal: 4m 14s\tremaining: 1m 13s\n",
      "776:\tlearn: 0.0011283\ttotal: 4m 15s\tremaining: 1m 13s\n",
      "777:\tlearn: 0.0011283\ttotal: 4m 15s\tremaining: 1m 12s\n",
      "778:\tlearn: 0.0011282\ttotal: 4m 15s\tremaining: 1m 12s\n",
      "779:\tlearn: 0.0011282\ttotal: 4m 16s\tremaining: 1m 12s\n",
      "780:\tlearn: 0.0011280\ttotal: 4m 16s\tremaining: 1m 11s\n",
      "781:\tlearn: 0.0011280\ttotal: 4m 16s\tremaining: 1m 11s\n",
      "782:\tlearn: 0.0011280\ttotal: 4m 17s\tremaining: 1m 11s\n",
      "783:\tlearn: 0.0011280\ttotal: 4m 17s\tremaining: 1m 10s\n",
      "784:\tlearn: 0.0011280\ttotal: 4m 17s\tremaining: 1m 10s\n",
      "785:\tlearn: 0.0011280\ttotal: 4m 18s\tremaining: 1m 10s\n",
      "786:\tlearn: 0.0011277\ttotal: 4m 18s\tremaining: 1m 9s\n",
      "787:\tlearn: 0.0011277\ttotal: 4m 18s\tremaining: 1m 9s\n",
      "788:\tlearn: 0.0011277\ttotal: 4m 19s\tremaining: 1m 9s\n",
      "789:\tlearn: 0.0011277\ttotal: 4m 19s\tremaining: 1m 8s\n",
      "790:\tlearn: 0.0011276\ttotal: 4m 19s\tremaining: 1m 8s\n",
      "791:\tlearn: 0.0011275\ttotal: 4m 20s\tremaining: 1m 8s\n",
      "792:\tlearn: 0.0011275\ttotal: 4m 20s\tremaining: 1m 8s\n",
      "793:\tlearn: 0.0011275\ttotal: 4m 20s\tremaining: 1m 7s\n",
      "794:\tlearn: 0.0011275\ttotal: 4m 21s\tremaining: 1m 7s\n",
      "795:\tlearn: 0.0011275\ttotal: 4m 21s\tremaining: 1m 7s\n",
      "796:\tlearn: 0.0011274\ttotal: 4m 21s\tremaining: 1m 6s\n",
      "797:\tlearn: 0.0011274\ttotal: 4m 22s\tremaining: 1m 6s\n",
      "798:\tlearn: 0.0011274\ttotal: 4m 22s\tremaining: 1m 6s\n",
      "799:\tlearn: 0.0011273\ttotal: 4m 22s\tremaining: 1m 5s\n",
      "800:\tlearn: 0.0011273\ttotal: 4m 23s\tremaining: 1m 5s\n",
      "801:\tlearn: 0.0011272\ttotal: 4m 23s\tremaining: 1m 5s\n",
      "802:\tlearn: 0.0011272\ttotal: 4m 23s\tremaining: 1m 4s\n",
      "803:\tlearn: 0.0011251\ttotal: 4m 24s\tremaining: 1m 4s\n",
      "804:\tlearn: 0.0011250\ttotal: 4m 24s\tremaining: 1m 4s\n",
      "805:\tlearn: 0.0011250\ttotal: 4m 24s\tremaining: 1m 3s\n",
      "806:\tlearn: 0.0011250\ttotal: 4m 25s\tremaining: 1m 3s\n",
      "807:\tlearn: 0.0011250\ttotal: 4m 25s\tremaining: 1m 3s\n",
      "808:\tlearn: 0.0011249\ttotal: 4m 25s\tremaining: 1m 2s\n",
      "809:\tlearn: 0.0011249\ttotal: 4m 26s\tremaining: 1m 2s\n",
      "810:\tlearn: 0.0011248\ttotal: 4m 26s\tremaining: 1m 2s\n",
      "811:\tlearn: 0.0011246\ttotal: 4m 26s\tremaining: 1m 1s\n",
      "812:\tlearn: 0.0011245\ttotal: 4m 27s\tremaining: 1m 1s\n",
      "813:\tlearn: 0.0011244\ttotal: 4m 27s\tremaining: 1m 1s\n",
      "814:\tlearn: 0.0011242\ttotal: 4m 27s\tremaining: 1m\n",
      "815:\tlearn: 0.0011226\ttotal: 4m 28s\tremaining: 1m\n",
      "816:\tlearn: 0.0011226\ttotal: 4m 28s\tremaining: 1m\n",
      "817:\tlearn: 0.0011223\ttotal: 4m 28s\tremaining: 59.8s\n",
      "818:\tlearn: 0.0011222\ttotal: 4m 29s\tremaining: 59.5s\n",
      "819:\tlearn: 0.0011222\ttotal: 4m 29s\tremaining: 59.1s\n",
      "820:\tlearn: 0.0011222\ttotal: 4m 29s\tremaining: 58.8s\n",
      "821:\tlearn: 0.0011222\ttotal: 4m 30s\tremaining: 58.5s\n",
      "822:\tlearn: 0.0011222\ttotal: 4m 30s\tremaining: 58.2s\n",
      "823:\tlearn: 0.0011221\ttotal: 4m 30s\tremaining: 57.8s\n",
      "824:\tlearn: 0.0011220\ttotal: 4m 31s\tremaining: 57.5s\n",
      "825:\tlearn: 0.0011220\ttotal: 4m 31s\tremaining: 57.2s\n",
      "826:\tlearn: 0.0011220\ttotal: 4m 31s\tremaining: 56.9s\n",
      "827:\tlearn: 0.0011218\ttotal: 4m 32s\tremaining: 56.5s\n",
      "828:\tlearn: 0.0011200\ttotal: 4m 32s\tremaining: 56.2s\n",
      "829:\tlearn: 0.0011199\ttotal: 4m 32s\tremaining: 55.9s\n",
      "830:\tlearn: 0.0011199\ttotal: 4m 33s\tremaining: 55.5s\n",
      "831:\tlearn: 0.0011198\ttotal: 4m 33s\tremaining: 55.2s\n",
      "832:\tlearn: 0.0011198\ttotal: 4m 33s\tremaining: 54.9s\n",
      "833:\tlearn: 0.0011198\ttotal: 4m 34s\tremaining: 54.6s\n",
      "834:\tlearn: 0.0011187\ttotal: 4m 34s\tremaining: 54.2s\n",
      "835:\tlearn: 0.0011185\ttotal: 4m 34s\tremaining: 53.9s\n",
      "836:\tlearn: 0.0011185\ttotal: 4m 35s\tremaining: 53.6s\n",
      "837:\tlearn: 0.0011184\ttotal: 4m 35s\tremaining: 53.2s\n",
      "838:\tlearn: 0.0011184\ttotal: 4m 35s\tremaining: 52.9s\n",
      "839:\tlearn: 0.0011184\ttotal: 4m 36s\tremaining: 52.6s\n",
      "840:\tlearn: 0.0011157\ttotal: 4m 36s\tremaining: 52.3s\n",
      "841:\tlearn: 0.0011157\ttotal: 4m 36s\tremaining: 51.9s\n",
      "842:\tlearn: 0.0011157\ttotal: 4m 37s\tremaining: 51.6s\n",
      "843:\tlearn: 0.0011157\ttotal: 4m 37s\tremaining: 51.3s\n",
      "844:\tlearn: 0.0011157\ttotal: 4m 37s\tremaining: 50.9s\n",
      "845:\tlearn: 0.0011135\ttotal: 4m 38s\tremaining: 50.6s\n",
      "846:\tlearn: 0.0011135\ttotal: 4m 38s\tremaining: 50.3s\n",
      "847:\tlearn: 0.0011135\ttotal: 4m 38s\tremaining: 50s\n",
      "848:\tlearn: 0.0011134\ttotal: 4m 39s\tremaining: 49.6s\n",
      "849:\tlearn: 0.0011134\ttotal: 4m 39s\tremaining: 49.3s\n",
      "850:\tlearn: 0.0011133\ttotal: 4m 39s\tremaining: 49s\n",
      "851:\tlearn: 0.0011133\ttotal: 4m 40s\tremaining: 48.6s\n",
      "852:\tlearn: 0.0011133\ttotal: 4m 40s\tremaining: 48.3s\n",
      "853:\tlearn: 0.0011133\ttotal: 4m 40s\tremaining: 48s\n",
      "854:\tlearn: 0.0011132\ttotal: 4m 40s\tremaining: 47.7s\n",
      "855:\tlearn: 0.0011131\ttotal: 4m 41s\tremaining: 47.3s\n",
      "856:\tlearn: 0.0011131\ttotal: 4m 41s\tremaining: 47s\n",
      "857:\tlearn: 0.0011099\ttotal: 4m 41s\tremaining: 46.7s\n",
      "858:\tlearn: 0.0011098\ttotal: 4m 42s\tremaining: 46.3s\n",
      "859:\tlearn: 0.0011098\ttotal: 4m 42s\tremaining: 46s\n",
      "860:\tlearn: 0.0011098\ttotal: 4m 43s\tremaining: 45.7s\n",
      "861:\tlearn: 0.0011098\ttotal: 4m 43s\tremaining: 45.4s\n",
      "862:\tlearn: 0.0011098\ttotal: 4m 43s\tremaining: 45s\n",
      "863:\tlearn: 0.0011098\ttotal: 4m 44s\tremaining: 44.7s\n",
      "864:\tlearn: 0.0011098\ttotal: 4m 44s\tremaining: 44.4s\n",
      "865:\tlearn: 0.0011097\ttotal: 4m 44s\tremaining: 44.1s\n",
      "866:\tlearn: 0.0011096\ttotal: 4m 45s\tremaining: 43.7s\n",
      "867:\tlearn: 0.0011095\ttotal: 4m 45s\tremaining: 43.4s\n",
      "868:\tlearn: 0.0011095\ttotal: 4m 45s\tremaining: 43.1s\n",
      "869:\tlearn: 0.0011095\ttotal: 4m 46s\tremaining: 42.7s\n",
      "870:\tlearn: 0.0011095\ttotal: 4m 46s\tremaining: 42.4s\n",
      "871:\tlearn: 0.0011095\ttotal: 4m 46s\tremaining: 42.1s\n",
      "872:\tlearn: 0.0011095\ttotal: 4m 46s\tremaining: 41.7s\n",
      "873:\tlearn: 0.0011095\ttotal: 4m 47s\tremaining: 41.4s\n",
      "874:\tlearn: 0.0011094\ttotal: 4m 47s\tremaining: 41.1s\n",
      "875:\tlearn: 0.0011094\ttotal: 4m 47s\tremaining: 40.8s\n",
      "876:\tlearn: 0.0011094\ttotal: 4m 48s\tremaining: 40.4s\n",
      "877:\tlearn: 0.0011094\ttotal: 4m 48s\tremaining: 40.1s\n",
      "878:\tlearn: 0.0011092\ttotal: 4m 48s\tremaining: 39.8s\n",
      "879:\tlearn: 0.0011091\ttotal: 4m 49s\tremaining: 39.4s\n",
      "880:\tlearn: 0.0011091\ttotal: 4m 49s\tremaining: 39.1s\n",
      "881:\tlearn: 0.0011090\ttotal: 4m 49s\tremaining: 38.8s\n",
      "882:\tlearn: 0.0011090\ttotal: 4m 50s\tremaining: 38.5s\n",
      "883:\tlearn: 0.0011090\ttotal: 4m 50s\tremaining: 38.1s\n",
      "884:\tlearn: 0.0011087\ttotal: 4m 50s\tremaining: 37.8s\n",
      "885:\tlearn: 0.0011086\ttotal: 4m 51s\tremaining: 37.5s\n",
      "886:\tlearn: 0.0011086\ttotal: 4m 51s\tremaining: 37.1s\n",
      "887:\tlearn: 0.0011085\ttotal: 4m 51s\tremaining: 36.8s\n",
      "888:\tlearn: 0.0011085\ttotal: 4m 52s\tremaining: 36.5s\n",
      "889:\tlearn: 0.0011084\ttotal: 4m 52s\tremaining: 36.2s\n",
      "890:\tlearn: 0.0011083\ttotal: 4m 52s\tremaining: 35.8s\n",
      "891:\tlearn: 0.0011083\ttotal: 4m 53s\tremaining: 35.5s\n",
      "892:\tlearn: 0.0011082\ttotal: 4m 53s\tremaining: 35.2s\n",
      "893:\tlearn: 0.0011082\ttotal: 4m 53s\tremaining: 34.8s\n",
      "894:\tlearn: 0.0011068\ttotal: 4m 54s\tremaining: 34.5s\n",
      "895:\tlearn: 0.0011067\ttotal: 4m 54s\tremaining: 34.2s\n",
      "896:\tlearn: 0.0011062\ttotal: 4m 54s\tremaining: 33.9s\n",
      "897:\tlearn: 0.0011061\ttotal: 4m 55s\tremaining: 33.5s\n",
      "898:\tlearn: 0.0011061\ttotal: 4m 55s\tremaining: 33.2s\n",
      "899:\tlearn: 0.0011061\ttotal: 4m 55s\tremaining: 32.9s\n",
      "900:\tlearn: 0.0011060\ttotal: 4m 56s\tremaining: 32.5s\n",
      "901:\tlearn: 0.0011043\ttotal: 4m 56s\tremaining: 32.2s\n",
      "902:\tlearn: 0.0011041\ttotal: 4m 56s\tremaining: 31.9s\n",
      "903:\tlearn: 0.0011037\ttotal: 4m 57s\tremaining: 31.6s\n",
      "904:\tlearn: 0.0011037\ttotal: 4m 57s\tremaining: 31.2s\n",
      "905:\tlearn: 0.0011037\ttotal: 4m 57s\tremaining: 30.9s\n",
      "906:\tlearn: 0.0011037\ttotal: 4m 58s\tremaining: 30.6s\n",
      "907:\tlearn: 0.0011037\ttotal: 4m 58s\tremaining: 30.2s\n",
      "908:\tlearn: 0.0011037\ttotal: 4m 58s\tremaining: 29.9s\n",
      "909:\tlearn: 0.0011036\ttotal: 4m 59s\tremaining: 29.6s\n",
      "910:\tlearn: 0.0011031\ttotal: 4m 59s\tremaining: 29.3s\n",
      "911:\tlearn: 0.0011031\ttotal: 4m 59s\tremaining: 28.9s\n",
      "912:\tlearn: 0.0011031\ttotal: 5m\tremaining: 28.6s\n",
      "913:\tlearn: 0.0011028\ttotal: 5m\tremaining: 28.3s\n",
      "914:\tlearn: 0.0011017\ttotal: 5m\tremaining: 27.9s\n",
      "915:\tlearn: 0.0011015\ttotal: 5m 1s\tremaining: 27.6s\n",
      "916:\tlearn: 0.0011015\ttotal: 5m 1s\tremaining: 27.3s\n",
      "917:\tlearn: 0.0011015\ttotal: 5m 1s\tremaining: 26.9s\n",
      "918:\tlearn: 0.0011014\ttotal: 5m 2s\tremaining: 26.6s\n",
      "919:\tlearn: 0.0011014\ttotal: 5m 2s\tremaining: 26.3s\n",
      "920:\tlearn: 0.0011014\ttotal: 5m 2s\tremaining: 26s\n",
      "921:\tlearn: 0.0011014\ttotal: 5m 3s\tremaining: 25.6s\n",
      "922:\tlearn: 0.0011014\ttotal: 5m 3s\tremaining: 25.3s\n",
      "923:\tlearn: 0.0011014\ttotal: 5m 3s\tremaining: 25s\n",
      "924:\tlearn: 0.0011013\ttotal: 5m 3s\tremaining: 24.6s\n",
      "925:\tlearn: 0.0011013\ttotal: 5m 4s\tremaining: 24.3s\n",
      "926:\tlearn: 0.0011012\ttotal: 5m 4s\tremaining: 24s\n",
      "927:\tlearn: 0.0011012\ttotal: 5m 4s\tremaining: 23.7s\n",
      "928:\tlearn: 0.0011012\ttotal: 5m 5s\tremaining: 23.3s\n",
      "929:\tlearn: 0.0011012\ttotal: 5m 5s\tremaining: 23s\n",
      "930:\tlearn: 0.0011009\ttotal: 5m 5s\tremaining: 22.7s\n",
      "931:\tlearn: 0.0011008\ttotal: 5m 6s\tremaining: 22.3s\n",
      "932:\tlearn: 0.0011008\ttotal: 5m 6s\tremaining: 22s\n",
      "933:\tlearn: 0.0011007\ttotal: 5m 6s\tremaining: 21.7s\n",
      "934:\tlearn: 0.0011007\ttotal: 5m 7s\tremaining: 21.4s\n",
      "935:\tlearn: 0.0010996\ttotal: 5m 7s\tremaining: 21s\n",
      "936:\tlearn: 0.0010974\ttotal: 5m 7s\tremaining: 20.7s\n",
      "937:\tlearn: 0.0010974\ttotal: 5m 8s\tremaining: 20.4s\n",
      "938:\tlearn: 0.0010974\ttotal: 5m 8s\tremaining: 20s\n",
      "939:\tlearn: 0.0010974\ttotal: 5m 8s\tremaining: 19.7s\n",
      "940:\tlearn: 0.0010974\ttotal: 5m 9s\tremaining: 19.4s\n",
      "941:\tlearn: 0.0010974\ttotal: 5m 9s\tremaining: 19.1s\n",
      "942:\tlearn: 0.0010974\ttotal: 5m 9s\tremaining: 18.7s\n",
      "943:\tlearn: 0.0010974\ttotal: 5m 10s\tremaining: 18.4s\n",
      "944:\tlearn: 0.0010971\ttotal: 5m 10s\tremaining: 18.1s\n",
      "945:\tlearn: 0.0010968\ttotal: 5m 10s\tremaining: 17.7s\n",
      "946:\tlearn: 0.0010966\ttotal: 5m 11s\tremaining: 17.4s\n",
      "947:\tlearn: 0.0010939\ttotal: 5m 11s\tremaining: 17.1s\n",
      "948:\tlearn: 0.0010912\ttotal: 5m 11s\tremaining: 16.8s\n",
      "949:\tlearn: 0.0010912\ttotal: 5m 12s\tremaining: 16.4s\n",
      "950:\tlearn: 0.0010912\ttotal: 5m 12s\tremaining: 16.1s\n",
      "951:\tlearn: 0.0010910\ttotal: 5m 12s\tremaining: 15.8s\n",
      "952:\tlearn: 0.0010910\ttotal: 5m 13s\tremaining: 15.4s\n",
      "953:\tlearn: 0.0010910\ttotal: 5m 13s\tremaining: 15.1s\n",
      "954:\tlearn: 0.0010910\ttotal: 5m 13s\tremaining: 14.8s\n",
      "955:\tlearn: 0.0010910\ttotal: 5m 14s\tremaining: 14.5s\n",
      "956:\tlearn: 0.0010878\ttotal: 5m 14s\tremaining: 14.1s\n",
      "957:\tlearn: 0.0010878\ttotal: 5m 14s\tremaining: 13.8s\n",
      "958:\tlearn: 0.0010877\ttotal: 5m 15s\tremaining: 13.5s\n",
      "959:\tlearn: 0.0010877\ttotal: 5m 15s\tremaining: 13.1s\n",
      "960:\tlearn: 0.0010877\ttotal: 5m 15s\tremaining: 12.8s\n",
      "961:\tlearn: 0.0010877\ttotal: 5m 16s\tremaining: 12.5s\n",
      "962:\tlearn: 0.0010877\ttotal: 5m 16s\tremaining: 12.2s\n",
      "963:\tlearn: 0.0010876\ttotal: 5m 16s\tremaining: 11.8s\n",
      "964:\tlearn: 0.0010876\ttotal: 5m 17s\tremaining: 11.5s\n",
      "965:\tlearn: 0.0010876\ttotal: 5m 17s\tremaining: 11.2s\n",
      "966:\tlearn: 0.0010876\ttotal: 5m 17s\tremaining: 10.8s\n",
      "967:\tlearn: 0.0010876\ttotal: 5m 18s\tremaining: 10.5s\n",
      "968:\tlearn: 0.0010875\ttotal: 5m 18s\tremaining: 10.2s\n",
      "969:\tlearn: 0.0010874\ttotal: 5m 18s\tremaining: 9.86s\n",
      "970:\tlearn: 0.0010874\ttotal: 5m 19s\tremaining: 9.53s\n",
      "971:\tlearn: 0.0010874\ttotal: 5m 19s\tremaining: 9.2s\n",
      "972:\tlearn: 0.0010874\ttotal: 5m 19s\tremaining: 8.87s\n",
      "973:\tlearn: 0.0010873\ttotal: 5m 20s\tremaining: 8.54s\n",
      "974:\tlearn: 0.0010873\ttotal: 5m 20s\tremaining: 8.21s\n",
      "975:\tlearn: 0.0010873\ttotal: 5m 20s\tremaining: 7.88s\n",
      "976:\tlearn: 0.0010868\ttotal: 5m 21s\tremaining: 7.56s\n",
      "977:\tlearn: 0.0010867\ttotal: 5m 21s\tremaining: 7.23s\n",
      "978:\tlearn: 0.0010867\ttotal: 5m 21s\tremaining: 6.9s\n",
      "979:\tlearn: 0.0010866\ttotal: 5m 22s\tremaining: 6.57s\n",
      "980:\tlearn: 0.0010865\ttotal: 5m 22s\tremaining: 6.24s\n",
      "981:\tlearn: 0.0010864\ttotal: 5m 22s\tremaining: 5.91s\n",
      "982:\tlearn: 0.0010862\ttotal: 5m 23s\tremaining: 5.59s\n",
      "983:\tlearn: 0.0010861\ttotal: 5m 23s\tremaining: 5.26s\n",
      "984:\tlearn: 0.0010860\ttotal: 5m 23s\tremaining: 4.93s\n",
      "985:\tlearn: 0.0010859\ttotal: 5m 24s\tremaining: 4.6s\n",
      "986:\tlearn: 0.0010859\ttotal: 5m 24s\tremaining: 4.27s\n",
      "987:\tlearn: 0.0010858\ttotal: 5m 24s\tremaining: 3.94s\n",
      "988:\tlearn: 0.0010858\ttotal: 5m 25s\tremaining: 3.61s\n",
      "989:\tlearn: 0.0010856\ttotal: 5m 25s\tremaining: 3.29s\n",
      "990:\tlearn: 0.0010856\ttotal: 5m 25s\tremaining: 2.96s\n",
      "991:\tlearn: 0.0010856\ttotal: 5m 26s\tremaining: 2.63s\n",
      "992:\tlearn: 0.0010855\ttotal: 5m 26s\tremaining: 2.3s\n",
      "993:\tlearn: 0.0010854\ttotal: 5m 26s\tremaining: 1.97s\n",
      "994:\tlearn: 0.0010854\ttotal: 5m 27s\tremaining: 1.64s\n",
      "995:\tlearn: 0.0010839\ttotal: 5m 27s\tremaining: 1.31s\n",
      "996:\tlearn: 0.0010839\ttotal: 5m 27s\tremaining: 986ms\n",
      "997:\tlearn: 0.0010822\ttotal: 5m 27s\tremaining: 657ms\n",
      "998:\tlearn: 0.0010819\ttotal: 5m 28s\tremaining: 329ms\n",
      "999:\tlearn: 0.0010819\ttotal: 5m 28s\tremaining: 0us\n",
      "0:\tlearn: 0.6342754\ttotal: 352ms\tremaining: 5m 51s\n",
      "1:\tlearn: 0.5876953\ttotal: 696ms\tremaining: 5m 47s\n",
      "2:\tlearn: 0.5312510\ttotal: 1.03s\tremaining: 5m 42s\n",
      "3:\tlearn: 0.4757040\ttotal: 1.38s\tremaining: 5m 42s\n",
      "4:\tlearn: 0.4385261\ttotal: 1.73s\tremaining: 5m 43s\n",
      "5:\tlearn: 0.4044310\ttotal: 2.05s\tremaining: 5m 39s\n",
      "6:\tlearn: 0.3740051\ttotal: 2.37s\tremaining: 5m 36s\n",
      "7:\tlearn: 0.3483739\ttotal: 2.7s\tremaining: 5m 35s\n",
      "8:\tlearn: 0.3041294\ttotal: 3.02s\tremaining: 5m 33s\n",
      "9:\tlearn: 0.2705344\ttotal: 3.35s\tremaining: 5m 32s\n",
      "10:\tlearn: 0.2548504\ttotal: 3.68s\tremaining: 5m 30s\n",
      "11:\tlearn: 0.2298349\ttotal: 4.01s\tremaining: 5m 30s\n",
      "12:\tlearn: 0.2155707\ttotal: 4.33s\tremaining: 5m 29s\n",
      "13:\tlearn: 0.1959561\ttotal: 4.66s\tremaining: 5m 27s\n",
      "14:\tlearn: 0.1825900\ttotal: 4.98s\tremaining: 5m 26s\n",
      "15:\tlearn: 0.1649153\ttotal: 5.3s\tremaining: 5m 26s\n",
      "16:\tlearn: 0.1490061\ttotal: 5.63s\tremaining: 5m 25s\n",
      "17:\tlearn: 0.1364319\ttotal: 5.96s\tremaining: 5m 25s\n",
      "18:\tlearn: 0.1234614\ttotal: 6.29s\tremaining: 5m 24s\n",
      "19:\tlearn: 0.1145826\ttotal: 6.62s\tremaining: 5m 24s\n",
      "20:\tlearn: 0.1023603\ttotal: 6.94s\tremaining: 5m 23s\n",
      "21:\tlearn: 0.0923388\ttotal: 7.27s\tremaining: 5m 23s\n",
      "22:\tlearn: 0.0825811\ttotal: 7.6s\tremaining: 5m 22s\n",
      "23:\tlearn: 0.0743766\ttotal: 7.92s\tremaining: 5m 22s\n",
      "24:\tlearn: 0.0675844\ttotal: 8.25s\tremaining: 5m 21s\n",
      "25:\tlearn: 0.0614657\ttotal: 8.58s\tremaining: 5m 21s\n",
      "26:\tlearn: 0.0553020\ttotal: 8.91s\tremaining: 5m 21s\n",
      "27:\tlearn: 0.0522720\ttotal: 9.23s\tremaining: 5m 20s\n",
      "28:\tlearn: 0.0481814\ttotal: 9.56s\tremaining: 5m 20s\n",
      "29:\tlearn: 0.0444448\ttotal: 9.88s\tremaining: 5m 19s\n",
      "30:\tlearn: 0.0410595\ttotal: 10.2s\tremaining: 5m 19s\n",
      "31:\tlearn: 0.0386157\ttotal: 10.5s\tremaining: 5m 18s\n",
      "32:\tlearn: 0.0366210\ttotal: 10.9s\tremaining: 5m 18s\n",
      "33:\tlearn: 0.0344465\ttotal: 11.2s\tremaining: 5m 17s\n",
      "34:\tlearn: 0.0329889\ttotal: 11.5s\tremaining: 5m 17s\n",
      "35:\tlearn: 0.0309909\ttotal: 11.8s\tremaining: 5m 16s\n",
      "36:\tlearn: 0.0289870\ttotal: 12.2s\tremaining: 5m 16s\n",
      "37:\tlearn: 0.0270769\ttotal: 12.5s\tremaining: 5m 16s\n",
      "38:\tlearn: 0.0259942\ttotal: 12.8s\tremaining: 5m 15s\n",
      "39:\tlearn: 0.0249546\ttotal: 13.1s\tremaining: 5m 15s\n",
      "40:\tlearn: 0.0236620\ttotal: 13.5s\tremaining: 5m 14s\n",
      "41:\tlearn: 0.0228804\ttotal: 13.8s\tremaining: 5m 14s\n",
      "42:\tlearn: 0.0218531\ttotal: 14.1s\tremaining: 5m 14s\n",
      "43:\tlearn: 0.0209908\ttotal: 14.4s\tremaining: 5m 13s\n",
      "44:\tlearn: 0.0203979\ttotal: 14.8s\tremaining: 5m 13s\n",
      "45:\tlearn: 0.0195539\ttotal: 15.1s\tremaining: 5m 13s\n",
      "46:\tlearn: 0.0188053\ttotal: 15.5s\tremaining: 5m 13s\n",
      "47:\tlearn: 0.0181266\ttotal: 15.8s\tremaining: 5m 13s\n",
      "48:\tlearn: 0.0174565\ttotal: 16.1s\tremaining: 5m 12s\n",
      "49:\tlearn: 0.0169434\ttotal: 16.4s\tremaining: 5m 12s\n",
      "50:\tlearn: 0.0162868\ttotal: 16.8s\tremaining: 5m 12s\n",
      "51:\tlearn: 0.0156091\ttotal: 17.1s\tremaining: 5m 11s\n",
      "52:\tlearn: 0.0150616\ttotal: 17.4s\tremaining: 5m 11s\n",
      "53:\tlearn: 0.0143357\ttotal: 17.7s\tremaining: 5m 10s\n",
      "54:\tlearn: 0.0139359\ttotal: 18.1s\tremaining: 5m 10s\n",
      "55:\tlearn: 0.0135438\ttotal: 18.4s\tremaining: 5m 10s\n",
      "56:\tlearn: 0.0131736\ttotal: 18.7s\tremaining: 5m 9s\n",
      "57:\tlearn: 0.0125640\ttotal: 19.1s\tremaining: 5m 9s\n",
      "58:\tlearn: 0.0121401\ttotal: 19.4s\tremaining: 5m 9s\n",
      "59:\tlearn: 0.0117737\ttotal: 19.7s\tremaining: 5m 8s\n",
      "60:\tlearn: 0.0114741\ttotal: 20s\tremaining: 5m 8s\n",
      "61:\tlearn: 0.0111540\ttotal: 20.4s\tremaining: 5m 8s\n",
      "62:\tlearn: 0.0107433\ttotal: 20.7s\tremaining: 5m 7s\n",
      "63:\tlearn: 0.0103337\ttotal: 21s\tremaining: 5m 7s\n",
      "64:\tlearn: 0.0100649\ttotal: 21.3s\tremaining: 5m 6s\n",
      "65:\tlearn: 0.0098884\ttotal: 21.7s\tremaining: 5m 6s\n",
      "66:\tlearn: 0.0096700\ttotal: 22s\tremaining: 5m 6s\n",
      "67:\tlearn: 0.0094757\ttotal: 22.3s\tremaining: 5m 6s\n",
      "68:\tlearn: 0.0092385\ttotal: 22.6s\tremaining: 5m 5s\n",
      "69:\tlearn: 0.0090525\ttotal: 23s\tremaining: 5m 5s\n",
      "70:\tlearn: 0.0088280\ttotal: 23.3s\tremaining: 5m 4s\n",
      "71:\tlearn: 0.0086504\ttotal: 23.6s\tremaining: 5m 4s\n",
      "72:\tlearn: 0.0084098\ttotal: 24s\tremaining: 5m 4s\n",
      "73:\tlearn: 0.0082638\ttotal: 24.3s\tremaining: 5m 3s\n",
      "74:\tlearn: 0.0081138\ttotal: 24.6s\tremaining: 5m 3s\n",
      "75:\tlearn: 0.0079608\ttotal: 24.9s\tremaining: 5m 3s\n",
      "76:\tlearn: 0.0078178\ttotal: 25.3s\tremaining: 5m 2s\n",
      "77:\tlearn: 0.0076064\ttotal: 25.6s\tremaining: 5m 2s\n",
      "78:\tlearn: 0.0074285\ttotal: 25.9s\tremaining: 5m 2s\n",
      "79:\tlearn: 0.0072906\ttotal: 26.2s\tremaining: 5m 1s\n",
      "80:\tlearn: 0.0071905\ttotal: 26.6s\tremaining: 5m 1s\n",
      "81:\tlearn: 0.0070859\ttotal: 26.9s\tremaining: 5m 1s\n",
      "82:\tlearn: 0.0069506\ttotal: 27.2s\tremaining: 5m\n",
      "83:\tlearn: 0.0068057\ttotal: 27.5s\tremaining: 5m\n",
      "84:\tlearn: 0.0066540\ttotal: 27.9s\tremaining: 5m\n",
      "85:\tlearn: 0.0065447\ttotal: 28.2s\tremaining: 4m 59s\n",
      "86:\tlearn: 0.0064399\ttotal: 28.5s\tremaining: 4m 59s\n",
      "87:\tlearn: 0.0063643\ttotal: 28.9s\tremaining: 4m 59s\n",
      "88:\tlearn: 0.0062747\ttotal: 29.2s\tremaining: 4m 58s\n",
      "89:\tlearn: 0.0061889\ttotal: 29.5s\tremaining: 4m 58s\n",
      "90:\tlearn: 0.0060908\ttotal: 29.9s\tremaining: 4m 58s\n",
      "91:\tlearn: 0.0059611\ttotal: 30.2s\tremaining: 4m 57s\n",
      "92:\tlearn: 0.0058625\ttotal: 30.5s\tremaining: 4m 57s\n",
      "93:\tlearn: 0.0058016\ttotal: 30.8s\tremaining: 4m 57s\n",
      "94:\tlearn: 0.0056992\ttotal: 31.2s\tremaining: 4m 56s\n",
      "95:\tlearn: 0.0056164\ttotal: 31.5s\tremaining: 4m 56s\n",
      "96:\tlearn: 0.0055465\ttotal: 31.8s\tremaining: 4m 56s\n",
      "97:\tlearn: 0.0054898\ttotal: 32.1s\tremaining: 4m 55s\n",
      "98:\tlearn: 0.0054075\ttotal: 32.5s\tremaining: 4m 55s\n",
      "99:\tlearn: 0.0053411\ttotal: 32.8s\tremaining: 4m 55s\n",
      "100:\tlearn: 0.0052831\ttotal: 33.1s\tremaining: 4m 54s\n",
      "101:\tlearn: 0.0052242\ttotal: 33.4s\tremaining: 4m 54s\n",
      "102:\tlearn: 0.0051683\ttotal: 33.8s\tremaining: 4m 54s\n",
      "103:\tlearn: 0.0050939\ttotal: 34.1s\tremaining: 4m 53s\n",
      "104:\tlearn: 0.0050322\ttotal: 34.4s\tremaining: 4m 53s\n",
      "105:\tlearn: 0.0049848\ttotal: 34.8s\tremaining: 4m 53s\n",
      "106:\tlearn: 0.0049142\ttotal: 35.1s\tremaining: 4m 52s\n",
      "107:\tlearn: 0.0048557\ttotal: 35.4s\tremaining: 4m 52s\n",
      "108:\tlearn: 0.0047783\ttotal: 35.7s\tremaining: 4m 52s\n",
      "109:\tlearn: 0.0047269\ttotal: 36.1s\tremaining: 4m 51s\n",
      "110:\tlearn: 0.0046780\ttotal: 36.4s\tremaining: 4m 51s\n",
      "111:\tlearn: 0.0046069\ttotal: 36.7s\tremaining: 4m 51s\n",
      "112:\tlearn: 0.0045522\ttotal: 37s\tremaining: 4m 50s\n",
      "113:\tlearn: 0.0045133\ttotal: 37.4s\tremaining: 4m 50s\n",
      "114:\tlearn: 0.0044605\ttotal: 37.7s\tremaining: 4m 50s\n",
      "115:\tlearn: 0.0044004\ttotal: 38s\tremaining: 4m 49s\n",
      "116:\tlearn: 0.0043269\ttotal: 38.4s\tremaining: 4m 49s\n",
      "117:\tlearn: 0.0042696\ttotal: 38.7s\tremaining: 4m 49s\n",
      "118:\tlearn: 0.0042159\ttotal: 39s\tremaining: 4m 48s\n",
      "119:\tlearn: 0.0041571\ttotal: 39.3s\tremaining: 4m 48s\n",
      "120:\tlearn: 0.0041228\ttotal: 39.7s\tremaining: 4m 48s\n",
      "121:\tlearn: 0.0040990\ttotal: 40s\tremaining: 4m 47s\n",
      "122:\tlearn: 0.0040695\ttotal: 40.3s\tremaining: 4m 47s\n",
      "123:\tlearn: 0.0040316\ttotal: 40.7s\tremaining: 4m 47s\n",
      "124:\tlearn: 0.0039911\ttotal: 41s\tremaining: 4m 46s\n",
      "125:\tlearn: 0.0039504\ttotal: 41.3s\tremaining: 4m 46s\n",
      "126:\tlearn: 0.0039310\ttotal: 41.6s\tremaining: 4m 46s\n",
      "127:\tlearn: 0.0038906\ttotal: 41.9s\tremaining: 4m 45s\n",
      "128:\tlearn: 0.0038534\ttotal: 42.3s\tremaining: 4m 45s\n",
      "129:\tlearn: 0.0038125\ttotal: 42.6s\tremaining: 4m 45s\n",
      "130:\tlearn: 0.0037647\ttotal: 42.9s\tremaining: 4m 44s\n",
      "131:\tlearn: 0.0037349\ttotal: 43.3s\tremaining: 4m 44s\n",
      "132:\tlearn: 0.0036909\ttotal: 43.6s\tremaining: 4m 44s\n",
      "133:\tlearn: 0.0036624\ttotal: 43.9s\tremaining: 4m 43s\n",
      "134:\tlearn: 0.0036402\ttotal: 44.2s\tremaining: 4m 43s\n",
      "135:\tlearn: 0.0036118\ttotal: 44.5s\tremaining: 4m 43s\n",
      "136:\tlearn: 0.0035854\ttotal: 44.9s\tremaining: 4m 42s\n",
      "137:\tlearn: 0.0035597\ttotal: 45.2s\tremaining: 4m 42s\n",
      "138:\tlearn: 0.0035107\ttotal: 45.5s\tremaining: 4m 42s\n",
      "139:\tlearn: 0.0034756\ttotal: 45.8s\tremaining: 4m 41s\n",
      "140:\tlearn: 0.0034329\ttotal: 46.2s\tremaining: 4m 41s\n",
      "141:\tlearn: 0.0034148\ttotal: 46.5s\tremaining: 4m 40s\n",
      "142:\tlearn: 0.0033687\ttotal: 46.8s\tremaining: 4m 40s\n",
      "143:\tlearn: 0.0033486\ttotal: 47.2s\tremaining: 4m 40s\n",
      "144:\tlearn: 0.0033154\ttotal: 47.5s\tremaining: 4m 39s\n",
      "145:\tlearn: 0.0032896\ttotal: 47.8s\tremaining: 4m 39s\n",
      "146:\tlearn: 0.0032655\ttotal: 48.1s\tremaining: 4m 39s\n",
      "147:\tlearn: 0.0032002\ttotal: 48.5s\tremaining: 4m 38s\n",
      "148:\tlearn: 0.0031698\ttotal: 48.8s\tremaining: 4m 38s\n",
      "149:\tlearn: 0.0031531\ttotal: 49.1s\tremaining: 4m 38s\n",
      "150:\tlearn: 0.0031209\ttotal: 49.4s\tremaining: 4m 37s\n",
      "151:\tlearn: 0.0030906\ttotal: 49.8s\tremaining: 4m 37s\n",
      "152:\tlearn: 0.0030619\ttotal: 50.1s\tremaining: 4m 37s\n",
      "153:\tlearn: 0.0030167\ttotal: 50.4s\tremaining: 4m 36s\n",
      "154:\tlearn: 0.0029959\ttotal: 50.7s\tremaining: 4m 36s\n",
      "155:\tlearn: 0.0029707\ttotal: 51.1s\tremaining: 4m 36s\n",
      "156:\tlearn: 0.0029495\ttotal: 51.4s\tremaining: 4m 35s\n",
      "157:\tlearn: 0.0029218\ttotal: 51.7s\tremaining: 4m 35s\n",
      "158:\tlearn: 0.0028879\ttotal: 52.1s\tremaining: 4m 35s\n",
      "159:\tlearn: 0.0028633\ttotal: 52.4s\tremaining: 4m 35s\n",
      "160:\tlearn: 0.0028240\ttotal: 52.7s\tremaining: 4m 34s\n",
      "161:\tlearn: 0.0027981\ttotal: 53s\tremaining: 4m 34s\n",
      "162:\tlearn: 0.0027733\ttotal: 53.4s\tremaining: 4m 34s\n",
      "163:\tlearn: 0.0027552\ttotal: 53.7s\tremaining: 4m 33s\n",
      "164:\tlearn: 0.0027409\ttotal: 54s\tremaining: 4m 33s\n",
      "165:\tlearn: 0.0027223\ttotal: 54.3s\tremaining: 4m 33s\n",
      "166:\tlearn: 0.0027079\ttotal: 54.7s\tremaining: 4m 32s\n",
      "167:\tlearn: 0.0026946\ttotal: 55s\tremaining: 4m 32s\n",
      "168:\tlearn: 0.0026736\ttotal: 55.3s\tremaining: 4m 32s\n",
      "169:\tlearn: 0.0026513\ttotal: 55.6s\tremaining: 4m 31s\n",
      "170:\tlearn: 0.0026321\ttotal: 56s\tremaining: 4m 31s\n",
      "171:\tlearn: 0.0026073\ttotal: 56.3s\tremaining: 4m 30s\n",
      "172:\tlearn: 0.0025901\ttotal: 56.6s\tremaining: 4m 30s\n",
      "173:\tlearn: 0.0025606\ttotal: 56.9s\tremaining: 4m 30s\n",
      "174:\tlearn: 0.0025426\ttotal: 57.3s\tremaining: 4m 29s\n",
      "175:\tlearn: 0.0025270\ttotal: 57.6s\tremaining: 4m 29s\n",
      "176:\tlearn: 0.0025059\ttotal: 57.9s\tremaining: 4m 29s\n",
      "177:\tlearn: 0.0024924\ttotal: 58.2s\tremaining: 4m 28s\n",
      "178:\tlearn: 0.0024764\ttotal: 58.6s\tremaining: 4m 28s\n",
      "179:\tlearn: 0.0024535\ttotal: 58.9s\tremaining: 4m 28s\n",
      "180:\tlearn: 0.0024396\ttotal: 59.2s\tremaining: 4m 27s\n",
      "181:\tlearn: 0.0024293\ttotal: 59.5s\tremaining: 4m 27s\n",
      "182:\tlearn: 0.0024152\ttotal: 59.9s\tremaining: 4m 27s\n",
      "183:\tlearn: 0.0023987\ttotal: 1m\tremaining: 4m 26s\n",
      "184:\tlearn: 0.0023815\ttotal: 1m\tremaining: 4m 26s\n",
      "185:\tlearn: 0.0023648\ttotal: 1m\tremaining: 4m 26s\n",
      "186:\tlearn: 0.0023431\ttotal: 1m 1s\tremaining: 4m 25s\n",
      "187:\tlearn: 0.0023269\ttotal: 1m 1s\tremaining: 4m 25s\n",
      "188:\tlearn: 0.0023150\ttotal: 1m 1s\tremaining: 4m 25s\n",
      "189:\tlearn: 0.0023068\ttotal: 1m 2s\tremaining: 4m 24s\n",
      "190:\tlearn: 0.0022945\ttotal: 1m 2s\tremaining: 4m 24s\n",
      "191:\tlearn: 0.0022829\ttotal: 1m 2s\tremaining: 4m 24s\n",
      "192:\tlearn: 0.0022654\ttotal: 1m 3s\tremaining: 4m 23s\n",
      "193:\tlearn: 0.0022514\ttotal: 1m 3s\tremaining: 4m 23s\n",
      "194:\tlearn: 0.0022409\ttotal: 1m 3s\tremaining: 4m 23s\n",
      "195:\tlearn: 0.0022223\ttotal: 1m 4s\tremaining: 4m 23s\n",
      "196:\tlearn: 0.0022102\ttotal: 1m 4s\tremaining: 4m 22s\n",
      "197:\tlearn: 0.0021965\ttotal: 1m 4s\tremaining: 4m 22s\n",
      "198:\tlearn: 0.0021870\ttotal: 1m 5s\tremaining: 4m 22s\n",
      "199:\tlearn: 0.0021741\ttotal: 1m 5s\tremaining: 4m 21s\n",
      "200:\tlearn: 0.0021554\ttotal: 1m 5s\tremaining: 4m 21s\n",
      "201:\tlearn: 0.0021554\ttotal: 1m 6s\tremaining: 4m 21s\n",
      "202:\tlearn: 0.0021305\ttotal: 1m 6s\tremaining: 4m 20s\n",
      "203:\tlearn: 0.0021195\ttotal: 1m 6s\tremaining: 4m 20s\n",
      "204:\tlearn: 0.0021113\ttotal: 1m 7s\tremaining: 4m 20s\n",
      "205:\tlearn: 0.0020977\ttotal: 1m 7s\tremaining: 4m 19s\n",
      "206:\tlearn: 0.0020858\ttotal: 1m 7s\tremaining: 4m 19s\n",
      "207:\tlearn: 0.0020694\ttotal: 1m 8s\tremaining: 4m 19s\n",
      "208:\tlearn: 0.0020595\ttotal: 1m 8s\tremaining: 4m 18s\n",
      "209:\tlearn: 0.0020510\ttotal: 1m 8s\tremaining: 4m 18s\n",
      "210:\tlearn: 0.0020358\ttotal: 1m 9s\tremaining: 4m 18s\n",
      "211:\tlearn: 0.0020270\ttotal: 1m 9s\tremaining: 4m 17s\n",
      "212:\tlearn: 0.0020201\ttotal: 1m 9s\tremaining: 4m 17s\n",
      "213:\tlearn: 0.0020057\ttotal: 1m 10s\tremaining: 4m 17s\n",
      "214:\tlearn: 0.0019978\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "215:\tlearn: 0.0019942\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "216:\tlearn: 0.0019858\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "217:\tlearn: 0.0019693\ttotal: 1m 11s\tremaining: 4m 15s\n",
      "218:\tlearn: 0.0019588\ttotal: 1m 11s\tremaining: 4m 15s\n",
      "219:\tlearn: 0.0019423\ttotal: 1m 11s\tremaining: 4m 15s\n",
      "220:\tlearn: 0.0019320\ttotal: 1m 12s\tremaining: 4m 14s\n",
      "221:\tlearn: 0.0019253\ttotal: 1m 12s\tremaining: 4m 14s\n",
      "222:\tlearn: 0.0019164\ttotal: 1m 12s\tremaining: 4m 14s\n",
      "223:\tlearn: 0.0019164\ttotal: 1m 13s\tremaining: 4m 13s\n",
      "224:\tlearn: 0.0019040\ttotal: 1m 13s\tremaining: 4m 13s\n",
      "225:\tlearn: 0.0018887\ttotal: 1m 13s\tremaining: 4m 13s\n",
      "226:\tlearn: 0.0018823\ttotal: 1m 14s\tremaining: 4m 12s\n",
      "227:\tlearn: 0.0018823\ttotal: 1m 14s\tremaining: 4m 12s\n",
      "228:\tlearn: 0.0018638\ttotal: 1m 14s\tremaining: 4m 12s\n",
      "229:\tlearn: 0.0018532\ttotal: 1m 15s\tremaining: 4m 11s\n",
      "230:\tlearn: 0.0018415\ttotal: 1m 15s\tremaining: 4m 11s\n",
      "231:\tlearn: 0.0018356\ttotal: 1m 15s\tremaining: 4m 11s\n",
      "232:\tlearn: 0.0018242\ttotal: 1m 16s\tremaining: 4m 10s\n",
      "233:\tlearn: 0.0018187\ttotal: 1m 16s\tremaining: 4m 10s\n",
      "234:\tlearn: 0.0018071\ttotal: 1m 16s\tremaining: 4m 10s\n",
      "235:\tlearn: 0.0017919\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "236:\tlearn: 0.0017822\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "237:\tlearn: 0.0017742\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "238:\tlearn: 0.0017639\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "239:\tlearn: 0.0017553\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "240:\tlearn: 0.0017453\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "241:\tlearn: 0.0017321\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "242:\tlearn: 0.0017226\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "243:\tlearn: 0.0017173\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "244:\tlearn: 0.0017107\ttotal: 1m 20s\tremaining: 4m 6s\n",
      "245:\tlearn: 0.0017030\ttotal: 1m 20s\tremaining: 4m 6s\n",
      "246:\tlearn: 0.0016944\ttotal: 1m 20s\tremaining: 4m 6s\n",
      "247:\tlearn: 0.0016943\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "248:\tlearn: 0.0016843\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "249:\tlearn: 0.0016763\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "250:\tlearn: 0.0016681\ttotal: 1m 22s\tremaining: 4m 5s\n",
      "251:\tlearn: 0.0016596\ttotal: 1m 22s\tremaining: 4m 4s\n",
      "252:\tlearn: 0.0016510\ttotal: 1m 22s\tremaining: 4m 4s\n",
      "253:\tlearn: 0.0016460\ttotal: 1m 23s\tremaining: 4m 3s\n",
      "254:\tlearn: 0.0016361\ttotal: 1m 23s\tremaining: 4m 3s\n",
      "255:\tlearn: 0.0016276\ttotal: 1m 23s\tremaining: 4m 3s\n",
      "256:\tlearn: 0.0016230\ttotal: 1m 24s\tremaining: 4m 2s\n",
      "257:\tlearn: 0.0016113\ttotal: 1m 24s\tremaining: 4m 2s\n",
      "258:\tlearn: 0.0016031\ttotal: 1m 24s\tremaining: 4m 2s\n",
      "259:\tlearn: 0.0015970\ttotal: 1m 25s\tremaining: 4m 1s\n",
      "260:\tlearn: 0.0015903\ttotal: 1m 25s\tremaining: 4m 1s\n",
      "261:\tlearn: 0.0015849\ttotal: 1m 25s\tremaining: 4m 1s\n",
      "262:\tlearn: 0.0015844\ttotal: 1m 26s\tremaining: 4m 1s\n",
      "263:\tlearn: 0.0015784\ttotal: 1m 26s\tremaining: 4m\n",
      "264:\tlearn: 0.0015739\ttotal: 1m 26s\tremaining: 4m\n",
      "265:\tlearn: 0.0015684\ttotal: 1m 26s\tremaining: 4m\n",
      "266:\tlearn: 0.0015590\ttotal: 1m 27s\tremaining: 3m 59s\n",
      "267:\tlearn: 0.0015527\ttotal: 1m 27s\tremaining: 3m 59s\n",
      "268:\tlearn: 0.0015525\ttotal: 1m 27s\tremaining: 3m 59s\n",
      "269:\tlearn: 0.0015525\ttotal: 1m 28s\tremaining: 3m 58s\n",
      "270:\tlearn: 0.0015487\ttotal: 1m 28s\tremaining: 3m 58s\n",
      "271:\tlearn: 0.0015472\ttotal: 1m 28s\tremaining: 3m 58s\n",
      "272:\tlearn: 0.0015405\ttotal: 1m 29s\tremaining: 3m 57s\n",
      "273:\tlearn: 0.0015328\ttotal: 1m 29s\tremaining: 3m 57s\n",
      "274:\tlearn: 0.0015286\ttotal: 1m 29s\tremaining: 3m 57s\n",
      "275:\tlearn: 0.0015223\ttotal: 1m 30s\tremaining: 3m 56s\n",
      "276:\tlearn: 0.0015223\ttotal: 1m 30s\tremaining: 3m 56s\n",
      "277:\tlearn: 0.0015223\ttotal: 1m 30s\tremaining: 3m 56s\n",
      "278:\tlearn: 0.0015182\ttotal: 1m 31s\tremaining: 3m 55s\n",
      "279:\tlearn: 0.0015096\ttotal: 1m 31s\tremaining: 3m 55s\n",
      "280:\tlearn: 0.0015096\ttotal: 1m 31s\tremaining: 3m 55s\n",
      "281:\tlearn: 0.0015024\ttotal: 1m 32s\tremaining: 3m 54s\n",
      "282:\tlearn: 0.0014971\ttotal: 1m 32s\tremaining: 3m 54s\n",
      "283:\tlearn: 0.0014907\ttotal: 1m 32s\tremaining: 3m 54s\n",
      "284:\tlearn: 0.0014907\ttotal: 1m 33s\tremaining: 3m 53s\n",
      "285:\tlearn: 0.0014864\ttotal: 1m 33s\tremaining: 3m 53s\n",
      "286:\tlearn: 0.0014864\ttotal: 1m 33s\tremaining: 3m 53s\n",
      "287:\tlearn: 0.0014863\ttotal: 1m 34s\tremaining: 3m 52s\n",
      "288:\tlearn: 0.0014786\ttotal: 1m 34s\tremaining: 3m 52s\n",
      "289:\tlearn: 0.0014755\ttotal: 1m 34s\tremaining: 3m 52s\n",
      "290:\tlearn: 0.0014701\ttotal: 1m 35s\tremaining: 3m 51s\n",
      "291:\tlearn: 0.0014700\ttotal: 1m 35s\tremaining: 3m 51s\n",
      "292:\tlearn: 0.0014655\ttotal: 1m 35s\tremaining: 3m 51s\n",
      "293:\tlearn: 0.0014599\ttotal: 1m 36s\tremaining: 3m 50s\n",
      "294:\tlearn: 0.0014591\ttotal: 1m 36s\tremaining: 3m 50s\n",
      "295:\tlearn: 0.0014591\ttotal: 1m 36s\tremaining: 3m 50s\n",
      "296:\tlearn: 0.0014532\ttotal: 1m 37s\tremaining: 3m 49s\n",
      "297:\tlearn: 0.0014477\ttotal: 1m 37s\tremaining: 3m 49s\n",
      "298:\tlearn: 0.0014405\ttotal: 1m 37s\tremaining: 3m 49s\n",
      "299:\tlearn: 0.0014363\ttotal: 1m 38s\tremaining: 3m 48s\n",
      "300:\tlearn: 0.0014300\ttotal: 1m 38s\tremaining: 3m 48s\n",
      "301:\tlearn: 0.0014299\ttotal: 1m 38s\tremaining: 3m 48s\n",
      "302:\tlearn: 0.0014299\ttotal: 1m 39s\tremaining: 3m 47s\n",
      "303:\tlearn: 0.0014230\ttotal: 1m 39s\tremaining: 3m 47s\n",
      "304:\tlearn: 0.0014230\ttotal: 1m 39s\tremaining: 3m 47s\n",
      "305:\tlearn: 0.0014230\ttotal: 1m 40s\tremaining: 3m 46s\n",
      "306:\tlearn: 0.0014230\ttotal: 1m 40s\tremaining: 3m 46s\n",
      "307:\tlearn: 0.0014230\ttotal: 1m 40s\tremaining: 3m 46s\n",
      "308:\tlearn: 0.0014230\ttotal: 1m 41s\tremaining: 3m 45s\n",
      "309:\tlearn: 0.0014228\ttotal: 1m 41s\tremaining: 3m 45s\n",
      "310:\tlearn: 0.0014188\ttotal: 1m 41s\tremaining: 3m 45s\n",
      "311:\tlearn: 0.0014188\ttotal: 1m 42s\tremaining: 3m 44s\n",
      "312:\tlearn: 0.0014134\ttotal: 1m 42s\tremaining: 3m 44s\n",
      "313:\tlearn: 0.0014111\ttotal: 1m 42s\tremaining: 3m 44s\n",
      "314:\tlearn: 0.0014111\ttotal: 1m 43s\tremaining: 3m 43s\n",
      "315:\tlearn: 0.0014049\ttotal: 1m 43s\tremaining: 3m 43s\n",
      "316:\tlearn: 0.0014048\ttotal: 1m 43s\tremaining: 3m 43s\n",
      "317:\tlearn: 0.0013992\ttotal: 1m 43s\tremaining: 3m 43s\n",
      "318:\tlearn: 0.0013937\ttotal: 1m 44s\tremaining: 3m 42s\n",
      "319:\tlearn: 0.0013937\ttotal: 1m 44s\tremaining: 3m 42s\n",
      "320:\tlearn: 0.0013887\ttotal: 1m 44s\tremaining: 3m 42s\n",
      "321:\tlearn: 0.0013886\ttotal: 1m 45s\tremaining: 3m 41s\n",
      "322:\tlearn: 0.0013886\ttotal: 1m 45s\tremaining: 3m 41s\n",
      "323:\tlearn: 0.0013886\ttotal: 1m 45s\tremaining: 3m 41s\n",
      "324:\tlearn: 0.0013885\ttotal: 1m 46s\tremaining: 3m 40s\n",
      "325:\tlearn: 0.0013884\ttotal: 1m 46s\tremaining: 3m 40s\n",
      "326:\tlearn: 0.0013842\ttotal: 1m 46s\tremaining: 3m 40s\n",
      "327:\tlearn: 0.0013842\ttotal: 1m 47s\tremaining: 3m 39s\n",
      "328:\tlearn: 0.0013795\ttotal: 1m 47s\tremaining: 3m 39s\n",
      "329:\tlearn: 0.0013794\ttotal: 1m 47s\tremaining: 3m 39s\n",
      "330:\tlearn: 0.0013749\ttotal: 1m 48s\tremaining: 3m 38s\n",
      "331:\tlearn: 0.0013748\ttotal: 1m 48s\tremaining: 3m 38s\n",
      "332:\tlearn: 0.0013699\ttotal: 1m 48s\tremaining: 3m 38s\n",
      "333:\tlearn: 0.0013685\ttotal: 1m 49s\tremaining: 3m 37s\n",
      "334:\tlearn: 0.0013685\ttotal: 1m 49s\tremaining: 3m 37s\n",
      "335:\tlearn: 0.0013636\ttotal: 1m 49s\tremaining: 3m 37s\n",
      "336:\tlearn: 0.0013572\ttotal: 1m 50s\tremaining: 3m 36s\n",
      "337:\tlearn: 0.0013569\ttotal: 1m 50s\tremaining: 3m 36s\n",
      "338:\tlearn: 0.0013568\ttotal: 1m 50s\tremaining: 3m 36s\n",
      "339:\tlearn: 0.0013567\ttotal: 1m 51s\tremaining: 3m 35s\n",
      "340:\tlearn: 0.0013537\ttotal: 1m 51s\tremaining: 3m 35s\n",
      "341:\tlearn: 0.0013509\ttotal: 1m 51s\tremaining: 3m 35s\n",
      "342:\tlearn: 0.0013494\ttotal: 1m 52s\tremaining: 3m 34s\n",
      "343:\tlearn: 0.0013494\ttotal: 1m 52s\tremaining: 3m 34s\n",
      "344:\tlearn: 0.0013493\ttotal: 1m 52s\tremaining: 3m 34s\n",
      "345:\tlearn: 0.0013457\ttotal: 1m 53s\tremaining: 3m 33s\n",
      "346:\tlearn: 0.0013457\ttotal: 1m 53s\tremaining: 3m 33s\n",
      "347:\tlearn: 0.0013433\ttotal: 1m 53s\tremaining: 3m 33s\n",
      "348:\tlearn: 0.0013412\ttotal: 1m 54s\tremaining: 3m 32s\n",
      "349:\tlearn: 0.0013356\ttotal: 1m 54s\tremaining: 3m 32s\n",
      "350:\tlearn: 0.0013329\ttotal: 1m 54s\tremaining: 3m 32s\n",
      "351:\tlearn: 0.0013329\ttotal: 1m 55s\tremaining: 3m 31s\n",
      "352:\tlearn: 0.0013327\ttotal: 1m 55s\tremaining: 3m 31s\n",
      "353:\tlearn: 0.0013307\ttotal: 1m 55s\tremaining: 3m 31s\n",
      "354:\tlearn: 0.0013305\ttotal: 1m 56s\tremaining: 3m 30s\n",
      "355:\tlearn: 0.0013305\ttotal: 1m 56s\tremaining: 3m 30s\n",
      "356:\tlearn: 0.0013304\ttotal: 1m 56s\tremaining: 3m 30s\n",
      "357:\tlearn: 0.0013304\ttotal: 1m 57s\tremaining: 3m 29s\n",
      "358:\tlearn: 0.0013280\ttotal: 1m 57s\tremaining: 3m 29s\n",
      "359:\tlearn: 0.0013264\ttotal: 1m 57s\tremaining: 3m 29s\n",
      "360:\tlearn: 0.0013264\ttotal: 1m 58s\tremaining: 3m 28s\n",
      "361:\tlearn: 0.0013228\ttotal: 1m 58s\tremaining: 3m 28s\n",
      "362:\tlearn: 0.0013184\ttotal: 1m 58s\tremaining: 3m 28s\n",
      "363:\tlearn: 0.0013167\ttotal: 1m 59s\tremaining: 3m 27s\n",
      "364:\tlearn: 0.0013166\ttotal: 1m 59s\tremaining: 3m 27s\n",
      "365:\tlearn: 0.0013130\ttotal: 1m 59s\tremaining: 3m 27s\n",
      "366:\tlearn: 0.0013083\ttotal: 2m\tremaining: 3m 26s\n",
      "367:\tlearn: 0.0013083\ttotal: 2m\tremaining: 3m 26s\n",
      "368:\tlearn: 0.0013082\ttotal: 2m\tremaining: 3m 26s\n",
      "369:\tlearn: 0.0013043\ttotal: 2m\tremaining: 3m 26s\n",
      "370:\tlearn: 0.0013043\ttotal: 2m 1s\tremaining: 3m 25s\n",
      "371:\tlearn: 0.0013043\ttotal: 2m 1s\tremaining: 3m 25s\n",
      "372:\tlearn: 0.0013040\ttotal: 2m 1s\tremaining: 3m 25s\n",
      "373:\tlearn: 0.0013017\ttotal: 2m 2s\tremaining: 3m 24s\n",
      "374:\tlearn: 0.0012998\ttotal: 2m 2s\tremaining: 3m 24s\n",
      "375:\tlearn: 0.0012996\ttotal: 2m 2s\tremaining: 3m 24s\n",
      "376:\tlearn: 0.0012968\ttotal: 2m 3s\tremaining: 3m 23s\n",
      "377:\tlearn: 0.0012968\ttotal: 2m 3s\tremaining: 3m 23s\n",
      "378:\tlearn: 0.0012967\ttotal: 2m 3s\tremaining: 3m 23s\n",
      "379:\tlearn: 0.0012967\ttotal: 2m 4s\tremaining: 3m 22s\n",
      "380:\tlearn: 0.0012962\ttotal: 2m 4s\tremaining: 3m 22s\n",
      "381:\tlearn: 0.0012960\ttotal: 2m 4s\tremaining: 3m 22s\n",
      "382:\tlearn: 0.0012926\ttotal: 2m 5s\tremaining: 3m 21s\n",
      "383:\tlearn: 0.0012926\ttotal: 2m 5s\tremaining: 3m 21s\n",
      "384:\tlearn: 0.0012925\ttotal: 2m 5s\tremaining: 3m 21s\n",
      "385:\tlearn: 0.0012924\ttotal: 2m 6s\tremaining: 3m 20s\n",
      "386:\tlearn: 0.0012866\ttotal: 2m 6s\tremaining: 3m 20s\n",
      "387:\tlearn: 0.0012866\ttotal: 2m 6s\tremaining: 3m 20s\n",
      "388:\tlearn: 0.0012866\ttotal: 2m 7s\tremaining: 3m 19s\n",
      "389:\tlearn: 0.0012841\ttotal: 2m 7s\tremaining: 3m 19s\n",
      "390:\tlearn: 0.0012840\ttotal: 2m 7s\tremaining: 3m 19s\n",
      "391:\tlearn: 0.0012798\ttotal: 2m 8s\tremaining: 3m 18s\n",
      "392:\tlearn: 0.0012798\ttotal: 2m 8s\tremaining: 3m 18s\n",
      "393:\tlearn: 0.0012794\ttotal: 2m 8s\tremaining: 3m 18s\n",
      "394:\tlearn: 0.0012764\ttotal: 2m 9s\tremaining: 3m 17s\n",
      "395:\tlearn: 0.0012744\ttotal: 2m 9s\tremaining: 3m 17s\n",
      "396:\tlearn: 0.0012736\ttotal: 2m 9s\tremaining: 3m 17s\n",
      "397:\tlearn: 0.0012736\ttotal: 2m 10s\tremaining: 3m 16s\n",
      "398:\tlearn: 0.0012700\ttotal: 2m 10s\tremaining: 3m 16s\n",
      "399:\tlearn: 0.0012700\ttotal: 2m 10s\tremaining: 3m 16s\n",
      "400:\tlearn: 0.0012700\ttotal: 2m 11s\tremaining: 3m 15s\n",
      "401:\tlearn: 0.0012698\ttotal: 2m 11s\tremaining: 3m 15s\n",
      "402:\tlearn: 0.0012698\ttotal: 2m 11s\tremaining: 3m 15s\n",
      "403:\tlearn: 0.0012666\ttotal: 2m 12s\tremaining: 3m 14s\n",
      "404:\tlearn: 0.0012666\ttotal: 2m 12s\tremaining: 3m 14s\n",
      "405:\tlearn: 0.0012666\ttotal: 2m 12s\tremaining: 3m 14s\n",
      "406:\tlearn: 0.0012666\ttotal: 2m 13s\tremaining: 3m 13s\n",
      "407:\tlearn: 0.0012664\ttotal: 2m 13s\tremaining: 3m 13s\n",
      "408:\tlearn: 0.0012610\ttotal: 2m 13s\tremaining: 3m 13s\n",
      "409:\tlearn: 0.0012610\ttotal: 2m 14s\tremaining: 3m 12s\n",
      "410:\tlearn: 0.0012609\ttotal: 2m 14s\tremaining: 3m 12s\n",
      "411:\tlearn: 0.0012574\ttotal: 2m 14s\tremaining: 3m 12s\n",
      "412:\tlearn: 0.0012571\ttotal: 2m 15s\tremaining: 3m 11s\n",
      "413:\tlearn: 0.0012531\ttotal: 2m 15s\tremaining: 3m 11s\n",
      "414:\tlearn: 0.0012490\ttotal: 2m 15s\tremaining: 3m 11s\n",
      "415:\tlearn: 0.0012486\ttotal: 2m 16s\tremaining: 3m 10s\n",
      "416:\tlearn: 0.0012486\ttotal: 2m 16s\tremaining: 3m 10s\n",
      "417:\tlearn: 0.0012426\ttotal: 2m 16s\tremaining: 3m 10s\n",
      "418:\tlearn: 0.0012426\ttotal: 2m 16s\tremaining: 3m 9s\n",
      "419:\tlearn: 0.0012425\ttotal: 2m 17s\tremaining: 3m 9s\n",
      "420:\tlearn: 0.0012425\ttotal: 2m 17s\tremaining: 3m 9s\n",
      "421:\tlearn: 0.0012425\ttotal: 2m 17s\tremaining: 3m 8s\n",
      "422:\tlearn: 0.0012401\ttotal: 2m 18s\tremaining: 3m 8s\n",
      "423:\tlearn: 0.0012348\ttotal: 2m 18s\tremaining: 3m 8s\n",
      "424:\tlearn: 0.0012348\ttotal: 2m 18s\tremaining: 3m 8s\n",
      "425:\tlearn: 0.0012347\ttotal: 2m 19s\tremaining: 3m 7s\n",
      "426:\tlearn: 0.0012347\ttotal: 2m 19s\tremaining: 3m 7s\n",
      "427:\tlearn: 0.0012330\ttotal: 2m 19s\tremaining: 3m 7s\n",
      "428:\tlearn: 0.0012329\ttotal: 2m 20s\tremaining: 3m 6s\n",
      "429:\tlearn: 0.0012249\ttotal: 2m 20s\tremaining: 3m 6s\n",
      "430:\tlearn: 0.0012249\ttotal: 2m 20s\tremaining: 3m 6s\n",
      "431:\tlearn: 0.0012248\ttotal: 2m 21s\tremaining: 3m 5s\n",
      "432:\tlearn: 0.0012203\ttotal: 2m 21s\tremaining: 3m 5s\n",
      "433:\tlearn: 0.0012169\ttotal: 2m 21s\tremaining: 3m 5s\n",
      "434:\tlearn: 0.0012166\ttotal: 2m 22s\tremaining: 3m 4s\n",
      "435:\tlearn: 0.0012166\ttotal: 2m 22s\tremaining: 3m 4s\n",
      "436:\tlearn: 0.0012130\ttotal: 2m 22s\tremaining: 3m 4s\n",
      "437:\tlearn: 0.0012130\ttotal: 2m 23s\tremaining: 3m 3s\n",
      "438:\tlearn: 0.0012130\ttotal: 2m 23s\tremaining: 3m 3s\n",
      "439:\tlearn: 0.0012129\ttotal: 2m 23s\tremaining: 3m 3s\n",
      "440:\tlearn: 0.0012129\ttotal: 2m 24s\tremaining: 3m 2s\n",
      "441:\tlearn: 0.0012129\ttotal: 2m 24s\tremaining: 3m 2s\n",
      "442:\tlearn: 0.0012129\ttotal: 2m 24s\tremaining: 3m 2s\n",
      "443:\tlearn: 0.0012129\ttotal: 2m 25s\tremaining: 3m 1s\n",
      "444:\tlearn: 0.0012124\ttotal: 2m 25s\tremaining: 3m 1s\n",
      "445:\tlearn: 0.0012122\ttotal: 2m 25s\tremaining: 3m 1s\n",
      "446:\tlearn: 0.0012093\ttotal: 2m 26s\tremaining: 3m\n",
      "447:\tlearn: 0.0012061\ttotal: 2m 26s\tremaining: 3m\n",
      "448:\tlearn: 0.0012059\ttotal: 2m 26s\tremaining: 3m\n",
      "449:\tlearn: 0.0012041\ttotal: 2m 27s\tremaining: 2m 59s\n",
      "450:\tlearn: 0.0012041\ttotal: 2m 27s\tremaining: 2m 59s\n",
      "451:\tlearn: 0.0012041\ttotal: 2m 27s\tremaining: 2m 59s\n",
      "452:\tlearn: 0.0012041\ttotal: 2m 28s\tremaining: 2m 58s\n",
      "453:\tlearn: 0.0012041\ttotal: 2m 28s\tremaining: 2m 58s\n",
      "454:\tlearn: 0.0012039\ttotal: 2m 28s\tremaining: 2m 58s\n",
      "455:\tlearn: 0.0012015\ttotal: 2m 29s\tremaining: 2m 57s\n",
      "456:\tlearn: 0.0011998\ttotal: 2m 29s\tremaining: 2m 57s\n",
      "457:\tlearn: 0.0011995\ttotal: 2m 29s\tremaining: 2m 57s\n",
      "458:\tlearn: 0.0011995\ttotal: 2m 30s\tremaining: 2m 56s\n",
      "459:\tlearn: 0.0011994\ttotal: 2m 30s\tremaining: 2m 56s\n",
      "460:\tlearn: 0.0011965\ttotal: 2m 30s\tremaining: 2m 56s\n",
      "461:\tlearn: 0.0011964\ttotal: 2m 31s\tremaining: 2m 55s\n",
      "462:\tlearn: 0.0011946\ttotal: 2m 31s\tremaining: 2m 55s\n",
      "463:\tlearn: 0.0011946\ttotal: 2m 31s\tremaining: 2m 55s\n",
      "464:\tlearn: 0.0011943\ttotal: 2m 32s\tremaining: 2m 54s\n",
      "465:\tlearn: 0.0011943\ttotal: 2m 32s\tremaining: 2m 54s\n",
      "466:\tlearn: 0.0011939\ttotal: 2m 32s\tremaining: 2m 54s\n",
      "467:\tlearn: 0.0011905\ttotal: 2m 33s\tremaining: 2m 53s\n",
      "468:\tlearn: 0.0011876\ttotal: 2m 33s\tremaining: 2m 53s\n",
      "469:\tlearn: 0.0011849\ttotal: 2m 33s\tremaining: 2m 53s\n",
      "470:\tlearn: 0.0011849\ttotal: 2m 34s\tremaining: 2m 53s\n",
      "471:\tlearn: 0.0011824\ttotal: 2m 34s\tremaining: 2m 52s\n",
      "472:\tlearn: 0.0011824\ttotal: 2m 34s\tremaining: 2m 52s\n",
      "473:\tlearn: 0.0011806\ttotal: 2m 35s\tremaining: 2m 52s\n",
      "474:\tlearn: 0.0011806\ttotal: 2m 35s\tremaining: 2m 51s\n",
      "475:\tlearn: 0.0011804\ttotal: 2m 35s\tremaining: 2m 51s\n",
      "476:\tlearn: 0.0011804\ttotal: 2m 35s\tremaining: 2m 51s\n",
      "477:\tlearn: 0.0011804\ttotal: 2m 36s\tremaining: 2m 50s\n",
      "478:\tlearn: 0.0011804\ttotal: 2m 36s\tremaining: 2m 50s\n",
      "479:\tlearn: 0.0011804\ttotal: 2m 36s\tremaining: 2m 50s\n",
      "480:\tlearn: 0.0011803\ttotal: 2m 37s\tremaining: 2m 49s\n",
      "481:\tlearn: 0.0011803\ttotal: 2m 37s\tremaining: 2m 49s\n",
      "482:\tlearn: 0.0011803\ttotal: 2m 37s\tremaining: 2m 49s\n",
      "483:\tlearn: 0.0011803\ttotal: 2m 38s\tremaining: 2m 48s\n",
      "484:\tlearn: 0.0011802\ttotal: 2m 38s\tremaining: 2m 48s\n",
      "485:\tlearn: 0.0011802\ttotal: 2m 38s\tremaining: 2m 48s\n",
      "486:\tlearn: 0.0011801\ttotal: 2m 39s\tremaining: 2m 47s\n",
      "487:\tlearn: 0.0011799\ttotal: 2m 39s\tremaining: 2m 47s\n",
      "488:\tlearn: 0.0011798\ttotal: 2m 39s\tremaining: 2m 47s\n",
      "489:\tlearn: 0.0011798\ttotal: 2m 40s\tremaining: 2m 46s\n",
      "490:\tlearn: 0.0011798\ttotal: 2m 40s\tremaining: 2m 46s\n",
      "491:\tlearn: 0.0011797\ttotal: 2m 40s\tremaining: 2m 46s\n",
      "492:\tlearn: 0.0011797\ttotal: 2m 41s\tremaining: 2m 45s\n",
      "493:\tlearn: 0.0011796\ttotal: 2m 41s\tremaining: 2m 45s\n",
      "494:\tlearn: 0.0011796\ttotal: 2m 41s\tremaining: 2m 45s\n",
      "495:\tlearn: 0.0011794\ttotal: 2m 42s\tremaining: 2m 44s\n",
      "496:\tlearn: 0.0011769\ttotal: 2m 42s\tremaining: 2m 44s\n",
      "497:\tlearn: 0.0011769\ttotal: 2m 42s\tremaining: 2m 44s\n",
      "498:\tlearn: 0.0011769\ttotal: 2m 43s\tremaining: 2m 43s\n",
      "499:\tlearn: 0.0011768\ttotal: 2m 43s\tremaining: 2m 43s\n",
      "500:\tlearn: 0.0011766\ttotal: 2m 43s\tremaining: 2m 43s\n",
      "501:\tlearn: 0.0011766\ttotal: 2m 44s\tremaining: 2m 42s\n",
      "502:\tlearn: 0.0011730\ttotal: 2m 44s\tremaining: 2m 42s\n",
      "503:\tlearn: 0.0011714\ttotal: 2m 44s\tremaining: 2m 42s\n",
      "504:\tlearn: 0.0011713\ttotal: 2m 45s\tremaining: 2m 41s\n",
      "505:\tlearn: 0.0011708\ttotal: 2m 45s\tremaining: 2m 41s\n",
      "506:\tlearn: 0.0011708\ttotal: 2m 45s\tremaining: 2m 41s\n",
      "507:\tlearn: 0.0011690\ttotal: 2m 46s\tremaining: 2m 40s\n",
      "508:\tlearn: 0.0011690\ttotal: 2m 46s\tremaining: 2m 40s\n",
      "509:\tlearn: 0.0011660\ttotal: 2m 46s\tremaining: 2m 40s\n",
      "510:\tlearn: 0.0011660\ttotal: 2m 47s\tremaining: 2m 39s\n",
      "511:\tlearn: 0.0011628\ttotal: 2m 47s\tremaining: 2m 39s\n",
      "512:\tlearn: 0.0011626\ttotal: 2m 47s\tremaining: 2m 39s\n",
      "513:\tlearn: 0.0011626\ttotal: 2m 48s\tremaining: 2m 38s\n",
      "514:\tlearn: 0.0011626\ttotal: 2m 48s\tremaining: 2m 38s\n",
      "515:\tlearn: 0.0011625\ttotal: 2m 48s\tremaining: 2m 38s\n",
      "516:\tlearn: 0.0011625\ttotal: 2m 49s\tremaining: 2m 37s\n",
      "517:\tlearn: 0.0011624\ttotal: 2m 49s\tremaining: 2m 37s\n",
      "518:\tlearn: 0.0011624\ttotal: 2m 49s\tremaining: 2m 37s\n",
      "519:\tlearn: 0.0011624\ttotal: 2m 50s\tremaining: 2m 36s\n",
      "520:\tlearn: 0.0011623\ttotal: 2m 50s\tremaining: 2m 36s\n",
      "521:\tlearn: 0.0011623\ttotal: 2m 50s\tremaining: 2m 36s\n",
      "522:\tlearn: 0.0011590\ttotal: 2m 51s\tremaining: 2m 35s\n",
      "523:\tlearn: 0.0011568\ttotal: 2m 51s\tremaining: 2m 35s\n",
      "524:\tlearn: 0.0011568\ttotal: 2m 51s\tremaining: 2m 35s\n",
      "525:\tlearn: 0.0011568\ttotal: 2m 51s\tremaining: 2m 34s\n",
      "526:\tlearn: 0.0011568\ttotal: 2m 52s\tremaining: 2m 34s\n",
      "527:\tlearn: 0.0011558\ttotal: 2m 52s\tremaining: 2m 34s\n",
      "528:\tlearn: 0.0011535\ttotal: 2m 52s\tremaining: 2m 34s\n",
      "529:\tlearn: 0.0011534\ttotal: 2m 53s\tremaining: 2m 33s\n",
      "530:\tlearn: 0.0011533\ttotal: 2m 53s\tremaining: 2m 33s\n",
      "531:\tlearn: 0.0011531\ttotal: 2m 53s\tremaining: 2m 33s\n",
      "532:\tlearn: 0.0011531\ttotal: 2m 54s\tremaining: 2m 32s\n",
      "533:\tlearn: 0.0011531\ttotal: 2m 54s\tremaining: 2m 32s\n",
      "534:\tlearn: 0.0011531\ttotal: 2m 54s\tremaining: 2m 32s\n",
      "535:\tlearn: 0.0011517\ttotal: 2m 55s\tremaining: 2m 31s\n",
      "536:\tlearn: 0.0011517\ttotal: 2m 55s\tremaining: 2m 31s\n",
      "537:\tlearn: 0.0011481\ttotal: 2m 55s\tremaining: 2m 31s\n",
      "538:\tlearn: 0.0011481\ttotal: 2m 56s\tremaining: 2m 30s\n",
      "539:\tlearn: 0.0011480\ttotal: 2m 56s\tremaining: 2m 30s\n",
      "540:\tlearn: 0.0011456\ttotal: 2m 56s\tremaining: 2m 30s\n",
      "541:\tlearn: 0.0011456\ttotal: 2m 57s\tremaining: 2m 29s\n",
      "542:\tlearn: 0.0011455\ttotal: 2m 57s\tremaining: 2m 29s\n",
      "543:\tlearn: 0.0011455\ttotal: 2m 57s\tremaining: 2m 29s\n",
      "544:\tlearn: 0.0011454\ttotal: 2m 58s\tremaining: 2m 28s\n",
      "545:\tlearn: 0.0011454\ttotal: 2m 58s\tremaining: 2m 28s\n",
      "546:\tlearn: 0.0011454\ttotal: 2m 58s\tremaining: 2m 28s\n",
      "547:\tlearn: 0.0011454\ttotal: 2m 59s\tremaining: 2m 27s\n",
      "548:\tlearn: 0.0011454\ttotal: 2m 59s\tremaining: 2m 27s\n",
      "549:\tlearn: 0.0011450\ttotal: 2m 59s\tremaining: 2m 27s\n",
      "550:\tlearn: 0.0011450\ttotal: 3m\tremaining: 2m 26s\n",
      "551:\tlearn: 0.0011412\ttotal: 3m\tremaining: 2m 26s\n",
      "552:\tlearn: 0.0011412\ttotal: 3m\tremaining: 2m 26s\n",
      "553:\tlearn: 0.0011410\ttotal: 3m 1s\tremaining: 2m 25s\n",
      "554:\tlearn: 0.0011410\ttotal: 3m 1s\tremaining: 2m 25s\n",
      "555:\tlearn: 0.0011409\ttotal: 3m 1s\tremaining: 2m 25s\n",
      "556:\tlearn: 0.0011409\ttotal: 3m 2s\tremaining: 2m 24s\n",
      "557:\tlearn: 0.0011409\ttotal: 3m 2s\tremaining: 2m 24s\n",
      "558:\tlearn: 0.0011409\ttotal: 3m 2s\tremaining: 2m 24s\n",
      "559:\tlearn: 0.0011409\ttotal: 3m 3s\tremaining: 2m 23s\n",
      "560:\tlearn: 0.0011382\ttotal: 3m 3s\tremaining: 2m 23s\n",
      "561:\tlearn: 0.0011346\ttotal: 3m 3s\tremaining: 2m 23s\n",
      "562:\tlearn: 0.0011346\ttotal: 3m 4s\tremaining: 2m 22s\n",
      "563:\tlearn: 0.0011345\ttotal: 3m 4s\tremaining: 2m 22s\n",
      "564:\tlearn: 0.0011315\ttotal: 3m 4s\tremaining: 2m 22s\n",
      "565:\tlearn: 0.0011315\ttotal: 3m 5s\tremaining: 2m 21s\n",
      "566:\tlearn: 0.0011315\ttotal: 3m 5s\tremaining: 2m 21s\n",
      "567:\tlearn: 0.0011314\ttotal: 3m 5s\tremaining: 2m 21s\n",
      "568:\tlearn: 0.0011303\ttotal: 3m 6s\tremaining: 2m 20s\n",
      "569:\tlearn: 0.0011303\ttotal: 3m 6s\tremaining: 2m 20s\n",
      "570:\tlearn: 0.0011303\ttotal: 3m 6s\tremaining: 2m 20s\n",
      "571:\tlearn: 0.0011303\ttotal: 3m 6s\tremaining: 2m 19s\n",
      "572:\tlearn: 0.0011303\ttotal: 3m 7s\tremaining: 2m 19s\n",
      "573:\tlearn: 0.0011298\ttotal: 3m 7s\tremaining: 2m 19s\n",
      "574:\tlearn: 0.0011297\ttotal: 3m 7s\tremaining: 2m 18s\n",
      "575:\tlearn: 0.0011295\ttotal: 3m 8s\tremaining: 2m 18s\n",
      "576:\tlearn: 0.0011295\ttotal: 3m 8s\tremaining: 2m 18s\n",
      "577:\tlearn: 0.0011294\ttotal: 3m 8s\tremaining: 2m 17s\n",
      "578:\tlearn: 0.0011262\ttotal: 3m 9s\tremaining: 2m 17s\n",
      "579:\tlearn: 0.0011261\ttotal: 3m 9s\tremaining: 2m 17s\n",
      "580:\tlearn: 0.0011218\ttotal: 3m 9s\tremaining: 2m 16s\n",
      "581:\tlearn: 0.0011217\ttotal: 3m 10s\tremaining: 2m 16s\n",
      "582:\tlearn: 0.0011216\ttotal: 3m 10s\tremaining: 2m 16s\n",
      "583:\tlearn: 0.0011216\ttotal: 3m 10s\tremaining: 2m 15s\n",
      "584:\tlearn: 0.0011216\ttotal: 3m 11s\tremaining: 2m 15s\n",
      "585:\tlearn: 0.0011216\ttotal: 3m 11s\tremaining: 2m 15s\n",
      "586:\tlearn: 0.0011216\ttotal: 3m 11s\tremaining: 2m 14s\n",
      "587:\tlearn: 0.0011215\ttotal: 3m 12s\tremaining: 2m 14s\n",
      "588:\tlearn: 0.0011215\ttotal: 3m 12s\tremaining: 2m 14s\n",
      "589:\tlearn: 0.0011215\ttotal: 3m 12s\tremaining: 2m 14s\n",
      "590:\tlearn: 0.0011215\ttotal: 3m 13s\tremaining: 2m 13s\n",
      "591:\tlearn: 0.0011213\ttotal: 3m 13s\tremaining: 2m 13s\n",
      "592:\tlearn: 0.0011212\ttotal: 3m 13s\tremaining: 2m 13s\n",
      "593:\tlearn: 0.0011203\ttotal: 3m 14s\tremaining: 2m 12s\n",
      "594:\tlearn: 0.0011203\ttotal: 3m 14s\tremaining: 2m 12s\n",
      "595:\tlearn: 0.0011172\ttotal: 3m 14s\tremaining: 2m 12s\n",
      "596:\tlearn: 0.0011171\ttotal: 3m 15s\tremaining: 2m 11s\n",
      "597:\tlearn: 0.0011171\ttotal: 3m 15s\tremaining: 2m 11s\n",
      "598:\tlearn: 0.0011171\ttotal: 3m 15s\tremaining: 2m 11s\n",
      "599:\tlearn: 0.0011171\ttotal: 3m 16s\tremaining: 2m 10s\n",
      "600:\tlearn: 0.0011170\ttotal: 3m 16s\tremaining: 2m 10s\n",
      "601:\tlearn: 0.0011169\ttotal: 3m 16s\tremaining: 2m 10s\n",
      "602:\tlearn: 0.0011169\ttotal: 3m 17s\tremaining: 2m 9s\n",
      "603:\tlearn: 0.0011169\ttotal: 3m 17s\tremaining: 2m 9s\n",
      "604:\tlearn: 0.0011169\ttotal: 3m 17s\tremaining: 2m 9s\n",
      "605:\tlearn: 0.0011169\ttotal: 3m 18s\tremaining: 2m 8s\n",
      "606:\tlearn: 0.0011169\ttotal: 3m 18s\tremaining: 2m 8s\n",
      "607:\tlearn: 0.0011169\ttotal: 3m 18s\tremaining: 2m 8s\n",
      "608:\tlearn: 0.0011153\ttotal: 3m 19s\tremaining: 2m 7s\n",
      "609:\tlearn: 0.0011153\ttotal: 3m 19s\tremaining: 2m 7s\n",
      "610:\tlearn: 0.0011153\ttotal: 3m 19s\tremaining: 2m 7s\n",
      "611:\tlearn: 0.0011152\ttotal: 3m 20s\tremaining: 2m 6s\n",
      "612:\tlearn: 0.0011152\ttotal: 3m 20s\tremaining: 2m 6s\n",
      "613:\tlearn: 0.0011152\ttotal: 3m 20s\tremaining: 2m 6s\n",
      "614:\tlearn: 0.0011151\ttotal: 3m 21s\tremaining: 2m 5s\n",
      "615:\tlearn: 0.0011151\ttotal: 3m 21s\tremaining: 2m 5s\n",
      "616:\tlearn: 0.0011151\ttotal: 3m 21s\tremaining: 2m 5s\n",
      "617:\tlearn: 0.0011151\ttotal: 3m 21s\tremaining: 2m 4s\n",
      "618:\tlearn: 0.0011150\ttotal: 3m 22s\tremaining: 2m 4s\n",
      "619:\tlearn: 0.0011149\ttotal: 3m 22s\tremaining: 2m 4s\n",
      "620:\tlearn: 0.0011147\ttotal: 3m 22s\tremaining: 2m 3s\n",
      "621:\tlearn: 0.0011147\ttotal: 3m 23s\tremaining: 2m 3s\n",
      "622:\tlearn: 0.0011094\ttotal: 3m 23s\tremaining: 2m 3s\n",
      "623:\tlearn: 0.0011094\ttotal: 3m 23s\tremaining: 2m 2s\n",
      "624:\tlearn: 0.0011094\ttotal: 3m 24s\tremaining: 2m 2s\n",
      "625:\tlearn: 0.0011094\ttotal: 3m 24s\tremaining: 2m 2s\n",
      "626:\tlearn: 0.0011093\ttotal: 3m 24s\tremaining: 2m 1s\n",
      "627:\tlearn: 0.0011092\ttotal: 3m 25s\tremaining: 2m 1s\n",
      "628:\tlearn: 0.0011092\ttotal: 3m 25s\tremaining: 2m 1s\n",
      "629:\tlearn: 0.0011092\ttotal: 3m 25s\tremaining: 2m\n",
      "630:\tlearn: 0.0011066\ttotal: 3m 26s\tremaining: 2m\n",
      "631:\tlearn: 0.0011065\ttotal: 3m 26s\tremaining: 2m\n",
      "632:\tlearn: 0.0011065\ttotal: 3m 26s\tremaining: 1m 59s\n",
      "633:\tlearn: 0.0011065\ttotal: 3m 27s\tremaining: 1m 59s\n",
      "634:\tlearn: 0.0011064\ttotal: 3m 27s\tremaining: 1m 59s\n",
      "635:\tlearn: 0.0011064\ttotal: 3m 27s\tremaining: 1m 58s\n",
      "636:\tlearn: 0.0011064\ttotal: 3m 28s\tremaining: 1m 58s\n",
      "637:\tlearn: 0.0011063\ttotal: 3m 28s\tremaining: 1m 58s\n",
      "638:\tlearn: 0.0011062\ttotal: 3m 28s\tremaining: 1m 58s\n",
      "639:\tlearn: 0.0011061\ttotal: 3m 29s\tremaining: 1m 57s\n",
      "640:\tlearn: 0.0011060\ttotal: 3m 29s\tremaining: 1m 57s\n",
      "641:\tlearn: 0.0011059\ttotal: 3m 29s\tremaining: 1m 57s\n",
      "642:\tlearn: 0.0011035\ttotal: 3m 30s\tremaining: 1m 56s\n",
      "643:\tlearn: 0.0011032\ttotal: 3m 30s\tremaining: 1m 56s\n",
      "644:\tlearn: 0.0011032\ttotal: 3m 30s\tremaining: 1m 56s\n",
      "645:\tlearn: 0.0011029\ttotal: 3m 31s\tremaining: 1m 55s\n",
      "646:\tlearn: 0.0011028\ttotal: 3m 31s\tremaining: 1m 55s\n",
      "647:\tlearn: 0.0011028\ttotal: 3m 31s\tremaining: 1m 55s\n",
      "648:\tlearn: 0.0011028\ttotal: 3m 32s\tremaining: 1m 54s\n",
      "649:\tlearn: 0.0011027\ttotal: 3m 32s\tremaining: 1m 54s\n",
      "650:\tlearn: 0.0011026\ttotal: 3m 32s\tremaining: 1m 54s\n",
      "651:\tlearn: 0.0010991\ttotal: 3m 33s\tremaining: 1m 53s\n",
      "652:\tlearn: 0.0010973\ttotal: 3m 33s\tremaining: 1m 53s\n",
      "653:\tlearn: 0.0010972\ttotal: 3m 33s\tremaining: 1m 53s\n",
      "654:\tlearn: 0.0010972\ttotal: 3m 34s\tremaining: 1m 52s\n",
      "655:\tlearn: 0.0010959\ttotal: 3m 34s\tremaining: 1m 52s\n",
      "656:\tlearn: 0.0010958\ttotal: 3m 34s\tremaining: 1m 52s\n",
      "657:\tlearn: 0.0010958\ttotal: 3m 35s\tremaining: 1m 51s\n",
      "658:\tlearn: 0.0010958\ttotal: 3m 35s\tremaining: 1m 51s\n",
      "659:\tlearn: 0.0010932\ttotal: 3m 35s\tremaining: 1m 51s\n",
      "660:\tlearn: 0.0010932\ttotal: 3m 36s\tremaining: 1m 50s\n",
      "661:\tlearn: 0.0010932\ttotal: 3m 36s\tremaining: 1m 50s\n",
      "662:\tlearn: 0.0010932\ttotal: 3m 36s\tremaining: 1m 50s\n",
      "663:\tlearn: 0.0010931\ttotal: 3m 37s\tremaining: 1m 49s\n",
      "664:\tlearn: 0.0010913\ttotal: 3m 37s\tremaining: 1m 49s\n",
      "665:\tlearn: 0.0010897\ttotal: 3m 37s\tremaining: 1m 49s\n",
      "666:\tlearn: 0.0010897\ttotal: 3m 38s\tremaining: 1m 48s\n",
      "667:\tlearn: 0.0010896\ttotal: 3m 38s\tremaining: 1m 48s\n",
      "668:\tlearn: 0.0010896\ttotal: 3m 38s\tremaining: 1m 48s\n",
      "669:\tlearn: 0.0010896\ttotal: 3m 38s\tremaining: 1m 47s\n",
      "670:\tlearn: 0.0010896\ttotal: 3m 39s\tremaining: 1m 47s\n",
      "671:\tlearn: 0.0010896\ttotal: 3m 39s\tremaining: 1m 47s\n",
      "672:\tlearn: 0.0010895\ttotal: 3m 39s\tremaining: 1m 46s\n",
      "673:\tlearn: 0.0010895\ttotal: 3m 40s\tremaining: 1m 46s\n",
      "674:\tlearn: 0.0010894\ttotal: 3m 40s\tremaining: 1m 46s\n",
      "675:\tlearn: 0.0010894\ttotal: 3m 40s\tremaining: 1m 45s\n",
      "676:\tlearn: 0.0010871\ttotal: 3m 41s\tremaining: 1m 45s\n",
      "677:\tlearn: 0.0010871\ttotal: 3m 41s\tremaining: 1m 45s\n",
      "678:\tlearn: 0.0010870\ttotal: 3m 41s\tremaining: 1m 44s\n",
      "679:\tlearn: 0.0010853\ttotal: 3m 42s\tremaining: 1m 44s\n",
      "680:\tlearn: 0.0010852\ttotal: 3m 42s\tremaining: 1m 44s\n",
      "681:\tlearn: 0.0010852\ttotal: 3m 42s\tremaining: 1m 43s\n",
      "682:\tlearn: 0.0010832\ttotal: 3m 43s\tremaining: 1m 43s\n",
      "683:\tlearn: 0.0010830\ttotal: 3m 43s\tremaining: 1m 43s\n",
      "684:\tlearn: 0.0010830\ttotal: 3m 43s\tremaining: 1m 42s\n",
      "685:\tlearn: 0.0010830\ttotal: 3m 44s\tremaining: 1m 42s\n",
      "686:\tlearn: 0.0010829\ttotal: 3m 44s\tremaining: 1m 42s\n",
      "687:\tlearn: 0.0010829\ttotal: 3m 44s\tremaining: 1m 41s\n",
      "688:\tlearn: 0.0010829\ttotal: 3m 45s\tremaining: 1m 41s\n",
      "689:\tlearn: 0.0010829\ttotal: 3m 45s\tremaining: 1m 41s\n",
      "690:\tlearn: 0.0010829\ttotal: 3m 45s\tremaining: 1m 41s\n",
      "691:\tlearn: 0.0010822\ttotal: 3m 46s\tremaining: 1m 40s\n",
      "692:\tlearn: 0.0010822\ttotal: 3m 46s\tremaining: 1m 40s\n",
      "693:\tlearn: 0.0010821\ttotal: 3m 46s\tremaining: 1m 40s\n",
      "694:\tlearn: 0.0010814\ttotal: 3m 47s\tremaining: 1m 39s\n",
      "695:\tlearn: 0.0010814\ttotal: 3m 47s\tremaining: 1m 39s\n",
      "696:\tlearn: 0.0010813\ttotal: 3m 47s\tremaining: 1m 39s\n",
      "697:\tlearn: 0.0010813\ttotal: 3m 48s\tremaining: 1m 38s\n",
      "698:\tlearn: 0.0010813\ttotal: 3m 48s\tremaining: 1m 38s\n",
      "699:\tlearn: 0.0010813\ttotal: 3m 48s\tremaining: 1m 38s\n",
      "700:\tlearn: 0.0010812\ttotal: 3m 49s\tremaining: 1m 37s\n",
      "701:\tlearn: 0.0010812\ttotal: 3m 49s\tremaining: 1m 37s\n",
      "702:\tlearn: 0.0010812\ttotal: 3m 49s\tremaining: 1m 37s\n",
      "703:\tlearn: 0.0010811\ttotal: 3m 50s\tremaining: 1m 36s\n",
      "704:\tlearn: 0.0010810\ttotal: 3m 50s\tremaining: 1m 36s\n",
      "705:\tlearn: 0.0010810\ttotal: 3m 50s\tremaining: 1m 36s\n",
      "706:\tlearn: 0.0010810\ttotal: 3m 51s\tremaining: 1m 35s\n",
      "707:\tlearn: 0.0010786\ttotal: 3m 51s\tremaining: 1m 35s\n",
      "708:\tlearn: 0.0010786\ttotal: 3m 51s\tremaining: 1m 35s\n",
      "709:\tlearn: 0.0010786\ttotal: 3m 52s\tremaining: 1m 34s\n",
      "710:\tlearn: 0.0010786\ttotal: 3m 52s\tremaining: 1m 34s\n",
      "711:\tlearn: 0.0010786\ttotal: 3m 52s\tremaining: 1m 34s\n",
      "712:\tlearn: 0.0010776\ttotal: 3m 53s\tremaining: 1m 33s\n",
      "713:\tlearn: 0.0010776\ttotal: 3m 53s\tremaining: 1m 33s\n",
      "714:\tlearn: 0.0010775\ttotal: 3m 53s\tremaining: 1m 33s\n",
      "715:\tlearn: 0.0010760\ttotal: 3m 54s\tremaining: 1m 32s\n",
      "716:\tlearn: 0.0010760\ttotal: 3m 54s\tremaining: 1m 32s\n",
      "717:\tlearn: 0.0010757\ttotal: 3m 54s\tremaining: 1m 32s\n",
      "718:\tlearn: 0.0010757\ttotal: 3m 55s\tremaining: 1m 31s\n",
      "719:\tlearn: 0.0010757\ttotal: 3m 55s\tremaining: 1m 31s\n",
      "720:\tlearn: 0.0010757\ttotal: 3m 55s\tremaining: 1m 31s\n",
      "721:\tlearn: 0.0010757\ttotal: 3m 56s\tremaining: 1m 30s\n",
      "722:\tlearn: 0.0010738\ttotal: 3m 56s\tremaining: 1m 30s\n",
      "723:\tlearn: 0.0010738\ttotal: 3m 56s\tremaining: 1m 30s\n",
      "724:\tlearn: 0.0010738\ttotal: 3m 57s\tremaining: 1m 29s\n",
      "725:\tlearn: 0.0010738\ttotal: 3m 57s\tremaining: 1m 29s\n",
      "726:\tlearn: 0.0010726\ttotal: 3m 57s\tremaining: 1m 29s\n",
      "727:\tlearn: 0.0010724\ttotal: 3m 57s\tremaining: 1m 28s\n",
      "728:\tlearn: 0.0010724\ttotal: 3m 58s\tremaining: 1m 28s\n",
      "729:\tlearn: 0.0010723\ttotal: 3m 58s\tremaining: 1m 28s\n",
      "730:\tlearn: 0.0010722\ttotal: 3m 58s\tremaining: 1m 27s\n",
      "731:\tlearn: 0.0010722\ttotal: 3m 59s\tremaining: 1m 27s\n",
      "732:\tlearn: 0.0010722\ttotal: 3m 59s\tremaining: 1m 27s\n",
      "733:\tlearn: 0.0010702\ttotal: 3m 59s\tremaining: 1m 26s\n",
      "734:\tlearn: 0.0010701\ttotal: 4m\tremaining: 1m 26s\n",
      "735:\tlearn: 0.0010701\ttotal: 4m\tremaining: 1m 26s\n",
      "736:\tlearn: 0.0010698\ttotal: 4m\tremaining: 1m 25s\n",
      "737:\tlearn: 0.0010695\ttotal: 4m 1s\tremaining: 1m 25s\n",
      "738:\tlearn: 0.0010666\ttotal: 4m 1s\tremaining: 1m 25s\n",
      "739:\tlearn: 0.0010632\ttotal: 4m 1s\tremaining: 1m 24s\n",
      "740:\tlearn: 0.0010631\ttotal: 4m 2s\tremaining: 1m 24s\n",
      "741:\tlearn: 0.0010620\ttotal: 4m 2s\tremaining: 1m 24s\n",
      "742:\tlearn: 0.0010620\ttotal: 4m 2s\tremaining: 1m 24s\n",
      "743:\tlearn: 0.0010619\ttotal: 4m 3s\tremaining: 1m 23s\n",
      "744:\tlearn: 0.0010614\ttotal: 4m 3s\tremaining: 1m 23s\n",
      "745:\tlearn: 0.0010614\ttotal: 4m 3s\tremaining: 1m 23s\n",
      "746:\tlearn: 0.0010614\ttotal: 4m 4s\tremaining: 1m 22s\n",
      "747:\tlearn: 0.0010613\ttotal: 4m 4s\tremaining: 1m 22s\n",
      "748:\tlearn: 0.0010588\ttotal: 4m 4s\tremaining: 1m 22s\n",
      "749:\tlearn: 0.0010575\ttotal: 4m 5s\tremaining: 1m 21s\n",
      "750:\tlearn: 0.0010559\ttotal: 4m 5s\tremaining: 1m 21s\n",
      "751:\tlearn: 0.0010544\ttotal: 4m 5s\tremaining: 1m 21s\n",
      "752:\tlearn: 0.0010543\ttotal: 4m 6s\tremaining: 1m 20s\n",
      "753:\tlearn: 0.0010505\ttotal: 4m 6s\tremaining: 1m 20s\n",
      "754:\tlearn: 0.0010505\ttotal: 4m 6s\tremaining: 1m 20s\n",
      "755:\tlearn: 0.0010505\ttotal: 4m 7s\tremaining: 1m 19s\n",
      "756:\tlearn: 0.0010488\ttotal: 4m 7s\tremaining: 1m 19s\n",
      "757:\tlearn: 0.0010487\ttotal: 4m 7s\tremaining: 1m 19s\n",
      "758:\tlearn: 0.0010487\ttotal: 4m 8s\tremaining: 1m 18s\n",
      "759:\tlearn: 0.0010487\ttotal: 4m 8s\tremaining: 1m 18s\n",
      "760:\tlearn: 0.0010487\ttotal: 4m 8s\tremaining: 1m 18s\n",
      "761:\tlearn: 0.0010487\ttotal: 4m 9s\tremaining: 1m 17s\n",
      "762:\tlearn: 0.0010487\ttotal: 4m 9s\tremaining: 1m 17s\n",
      "763:\tlearn: 0.0010461\ttotal: 4m 9s\tremaining: 1m 17s\n",
      "764:\tlearn: 0.0010459\ttotal: 4m 10s\tremaining: 1m 16s\n",
      "765:\tlearn: 0.0010458\ttotal: 4m 10s\tremaining: 1m 16s\n",
      "766:\tlearn: 0.0010458\ttotal: 4m 10s\tremaining: 1m 16s\n",
      "767:\tlearn: 0.0010458\ttotal: 4m 11s\tremaining: 1m 15s\n",
      "768:\tlearn: 0.0010458\ttotal: 4m 11s\tremaining: 1m 15s\n",
      "769:\tlearn: 0.0010458\ttotal: 4m 11s\tremaining: 1m 15s\n",
      "770:\tlearn: 0.0010458\ttotal: 4m 12s\tremaining: 1m 14s\n",
      "771:\tlearn: 0.0010458\ttotal: 4m 12s\tremaining: 1m 14s\n",
      "772:\tlearn: 0.0010457\ttotal: 4m 12s\tremaining: 1m 14s\n",
      "773:\tlearn: 0.0010457\ttotal: 4m 12s\tremaining: 1m 13s\n",
      "774:\tlearn: 0.0010457\ttotal: 4m 13s\tremaining: 1m 13s\n",
      "775:\tlearn: 0.0010456\ttotal: 4m 13s\tremaining: 1m 13s\n",
      "776:\tlearn: 0.0010456\ttotal: 4m 13s\tremaining: 1m 12s\n",
      "777:\tlearn: 0.0010456\ttotal: 4m 14s\tremaining: 1m 12s\n",
      "778:\tlearn: 0.0010454\ttotal: 4m 14s\tremaining: 1m 12s\n",
      "779:\tlearn: 0.0010453\ttotal: 4m 15s\tremaining: 1m 11s\n",
      "780:\tlearn: 0.0010453\ttotal: 4m 15s\tremaining: 1m 11s\n",
      "781:\tlearn: 0.0010452\ttotal: 4m 15s\tremaining: 1m 11s\n",
      "782:\tlearn: 0.0010441\ttotal: 4m 16s\tremaining: 1m 10s\n",
      "783:\tlearn: 0.0010440\ttotal: 4m 16s\tremaining: 1m 10s\n",
      "784:\tlearn: 0.0010439\ttotal: 4m 16s\tremaining: 1m 10s\n",
      "785:\tlearn: 0.0010439\ttotal: 4m 16s\tremaining: 1m 9s\n",
      "786:\tlearn: 0.0010435\ttotal: 4m 17s\tremaining: 1m 9s\n",
      "787:\tlearn: 0.0010434\ttotal: 4m 17s\tremaining: 1m 9s\n",
      "788:\tlearn: 0.0010434\ttotal: 4m 17s\tremaining: 1m 8s\n",
      "789:\tlearn: 0.0010429\ttotal: 4m 18s\tremaining: 1m 8s\n",
      "790:\tlearn: 0.0010428\ttotal: 4m 18s\tremaining: 1m 8s\n",
      "791:\tlearn: 0.0010427\ttotal: 4m 18s\tremaining: 1m 8s\n",
      "792:\tlearn: 0.0010427\ttotal: 4m 19s\tremaining: 1m 7s\n",
      "793:\tlearn: 0.0010427\ttotal: 4m 19s\tremaining: 1m 7s\n",
      "794:\tlearn: 0.0010427\ttotal: 4m 19s\tremaining: 1m 7s\n",
      "795:\tlearn: 0.0010427\ttotal: 4m 20s\tremaining: 1m 6s\n",
      "796:\tlearn: 0.0010425\ttotal: 4m 20s\tremaining: 1m 6s\n",
      "797:\tlearn: 0.0010397\ttotal: 4m 20s\tremaining: 1m 6s\n",
      "798:\tlearn: 0.0010397\ttotal: 4m 21s\tremaining: 1m 5s\n",
      "799:\tlearn: 0.0010397\ttotal: 4m 21s\tremaining: 1m 5s\n",
      "800:\tlearn: 0.0010395\ttotal: 4m 21s\tremaining: 1m 5s\n",
      "801:\tlearn: 0.0010394\ttotal: 4m 22s\tremaining: 1m 4s\n",
      "802:\tlearn: 0.0010373\ttotal: 4m 22s\tremaining: 1m 4s\n",
      "803:\tlearn: 0.0010372\ttotal: 4m 22s\tremaining: 1m 4s\n",
      "804:\tlearn: 0.0010372\ttotal: 4m 23s\tremaining: 1m 3s\n",
      "805:\tlearn: 0.0010371\ttotal: 4m 23s\tremaining: 1m 3s\n",
      "806:\tlearn: 0.0010371\ttotal: 4m 23s\tremaining: 1m 3s\n",
      "807:\tlearn: 0.0010371\ttotal: 4m 24s\tremaining: 1m 2s\n",
      "808:\tlearn: 0.0010371\ttotal: 4m 24s\tremaining: 1m 2s\n",
      "809:\tlearn: 0.0010369\ttotal: 4m 24s\tremaining: 1m 2s\n",
      "810:\tlearn: 0.0010369\ttotal: 4m 25s\tremaining: 1m 1s\n",
      "811:\tlearn: 0.0010369\ttotal: 4m 25s\tremaining: 1m 1s\n",
      "812:\tlearn: 0.0010369\ttotal: 4m 25s\tremaining: 1m 1s\n",
      "813:\tlearn: 0.0010369\ttotal: 4m 26s\tremaining: 1m\n",
      "814:\tlearn: 0.0010366\ttotal: 4m 26s\tremaining: 1m\n",
      "815:\tlearn: 0.0010366\ttotal: 4m 26s\tremaining: 1m\n",
      "816:\tlearn: 0.0010363\ttotal: 4m 27s\tremaining: 59.8s\n",
      "817:\tlearn: 0.0010363\ttotal: 4m 27s\tremaining: 59.5s\n",
      "818:\tlearn: 0.0010363\ttotal: 4m 27s\tremaining: 59.2s\n",
      "819:\tlearn: 0.0010363\ttotal: 4m 28s\tremaining: 58.9s\n",
      "820:\tlearn: 0.0010363\ttotal: 4m 28s\tremaining: 58.5s\n",
      "821:\tlearn: 0.0010342\ttotal: 4m 28s\tremaining: 58.2s\n",
      "822:\tlearn: 0.0010340\ttotal: 4m 29s\tremaining: 57.9s\n",
      "823:\tlearn: 0.0010340\ttotal: 4m 29s\tremaining: 57.5s\n",
      "824:\tlearn: 0.0010340\ttotal: 4m 29s\tremaining: 57.2s\n",
      "825:\tlearn: 0.0010339\ttotal: 4m 30s\tremaining: 56.9s\n",
      "826:\tlearn: 0.0010339\ttotal: 4m 30s\tremaining: 56.6s\n",
      "827:\tlearn: 0.0010339\ttotal: 4m 30s\tremaining: 56.2s\n",
      "828:\tlearn: 0.0010302\ttotal: 4m 31s\tremaining: 55.9s\n",
      "829:\tlearn: 0.0010302\ttotal: 4m 31s\tremaining: 55.6s\n",
      "830:\tlearn: 0.0010301\ttotal: 4m 31s\tremaining: 55.3s\n",
      "831:\tlearn: 0.0010301\ttotal: 4m 32s\tremaining: 54.9s\n",
      "832:\tlearn: 0.0010301\ttotal: 4m 32s\tremaining: 54.6s\n",
      "833:\tlearn: 0.0010300\ttotal: 4m 32s\tremaining: 54.3s\n",
      "834:\tlearn: 0.0010299\ttotal: 4m 32s\tremaining: 53.9s\n",
      "835:\tlearn: 0.0010299\ttotal: 4m 33s\tremaining: 53.6s\n",
      "836:\tlearn: 0.0010299\ttotal: 4m 33s\tremaining: 53.3s\n",
      "837:\tlearn: 0.0010298\ttotal: 4m 33s\tremaining: 53s\n",
      "838:\tlearn: 0.0010298\ttotal: 4m 34s\tremaining: 52.6s\n",
      "839:\tlearn: 0.0010297\ttotal: 4m 34s\tremaining: 52.3s\n",
      "840:\tlearn: 0.0010297\ttotal: 4m 34s\tremaining: 52s\n",
      "841:\tlearn: 0.0010296\ttotal: 4m 35s\tremaining: 51.7s\n",
      "842:\tlearn: 0.0010296\ttotal: 4m 35s\tremaining: 51.3s\n",
      "843:\tlearn: 0.0010296\ttotal: 4m 35s\tremaining: 51s\n",
      "844:\tlearn: 0.0010295\ttotal: 4m 36s\tremaining: 50.7s\n",
      "845:\tlearn: 0.0010295\ttotal: 4m 36s\tremaining: 50.3s\n",
      "846:\tlearn: 0.0010293\ttotal: 4m 36s\tremaining: 50s\n",
      "847:\tlearn: 0.0010293\ttotal: 4m 37s\tremaining: 49.7s\n",
      "848:\tlearn: 0.0010293\ttotal: 4m 37s\tremaining: 49.4s\n",
      "849:\tlearn: 0.0010293\ttotal: 4m 37s\tremaining: 49s\n",
      "850:\tlearn: 0.0010292\ttotal: 4m 38s\tremaining: 48.7s\n",
      "851:\tlearn: 0.0010284\ttotal: 4m 38s\tremaining: 48.4s\n",
      "852:\tlearn: 0.0010284\ttotal: 4m 38s\tremaining: 48s\n",
      "853:\tlearn: 0.0010284\ttotal: 4m 39s\tremaining: 47.7s\n",
      "854:\tlearn: 0.0010284\ttotal: 4m 39s\tremaining: 47.4s\n",
      "855:\tlearn: 0.0010283\ttotal: 4m 39s\tremaining: 47.1s\n",
      "856:\tlearn: 0.0010258\ttotal: 4m 40s\tremaining: 46.7s\n",
      "857:\tlearn: 0.0010258\ttotal: 4m 40s\tremaining: 46.4s\n",
      "858:\tlearn: 0.0010258\ttotal: 4m 40s\tremaining: 46.1s\n",
      "859:\tlearn: 0.0010257\ttotal: 4m 41s\tremaining: 45.8s\n",
      "860:\tlearn: 0.0010257\ttotal: 4m 41s\tremaining: 45.4s\n",
      "861:\tlearn: 0.0010257\ttotal: 4m 41s\tremaining: 45.1s\n",
      "862:\tlearn: 0.0010256\ttotal: 4m 42s\tremaining: 44.8s\n",
      "863:\tlearn: 0.0010255\ttotal: 4m 42s\tremaining: 44.5s\n",
      "864:\tlearn: 0.0010255\ttotal: 4m 42s\tremaining: 44.1s\n",
      "865:\tlearn: 0.0010254\ttotal: 4m 43s\tremaining: 43.8s\n",
      "866:\tlearn: 0.0010254\ttotal: 4m 43s\tremaining: 43.5s\n",
      "867:\tlearn: 0.0010254\ttotal: 4m 43s\tremaining: 43.1s\n",
      "868:\tlearn: 0.0010253\ttotal: 4m 44s\tremaining: 42.8s\n",
      "869:\tlearn: 0.0010253\ttotal: 4m 44s\tremaining: 42.5s\n",
      "870:\tlearn: 0.0010252\ttotal: 4m 44s\tremaining: 42.2s\n",
      "871:\tlearn: 0.0010252\ttotal: 4m 44s\tremaining: 41.8s\n",
      "872:\tlearn: 0.0010252\ttotal: 4m 45s\tremaining: 41.5s\n",
      "873:\tlearn: 0.0010251\ttotal: 4m 45s\tremaining: 41.2s\n",
      "874:\tlearn: 0.0010229\ttotal: 4m 45s\tremaining: 40.9s\n",
      "875:\tlearn: 0.0010229\ttotal: 4m 46s\tremaining: 40.5s\n",
      "876:\tlearn: 0.0010229\ttotal: 4m 46s\tremaining: 40.2s\n",
      "877:\tlearn: 0.0010229\ttotal: 4m 46s\tremaining: 39.9s\n",
      "878:\tlearn: 0.0010228\ttotal: 4m 47s\tremaining: 39.5s\n",
      "879:\tlearn: 0.0010227\ttotal: 4m 47s\tremaining: 39.2s\n",
      "880:\tlearn: 0.0010210\ttotal: 4m 47s\tremaining: 38.9s\n",
      "881:\tlearn: 0.0010195\ttotal: 4m 48s\tremaining: 38.6s\n",
      "882:\tlearn: 0.0010194\ttotal: 4m 48s\tremaining: 38.2s\n",
      "883:\tlearn: 0.0010194\ttotal: 4m 48s\tremaining: 37.9s\n",
      "884:\tlearn: 0.0010194\ttotal: 4m 49s\tremaining: 37.6s\n",
      "885:\tlearn: 0.0010194\ttotal: 4m 49s\tremaining: 37.3s\n",
      "886:\tlearn: 0.0010193\ttotal: 4m 49s\tremaining: 36.9s\n",
      "887:\tlearn: 0.0010193\ttotal: 4m 50s\tremaining: 36.6s\n",
      "888:\tlearn: 0.0010174\ttotal: 4m 50s\tremaining: 36.3s\n",
      "889:\tlearn: 0.0010174\ttotal: 4m 50s\tremaining: 35.9s\n",
      "890:\tlearn: 0.0010173\ttotal: 4m 51s\tremaining: 35.6s\n",
      "891:\tlearn: 0.0010161\ttotal: 4m 51s\tremaining: 35.3s\n",
      "892:\tlearn: 0.0010161\ttotal: 4m 51s\tremaining: 35s\n",
      "893:\tlearn: 0.0010160\ttotal: 4m 52s\tremaining: 34.6s\n",
      "894:\tlearn: 0.0010137\ttotal: 4m 52s\tremaining: 34.3s\n",
      "895:\tlearn: 0.0010137\ttotal: 4m 52s\tremaining: 34s\n",
      "896:\tlearn: 0.0010136\ttotal: 4m 53s\tremaining: 33.7s\n",
      "897:\tlearn: 0.0010126\ttotal: 4m 53s\tremaining: 33.3s\n",
      "898:\tlearn: 0.0010116\ttotal: 4m 53s\tremaining: 33s\n",
      "899:\tlearn: 0.0010089\ttotal: 4m 54s\tremaining: 32.7s\n",
      "900:\tlearn: 0.0010089\ttotal: 4m 54s\tremaining: 32.4s\n",
      "901:\tlearn: 0.0010069\ttotal: 4m 54s\tremaining: 32s\n",
      "902:\tlearn: 0.0010069\ttotal: 4m 55s\tremaining: 31.7s\n",
      "903:\tlearn: 0.0010068\ttotal: 4m 55s\tremaining: 31.4s\n",
      "904:\tlearn: 0.0010068\ttotal: 4m 55s\tremaining: 31s\n",
      "905:\tlearn: 0.0010068\ttotal: 4m 56s\tremaining: 30.7s\n",
      "906:\tlearn: 0.0010068\ttotal: 4m 56s\tremaining: 30.4s\n",
      "907:\tlearn: 0.0010058\ttotal: 4m 56s\tremaining: 30.1s\n",
      "908:\tlearn: 0.0010058\ttotal: 4m 57s\tremaining: 29.7s\n",
      "909:\tlearn: 0.0010058\ttotal: 4m 57s\tremaining: 29.4s\n",
      "910:\tlearn: 0.0010058\ttotal: 4m 57s\tremaining: 29.1s\n",
      "911:\tlearn: 0.0010058\ttotal: 4m 58s\tremaining: 28.8s\n",
      "912:\tlearn: 0.0010058\ttotal: 4m 58s\tremaining: 28.4s\n",
      "913:\tlearn: 0.0010057\ttotal: 4m 58s\tremaining: 28.1s\n",
      "914:\tlearn: 0.0010056\ttotal: 4m 59s\tremaining: 27.8s\n",
      "915:\tlearn: 0.0010056\ttotal: 4m 59s\tremaining: 27.4s\n",
      "916:\tlearn: 0.0010056\ttotal: 4m 59s\tremaining: 27.1s\n",
      "917:\tlearn: 0.0010056\ttotal: 4m 59s\tremaining: 26.8s\n",
      "918:\tlearn: 0.0010055\ttotal: 5m\tremaining: 26.5s\n",
      "919:\tlearn: 0.0010055\ttotal: 5m\tremaining: 26.1s\n",
      "920:\tlearn: 0.0010052\ttotal: 5m\tremaining: 25.8s\n",
      "921:\tlearn: 0.0010052\ttotal: 5m 1s\tremaining: 25.5s\n",
      "922:\tlearn: 0.0010051\ttotal: 5m 1s\tremaining: 25.2s\n",
      "923:\tlearn: 0.0010051\ttotal: 5m 1s\tremaining: 24.8s\n",
      "924:\tlearn: 0.0010050\ttotal: 5m 2s\tremaining: 24.5s\n",
      "925:\tlearn: 0.0010050\ttotal: 5m 2s\tremaining: 24.2s\n",
      "926:\tlearn: 0.0010050\ttotal: 5m 2s\tremaining: 23.9s\n",
      "927:\tlearn: 0.0010050\ttotal: 5m 3s\tremaining: 23.5s\n",
      "928:\tlearn: 0.0010049\ttotal: 5m 3s\tremaining: 23.2s\n",
      "929:\tlearn: 0.0010049\ttotal: 5m 3s\tremaining: 22.9s\n",
      "930:\tlearn: 0.0010049\ttotal: 5m 4s\tremaining: 22.5s\n",
      "931:\tlearn: 0.0010048\ttotal: 5m 4s\tremaining: 22.2s\n",
      "932:\tlearn: 0.0010048\ttotal: 5m 4s\tremaining: 21.9s\n",
      "933:\tlearn: 0.0010040\ttotal: 5m 5s\tremaining: 21.6s\n",
      "934:\tlearn: 0.0010018\ttotal: 5m 5s\tremaining: 21.2s\n",
      "935:\tlearn: 0.0010018\ttotal: 5m 5s\tremaining: 20.9s\n",
      "936:\tlearn: 0.0010018\ttotal: 5m 6s\tremaining: 20.6s\n",
      "937:\tlearn: 0.0010017\ttotal: 5m 6s\tremaining: 20.3s\n",
      "938:\tlearn: 0.0010017\ttotal: 5m 6s\tremaining: 19.9s\n",
      "939:\tlearn: 0.0010017\ttotal: 5m 7s\tremaining: 19.6s\n",
      "940:\tlearn: 0.0010016\ttotal: 5m 7s\tremaining: 19.3s\n",
      "941:\tlearn: 0.0009991\ttotal: 5m 7s\tremaining: 19s\n",
      "942:\tlearn: 0.0009991\ttotal: 5m 8s\tremaining: 18.6s\n",
      "943:\tlearn: 0.0009971\ttotal: 5m 8s\tremaining: 18.3s\n",
      "944:\tlearn: 0.0009971\ttotal: 5m 8s\tremaining: 18s\n",
      "945:\tlearn: 0.0009970\ttotal: 5m 9s\tremaining: 17.7s\n",
      "946:\tlearn: 0.0009967\ttotal: 5m 9s\tremaining: 17.3s\n",
      "947:\tlearn: 0.0009967\ttotal: 5m 9s\tremaining: 17s\n",
      "948:\tlearn: 0.0009966\ttotal: 5m 10s\tremaining: 16.7s\n",
      "949:\tlearn: 0.0009964\ttotal: 5m 10s\tremaining: 16.3s\n",
      "950:\tlearn: 0.0009955\ttotal: 5m 10s\tremaining: 16s\n",
      "951:\tlearn: 0.0009955\ttotal: 5m 11s\tremaining: 15.7s\n",
      "952:\tlearn: 0.0009955\ttotal: 5m 11s\tremaining: 15.4s\n",
      "953:\tlearn: 0.0009954\ttotal: 5m 11s\tremaining: 15s\n",
      "954:\tlearn: 0.0009953\ttotal: 5m 12s\tremaining: 14.7s\n",
      "955:\tlearn: 0.0009941\ttotal: 5m 12s\tremaining: 14.4s\n",
      "956:\tlearn: 0.0009940\ttotal: 5m 12s\tremaining: 14.1s\n",
      "957:\tlearn: 0.0009940\ttotal: 5m 13s\tremaining: 13.7s\n",
      "958:\tlearn: 0.0009940\ttotal: 5m 13s\tremaining: 13.4s\n",
      "959:\tlearn: 0.0009940\ttotal: 5m 13s\tremaining: 13.1s\n",
      "960:\tlearn: 0.0009940\ttotal: 5m 14s\tremaining: 12.8s\n",
      "961:\tlearn: 0.0009940\ttotal: 5m 14s\tremaining: 12.4s\n",
      "962:\tlearn: 0.0009940\ttotal: 5m 14s\tremaining: 12.1s\n",
      "963:\tlearn: 0.0009940\ttotal: 5m 15s\tremaining: 11.8s\n",
      "964:\tlearn: 0.0009940\ttotal: 5m 15s\tremaining: 11.4s\n",
      "965:\tlearn: 0.0009940\ttotal: 5m 15s\tremaining: 11.1s\n",
      "966:\tlearn: 0.0009940\ttotal: 5m 16s\tremaining: 10.8s\n",
      "967:\tlearn: 0.0009940\ttotal: 5m 16s\tremaining: 10.5s\n",
      "968:\tlearn: 0.0009940\ttotal: 5m 16s\tremaining: 10.1s\n",
      "969:\tlearn: 0.0009940\ttotal: 5m 17s\tremaining: 9.81s\n",
      "970:\tlearn: 0.0009940\ttotal: 5m 17s\tremaining: 9.48s\n",
      "971:\tlearn: 0.0009940\ttotal: 5m 17s\tremaining: 9.15s\n",
      "972:\tlearn: 0.0009940\ttotal: 5m 18s\tremaining: 8.83s\n",
      "973:\tlearn: 0.0009940\ttotal: 5m 18s\tremaining: 8.5s\n",
      "974:\tlearn: 0.0009939\ttotal: 5m 18s\tremaining: 8.17s\n",
      "975:\tlearn: 0.0009938\ttotal: 5m 19s\tremaining: 7.85s\n",
      "976:\tlearn: 0.0009937\ttotal: 5m 19s\tremaining: 7.52s\n",
      "977:\tlearn: 0.0009935\ttotal: 5m 19s\tremaining: 7.19s\n",
      "978:\tlearn: 0.0009935\ttotal: 5m 20s\tremaining: 6.87s\n",
      "979:\tlearn: 0.0009935\ttotal: 5m 20s\tremaining: 6.54s\n",
      "980:\tlearn: 0.0009935\ttotal: 5m 20s\tremaining: 6.21s\n",
      "981:\tlearn: 0.0009935\ttotal: 5m 21s\tremaining: 5.88s\n",
      "982:\tlearn: 0.0009934\ttotal: 5m 21s\tremaining: 5.56s\n",
      "983:\tlearn: 0.0009934\ttotal: 5m 21s\tremaining: 5.23s\n",
      "984:\tlearn: 0.0009933\ttotal: 5m 22s\tremaining: 4.9s\n",
      "985:\tlearn: 0.0009933\ttotal: 5m 22s\tremaining: 4.58s\n",
      "986:\tlearn: 0.0009933\ttotal: 5m 22s\tremaining: 4.25s\n",
      "987:\tlearn: 0.0009933\ttotal: 5m 23s\tremaining: 3.92s\n",
      "988:\tlearn: 0.0009867\ttotal: 5m 23s\tremaining: 3.6s\n",
      "989:\tlearn: 0.0009867\ttotal: 5m 23s\tremaining: 3.27s\n",
      "990:\tlearn: 0.0009866\ttotal: 5m 24s\tremaining: 2.94s\n",
      "991:\tlearn: 0.0009865\ttotal: 5m 24s\tremaining: 2.62s\n",
      "992:\tlearn: 0.0009864\ttotal: 5m 24s\tremaining: 2.29s\n",
      "993:\tlearn: 0.0009863\ttotal: 5m 25s\tremaining: 1.96s\n",
      "994:\tlearn: 0.0009863\ttotal: 5m 25s\tremaining: 1.63s\n",
      "995:\tlearn: 0.0009863\ttotal: 5m 25s\tremaining: 1.31s\n",
      "996:\tlearn: 0.0009862\ttotal: 5m 25s\tremaining: 981ms\n",
      "997:\tlearn: 0.0009860\ttotal: 5m 26s\tremaining: 654ms\n",
      "998:\tlearn: 0.0009860\ttotal: 5m 26s\tremaining: 327ms\n",
      "999:\tlearn: 0.0009859\ttotal: 5m 26s\tremaining: 0us\n",
      "0:\tlearn: 0.6155744\ttotal: 353ms\tremaining: 5m 52s\n",
      "1:\tlearn: 0.5502219\ttotal: 685ms\tremaining: 5m 41s\n",
      "2:\tlearn: 0.4726118\ttotal: 1.02s\tremaining: 5m 38s\n",
      "3:\tlearn: 0.4072536\ttotal: 1.34s\tremaining: 5m 34s\n",
      "4:\tlearn: 0.3658331\ttotal: 1.67s\tremaining: 5m 32s\n",
      "5:\tlearn: 0.3323415\ttotal: 1.99s\tremaining: 5m 29s\n",
      "6:\tlearn: 0.3079206\ttotal: 2.32s\tremaining: 5m 29s\n",
      "7:\tlearn: 0.2803701\ttotal: 2.65s\tremaining: 5m 28s\n",
      "8:\tlearn: 0.2600463\ttotal: 2.98s\tremaining: 5m 28s\n",
      "9:\tlearn: 0.2337118\ttotal: 3.31s\tremaining: 5m 27s\n",
      "10:\tlearn: 0.2137266\ttotal: 3.63s\tremaining: 5m 26s\n",
      "11:\tlearn: 0.1941089\ttotal: 3.95s\tremaining: 5m 25s\n",
      "12:\tlearn: 0.1790636\ttotal: 4.28s\tremaining: 5m 25s\n",
      "13:\tlearn: 0.1650671\ttotal: 4.61s\tremaining: 5m 24s\n",
      "14:\tlearn: 0.1506112\ttotal: 4.95s\tremaining: 5m 24s\n",
      "15:\tlearn: 0.1351221\ttotal: 5.28s\tremaining: 5m 24s\n",
      "16:\tlearn: 0.1218833\ttotal: 5.61s\tremaining: 5m 24s\n",
      "17:\tlearn: 0.1125112\ttotal: 5.93s\tremaining: 5m 23s\n",
      "18:\tlearn: 0.1006523\ttotal: 6.26s\tremaining: 5m 23s\n",
      "19:\tlearn: 0.0932300\ttotal: 6.58s\tremaining: 5m 22s\n",
      "20:\tlearn: 0.0864009\ttotal: 6.9s\tremaining: 5m 21s\n",
      "21:\tlearn: 0.0762873\ttotal: 7.24s\tremaining: 5m 21s\n",
      "22:\tlearn: 0.0699996\ttotal: 7.56s\tremaining: 5m 21s\n",
      "23:\tlearn: 0.0635581\ttotal: 7.88s\tremaining: 5m 20s\n",
      "24:\tlearn: 0.0592017\ttotal: 8.21s\tremaining: 5m 20s\n",
      "25:\tlearn: 0.0543965\ttotal: 8.54s\tremaining: 5m 19s\n",
      "26:\tlearn: 0.0494716\ttotal: 8.87s\tremaining: 5m 19s\n",
      "27:\tlearn: 0.0459813\ttotal: 9.19s\tremaining: 5m 19s\n",
      "28:\tlearn: 0.0418663\ttotal: 9.52s\tremaining: 5m 18s\n",
      "29:\tlearn: 0.0392579\ttotal: 9.85s\tremaining: 5m 18s\n",
      "30:\tlearn: 0.0365684\ttotal: 10.2s\tremaining: 5m 17s\n",
      "31:\tlearn: 0.0347315\ttotal: 10.5s\tremaining: 5m 17s\n",
      "32:\tlearn: 0.0325867\ttotal: 10.8s\tremaining: 5m 17s\n",
      "33:\tlearn: 0.0305745\ttotal: 11.2s\tremaining: 5m 17s\n",
      "34:\tlearn: 0.0288931\ttotal: 11.5s\tremaining: 5m 16s\n",
      "35:\tlearn: 0.0273747\ttotal: 11.8s\tremaining: 5m 16s\n",
      "36:\tlearn: 0.0257188\ttotal: 12.1s\tremaining: 5m 15s\n",
      "37:\tlearn: 0.0243146\ttotal: 12.5s\tremaining: 5m 15s\n",
      "38:\tlearn: 0.0232323\ttotal: 12.8s\tremaining: 5m 15s\n",
      "39:\tlearn: 0.0222722\ttotal: 13.1s\tremaining: 5m 15s\n",
      "40:\tlearn: 0.0211033\ttotal: 13.5s\tremaining: 5m 14s\n",
      "41:\tlearn: 0.0199236\ttotal: 13.8s\tremaining: 5m 14s\n",
      "42:\tlearn: 0.0192139\ttotal: 14.1s\tremaining: 5m 14s\n",
      "43:\tlearn: 0.0186087\ttotal: 14.4s\tremaining: 5m 13s\n",
      "44:\tlearn: 0.0179445\ttotal: 14.8s\tremaining: 5m 13s\n",
      "45:\tlearn: 0.0173529\ttotal: 15.1s\tremaining: 5m 12s\n",
      "46:\tlearn: 0.0167296\ttotal: 15.4s\tremaining: 5m 12s\n",
      "47:\tlearn: 0.0160198\ttotal: 15.8s\tremaining: 5m 12s\n",
      "48:\tlearn: 0.0154800\ttotal: 16.1s\tremaining: 5m 11s\n",
      "49:\tlearn: 0.0148733\ttotal: 16.4s\tremaining: 5m 11s\n",
      "50:\tlearn: 0.0143099\ttotal: 16.7s\tremaining: 5m 11s\n",
      "51:\tlearn: 0.0138892\ttotal: 17.1s\tremaining: 5m 10s\n",
      "52:\tlearn: 0.0131624\ttotal: 17.4s\tremaining: 5m 10s\n",
      "53:\tlearn: 0.0127304\ttotal: 17.7s\tremaining: 5m 10s\n",
      "54:\tlearn: 0.0123622\ttotal: 18s\tremaining: 5m 9s\n",
      "55:\tlearn: 0.0119900\ttotal: 18.4s\tremaining: 5m 9s\n",
      "56:\tlearn: 0.0115559\ttotal: 18.7s\tremaining: 5m 9s\n",
      "57:\tlearn: 0.0112380\ttotal: 19s\tremaining: 5m 8s\n",
      "58:\tlearn: 0.0110151\ttotal: 19.3s\tremaining: 5m 8s\n",
      "59:\tlearn: 0.0107188\ttotal: 19.7s\tremaining: 5m 8s\n",
      "60:\tlearn: 0.0104932\ttotal: 20s\tremaining: 5m 7s\n",
      "61:\tlearn: 0.0102316\ttotal: 20.3s\tremaining: 5m 7s\n",
      "62:\tlearn: 0.0100115\ttotal: 20.7s\tremaining: 5m 7s\n",
      "63:\tlearn: 0.0097164\ttotal: 21s\tremaining: 5m 6s\n",
      "64:\tlearn: 0.0094559\ttotal: 21.3s\tremaining: 5m 6s\n",
      "65:\tlearn: 0.0090910\ttotal: 21.6s\tremaining: 5m 6s\n",
      "66:\tlearn: 0.0089303\ttotal: 22s\tremaining: 5m 5s\n",
      "67:\tlearn: 0.0087819\ttotal: 22.3s\tremaining: 5m 5s\n",
      "68:\tlearn: 0.0085730\ttotal: 22.6s\tremaining: 5m 5s\n",
      "69:\tlearn: 0.0083787\ttotal: 22.9s\tremaining: 5m 4s\n",
      "70:\tlearn: 0.0082558\ttotal: 23.3s\tremaining: 5m 4s\n",
      "71:\tlearn: 0.0080902\ttotal: 23.6s\tremaining: 5m 4s\n",
      "72:\tlearn: 0.0079516\ttotal: 23.9s\tremaining: 5m 3s\n",
      "73:\tlearn: 0.0076989\ttotal: 24.3s\tremaining: 5m 3s\n",
      "74:\tlearn: 0.0075884\ttotal: 24.6s\tremaining: 5m 3s\n",
      "75:\tlearn: 0.0074305\ttotal: 24.9s\tremaining: 5m 2s\n",
      "76:\tlearn: 0.0072576\ttotal: 25.2s\tremaining: 5m 2s\n",
      "77:\tlearn: 0.0071324\ttotal: 25.6s\tremaining: 5m 2s\n",
      "78:\tlearn: 0.0069660\ttotal: 25.9s\tremaining: 5m 1s\n",
      "79:\tlearn: 0.0068646\ttotal: 26.2s\tremaining: 5m 1s\n",
      "80:\tlearn: 0.0067766\ttotal: 26.5s\tremaining: 5m 1s\n",
      "81:\tlearn: 0.0066666\ttotal: 26.9s\tremaining: 5m\n",
      "82:\tlearn: 0.0065500\ttotal: 27.2s\tremaining: 5m\n",
      "83:\tlearn: 0.0064593\ttotal: 27.5s\tremaining: 5m\n",
      "84:\tlearn: 0.0063552\ttotal: 27.8s\tremaining: 4m 59s\n",
      "85:\tlearn: 0.0062492\ttotal: 28.2s\tremaining: 4m 59s\n",
      "86:\tlearn: 0.0061663\ttotal: 28.5s\tremaining: 4m 59s\n",
      "87:\tlearn: 0.0060449\ttotal: 28.8s\tremaining: 4m 58s\n",
      "88:\tlearn: 0.0059623\ttotal: 29.1s\tremaining: 4m 58s\n",
      "89:\tlearn: 0.0058645\ttotal: 29.5s\tremaining: 4m 58s\n",
      "90:\tlearn: 0.0057715\ttotal: 29.8s\tremaining: 4m 57s\n",
      "91:\tlearn: 0.0056839\ttotal: 30.1s\tremaining: 4m 57s\n",
      "92:\tlearn: 0.0056095\ttotal: 30.5s\tremaining: 4m 57s\n",
      "93:\tlearn: 0.0055405\ttotal: 30.8s\tremaining: 4m 56s\n",
      "94:\tlearn: 0.0054632\ttotal: 31.1s\tremaining: 4m 56s\n",
      "95:\tlearn: 0.0053517\ttotal: 31.4s\tremaining: 4m 56s\n",
      "96:\tlearn: 0.0052867\ttotal: 31.8s\tremaining: 4m 55s\n",
      "97:\tlearn: 0.0052182\ttotal: 32.1s\tremaining: 4m 55s\n",
      "98:\tlearn: 0.0051354\ttotal: 32.4s\tremaining: 4m 54s\n",
      "99:\tlearn: 0.0050823\ttotal: 32.7s\tremaining: 4m 54s\n",
      "100:\tlearn: 0.0050177\ttotal: 33.1s\tremaining: 4m 54s\n",
      "101:\tlearn: 0.0049037\ttotal: 33.4s\tremaining: 4m 53s\n",
      "102:\tlearn: 0.0048343\ttotal: 33.7s\tremaining: 4m 53s\n",
      "103:\tlearn: 0.0047983\ttotal: 34s\tremaining: 4m 53s\n",
      "104:\tlearn: 0.0047354\ttotal: 34.4s\tremaining: 4m 52s\n",
      "105:\tlearn: 0.0046876\ttotal: 34.7s\tremaining: 4m 52s\n",
      "106:\tlearn: 0.0046423\ttotal: 35s\tremaining: 4m 52s\n",
      "107:\tlearn: 0.0045840\ttotal: 35.3s\tremaining: 4m 51s\n",
      "108:\tlearn: 0.0045465\ttotal: 35.7s\tremaining: 4m 51s\n",
      "109:\tlearn: 0.0044840\ttotal: 36s\tremaining: 4m 51s\n",
      "110:\tlearn: 0.0044095\ttotal: 36.3s\tremaining: 4m 50s\n",
      "111:\tlearn: 0.0043593\ttotal: 36.6s\tremaining: 4m 50s\n",
      "112:\tlearn: 0.0043087\ttotal: 37s\tremaining: 4m 50s\n",
      "113:\tlearn: 0.0042464\ttotal: 37.3s\tremaining: 4m 49s\n",
      "114:\tlearn: 0.0041990\ttotal: 37.6s\tremaining: 4m 49s\n",
      "115:\tlearn: 0.0041662\ttotal: 38s\tremaining: 4m 49s\n",
      "116:\tlearn: 0.0041193\ttotal: 38.3s\tremaining: 4m 48s\n",
      "117:\tlearn: 0.0040716\ttotal: 38.6s\tremaining: 4m 48s\n",
      "118:\tlearn: 0.0039804\ttotal: 38.9s\tremaining: 4m 48s\n",
      "119:\tlearn: 0.0038855\ttotal: 39.3s\tremaining: 4m 47s\n",
      "120:\tlearn: 0.0037888\ttotal: 39.6s\tremaining: 4m 47s\n",
      "121:\tlearn: 0.0037219\ttotal: 39.9s\tremaining: 4m 47s\n",
      "122:\tlearn: 0.0036622\ttotal: 40.2s\tremaining: 4m 46s\n",
      "123:\tlearn: 0.0036106\ttotal: 40.6s\tremaining: 4m 46s\n",
      "124:\tlearn: 0.0035734\ttotal: 40.9s\tremaining: 4m 46s\n",
      "125:\tlearn: 0.0035211\ttotal: 41.2s\tremaining: 4m 45s\n",
      "126:\tlearn: 0.0034694\ttotal: 41.5s\tremaining: 4m 45s\n",
      "127:\tlearn: 0.0034354\ttotal: 41.9s\tremaining: 4m 45s\n",
      "128:\tlearn: 0.0033973\ttotal: 42.2s\tremaining: 4m 44s\n",
      "129:\tlearn: 0.0033500\ttotal: 42.5s\tremaining: 4m 44s\n",
      "130:\tlearn: 0.0033212\ttotal: 42.8s\tremaining: 4m 44s\n",
      "131:\tlearn: 0.0032929\ttotal: 43.2s\tremaining: 4m 43s\n",
      "132:\tlearn: 0.0032587\ttotal: 43.5s\tremaining: 4m 43s\n",
      "133:\tlearn: 0.0032325\ttotal: 43.8s\tremaining: 4m 43s\n",
      "134:\tlearn: 0.0031939\ttotal: 44.1s\tremaining: 4m 42s\n",
      "135:\tlearn: 0.0031613\ttotal: 44.5s\tremaining: 4m 42s\n",
      "136:\tlearn: 0.0031333\ttotal: 44.8s\tremaining: 4m 42s\n",
      "137:\tlearn: 0.0031017\ttotal: 45.1s\tremaining: 4m 41s\n",
      "138:\tlearn: 0.0030793\ttotal: 45.4s\tremaining: 4m 41s\n",
      "139:\tlearn: 0.0030557\ttotal: 45.8s\tremaining: 4m 41s\n",
      "140:\tlearn: 0.0030104\ttotal: 46.1s\tremaining: 4m 40s\n",
      "141:\tlearn: 0.0029656\ttotal: 46.4s\tremaining: 4m 40s\n",
      "142:\tlearn: 0.0029409\ttotal: 46.7s\tremaining: 4m 40s\n",
      "143:\tlearn: 0.0029175\ttotal: 47.1s\tremaining: 4m 39s\n",
      "144:\tlearn: 0.0028973\ttotal: 47.4s\tremaining: 4m 39s\n",
      "145:\tlearn: 0.0028654\ttotal: 47.7s\tremaining: 4m 39s\n",
      "146:\tlearn: 0.0028416\ttotal: 48s\tremaining: 4m 38s\n",
      "147:\tlearn: 0.0028302\ttotal: 48.4s\tremaining: 4m 38s\n",
      "148:\tlearn: 0.0028038\ttotal: 48.7s\tremaining: 4m 38s\n",
      "149:\tlearn: 0.0027840\ttotal: 49s\tremaining: 4m 37s\n",
      "150:\tlearn: 0.0027601\ttotal: 49.4s\tremaining: 4m 37s\n",
      "151:\tlearn: 0.0027383\ttotal: 49.7s\tremaining: 4m 37s\n",
      "152:\tlearn: 0.0027303\ttotal: 50s\tremaining: 4m 36s\n",
      "153:\tlearn: 0.0027124\ttotal: 50.3s\tremaining: 4m 36s\n",
      "154:\tlearn: 0.0026945\ttotal: 50.7s\tremaining: 4m 36s\n",
      "155:\tlearn: 0.0026770\ttotal: 51s\tremaining: 4m 35s\n",
      "156:\tlearn: 0.0026366\ttotal: 51.3s\tremaining: 4m 35s\n",
      "157:\tlearn: 0.0026125\ttotal: 51.6s\tremaining: 4m 35s\n",
      "158:\tlearn: 0.0025855\ttotal: 52s\tremaining: 4m 34s\n",
      "159:\tlearn: 0.0025707\ttotal: 52.3s\tremaining: 4m 34s\n",
      "160:\tlearn: 0.0025494\ttotal: 52.6s\tremaining: 4m 34s\n",
      "161:\tlearn: 0.0025373\ttotal: 53s\tremaining: 4m 33s\n",
      "162:\tlearn: 0.0025164\ttotal: 53.3s\tremaining: 4m 33s\n",
      "163:\tlearn: 0.0024929\ttotal: 53.6s\tremaining: 4m 33s\n",
      "164:\tlearn: 0.0024736\ttotal: 53.9s\tremaining: 4m 32s\n",
      "165:\tlearn: 0.0024595\ttotal: 54.3s\tremaining: 4m 32s\n",
      "166:\tlearn: 0.0024523\ttotal: 54.6s\tremaining: 4m 32s\n",
      "167:\tlearn: 0.0024373\ttotal: 54.9s\tremaining: 4m 31s\n",
      "168:\tlearn: 0.0024201\ttotal: 55.2s\tremaining: 4m 31s\n",
      "169:\tlearn: 0.0024004\ttotal: 55.5s\tremaining: 4m 31s\n",
      "170:\tlearn: 0.0023865\ttotal: 55.9s\tremaining: 4m 30s\n",
      "171:\tlearn: 0.0023747\ttotal: 56.2s\tremaining: 4m 30s\n",
      "172:\tlearn: 0.0023541\ttotal: 56.5s\tremaining: 4m 30s\n",
      "173:\tlearn: 0.0023259\ttotal: 56.8s\tremaining: 4m 29s\n",
      "174:\tlearn: 0.0023100\ttotal: 57.2s\tremaining: 4m 29s\n",
      "175:\tlearn: 0.0022967\ttotal: 57.5s\tremaining: 4m 29s\n",
      "176:\tlearn: 0.0022753\ttotal: 57.8s\tremaining: 4m 28s\n",
      "177:\tlearn: 0.0022518\ttotal: 58.1s\tremaining: 4m 28s\n",
      "178:\tlearn: 0.0022392\ttotal: 58.5s\tremaining: 4m 28s\n",
      "179:\tlearn: 0.0022162\ttotal: 58.8s\tremaining: 4m 27s\n",
      "180:\tlearn: 0.0021980\ttotal: 59.1s\tremaining: 4m 27s\n",
      "181:\tlearn: 0.0021873\ttotal: 59.4s\tremaining: 4m 27s\n",
      "182:\tlearn: 0.0021779\ttotal: 59.8s\tremaining: 4m 26s\n",
      "183:\tlearn: 0.0021650\ttotal: 1m\tremaining: 4m 26s\n",
      "184:\tlearn: 0.0021507\ttotal: 1m\tremaining: 4m 26s\n",
      "185:\tlearn: 0.0021418\ttotal: 1m\tremaining: 4m 25s\n",
      "186:\tlearn: 0.0021216\ttotal: 1m 1s\tremaining: 4m 25s\n",
      "187:\tlearn: 0.0021022\ttotal: 1m 1s\tremaining: 4m 25s\n",
      "188:\tlearn: 0.0020858\ttotal: 1m 1s\tremaining: 4m 24s\n",
      "189:\tlearn: 0.0020627\ttotal: 1m 2s\tremaining: 4m 24s\n",
      "190:\tlearn: 0.0020486\ttotal: 1m 2s\tremaining: 4m 24s\n",
      "191:\tlearn: 0.0020346\ttotal: 1m 2s\tremaining: 4m 23s\n",
      "192:\tlearn: 0.0020208\ttotal: 1m 3s\tremaining: 4m 23s\n",
      "193:\tlearn: 0.0020016\ttotal: 1m 3s\tremaining: 4m 23s\n",
      "194:\tlearn: 0.0019811\ttotal: 1m 3s\tremaining: 4m 22s\n",
      "195:\tlearn: 0.0019725\ttotal: 1m 4s\tremaining: 4m 22s\n",
      "196:\tlearn: 0.0019646\ttotal: 1m 4s\tremaining: 4m 22s\n",
      "197:\tlearn: 0.0019374\ttotal: 1m 4s\tremaining: 4m 21s\n",
      "198:\tlearn: 0.0019233\ttotal: 1m 4s\tremaining: 4m 21s\n",
      "199:\tlearn: 0.0019140\ttotal: 1m 5s\tremaining: 4m 21s\n",
      "200:\tlearn: 0.0019073\ttotal: 1m 5s\tremaining: 4m 20s\n",
      "201:\tlearn: 0.0018990\ttotal: 1m 5s\tremaining: 4m 20s\n",
      "202:\tlearn: 0.0018990\ttotal: 1m 6s\tremaining: 4m 20s\n",
      "203:\tlearn: 0.0018990\ttotal: 1m 6s\tremaining: 4m 19s\n",
      "204:\tlearn: 0.0018990\ttotal: 1m 6s\tremaining: 4m 19s\n",
      "205:\tlearn: 0.0018828\ttotal: 1m 7s\tremaining: 4m 19s\n",
      "206:\tlearn: 0.0018711\ttotal: 1m 7s\tremaining: 4m 19s\n",
      "207:\tlearn: 0.0018632\ttotal: 1m 7s\tremaining: 4m 18s\n",
      "208:\tlearn: 0.0018513\ttotal: 1m 8s\tremaining: 4m 18s\n",
      "209:\tlearn: 0.0018444\ttotal: 1m 8s\tremaining: 4m 18s\n",
      "210:\tlearn: 0.0018371\ttotal: 1m 8s\tremaining: 4m 17s\n",
      "211:\tlearn: 0.0018225\ttotal: 1m 9s\tremaining: 4m 17s\n",
      "212:\tlearn: 0.0018225\ttotal: 1m 9s\tremaining: 4m 17s\n",
      "213:\tlearn: 0.0018153\ttotal: 1m 9s\tremaining: 4m 16s\n",
      "214:\tlearn: 0.0018020\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "215:\tlearn: 0.0017931\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "216:\tlearn: 0.0017869\ttotal: 1m 10s\tremaining: 4m 15s\n",
      "217:\tlearn: 0.0017726\ttotal: 1m 11s\tremaining: 4m 15s\n",
      "218:\tlearn: 0.0017546\ttotal: 1m 11s\tremaining: 4m 15s\n",
      "219:\tlearn: 0.0017462\ttotal: 1m 11s\tremaining: 4m 14s\n",
      "220:\tlearn: 0.0017359\ttotal: 1m 12s\tremaining: 4m 14s\n",
      "221:\tlearn: 0.0017266\ttotal: 1m 12s\tremaining: 4m 14s\n",
      "222:\tlearn: 0.0017135\ttotal: 1m 12s\tremaining: 4m 13s\n",
      "223:\tlearn: 0.0017060\ttotal: 1m 13s\tremaining: 4m 13s\n",
      "224:\tlearn: 0.0017035\ttotal: 1m 13s\tremaining: 4m 13s\n",
      "225:\tlearn: 0.0016944\ttotal: 1m 13s\tremaining: 4m 12s\n",
      "226:\tlearn: 0.0016913\ttotal: 1m 14s\tremaining: 4m 12s\n",
      "227:\tlearn: 0.0016852\ttotal: 1m 14s\tremaining: 4m 12s\n",
      "228:\tlearn: 0.0016852\ttotal: 1m 14s\tremaining: 4m 11s\n",
      "229:\tlearn: 0.0016739\ttotal: 1m 15s\tremaining: 4m 11s\n",
      "230:\tlearn: 0.0016671\ttotal: 1m 15s\tremaining: 4m 11s\n",
      "231:\tlearn: 0.0016617\ttotal: 1m 15s\tremaining: 4m 10s\n",
      "232:\tlearn: 0.0016558\ttotal: 1m 16s\tremaining: 4m 10s\n",
      "233:\tlearn: 0.0016556\ttotal: 1m 16s\tremaining: 4m 10s\n",
      "234:\tlearn: 0.0016552\ttotal: 1m 16s\tremaining: 4m 9s\n",
      "235:\tlearn: 0.0016451\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "236:\tlearn: 0.0016378\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "237:\tlearn: 0.0016286\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "238:\tlearn: 0.0016227\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "239:\tlearn: 0.0016125\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "240:\tlearn: 0.0016063\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "241:\tlearn: 0.0016063\ttotal: 1m 19s\tremaining: 4m 8s\n",
      "242:\tlearn: 0.0016011\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "243:\tlearn: 0.0015938\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "244:\tlearn: 0.0015857\ttotal: 1m 20s\tremaining: 4m 7s\n",
      "245:\tlearn: 0.0015753\ttotal: 1m 20s\tremaining: 4m 7s\n",
      "246:\tlearn: 0.0015706\ttotal: 1m 20s\tremaining: 4m 6s\n",
      "247:\tlearn: 0.0015649\ttotal: 1m 21s\tremaining: 4m 6s\n",
      "248:\tlearn: 0.0015599\ttotal: 1m 21s\tremaining: 4m 6s\n",
      "249:\tlearn: 0.0015421\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "250:\tlearn: 0.0015364\ttotal: 1m 22s\tremaining: 4m 5s\n",
      "251:\tlearn: 0.0015364\ttotal: 1m 22s\tremaining: 4m 5s\n",
      "252:\tlearn: 0.0015315\ttotal: 1m 22s\tremaining: 4m 4s\n",
      "253:\tlearn: 0.0015313\ttotal: 1m 23s\tremaining: 4m 4s\n",
      "254:\tlearn: 0.0015267\ttotal: 1m 23s\tremaining: 4m 4s\n",
      "255:\tlearn: 0.0015203\ttotal: 1m 23s\tremaining: 4m 3s\n",
      "256:\tlearn: 0.0015173\ttotal: 1m 24s\tremaining: 4m 3s\n",
      "257:\tlearn: 0.0015171\ttotal: 1m 24s\tremaining: 4m 3s\n",
      "258:\tlearn: 0.0015164\ttotal: 1m 24s\tremaining: 4m 3s\n",
      "259:\tlearn: 0.0015091\ttotal: 1m 25s\tremaining: 4m 2s\n",
      "260:\tlearn: 0.0015057\ttotal: 1m 25s\tremaining: 4m 2s\n",
      "261:\tlearn: 0.0014997\ttotal: 1m 26s\tremaining: 4m 2s\n",
      "262:\tlearn: 0.0014997\ttotal: 1m 26s\tremaining: 4m 2s\n",
      "263:\tlearn: 0.0014950\ttotal: 1m 26s\tremaining: 4m 1s\n",
      "264:\tlearn: 0.0014906\ttotal: 1m 27s\tremaining: 4m 1s\n",
      "265:\tlearn: 0.0014903\ttotal: 1m 27s\tremaining: 4m 1s\n",
      "266:\tlearn: 0.0014864\ttotal: 1m 27s\tremaining: 4m\n",
      "267:\tlearn: 0.0014864\ttotal: 1m 28s\tremaining: 4m\n",
      "268:\tlearn: 0.0014811\ttotal: 1m 28s\tremaining: 4m\n",
      "269:\tlearn: 0.0014809\ttotal: 1m 28s\tremaining: 3m 59s\n",
      "270:\tlearn: 0.0014809\ttotal: 1m 29s\tremaining: 3m 59s\n",
      "271:\tlearn: 0.0014808\ttotal: 1m 29s\tremaining: 3m 59s\n",
      "272:\tlearn: 0.0014799\ttotal: 1m 29s\tremaining: 3m 58s\n",
      "273:\tlearn: 0.0014734\ttotal: 1m 30s\tremaining: 3m 58s\n",
      "274:\tlearn: 0.0014680\ttotal: 1m 30s\tremaining: 3m 58s\n",
      "275:\tlearn: 0.0014616\ttotal: 1m 30s\tremaining: 3m 58s\n",
      "276:\tlearn: 0.0014524\ttotal: 1m 31s\tremaining: 3m 57s\n",
      "277:\tlearn: 0.0014524\ttotal: 1m 31s\tremaining: 3m 57s\n",
      "278:\tlearn: 0.0014524\ttotal: 1m 31s\tremaining: 3m 57s\n",
      "279:\tlearn: 0.0014522\ttotal: 1m 32s\tremaining: 3m 56s\n",
      "280:\tlearn: 0.0014491\ttotal: 1m 32s\tremaining: 3m 56s\n",
      "281:\tlearn: 0.0014491\ttotal: 1m 32s\tremaining: 3m 56s\n",
      "282:\tlearn: 0.0014490\ttotal: 1m 33s\tremaining: 3m 55s\n",
      "283:\tlearn: 0.0014434\ttotal: 1m 33s\tremaining: 3m 55s\n",
      "284:\tlearn: 0.0014389\ttotal: 1m 33s\tremaining: 3m 55s\n",
      "285:\tlearn: 0.0014364\ttotal: 1m 34s\tremaining: 3m 54s\n",
      "286:\tlearn: 0.0014364\ttotal: 1m 34s\tremaining: 3m 54s\n",
      "287:\tlearn: 0.0014328\ttotal: 1m 34s\tremaining: 3m 54s\n",
      "288:\tlearn: 0.0014327\ttotal: 1m 35s\tremaining: 3m 53s\n",
      "289:\tlearn: 0.0014287\ttotal: 1m 35s\tremaining: 3m 53s\n",
      "290:\tlearn: 0.0014269\ttotal: 1m 35s\tremaining: 3m 53s\n",
      "291:\tlearn: 0.0014216\ttotal: 1m 36s\tremaining: 3m 52s\n",
      "292:\tlearn: 0.0014216\ttotal: 1m 36s\tremaining: 3m 52s\n",
      "293:\tlearn: 0.0014216\ttotal: 1m 36s\tremaining: 3m 52s\n",
      "294:\tlearn: 0.0014216\ttotal: 1m 37s\tremaining: 3m 51s\n",
      "295:\tlearn: 0.0014172\ttotal: 1m 37s\tremaining: 3m 51s\n",
      "296:\tlearn: 0.0014169\ttotal: 1m 37s\tremaining: 3m 51s\n",
      "297:\tlearn: 0.0014165\ttotal: 1m 38s\tremaining: 3m 50s\n",
      "298:\tlearn: 0.0014108\ttotal: 1m 38s\tremaining: 3m 50s\n",
      "299:\tlearn: 0.0014065\ttotal: 1m 38s\tremaining: 3m 50s\n",
      "300:\tlearn: 0.0014065\ttotal: 1m 38s\tremaining: 3m 49s\n",
      "301:\tlearn: 0.0014063\ttotal: 1m 39s\tremaining: 3m 49s\n",
      "302:\tlearn: 0.0014063\ttotal: 1m 39s\tremaining: 3m 49s\n",
      "303:\tlearn: 0.0014063\ttotal: 1m 39s\tremaining: 3m 48s\n",
      "304:\tlearn: 0.0014062\ttotal: 1m 40s\tremaining: 3m 48s\n",
      "305:\tlearn: 0.0014036\ttotal: 1m 40s\tremaining: 3m 48s\n",
      "306:\tlearn: 0.0013986\ttotal: 1m 40s\tremaining: 3m 47s\n",
      "307:\tlearn: 0.0013982\ttotal: 1m 41s\tremaining: 3m 47s\n",
      "308:\tlearn: 0.0013941\ttotal: 1m 41s\tremaining: 3m 47s\n",
      "309:\tlearn: 0.0013941\ttotal: 1m 41s\tremaining: 3m 46s\n",
      "310:\tlearn: 0.0013940\ttotal: 1m 42s\tremaining: 3m 46s\n",
      "311:\tlearn: 0.0013894\ttotal: 1m 42s\tremaining: 3m 46s\n",
      "312:\tlearn: 0.0013894\ttotal: 1m 42s\tremaining: 3m 45s\n",
      "313:\tlearn: 0.0013874\ttotal: 1m 43s\tremaining: 3m 45s\n",
      "314:\tlearn: 0.0013874\ttotal: 1m 43s\tremaining: 3m 45s\n",
      "315:\tlearn: 0.0013873\ttotal: 1m 43s\tremaining: 3m 44s\n",
      "316:\tlearn: 0.0013827\ttotal: 1m 44s\tremaining: 3m 44s\n",
      "317:\tlearn: 0.0013827\ttotal: 1m 44s\tremaining: 3m 44s\n",
      "318:\tlearn: 0.0013825\ttotal: 1m 44s\tremaining: 3m 43s\n",
      "319:\tlearn: 0.0013824\ttotal: 1m 45s\tremaining: 3m 43s\n",
      "320:\tlearn: 0.0013824\ttotal: 1m 45s\tremaining: 3m 43s\n",
      "321:\tlearn: 0.0013794\ttotal: 1m 45s\tremaining: 3m 42s\n",
      "322:\tlearn: 0.0013735\ttotal: 1m 46s\tremaining: 3m 42s\n",
      "323:\tlearn: 0.0013686\ttotal: 1m 46s\tremaining: 3m 42s\n",
      "324:\tlearn: 0.0013686\ttotal: 1m 46s\tremaining: 3m 41s\n",
      "325:\tlearn: 0.0013625\ttotal: 1m 47s\tremaining: 3m 41s\n",
      "326:\tlearn: 0.0013625\ttotal: 1m 47s\tremaining: 3m 41s\n",
      "327:\tlearn: 0.0013625\ttotal: 1m 47s\tremaining: 3m 40s\n",
      "328:\tlearn: 0.0013555\ttotal: 1m 48s\tremaining: 3m 40s\n",
      "329:\tlearn: 0.0013555\ttotal: 1m 48s\tremaining: 3m 40s\n",
      "330:\tlearn: 0.0013555\ttotal: 1m 48s\tremaining: 3m 39s\n",
      "331:\tlearn: 0.0013555\ttotal: 1m 49s\tremaining: 3m 39s\n",
      "332:\tlearn: 0.0013555\ttotal: 1m 49s\tremaining: 3m 39s\n",
      "333:\tlearn: 0.0013534\ttotal: 1m 49s\tremaining: 3m 38s\n",
      "334:\tlearn: 0.0013532\ttotal: 1m 50s\tremaining: 3m 38s\n",
      "335:\tlearn: 0.0013531\ttotal: 1m 50s\tremaining: 3m 38s\n",
      "336:\tlearn: 0.0013491\ttotal: 1m 50s\tremaining: 3m 37s\n",
      "337:\tlearn: 0.0013491\ttotal: 1m 51s\tremaining: 3m 37s\n",
      "338:\tlearn: 0.0013490\ttotal: 1m 51s\tremaining: 3m 37s\n",
      "339:\tlearn: 0.0013407\ttotal: 1m 51s\tremaining: 3m 36s\n",
      "340:\tlearn: 0.0013383\ttotal: 1m 52s\tremaining: 3m 36s\n",
      "341:\tlearn: 0.0013351\ttotal: 1m 52s\tremaining: 3m 36s\n",
      "342:\tlearn: 0.0013304\ttotal: 1m 52s\tremaining: 3m 35s\n",
      "343:\tlearn: 0.0013302\ttotal: 1m 53s\tremaining: 3m 35s\n",
      "344:\tlearn: 0.0013302\ttotal: 1m 53s\tremaining: 3m 35s\n",
      "345:\tlearn: 0.0013302\ttotal: 1m 53s\tremaining: 3m 34s\n",
      "346:\tlearn: 0.0013231\ttotal: 1m 54s\tremaining: 3m 34s\n",
      "347:\tlearn: 0.0013231\ttotal: 1m 54s\tremaining: 3m 34s\n",
      "348:\tlearn: 0.0013177\ttotal: 1m 54s\tremaining: 3m 33s\n",
      "349:\tlearn: 0.0013177\ttotal: 1m 55s\tremaining: 3m 33s\n",
      "350:\tlearn: 0.0013177\ttotal: 1m 55s\tremaining: 3m 33s\n",
      "351:\tlearn: 0.0013124\ttotal: 1m 55s\tremaining: 3m 32s\n",
      "352:\tlearn: 0.0013083\ttotal: 1m 55s\tremaining: 3m 32s\n",
      "353:\tlearn: 0.0013082\ttotal: 1m 56s\tremaining: 3m 32s\n",
      "354:\tlearn: 0.0013073\ttotal: 1m 56s\tremaining: 3m 31s\n",
      "355:\tlearn: 0.0013072\ttotal: 1m 56s\tremaining: 3m 31s\n",
      "356:\tlearn: 0.0013067\ttotal: 1m 57s\tremaining: 3m 31s\n",
      "357:\tlearn: 0.0013020\ttotal: 1m 57s\tremaining: 3m 30s\n",
      "358:\tlearn: 0.0013019\ttotal: 1m 57s\tremaining: 3m 30s\n",
      "359:\tlearn: 0.0013019\ttotal: 1m 58s\tremaining: 3m 30s\n",
      "360:\tlearn: 0.0013019\ttotal: 1m 58s\tremaining: 3m 29s\n",
      "361:\tlearn: 0.0013019\ttotal: 1m 58s\tremaining: 3m 29s\n",
      "362:\tlearn: 0.0013019\ttotal: 1m 59s\tremaining: 3m 29s\n",
      "363:\tlearn: 0.0013019\ttotal: 1m 59s\tremaining: 3m 28s\n",
      "364:\tlearn: 0.0013018\ttotal: 1m 59s\tremaining: 3m 28s\n",
      "365:\tlearn: 0.0013014\ttotal: 2m\tremaining: 3m 28s\n",
      "366:\tlearn: 0.0012976\ttotal: 2m\tremaining: 3m 27s\n",
      "367:\tlearn: 0.0012971\ttotal: 2m\tremaining: 3m 27s\n",
      "368:\tlearn: 0.0012938\ttotal: 2m 1s\tremaining: 3m 27s\n",
      "369:\tlearn: 0.0012917\ttotal: 2m 1s\tremaining: 3m 26s\n",
      "370:\tlearn: 0.0012916\ttotal: 2m 1s\tremaining: 3m 26s\n",
      "371:\tlearn: 0.0012916\ttotal: 2m 2s\tremaining: 3m 26s\n",
      "372:\tlearn: 0.0012916\ttotal: 2m 2s\tremaining: 3m 26s\n",
      "373:\tlearn: 0.0012906\ttotal: 2m 2s\tremaining: 3m 25s\n",
      "374:\tlearn: 0.0012906\ttotal: 2m 3s\tremaining: 3m 25s\n",
      "375:\tlearn: 0.0012906\ttotal: 2m 3s\tremaining: 3m 25s\n",
      "376:\tlearn: 0.0012872\ttotal: 2m 3s\tremaining: 3m 24s\n",
      "377:\tlearn: 0.0012871\ttotal: 2m 4s\tremaining: 3m 24s\n",
      "378:\tlearn: 0.0012871\ttotal: 2m 4s\tremaining: 3m 23s\n",
      "379:\tlearn: 0.0012871\ttotal: 2m 4s\tremaining: 3m 23s\n",
      "380:\tlearn: 0.0012871\ttotal: 2m 5s\tremaining: 3m 23s\n",
      "381:\tlearn: 0.0012837\ttotal: 2m 5s\tremaining: 3m 23s\n",
      "382:\tlearn: 0.0012837\ttotal: 2m 5s\tremaining: 3m 22s\n",
      "383:\tlearn: 0.0012794\ttotal: 2m 6s\tremaining: 3m 22s\n",
      "384:\tlearn: 0.0012743\ttotal: 2m 6s\tremaining: 3m 22s\n",
      "385:\tlearn: 0.0012728\ttotal: 2m 6s\tremaining: 3m 21s\n",
      "386:\tlearn: 0.0012726\ttotal: 2m 7s\tremaining: 3m 21s\n",
      "387:\tlearn: 0.0012725\ttotal: 2m 7s\tremaining: 3m 21s\n",
      "388:\tlearn: 0.0012725\ttotal: 2m 7s\tremaining: 3m 20s\n",
      "389:\tlearn: 0.0012724\ttotal: 2m 8s\tremaining: 3m 20s\n",
      "390:\tlearn: 0.0012723\ttotal: 2m 8s\tremaining: 3m 20s\n",
      "391:\tlearn: 0.0012723\ttotal: 2m 8s\tremaining: 3m 19s\n",
      "392:\tlearn: 0.0012722\ttotal: 2m 9s\tremaining: 3m 19s\n",
      "393:\tlearn: 0.0012722\ttotal: 2m 9s\tremaining: 3m 19s\n",
      "394:\tlearn: 0.0012667\ttotal: 2m 9s\tremaining: 3m 18s\n",
      "395:\tlearn: 0.0012631\ttotal: 2m 10s\tremaining: 3m 18s\n",
      "396:\tlearn: 0.0012631\ttotal: 2m 10s\tremaining: 3m 18s\n",
      "397:\tlearn: 0.0012630\ttotal: 2m 10s\tremaining: 3m 17s\n",
      "398:\tlearn: 0.0012630\ttotal: 2m 11s\tremaining: 3m 17s\n",
      "399:\tlearn: 0.0012630\ttotal: 2m 11s\tremaining: 3m 17s\n",
      "400:\tlearn: 0.0012611\ttotal: 2m 11s\tremaining: 3m 16s\n",
      "401:\tlearn: 0.0012587\ttotal: 2m 12s\tremaining: 3m 16s\n",
      "402:\tlearn: 0.0012586\ttotal: 2m 12s\tremaining: 3m 16s\n",
      "403:\tlearn: 0.0012552\ttotal: 2m 12s\tremaining: 3m 15s\n",
      "404:\tlearn: 0.0012534\ttotal: 2m 12s\tremaining: 3m 15s\n",
      "405:\tlearn: 0.0012533\ttotal: 2m 13s\tremaining: 3m 15s\n",
      "406:\tlearn: 0.0012532\ttotal: 2m 13s\tremaining: 3m 14s\n",
      "407:\tlearn: 0.0012532\ttotal: 2m 13s\tremaining: 3m 14s\n",
      "408:\tlearn: 0.0012532\ttotal: 2m 14s\tremaining: 3m 14s\n",
      "409:\tlearn: 0.0012531\ttotal: 2m 14s\tremaining: 3m 13s\n",
      "410:\tlearn: 0.0012530\ttotal: 2m 14s\tremaining: 3m 13s\n",
      "411:\tlearn: 0.0012530\ttotal: 2m 15s\tremaining: 3m 13s\n",
      "412:\tlearn: 0.0012530\ttotal: 2m 15s\tremaining: 3m 12s\n",
      "413:\tlearn: 0.0012509\ttotal: 2m 15s\tremaining: 3m 12s\n",
      "414:\tlearn: 0.0012508\ttotal: 2m 16s\tremaining: 3m 12s\n",
      "415:\tlearn: 0.0012491\ttotal: 2m 16s\tremaining: 3m 11s\n",
      "416:\tlearn: 0.0012459\ttotal: 2m 16s\tremaining: 3m 11s\n",
      "417:\tlearn: 0.0012458\ttotal: 2m 17s\tremaining: 3m 11s\n",
      "418:\tlearn: 0.0012436\ttotal: 2m 17s\tremaining: 3m 10s\n",
      "419:\tlearn: 0.0012435\ttotal: 2m 17s\tremaining: 3m 10s\n",
      "420:\tlearn: 0.0012435\ttotal: 2m 18s\tremaining: 3m 10s\n",
      "421:\tlearn: 0.0012435\ttotal: 2m 18s\tremaining: 3m 9s\n",
      "422:\tlearn: 0.0012435\ttotal: 2m 18s\tremaining: 3m 9s\n",
      "423:\tlearn: 0.0012435\ttotal: 2m 19s\tremaining: 3m 9s\n",
      "424:\tlearn: 0.0012400\ttotal: 2m 19s\tremaining: 3m 8s\n",
      "425:\tlearn: 0.0012400\ttotal: 2m 19s\tremaining: 3m 8s\n",
      "426:\tlearn: 0.0012353\ttotal: 2m 20s\tremaining: 3m 8s\n",
      "427:\tlearn: 0.0012353\ttotal: 2m 20s\tremaining: 3m 7s\n",
      "428:\tlearn: 0.0012352\ttotal: 2m 20s\tremaining: 3m 7s\n",
      "429:\tlearn: 0.0012352\ttotal: 2m 21s\tremaining: 3m 7s\n",
      "430:\tlearn: 0.0012352\ttotal: 2m 21s\tremaining: 3m 6s\n",
      "431:\tlearn: 0.0012351\ttotal: 2m 21s\tremaining: 3m 6s\n",
      "432:\tlearn: 0.0012351\ttotal: 2m 22s\tremaining: 3m 6s\n",
      "433:\tlearn: 0.0012351\ttotal: 2m 22s\tremaining: 3m 5s\n",
      "434:\tlearn: 0.0012351\ttotal: 2m 22s\tremaining: 3m 5s\n",
      "435:\tlearn: 0.0012350\ttotal: 2m 23s\tremaining: 3m 5s\n",
      "436:\tlearn: 0.0012350\ttotal: 2m 23s\tremaining: 3m 4s\n",
      "437:\tlearn: 0.0012346\ttotal: 2m 23s\tremaining: 3m 4s\n",
      "438:\tlearn: 0.0012345\ttotal: 2m 24s\tremaining: 3m 4s\n",
      "439:\tlearn: 0.0012342\ttotal: 2m 24s\tremaining: 3m 3s\n",
      "440:\tlearn: 0.0012342\ttotal: 2m 24s\tremaining: 3m 3s\n",
      "441:\tlearn: 0.0012339\ttotal: 2m 25s\tremaining: 3m 3s\n",
      "442:\tlearn: 0.0012339\ttotal: 2m 25s\tremaining: 3m 2s\n",
      "443:\tlearn: 0.0012338\ttotal: 2m 25s\tremaining: 3m 2s\n",
      "444:\tlearn: 0.0012315\ttotal: 2m 26s\tremaining: 3m 2s\n",
      "445:\tlearn: 0.0012314\ttotal: 2m 26s\tremaining: 3m 1s\n",
      "446:\tlearn: 0.0012314\ttotal: 2m 26s\tremaining: 3m 1s\n",
      "447:\tlearn: 0.0012311\ttotal: 2m 27s\tremaining: 3m 1s\n",
      "448:\tlearn: 0.0012286\ttotal: 2m 27s\tremaining: 3m\n",
      "449:\tlearn: 0.0012285\ttotal: 2m 27s\tremaining: 3m\n",
      "450:\tlearn: 0.0012284\ttotal: 2m 28s\tremaining: 3m\n",
      "451:\tlearn: 0.0012284\ttotal: 2m 28s\tremaining: 2m 59s\n",
      "452:\tlearn: 0.0012284\ttotal: 2m 28s\tremaining: 2m 59s\n",
      "453:\tlearn: 0.0012273\ttotal: 2m 29s\tremaining: 2m 59s\n",
      "454:\tlearn: 0.0012273\ttotal: 2m 29s\tremaining: 2m 58s\n",
      "455:\tlearn: 0.0012260\ttotal: 2m 29s\tremaining: 2m 58s\n",
      "456:\tlearn: 0.0012257\ttotal: 2m 30s\tremaining: 2m 58s\n",
      "457:\tlearn: 0.0012256\ttotal: 2m 30s\tremaining: 2m 57s\n",
      "458:\tlearn: 0.0012225\ttotal: 2m 30s\tremaining: 2m 57s\n",
      "459:\tlearn: 0.0012210\ttotal: 2m 31s\tremaining: 2m 57s\n",
      "460:\tlearn: 0.0012188\ttotal: 2m 31s\tremaining: 2m 56s\n",
      "461:\tlearn: 0.0012187\ttotal: 2m 31s\tremaining: 2m 56s\n",
      "462:\tlearn: 0.0012187\ttotal: 2m 31s\tremaining: 2m 56s\n",
      "463:\tlearn: 0.0012154\ttotal: 2m 32s\tremaining: 2m 55s\n",
      "464:\tlearn: 0.0012154\ttotal: 2m 32s\tremaining: 2m 55s\n",
      "465:\tlearn: 0.0012154\ttotal: 2m 32s\tremaining: 2m 55s\n",
      "466:\tlearn: 0.0012149\ttotal: 2m 33s\tremaining: 2m 54s\n",
      "467:\tlearn: 0.0012149\ttotal: 2m 33s\tremaining: 2m 54s\n",
      "468:\tlearn: 0.0012149\ttotal: 2m 33s\tremaining: 2m 54s\n",
      "469:\tlearn: 0.0012149\ttotal: 2m 34s\tremaining: 2m 53s\n",
      "470:\tlearn: 0.0012149\ttotal: 2m 34s\tremaining: 2m 53s\n",
      "471:\tlearn: 0.0012149\ttotal: 2m 34s\tremaining: 2m 53s\n",
      "472:\tlearn: 0.0012149\ttotal: 2m 35s\tremaining: 2m 52s\n",
      "473:\tlearn: 0.0012149\ttotal: 2m 35s\tremaining: 2m 52s\n",
      "474:\tlearn: 0.0012147\ttotal: 2m 35s\tremaining: 2m 52s\n",
      "475:\tlearn: 0.0012138\ttotal: 2m 36s\tremaining: 2m 51s\n",
      "476:\tlearn: 0.0012127\ttotal: 2m 36s\tremaining: 2m 51s\n",
      "477:\tlearn: 0.0012127\ttotal: 2m 36s\tremaining: 2m 51s\n",
      "478:\tlearn: 0.0012127\ttotal: 2m 37s\tremaining: 2m 51s\n",
      "479:\tlearn: 0.0012127\ttotal: 2m 37s\tremaining: 2m 50s\n",
      "480:\tlearn: 0.0012126\ttotal: 2m 38s\tremaining: 2m 50s\n",
      "481:\tlearn: 0.0012094\ttotal: 2m 38s\tremaining: 2m 50s\n",
      "482:\tlearn: 0.0012011\ttotal: 2m 38s\tremaining: 2m 49s\n",
      "483:\tlearn: 0.0012010\ttotal: 2m 39s\tremaining: 2m 49s\n",
      "484:\tlearn: 0.0011986\ttotal: 2m 39s\tremaining: 2m 49s\n",
      "485:\tlearn: 0.0011984\ttotal: 2m 39s\tremaining: 2m 48s\n",
      "486:\tlearn: 0.0011984\ttotal: 2m 40s\tremaining: 2m 48s\n",
      "487:\tlearn: 0.0011984\ttotal: 2m 41s\tremaining: 2m 49s\n",
      "488:\tlearn: 0.0011984\ttotal: 2m 41s\tremaining: 2m 48s\n",
      "489:\tlearn: 0.0011983\ttotal: 2m 42s\tremaining: 2m 48s\n",
      "490:\tlearn: 0.0011971\ttotal: 2m 42s\tremaining: 2m 48s\n",
      "491:\tlearn: 0.0011971\ttotal: 2m 42s\tremaining: 2m 48s\n",
      "492:\tlearn: 0.0011970\ttotal: 2m 43s\tremaining: 2m 47s\n",
      "493:\tlearn: 0.0011970\ttotal: 2m 43s\tremaining: 2m 47s\n",
      "494:\tlearn: 0.0011937\ttotal: 2m 43s\tremaining: 2m 47s\n",
      "495:\tlearn: 0.0011908\ttotal: 2m 44s\tremaining: 2m 46s\n",
      "496:\tlearn: 0.0011862\ttotal: 2m 44s\tremaining: 2m 46s\n",
      "497:\tlearn: 0.0011832\ttotal: 2m 45s\tremaining: 2m 46s\n",
      "498:\tlearn: 0.0011803\ttotal: 2m 45s\tremaining: 2m 46s\n",
      "499:\tlearn: 0.0011768\ttotal: 2m 45s\tremaining: 2m 45s\n",
      "500:\tlearn: 0.0011745\ttotal: 2m 46s\tremaining: 2m 45s\n",
      "501:\tlearn: 0.0011744\ttotal: 2m 46s\tremaining: 2m 45s\n",
      "502:\tlearn: 0.0011744\ttotal: 2m 47s\tremaining: 2m 45s\n",
      "503:\tlearn: 0.0011743\ttotal: 2m 47s\tremaining: 2m 45s\n",
      "504:\tlearn: 0.0011743\ttotal: 2m 48s\tremaining: 2m 44s\n",
      "505:\tlearn: 0.0011743\ttotal: 2m 48s\tremaining: 2m 44s\n",
      "506:\tlearn: 0.0011743\ttotal: 2m 49s\tremaining: 2m 44s\n",
      "507:\tlearn: 0.0011742\ttotal: 2m 49s\tremaining: 2m 44s\n",
      "508:\tlearn: 0.0011742\ttotal: 2m 49s\tremaining: 2m 43s\n",
      "509:\tlearn: 0.0011703\ttotal: 2m 50s\tremaining: 2m 43s\n",
      "510:\tlearn: 0.0011669\ttotal: 2m 50s\tremaining: 2m 43s\n",
      "511:\tlearn: 0.0011668\ttotal: 2m 51s\tremaining: 2m 43s\n",
      "512:\tlearn: 0.0011634\ttotal: 2m 51s\tremaining: 2m 42s\n",
      "513:\tlearn: 0.0011633\ttotal: 2m 52s\tremaining: 2m 42s\n",
      "514:\tlearn: 0.0011633\ttotal: 2m 52s\tremaining: 2m 42s\n",
      "515:\tlearn: 0.0011633\ttotal: 2m 52s\tremaining: 2m 42s\n",
      "516:\tlearn: 0.0011600\ttotal: 2m 53s\tremaining: 2m 41s\n",
      "517:\tlearn: 0.0011569\ttotal: 2m 53s\tremaining: 2m 41s\n",
      "518:\tlearn: 0.0011568\ttotal: 2m 53s\tremaining: 2m 41s\n",
      "519:\tlearn: 0.0011567\ttotal: 2m 54s\tremaining: 2m 40s\n",
      "520:\tlearn: 0.0011567\ttotal: 2m 54s\tremaining: 2m 40s\n",
      "521:\tlearn: 0.0011567\ttotal: 2m 55s\tremaining: 2m 40s\n",
      "522:\tlearn: 0.0011567\ttotal: 2m 55s\tremaining: 2m 40s\n",
      "523:\tlearn: 0.0011567\ttotal: 2m 55s\tremaining: 2m 39s\n",
      "524:\tlearn: 0.0011566\ttotal: 2m 56s\tremaining: 2m 39s\n",
      "525:\tlearn: 0.0011544\ttotal: 2m 56s\tremaining: 2m 39s\n",
      "526:\tlearn: 0.0011544\ttotal: 2m 57s\tremaining: 2m 38s\n",
      "527:\tlearn: 0.0011504\ttotal: 2m 57s\tremaining: 2m 38s\n",
      "528:\tlearn: 0.0011466\ttotal: 2m 57s\tremaining: 2m 38s\n",
      "529:\tlearn: 0.0011466\ttotal: 2m 58s\tremaining: 2m 38s\n",
      "530:\tlearn: 0.0011466\ttotal: 2m 58s\tremaining: 2m 37s\n",
      "531:\tlearn: 0.0011466\ttotal: 2m 59s\tremaining: 2m 37s\n",
      "532:\tlearn: 0.0011465\ttotal: 2m 59s\tremaining: 2m 37s\n",
      "533:\tlearn: 0.0011441\ttotal: 2m 59s\tremaining: 2m 36s\n",
      "534:\tlearn: 0.0011441\ttotal: 3m\tremaining: 2m 36s\n",
      "535:\tlearn: 0.0011441\ttotal: 3m\tremaining: 2m 36s\n",
      "536:\tlearn: 0.0011441\ttotal: 3m\tremaining: 2m 35s\n",
      "537:\tlearn: 0.0011439\ttotal: 3m 1s\tremaining: 2m 35s\n",
      "538:\tlearn: 0.0011439\ttotal: 3m 1s\tremaining: 2m 35s\n",
      "539:\tlearn: 0.0011438\ttotal: 3m 1s\tremaining: 2m 35s\n",
      "540:\tlearn: 0.0011437\ttotal: 3m 2s\tremaining: 2m 34s\n",
      "541:\tlearn: 0.0011435\ttotal: 3m 2s\tremaining: 2m 34s\n",
      "542:\tlearn: 0.0011435\ttotal: 3m 3s\tremaining: 2m 34s\n",
      "543:\tlearn: 0.0011435\ttotal: 3m 3s\tremaining: 2m 33s\n",
      "544:\tlearn: 0.0011435\ttotal: 3m 3s\tremaining: 2m 33s\n",
      "545:\tlearn: 0.0011435\ttotal: 3m 4s\tremaining: 2m 33s\n",
      "546:\tlearn: 0.0011435\ttotal: 3m 4s\tremaining: 2m 33s\n",
      "547:\tlearn: 0.0011435\ttotal: 3m 5s\tremaining: 2m 32s\n",
      "548:\tlearn: 0.0011434\ttotal: 3m 5s\tremaining: 2m 32s\n",
      "549:\tlearn: 0.0011434\ttotal: 3m 5s\tremaining: 2m 32s\n",
      "550:\tlearn: 0.0011433\ttotal: 3m 6s\tremaining: 2m 31s\n",
      "551:\tlearn: 0.0011433\ttotal: 3m 6s\tremaining: 2m 31s\n",
      "552:\tlearn: 0.0011415\ttotal: 3m 7s\tremaining: 2m 31s\n",
      "553:\tlearn: 0.0011414\ttotal: 3m 7s\tremaining: 2m 30s\n",
      "554:\tlearn: 0.0011413\ttotal: 3m 7s\tremaining: 2m 30s\n",
      "555:\tlearn: 0.0011413\ttotal: 3m 8s\tremaining: 2m 30s\n",
      "556:\tlearn: 0.0011413\ttotal: 3m 8s\tremaining: 2m 29s\n",
      "557:\tlearn: 0.0011413\ttotal: 3m 8s\tremaining: 2m 29s\n",
      "558:\tlearn: 0.0011413\ttotal: 3m 9s\tremaining: 2m 29s\n",
      "559:\tlearn: 0.0011412\ttotal: 3m 9s\tremaining: 2m 28s\n",
      "560:\tlearn: 0.0011412\ttotal: 3m 10s\tremaining: 2m 28s\n",
      "561:\tlearn: 0.0011412\ttotal: 3m 10s\tremaining: 2m 28s\n",
      "562:\tlearn: 0.0011412\ttotal: 3m 10s\tremaining: 2m 28s\n",
      "563:\tlearn: 0.0011411\ttotal: 3m 11s\tremaining: 2m 27s\n",
      "564:\tlearn: 0.0011409\ttotal: 3m 11s\tremaining: 2m 27s\n",
      "565:\tlearn: 0.0011408\ttotal: 3m 11s\tremaining: 2m 27s\n",
      "566:\tlearn: 0.0011407\ttotal: 3m 12s\tremaining: 2m 26s\n",
      "567:\tlearn: 0.0011407\ttotal: 3m 12s\tremaining: 2m 26s\n",
      "568:\tlearn: 0.0011406\ttotal: 3m 12s\tremaining: 2m 26s\n",
      "569:\tlearn: 0.0011406\ttotal: 3m 13s\tremaining: 2m 25s\n",
      "570:\tlearn: 0.0011405\ttotal: 3m 13s\tremaining: 2m 25s\n",
      "571:\tlearn: 0.0011405\ttotal: 3m 14s\tremaining: 2m 25s\n",
      "572:\tlearn: 0.0011405\ttotal: 3m 14s\tremaining: 2m 25s\n",
      "573:\tlearn: 0.0011405\ttotal: 3m 15s\tremaining: 2m 24s\n",
      "574:\tlearn: 0.0011405\ttotal: 3m 15s\tremaining: 2m 24s\n",
      "575:\tlearn: 0.0011405\ttotal: 3m 15s\tremaining: 2m 24s\n",
      "576:\tlearn: 0.0011405\ttotal: 3m 16s\tremaining: 2m 23s\n",
      "577:\tlearn: 0.0011405\ttotal: 3m 16s\tremaining: 2m 23s\n",
      "578:\tlearn: 0.0011405\ttotal: 3m 16s\tremaining: 2m 23s\n",
      "579:\tlearn: 0.0011405\ttotal: 3m 17s\tremaining: 2m 22s\n",
      "580:\tlearn: 0.0011405\ttotal: 3m 17s\tremaining: 2m 22s\n",
      "581:\tlearn: 0.0011405\ttotal: 3m 17s\tremaining: 2m 22s\n",
      "582:\tlearn: 0.0011404\ttotal: 3m 18s\tremaining: 2m 21s\n",
      "583:\tlearn: 0.0011403\ttotal: 3m 18s\tremaining: 2m 21s\n",
      "584:\tlearn: 0.0011380\ttotal: 3m 18s\tremaining: 2m 21s\n",
      "585:\tlearn: 0.0011380\ttotal: 3m 19s\tremaining: 2m 20s\n",
      "586:\tlearn: 0.0011376\ttotal: 3m 19s\tremaining: 2m 20s\n",
      "587:\tlearn: 0.0011376\ttotal: 3m 19s\tremaining: 2m 20s\n",
      "588:\tlearn: 0.0011375\ttotal: 3m 20s\tremaining: 2m 19s\n",
      "589:\tlearn: 0.0011374\ttotal: 3m 20s\tremaining: 2m 19s\n",
      "590:\tlearn: 0.0011374\ttotal: 3m 20s\tremaining: 2m 19s\n",
      "591:\tlearn: 0.0011353\ttotal: 3m 21s\tremaining: 2m 18s\n",
      "592:\tlearn: 0.0011327\ttotal: 3m 21s\tremaining: 2m 18s\n",
      "593:\tlearn: 0.0011326\ttotal: 3m 22s\tremaining: 2m 18s\n",
      "594:\tlearn: 0.0011326\ttotal: 3m 22s\tremaining: 2m 17s\n",
      "595:\tlearn: 0.0011325\ttotal: 3m 22s\tremaining: 2m 17s\n",
      "596:\tlearn: 0.0011321\ttotal: 3m 23s\tremaining: 2m 17s\n",
      "597:\tlearn: 0.0011319\ttotal: 3m 23s\tremaining: 2m 16s\n",
      "598:\tlearn: 0.0011319\ttotal: 3m 23s\tremaining: 2m 16s\n",
      "599:\tlearn: 0.0011317\ttotal: 3m 24s\tremaining: 2m 16s\n",
      "600:\tlearn: 0.0011317\ttotal: 3m 24s\tremaining: 2m 15s\n",
      "601:\tlearn: 0.0011277\ttotal: 3m 24s\tremaining: 2m 15s\n",
      "602:\tlearn: 0.0011276\ttotal: 3m 25s\tremaining: 2m 15s\n",
      "603:\tlearn: 0.0011275\ttotal: 3m 25s\tremaining: 2m 14s\n",
      "604:\tlearn: 0.0011275\ttotal: 3m 25s\tremaining: 2m 14s\n",
      "605:\tlearn: 0.0011274\ttotal: 3m 26s\tremaining: 2m 14s\n",
      "606:\tlearn: 0.0011274\ttotal: 3m 26s\tremaining: 2m 13s\n",
      "607:\tlearn: 0.0011274\ttotal: 3m 26s\tremaining: 2m 13s\n",
      "608:\tlearn: 0.0011274\ttotal: 3m 27s\tremaining: 2m 13s\n",
      "609:\tlearn: 0.0011274\ttotal: 3m 27s\tremaining: 2m 12s\n",
      "610:\tlearn: 0.0011246\ttotal: 3m 28s\tremaining: 2m 12s\n",
      "611:\tlearn: 0.0011246\ttotal: 3m 28s\tremaining: 2m 12s\n",
      "612:\tlearn: 0.0011245\ttotal: 3m 28s\tremaining: 2m 11s\n",
      "613:\tlearn: 0.0011245\ttotal: 3m 29s\tremaining: 2m 11s\n",
      "614:\tlearn: 0.0011245\ttotal: 3m 29s\tremaining: 2m 11s\n",
      "615:\tlearn: 0.0011238\ttotal: 3m 30s\tremaining: 2m 11s\n",
      "616:\tlearn: 0.0011234\ttotal: 3m 30s\tremaining: 2m 10s\n",
      "617:\tlearn: 0.0011234\ttotal: 3m 31s\tremaining: 2m 10s\n",
      "618:\tlearn: 0.0011234\ttotal: 3m 31s\tremaining: 2m 10s\n",
      "619:\tlearn: 0.0011209\ttotal: 3m 32s\tremaining: 2m 9s\n",
      "620:\tlearn: 0.0011209\ttotal: 3m 32s\tremaining: 2m 9s\n",
      "621:\tlearn: 0.0011209\ttotal: 3m 32s\tremaining: 2m 9s\n",
      "622:\tlearn: 0.0011208\ttotal: 3m 33s\tremaining: 2m 8s\n",
      "623:\tlearn: 0.0011199\ttotal: 3m 33s\tremaining: 2m 8s\n",
      "624:\tlearn: 0.0011198\ttotal: 3m 33s\tremaining: 2m 8s\n",
      "625:\tlearn: 0.0011197\ttotal: 3m 34s\tremaining: 2m 8s\n",
      "626:\tlearn: 0.0011195\ttotal: 3m 34s\tremaining: 2m 7s\n",
      "627:\tlearn: 0.0011195\ttotal: 3m 35s\tremaining: 2m 7s\n",
      "628:\tlearn: 0.0011195\ttotal: 3m 35s\tremaining: 2m 7s\n",
      "629:\tlearn: 0.0011193\ttotal: 3m 35s\tremaining: 2m 6s\n",
      "630:\tlearn: 0.0011193\ttotal: 3m 36s\tremaining: 2m 6s\n",
      "631:\tlearn: 0.0011192\ttotal: 3m 36s\tremaining: 2m 6s\n",
      "632:\tlearn: 0.0011171\ttotal: 3m 37s\tremaining: 2m 5s\n",
      "633:\tlearn: 0.0011169\ttotal: 3m 37s\tremaining: 2m 5s\n",
      "634:\tlearn: 0.0011140\ttotal: 3m 37s\tremaining: 2m 5s\n",
      "635:\tlearn: 0.0011140\ttotal: 3m 38s\tremaining: 2m 4s\n",
      "636:\tlearn: 0.0011139\ttotal: 3m 38s\tremaining: 2m 4s\n",
      "637:\tlearn: 0.0011139\ttotal: 3m 39s\tremaining: 2m 4s\n",
      "638:\tlearn: 0.0011139\ttotal: 3m 39s\tremaining: 2m 3s\n",
      "639:\tlearn: 0.0011138\ttotal: 3m 39s\tremaining: 2m 3s\n",
      "640:\tlearn: 0.0011137\ttotal: 3m 40s\tremaining: 2m 3s\n",
      "641:\tlearn: 0.0011137\ttotal: 3m 40s\tremaining: 2m 3s\n",
      "642:\tlearn: 0.0011118\ttotal: 3m 41s\tremaining: 2m 2s\n",
      "643:\tlearn: 0.0011117\ttotal: 3m 41s\tremaining: 2m 2s\n",
      "644:\tlearn: 0.0011116\ttotal: 3m 41s\tremaining: 2m 2s\n",
      "645:\tlearn: 0.0011115\ttotal: 3m 42s\tremaining: 2m 1s\n",
      "646:\tlearn: 0.0011115\ttotal: 3m 42s\tremaining: 2m 1s\n",
      "647:\tlearn: 0.0011113\ttotal: 3m 42s\tremaining: 2m 1s\n",
      "648:\tlearn: 0.0011092\ttotal: 3m 43s\tremaining: 2m\n",
      "649:\tlearn: 0.0011092\ttotal: 3m 43s\tremaining: 2m\n",
      "650:\tlearn: 0.0011090\ttotal: 3m 44s\tremaining: 2m\n",
      "651:\tlearn: 0.0011082\ttotal: 3m 44s\tremaining: 1m 59s\n",
      "652:\tlearn: 0.0011082\ttotal: 3m 44s\tremaining: 1m 59s\n",
      "653:\tlearn: 0.0011081\ttotal: 3m 45s\tremaining: 1m 59s\n",
      "654:\tlearn: 0.0011081\ttotal: 3m 45s\tremaining: 1m 58s\n",
      "655:\tlearn: 0.0011081\ttotal: 3m 45s\tremaining: 1m 58s\n",
      "656:\tlearn: 0.0011081\ttotal: 3m 46s\tremaining: 1m 58s\n",
      "657:\tlearn: 0.0011079\ttotal: 3m 46s\tremaining: 1m 57s\n",
      "658:\tlearn: 0.0011079\ttotal: 3m 47s\tremaining: 1m 57s\n",
      "659:\tlearn: 0.0011079\ttotal: 3m 47s\tremaining: 1m 57s\n",
      "660:\tlearn: 0.0011079\ttotal: 3m 47s\tremaining: 1m 56s\n",
      "661:\tlearn: 0.0011060\ttotal: 3m 48s\tremaining: 1m 56s\n",
      "662:\tlearn: 0.0011060\ttotal: 3m 48s\tremaining: 1m 56s\n",
      "663:\tlearn: 0.0011060\ttotal: 3m 49s\tremaining: 1m 55s\n",
      "664:\tlearn: 0.0011060\ttotal: 3m 49s\tremaining: 1m 55s\n",
      "665:\tlearn: 0.0011060\ttotal: 3m 49s\tremaining: 1m 55s\n",
      "666:\tlearn: 0.0011013\ttotal: 3m 50s\tremaining: 1m 54s\n",
      "667:\tlearn: 0.0011013\ttotal: 3m 50s\tremaining: 1m 54s\n",
      "668:\tlearn: 0.0011013\ttotal: 3m 50s\tremaining: 1m 54s\n",
      "669:\tlearn: 0.0011013\ttotal: 3m 51s\tremaining: 1m 53s\n",
      "670:\tlearn: 0.0011013\ttotal: 3m 51s\tremaining: 1m 53s\n",
      "671:\tlearn: 0.0010993\ttotal: 3m 51s\tremaining: 1m 53s\n",
      "672:\tlearn: 0.0010993\ttotal: 3m 52s\tremaining: 1m 52s\n",
      "673:\tlearn: 0.0010989\ttotal: 3m 52s\tremaining: 1m 52s\n",
      "674:\tlearn: 0.0010989\ttotal: 3m 53s\tremaining: 1m 52s\n",
      "675:\tlearn: 0.0010988\ttotal: 3m 53s\tremaining: 1m 51s\n",
      "676:\tlearn: 0.0010986\ttotal: 3m 53s\tremaining: 1m 51s\n",
      "677:\tlearn: 0.0010986\ttotal: 3m 54s\tremaining: 1m 51s\n",
      "678:\tlearn: 0.0010986\ttotal: 3m 54s\tremaining: 1m 50s\n",
      "679:\tlearn: 0.0010986\ttotal: 3m 55s\tremaining: 1m 50s\n",
      "680:\tlearn: 0.0010985\ttotal: 3m 55s\tremaining: 1m 50s\n",
      "681:\tlearn: 0.0010985\ttotal: 3m 55s\tremaining: 1m 49s\n",
      "682:\tlearn: 0.0010984\ttotal: 3m 56s\tremaining: 1m 49s\n",
      "683:\tlearn: 0.0010969\ttotal: 3m 56s\tremaining: 1m 49s\n",
      "684:\tlearn: 0.0010969\ttotal: 3m 56s\tremaining: 1m 48s\n",
      "685:\tlearn: 0.0010969\ttotal: 3m 57s\tremaining: 1m 48s\n",
      "686:\tlearn: 0.0010947\ttotal: 3m 57s\tremaining: 1m 48s\n",
      "687:\tlearn: 0.0010947\ttotal: 3m 57s\tremaining: 1m 47s\n",
      "688:\tlearn: 0.0010946\ttotal: 3m 58s\tremaining: 1m 47s\n",
      "689:\tlearn: 0.0010932\ttotal: 3m 58s\tremaining: 1m 47s\n",
      "690:\tlearn: 0.0010929\ttotal: 3m 58s\tremaining: 1m 46s\n",
      "691:\tlearn: 0.0010928\ttotal: 3m 59s\tremaining: 1m 46s\n",
      "692:\tlearn: 0.0010928\ttotal: 3m 59s\tremaining: 1m 46s\n",
      "693:\tlearn: 0.0010927\ttotal: 4m\tremaining: 1m 45s\n",
      "694:\tlearn: 0.0010925\ttotal: 4m\tremaining: 1m 45s\n",
      "695:\tlearn: 0.0010925\ttotal: 4m\tremaining: 1m 45s\n",
      "696:\tlearn: 0.0010925\ttotal: 4m 1s\tremaining: 1m 44s\n",
      "697:\tlearn: 0.0010910\ttotal: 4m 1s\tremaining: 1m 44s\n",
      "698:\tlearn: 0.0010871\ttotal: 4m 1s\tremaining: 1m 44s\n",
      "699:\tlearn: 0.0010871\ttotal: 4m 2s\tremaining: 1m 43s\n",
      "700:\tlearn: 0.0010871\ttotal: 4m 2s\tremaining: 1m 43s\n",
      "701:\tlearn: 0.0010871\ttotal: 4m 2s\tremaining: 1m 43s\n",
      "702:\tlearn: 0.0010870\ttotal: 4m 3s\tremaining: 1m 42s\n",
      "703:\tlearn: 0.0010870\ttotal: 4m 3s\tremaining: 1m 42s\n",
      "704:\tlearn: 0.0010869\ttotal: 4m 3s\tremaining: 1m 42s\n",
      "705:\tlearn: 0.0010867\ttotal: 4m 4s\tremaining: 1m 41s\n",
      "706:\tlearn: 0.0010867\ttotal: 4m 4s\tremaining: 1m 41s\n",
      "707:\tlearn: 0.0010867\ttotal: 4m 4s\tremaining: 1m 40s\n",
      "708:\tlearn: 0.0010866\ttotal: 4m 5s\tremaining: 1m 40s\n",
      "709:\tlearn: 0.0010866\ttotal: 4m 5s\tremaining: 1m 40s\n",
      "710:\tlearn: 0.0010865\ttotal: 4m 5s\tremaining: 1m 39s\n",
      "711:\tlearn: 0.0010845\ttotal: 4m 6s\tremaining: 1m 39s\n",
      "712:\tlearn: 0.0010828\ttotal: 4m 6s\tremaining: 1m 39s\n",
      "713:\tlearn: 0.0010828\ttotal: 4m 6s\tremaining: 1m 38s\n",
      "714:\tlearn: 0.0010828\ttotal: 4m 7s\tremaining: 1m 38s\n",
      "715:\tlearn: 0.0010827\ttotal: 4m 7s\tremaining: 1m 38s\n",
      "716:\tlearn: 0.0010827\ttotal: 4m 7s\tremaining: 1m 37s\n",
      "717:\tlearn: 0.0010827\ttotal: 4m 8s\tremaining: 1m 37s\n",
      "718:\tlearn: 0.0010827\ttotal: 4m 8s\tremaining: 1m 37s\n",
      "719:\tlearn: 0.0010798\ttotal: 4m 8s\tremaining: 1m 36s\n",
      "720:\tlearn: 0.0010798\ttotal: 4m 9s\tremaining: 1m 36s\n",
      "721:\tlearn: 0.0010798\ttotal: 4m 9s\tremaining: 1m 36s\n",
      "722:\tlearn: 0.0010798\ttotal: 4m 10s\tremaining: 1m 35s\n",
      "723:\tlearn: 0.0010797\ttotal: 4m 10s\tremaining: 1m 35s\n",
      "724:\tlearn: 0.0010797\ttotal: 4m 10s\tremaining: 1m 35s\n",
      "725:\tlearn: 0.0010797\ttotal: 4m 11s\tremaining: 1m 34s\n",
      "726:\tlearn: 0.0010796\ttotal: 4m 11s\tremaining: 1m 34s\n",
      "727:\tlearn: 0.0010776\ttotal: 4m 11s\tremaining: 1m 34s\n",
      "728:\tlearn: 0.0010775\ttotal: 4m 12s\tremaining: 1m 33s\n",
      "729:\tlearn: 0.0010749\ttotal: 4m 12s\tremaining: 1m 33s\n",
      "730:\tlearn: 0.0010749\ttotal: 4m 12s\tremaining: 1m 33s\n",
      "731:\tlearn: 0.0010748\ttotal: 4m 13s\tremaining: 1m 32s\n",
      "732:\tlearn: 0.0010714\ttotal: 4m 13s\tremaining: 1m 32s\n",
      "733:\tlearn: 0.0010714\ttotal: 4m 13s\tremaining: 1m 32s\n",
      "734:\tlearn: 0.0010714\ttotal: 4m 14s\tremaining: 1m 31s\n",
      "735:\tlearn: 0.0010713\ttotal: 4m 14s\tremaining: 1m 31s\n",
      "736:\tlearn: 0.0010711\ttotal: 4m 14s\tremaining: 1m 30s\n",
      "737:\tlearn: 0.0010710\ttotal: 4m 15s\tremaining: 1m 30s\n",
      "738:\tlearn: 0.0010692\ttotal: 4m 15s\tremaining: 1m 30s\n",
      "739:\tlearn: 0.0010691\ttotal: 4m 15s\tremaining: 1m 29s\n",
      "740:\tlearn: 0.0010661\ttotal: 4m 16s\tremaining: 1m 29s\n",
      "741:\tlearn: 0.0010659\ttotal: 4m 16s\tremaining: 1m 29s\n",
      "742:\tlearn: 0.0010657\ttotal: 4m 17s\tremaining: 1m 28s\n",
      "743:\tlearn: 0.0010657\ttotal: 4m 17s\tremaining: 1m 28s\n",
      "744:\tlearn: 0.0010656\ttotal: 4m 17s\tremaining: 1m 28s\n",
      "745:\tlearn: 0.0010656\ttotal: 4m 18s\tremaining: 1m 27s\n",
      "746:\tlearn: 0.0010656\ttotal: 4m 18s\tremaining: 1m 27s\n",
      "747:\tlearn: 0.0010656\ttotal: 4m 18s\tremaining: 1m 27s\n",
      "748:\tlearn: 0.0010656\ttotal: 4m 19s\tremaining: 1m 26s\n",
      "749:\tlearn: 0.0010656\ttotal: 4m 19s\tremaining: 1m 26s\n",
      "750:\tlearn: 0.0010655\ttotal: 4m 19s\tremaining: 1m 26s\n",
      "751:\tlearn: 0.0010654\ttotal: 4m 20s\tremaining: 1m 25s\n",
      "752:\tlearn: 0.0010654\ttotal: 4m 20s\tremaining: 1m 25s\n",
      "753:\tlearn: 0.0010654\ttotal: 4m 21s\tremaining: 1m 25s\n",
      "754:\tlearn: 0.0010641\ttotal: 4m 21s\tremaining: 1m 24s\n",
      "755:\tlearn: 0.0010640\ttotal: 4m 21s\tremaining: 1m 24s\n",
      "756:\tlearn: 0.0010640\ttotal: 4m 22s\tremaining: 1m 24s\n",
      "757:\tlearn: 0.0010620\ttotal: 4m 22s\tremaining: 1m 23s\n",
      "758:\tlearn: 0.0010620\ttotal: 4m 22s\tremaining: 1m 23s\n",
      "759:\tlearn: 0.0010620\ttotal: 4m 23s\tremaining: 1m 23s\n",
      "760:\tlearn: 0.0010620\ttotal: 4m 23s\tremaining: 1m 22s\n",
      "761:\tlearn: 0.0010618\ttotal: 4m 24s\tremaining: 1m 22s\n",
      "762:\tlearn: 0.0010618\ttotal: 4m 24s\tremaining: 1m 22s\n",
      "763:\tlearn: 0.0010618\ttotal: 4m 24s\tremaining: 1m 21s\n",
      "764:\tlearn: 0.0010617\ttotal: 4m 25s\tremaining: 1m 21s\n",
      "765:\tlearn: 0.0010585\ttotal: 4m 25s\tremaining: 1m 21s\n",
      "766:\tlearn: 0.0010564\ttotal: 4m 25s\tremaining: 1m 20s\n",
      "767:\tlearn: 0.0010564\ttotal: 4m 26s\tremaining: 1m 20s\n",
      "768:\tlearn: 0.0010549\ttotal: 4m 26s\tremaining: 1m 20s\n",
      "769:\tlearn: 0.0010548\ttotal: 4m 26s\tremaining: 1m 19s\n",
      "770:\tlearn: 0.0010524\ttotal: 4m 27s\tremaining: 1m 19s\n",
      "771:\tlearn: 0.0010524\ttotal: 4m 27s\tremaining: 1m 19s\n",
      "772:\tlearn: 0.0010522\ttotal: 4m 28s\tremaining: 1m 18s\n",
      "773:\tlearn: 0.0010499\ttotal: 4m 28s\tremaining: 1m 18s\n",
      "774:\tlearn: 0.0010497\ttotal: 4m 28s\tremaining: 1m 18s\n",
      "775:\tlearn: 0.0010475\ttotal: 4m 29s\tremaining: 1m 17s\n",
      "776:\tlearn: 0.0010475\ttotal: 4m 29s\tremaining: 1m 17s\n",
      "777:\tlearn: 0.0010475\ttotal: 4m 30s\tremaining: 1m 17s\n",
      "778:\tlearn: 0.0010475\ttotal: 4m 30s\tremaining: 1m 16s\n",
      "779:\tlearn: 0.0010474\ttotal: 4m 30s\tremaining: 1m 16s\n",
      "780:\tlearn: 0.0010460\ttotal: 4m 31s\tremaining: 1m 16s\n",
      "781:\tlearn: 0.0010459\ttotal: 4m 31s\tremaining: 1m 15s\n",
      "782:\tlearn: 0.0010458\ttotal: 4m 31s\tremaining: 1m 15s\n",
      "783:\tlearn: 0.0010453\ttotal: 4m 32s\tremaining: 1m 15s\n",
      "784:\tlearn: 0.0010452\ttotal: 4m 32s\tremaining: 1m 14s\n",
      "785:\tlearn: 0.0010443\ttotal: 4m 33s\tremaining: 1m 14s\n",
      "786:\tlearn: 0.0010443\ttotal: 4m 33s\tremaining: 1m 13s\n",
      "787:\tlearn: 0.0010420\ttotal: 4m 33s\tremaining: 1m 13s\n",
      "788:\tlearn: 0.0010420\ttotal: 4m 34s\tremaining: 1m 13s\n",
      "789:\tlearn: 0.0010419\ttotal: 4m 34s\tremaining: 1m 12s\n",
      "790:\tlearn: 0.0010418\ttotal: 4m 34s\tremaining: 1m 12s\n",
      "791:\tlearn: 0.0010418\ttotal: 4m 35s\tremaining: 1m 12s\n",
      "792:\tlearn: 0.0010417\ttotal: 4m 35s\tremaining: 1m 11s\n",
      "793:\tlearn: 0.0010401\ttotal: 4m 35s\tremaining: 1m 11s\n",
      "794:\tlearn: 0.0010401\ttotal: 4m 36s\tremaining: 1m 11s\n",
      "795:\tlearn: 0.0010401\ttotal: 4m 36s\tremaining: 1m 10s\n",
      "796:\tlearn: 0.0010386\ttotal: 4m 37s\tremaining: 1m 10s\n",
      "797:\tlearn: 0.0010385\ttotal: 4m 37s\tremaining: 1m 10s\n",
      "798:\tlearn: 0.0010384\ttotal: 4m 37s\tremaining: 1m 9s\n",
      "799:\tlearn: 0.0010383\ttotal: 4m 38s\tremaining: 1m 9s\n",
      "800:\tlearn: 0.0010383\ttotal: 4m 38s\tremaining: 1m 9s\n",
      "801:\tlearn: 0.0010382\ttotal: 4m 39s\tremaining: 1m 8s\n",
      "802:\tlearn: 0.0010382\ttotal: 4m 39s\tremaining: 1m 8s\n",
      "803:\tlearn: 0.0010382\ttotal: 4m 39s\tremaining: 1m 8s\n",
      "804:\tlearn: 0.0010382\ttotal: 4m 40s\tremaining: 1m 7s\n",
      "805:\tlearn: 0.0010381\ttotal: 4m 40s\tremaining: 1m 7s\n",
      "806:\tlearn: 0.0010381\ttotal: 4m 40s\tremaining: 1m 7s\n",
      "807:\tlearn: 0.0010381\ttotal: 4m 41s\tremaining: 1m 6s\n",
      "808:\tlearn: 0.0010380\ttotal: 4m 41s\tremaining: 1m 6s\n",
      "809:\tlearn: 0.0010380\ttotal: 4m 42s\tremaining: 1m 6s\n",
      "810:\tlearn: 0.0010380\ttotal: 4m 42s\tremaining: 1m 5s\n",
      "811:\tlearn: 0.0010380\ttotal: 4m 42s\tremaining: 1m 5s\n",
      "812:\tlearn: 0.0010379\ttotal: 4m 43s\tremaining: 1m 5s\n",
      "813:\tlearn: 0.0010379\ttotal: 4m 43s\tremaining: 1m 4s\n",
      "814:\tlearn: 0.0010379\ttotal: 4m 43s\tremaining: 1m 4s\n",
      "815:\tlearn: 0.0010379\ttotal: 4m 44s\tremaining: 1m 4s\n",
      "816:\tlearn: 0.0010379\ttotal: 4m 44s\tremaining: 1m 3s\n",
      "817:\tlearn: 0.0010379\ttotal: 4m 45s\tremaining: 1m 3s\n",
      "818:\tlearn: 0.0010379\ttotal: 4m 45s\tremaining: 1m 3s\n",
      "819:\tlearn: 0.0010379\ttotal: 4m 45s\tremaining: 1m 2s\n",
      "820:\tlearn: 0.0010379\ttotal: 4m 46s\tremaining: 1m 2s\n",
      "821:\tlearn: 0.0010378\ttotal: 4m 46s\tremaining: 1m 2s\n",
      "822:\tlearn: 0.0010378\ttotal: 4m 46s\tremaining: 1m 1s\n",
      "823:\tlearn: 0.0010378\ttotal: 4m 47s\tremaining: 1m 1s\n",
      "824:\tlearn: 0.0010377\ttotal: 4m 47s\tremaining: 1m 1s\n",
      "825:\tlearn: 0.0010363\ttotal: 4m 48s\tremaining: 1m\n",
      "826:\tlearn: 0.0010363\ttotal: 4m 48s\tremaining: 1m\n",
      "827:\tlearn: 0.0010363\ttotal: 4m 48s\tremaining: 60s\n",
      "828:\tlearn: 0.0010363\ttotal: 4m 49s\tremaining: 59.6s\n",
      "829:\tlearn: 0.0010350\ttotal: 4m 49s\tremaining: 59.3s\n",
      "830:\tlearn: 0.0010349\ttotal: 4m 49s\tremaining: 59s\n",
      "831:\tlearn: 0.0010348\ttotal: 4m 50s\tremaining: 58.6s\n",
      "832:\tlearn: 0.0010348\ttotal: 4m 50s\tremaining: 58.3s\n",
      "833:\tlearn: 0.0010348\ttotal: 4m 51s\tremaining: 58s\n",
      "834:\tlearn: 0.0010348\ttotal: 4m 51s\tremaining: 57.6s\n",
      "835:\tlearn: 0.0010347\ttotal: 4m 52s\tremaining: 57.3s\n",
      "836:\tlearn: 0.0010345\ttotal: 4m 52s\tremaining: 57s\n",
      "837:\tlearn: 0.0010344\ttotal: 4m 53s\tremaining: 56.6s\n",
      "838:\tlearn: 0.0010315\ttotal: 4m 53s\tremaining: 56.3s\n",
      "839:\tlearn: 0.0010315\ttotal: 4m 53s\tremaining: 56s\n",
      "840:\tlearn: 0.0010304\ttotal: 4m 54s\tremaining: 55.6s\n",
      "841:\tlearn: 0.0010304\ttotal: 4m 54s\tremaining: 55.3s\n",
      "842:\tlearn: 0.0010304\ttotal: 4m 54s\tremaining: 54.9s\n",
      "843:\tlearn: 0.0010302\ttotal: 4m 55s\tremaining: 54.6s\n",
      "844:\tlearn: 0.0010301\ttotal: 4m 55s\tremaining: 54.2s\n",
      "845:\tlearn: 0.0010300\ttotal: 4m 56s\tremaining: 53.9s\n",
      "846:\tlearn: 0.0010300\ttotal: 4m 56s\tremaining: 53.5s\n",
      "847:\tlearn: 0.0010300\ttotal: 4m 56s\tremaining: 53.2s\n",
      "848:\tlearn: 0.0010300\ttotal: 4m 57s\tremaining: 52.8s\n",
      "849:\tlearn: 0.0010300\ttotal: 4m 57s\tremaining: 52.5s\n",
      "850:\tlearn: 0.0010300\ttotal: 4m 57s\tremaining: 52.1s\n",
      "851:\tlearn: 0.0010299\ttotal: 4m 58s\tremaining: 51.8s\n",
      "852:\tlearn: 0.0010298\ttotal: 4m 58s\tremaining: 51.4s\n",
      "853:\tlearn: 0.0010297\ttotal: 4m 58s\tremaining: 51.1s\n",
      "854:\tlearn: 0.0010271\ttotal: 4m 59s\tremaining: 50.7s\n",
      "855:\tlearn: 0.0010271\ttotal: 4m 59s\tremaining: 50.4s\n",
      "856:\tlearn: 0.0010271\ttotal: 4m 59s\tremaining: 50s\n",
      "857:\tlearn: 0.0010266\ttotal: 5m\tremaining: 49.7s\n",
      "858:\tlearn: 0.0010266\ttotal: 5m\tremaining: 49.3s\n",
      "859:\tlearn: 0.0010265\ttotal: 5m\tremaining: 49s\n",
      "860:\tlearn: 0.0010255\ttotal: 5m 1s\tremaining: 48.6s\n",
      "861:\tlearn: 0.0010255\ttotal: 5m 1s\tremaining: 48.2s\n",
      "862:\tlearn: 0.0010255\ttotal: 5m 1s\tremaining: 47.9s\n",
      "863:\tlearn: 0.0010255\ttotal: 5m 2s\tremaining: 47.5s\n",
      "864:\tlearn: 0.0010254\ttotal: 5m 2s\tremaining: 47.2s\n",
      "865:\tlearn: 0.0010254\ttotal: 5m 2s\tremaining: 46.8s\n",
      "866:\tlearn: 0.0010254\ttotal: 5m 2s\tremaining: 46.5s\n",
      "867:\tlearn: 0.0010254\ttotal: 5m 3s\tremaining: 46.1s\n",
      "868:\tlearn: 0.0010254\ttotal: 5m 3s\tremaining: 45.8s\n",
      "869:\tlearn: 0.0010248\ttotal: 5m 3s\tremaining: 45.4s\n",
      "870:\tlearn: 0.0010247\ttotal: 5m 4s\tremaining: 45.1s\n",
      "871:\tlearn: 0.0010246\ttotal: 5m 4s\tremaining: 44.7s\n",
      "872:\tlearn: 0.0010246\ttotal: 5m 4s\tremaining: 44.4s\n",
      "873:\tlearn: 0.0010220\ttotal: 5m 5s\tremaining: 44s\n",
      "874:\tlearn: 0.0010220\ttotal: 5m 5s\tremaining: 43.7s\n",
      "875:\tlearn: 0.0010220\ttotal: 5m 5s\tremaining: 43.3s\n",
      "876:\tlearn: 0.0010218\ttotal: 5m 6s\tremaining: 43s\n",
      "877:\tlearn: 0.0010218\ttotal: 5m 6s\tremaining: 42.6s\n",
      "878:\tlearn: 0.0010218\ttotal: 5m 6s\tremaining: 42.2s\n",
      "879:\tlearn: 0.0010218\ttotal: 5m 7s\tremaining: 41.9s\n",
      "880:\tlearn: 0.0010217\ttotal: 5m 7s\tremaining: 41.5s\n",
      "881:\tlearn: 0.0010217\ttotal: 5m 7s\tremaining: 41.2s\n",
      "882:\tlearn: 0.0010216\ttotal: 5m 8s\tremaining: 40.8s\n",
      "883:\tlearn: 0.0010216\ttotal: 5m 8s\tremaining: 40.5s\n",
      "884:\tlearn: 0.0010215\ttotal: 5m 8s\tremaining: 40.1s\n",
      "885:\tlearn: 0.0010215\ttotal: 5m 9s\tremaining: 39.8s\n",
      "886:\tlearn: 0.0010195\ttotal: 5m 9s\tremaining: 39.4s\n",
      "887:\tlearn: 0.0010194\ttotal: 5m 9s\tremaining: 39.1s\n",
      "888:\tlearn: 0.0010194\ttotal: 5m 10s\tremaining: 38.7s\n",
      "889:\tlearn: 0.0010169\ttotal: 5m 10s\tremaining: 38.4s\n",
      "890:\tlearn: 0.0010169\ttotal: 5m 10s\tremaining: 38s\n",
      "891:\tlearn: 0.0010167\ttotal: 5m 11s\tremaining: 37.7s\n",
      "892:\tlearn: 0.0010166\ttotal: 5m 11s\tremaining: 37.3s\n",
      "893:\tlearn: 0.0010166\ttotal: 5m 11s\tremaining: 37s\n",
      "894:\tlearn: 0.0010165\ttotal: 5m 12s\tremaining: 36.6s\n",
      "895:\tlearn: 0.0010165\ttotal: 5m 12s\tremaining: 36.3s\n",
      "896:\tlearn: 0.0010165\ttotal: 5m 12s\tremaining: 35.9s\n",
      "897:\tlearn: 0.0010164\ttotal: 5m 13s\tremaining: 35.6s\n",
      "898:\tlearn: 0.0010164\ttotal: 5m 13s\tremaining: 35.2s\n",
      "899:\tlearn: 0.0010164\ttotal: 5m 13s\tremaining: 34.9s\n",
      "900:\tlearn: 0.0010139\ttotal: 5m 14s\tremaining: 34.5s\n",
      "901:\tlearn: 0.0010139\ttotal: 5m 14s\tremaining: 34.2s\n",
      "902:\tlearn: 0.0010138\ttotal: 5m 14s\tremaining: 33.8s\n",
      "903:\tlearn: 0.0010137\ttotal: 5m 15s\tremaining: 33.5s\n",
      "904:\tlearn: 0.0010135\ttotal: 5m 15s\tremaining: 33.1s\n",
      "905:\tlearn: 0.0010135\ttotal: 5m 15s\tremaining: 32.8s\n",
      "906:\tlearn: 0.0010065\ttotal: 5m 16s\tremaining: 32.4s\n",
      "907:\tlearn: 0.0010065\ttotal: 5m 16s\tremaining: 32.1s\n",
      "908:\tlearn: 0.0010065\ttotal: 5m 16s\tremaining: 31.7s\n",
      "909:\tlearn: 0.0010064\ttotal: 5m 17s\tremaining: 31.4s\n",
      "910:\tlearn: 0.0010064\ttotal: 5m 17s\tremaining: 31s\n",
      "911:\tlearn: 0.0010064\ttotal: 5m 17s\tremaining: 30.7s\n",
      "912:\tlearn: 0.0010064\ttotal: 5m 17s\tremaining: 30.3s\n",
      "913:\tlearn: 0.0010064\ttotal: 5m 18s\tremaining: 30s\n",
      "914:\tlearn: 0.0010032\ttotal: 5m 18s\tremaining: 29.6s\n",
      "915:\tlearn: 0.0010032\ttotal: 5m 18s\tremaining: 29.3s\n",
      "916:\tlearn: 0.0010031\ttotal: 5m 19s\tremaining: 28.9s\n",
      "917:\tlearn: 0.0010031\ttotal: 5m 19s\tremaining: 28.5s\n",
      "918:\tlearn: 0.0010031\ttotal: 5m 19s\tremaining: 28.2s\n",
      "919:\tlearn: 0.0010030\ttotal: 5m 20s\tremaining: 27.8s\n",
      "920:\tlearn: 0.0010026\ttotal: 5m 20s\tremaining: 27.5s\n",
      "921:\tlearn: 0.0010026\ttotal: 5m 20s\tremaining: 27.2s\n",
      "922:\tlearn: 0.0010026\ttotal: 5m 21s\tremaining: 26.8s\n",
      "923:\tlearn: 0.0010026\ttotal: 5m 21s\tremaining: 26.5s\n",
      "924:\tlearn: 0.0010026\ttotal: 5m 21s\tremaining: 26.1s\n",
      "925:\tlearn: 0.0010013\ttotal: 5m 22s\tremaining: 25.8s\n",
      "926:\tlearn: 0.0010012\ttotal: 5m 22s\tremaining: 25.4s\n",
      "927:\tlearn: 0.0010012\ttotal: 5m 22s\tremaining: 25.1s\n",
      "928:\tlearn: 0.0010012\ttotal: 5m 23s\tremaining: 24.7s\n",
      "929:\tlearn: 0.0010012\ttotal: 5m 23s\tremaining: 24.4s\n",
      "930:\tlearn: 0.0010012\ttotal: 5m 23s\tremaining: 24s\n",
      "931:\tlearn: 0.0010012\ttotal: 5m 24s\tremaining: 23.7s\n",
      "932:\tlearn: 0.0010012\ttotal: 5m 24s\tremaining: 23.3s\n",
      "933:\tlearn: 0.0010011\ttotal: 5m 24s\tremaining: 23s\n",
      "934:\tlearn: 0.0010011\ttotal: 5m 25s\tremaining: 22.6s\n",
      "935:\tlearn: 0.0010011\ttotal: 5m 25s\tremaining: 22.3s\n",
      "936:\tlearn: 0.0010010\ttotal: 5m 25s\tremaining: 21.9s\n",
      "937:\tlearn: 0.0010010\ttotal: 5m 26s\tremaining: 21.6s\n",
      "938:\tlearn: 0.0010009\ttotal: 5m 26s\tremaining: 21.2s\n",
      "939:\tlearn: 0.0010008\ttotal: 5m 26s\tremaining: 20.9s\n",
      "940:\tlearn: 0.0010007\ttotal: 5m 27s\tremaining: 20.5s\n",
      "941:\tlearn: 0.0010007\ttotal: 5m 27s\tremaining: 20.2s\n",
      "942:\tlearn: 0.0010003\ttotal: 5m 27s\tremaining: 19.8s\n",
      "943:\tlearn: 0.0010002\ttotal: 5m 28s\tremaining: 19.5s\n",
      "944:\tlearn: 0.0010002\ttotal: 5m 28s\tremaining: 19.1s\n",
      "945:\tlearn: 0.0010002\ttotal: 5m 28s\tremaining: 18.8s\n",
      "946:\tlearn: 0.0009992\ttotal: 5m 29s\tremaining: 18.4s\n",
      "947:\tlearn: 0.0009991\ttotal: 5m 29s\tremaining: 18.1s\n",
      "948:\tlearn: 0.0009987\ttotal: 5m 29s\tremaining: 17.7s\n",
      "949:\tlearn: 0.0009987\ttotal: 5m 30s\tremaining: 17.4s\n",
      "950:\tlearn: 0.0009987\ttotal: 5m 30s\tremaining: 17s\n",
      "951:\tlearn: 0.0009987\ttotal: 5m 30s\tremaining: 16.7s\n",
      "952:\tlearn: 0.0009986\ttotal: 5m 31s\tremaining: 16.3s\n",
      "953:\tlearn: 0.0009985\ttotal: 5m 31s\tremaining: 16s\n",
      "954:\tlearn: 0.0009984\ttotal: 5m 31s\tremaining: 15.6s\n",
      "955:\tlearn: 0.0009984\ttotal: 5m 32s\tremaining: 15.3s\n",
      "956:\tlearn: 0.0009984\ttotal: 5m 32s\tremaining: 14.9s\n",
      "957:\tlearn: 0.0009984\ttotal: 5m 32s\tremaining: 14.6s\n",
      "958:\tlearn: 0.0009984\ttotal: 5m 32s\tremaining: 14.2s\n",
      "959:\tlearn: 0.0009984\ttotal: 5m 33s\tremaining: 13.9s\n",
      "960:\tlearn: 0.0009984\ttotal: 5m 33s\tremaining: 13.5s\n",
      "961:\tlearn: 0.0009984\ttotal: 5m 33s\tremaining: 13.2s\n",
      "962:\tlearn: 0.0009984\ttotal: 5m 34s\tremaining: 12.8s\n",
      "963:\tlearn: 0.0009984\ttotal: 5m 34s\tremaining: 12.5s\n",
      "964:\tlearn: 0.0009980\ttotal: 5m 34s\tremaining: 12.1s\n",
      "965:\tlearn: 0.0009979\ttotal: 5m 35s\tremaining: 11.8s\n",
      "966:\tlearn: 0.0009979\ttotal: 5m 35s\tremaining: 11.5s\n",
      "967:\tlearn: 0.0009978\ttotal: 5m 35s\tremaining: 11.1s\n",
      "968:\tlearn: 0.0009978\ttotal: 5m 36s\tremaining: 10.8s\n",
      "969:\tlearn: 0.0009977\ttotal: 5m 36s\tremaining: 10.4s\n",
      "970:\tlearn: 0.0009977\ttotal: 5m 36s\tremaining: 10.1s\n",
      "971:\tlearn: 0.0009929\ttotal: 5m 37s\tremaining: 9.71s\n",
      "972:\tlearn: 0.0009929\ttotal: 5m 37s\tremaining: 9.37s\n",
      "973:\tlearn: 0.0009929\ttotal: 5m 37s\tremaining: 9.02s\n",
      "974:\tlearn: 0.0009929\ttotal: 5m 38s\tremaining: 8.67s\n",
      "975:\tlearn: 0.0009929\ttotal: 5m 38s\tremaining: 8.32s\n",
      "976:\tlearn: 0.0009929\ttotal: 5m 38s\tremaining: 7.98s\n",
      "977:\tlearn: 0.0009928\ttotal: 5m 39s\tremaining: 7.63s\n",
      "978:\tlearn: 0.0009928\ttotal: 5m 39s\tremaining: 7.28s\n",
      "979:\tlearn: 0.0009928\ttotal: 5m 39s\tremaining: 6.93s\n",
      "980:\tlearn: 0.0009928\ttotal: 5m 40s\tremaining: 6.59s\n",
      "981:\tlearn: 0.0009927\ttotal: 5m 40s\tremaining: 6.24s\n",
      "982:\tlearn: 0.0009927\ttotal: 5m 40s\tremaining: 5.89s\n",
      "983:\tlearn: 0.0009924\ttotal: 5m 41s\tremaining: 5.55s\n",
      "984:\tlearn: 0.0009924\ttotal: 5m 41s\tremaining: 5.2s\n",
      "985:\tlearn: 0.0009924\ttotal: 5m 41s\tremaining: 4.85s\n",
      "986:\tlearn: 0.0009923\ttotal: 5m 42s\tremaining: 4.51s\n",
      "987:\tlearn: 0.0009923\ttotal: 5m 42s\tremaining: 4.16s\n",
      "988:\tlearn: 0.0009922\ttotal: 5m 42s\tremaining: 3.81s\n",
      "989:\tlearn: 0.0009922\ttotal: 5m 43s\tremaining: 3.46s\n",
      "990:\tlearn: 0.0009922\ttotal: 5m 43s\tremaining: 3.12s\n",
      "991:\tlearn: 0.0009922\ttotal: 5m 43s\tremaining: 2.77s\n",
      "992:\tlearn: 0.0009922\ttotal: 5m 44s\tremaining: 2.42s\n",
      "993:\tlearn: 0.0009922\ttotal: 5m 44s\tremaining: 2.08s\n",
      "994:\tlearn: 0.0009922\ttotal: 5m 44s\tremaining: 1.73s\n",
      "995:\tlearn: 0.0009922\ttotal: 5m 45s\tremaining: 1.39s\n",
      "996:\tlearn: 0.0009922\ttotal: 5m 45s\tremaining: 1.04s\n",
      "997:\tlearn: 0.0009921\ttotal: 5m 45s\tremaining: 693ms\n",
      "998:\tlearn: 0.0009921\ttotal: 5m 46s\tremaining: 346ms\n",
      "999:\tlearn: 0.0009921\ttotal: 5m 46s\tremaining: 0us\n",
      "0:\tlearn: 0.6133590\ttotal: 364ms\tremaining: 6m 3s\n",
      "1:\tlearn: 0.5598996\ttotal: 690ms\tremaining: 5m 44s\n",
      "2:\tlearn: 0.5023102\ttotal: 1.01s\tremaining: 5m 35s\n",
      "3:\tlearn: 0.4499588\ttotal: 1.34s\tremaining: 5m 32s\n",
      "4:\tlearn: 0.3873076\ttotal: 1.67s\tremaining: 5m 31s\n",
      "5:\tlearn: 0.3485652\ttotal: 1.99s\tremaining: 5m 30s\n",
      "6:\tlearn: 0.3213974\ttotal: 2.33s\tremaining: 5m 29s\n",
      "7:\tlearn: 0.2943711\ttotal: 2.65s\tremaining: 5m 28s\n",
      "8:\tlearn: 0.2705214\ttotal: 2.98s\tremaining: 5m 27s\n",
      "9:\tlearn: 0.2376517\ttotal: 3.31s\tremaining: 5m 27s\n",
      "10:\tlearn: 0.2111130\ttotal: 3.63s\tremaining: 5m 26s\n",
      "11:\tlearn: 0.1882092\ttotal: 3.96s\tremaining: 5m 26s\n",
      "12:\tlearn: 0.1766308\ttotal: 4.29s\tremaining: 5m 26s\n",
      "13:\tlearn: 0.1556552\ttotal: 4.62s\tremaining: 5m 25s\n",
      "14:\tlearn: 0.1435605\ttotal: 4.95s\tremaining: 5m 24s\n",
      "15:\tlearn: 0.1312956\ttotal: 5.27s\tremaining: 5m 24s\n",
      "16:\tlearn: 0.1207505\ttotal: 5.6s\tremaining: 5m 23s\n",
      "17:\tlearn: 0.1080650\ttotal: 5.92s\tremaining: 5m 23s\n",
      "18:\tlearn: 0.1007247\ttotal: 6.25s\tremaining: 5m 22s\n",
      "19:\tlearn: 0.0920979\ttotal: 6.57s\tremaining: 5m 21s\n",
      "20:\tlearn: 0.0853298\ttotal: 6.9s\tremaining: 5m 21s\n",
      "21:\tlearn: 0.0775218\ttotal: 7.22s\tremaining: 5m 21s\n",
      "22:\tlearn: 0.0708873\ttotal: 7.56s\tremaining: 5m 21s\n",
      "23:\tlearn: 0.0647229\ttotal: 7.88s\tremaining: 5m 20s\n",
      "24:\tlearn: 0.0588859\ttotal: 8.2s\tremaining: 5m 19s\n",
      "25:\tlearn: 0.0536906\ttotal: 8.53s\tremaining: 5m 19s\n",
      "26:\tlearn: 0.0488292\ttotal: 8.85s\tremaining: 5m 18s\n",
      "27:\tlearn: 0.0463605\ttotal: 9.18s\tremaining: 5m 18s\n",
      "28:\tlearn: 0.0441865\ttotal: 9.52s\tremaining: 5m 18s\n",
      "29:\tlearn: 0.0404059\ttotal: 9.84s\tremaining: 5m 18s\n",
      "30:\tlearn: 0.0370904\ttotal: 10.2s\tremaining: 5m 17s\n",
      "31:\tlearn: 0.0349212\ttotal: 10.5s\tremaining: 5m 17s\n",
      "32:\tlearn: 0.0326535\ttotal: 10.8s\tremaining: 5m 17s\n",
      "33:\tlearn: 0.0307934\ttotal: 11.2s\tremaining: 5m 16s\n",
      "34:\tlearn: 0.0287956\ttotal: 11.5s\tremaining: 5m 16s\n",
      "35:\tlearn: 0.0275829\ttotal: 11.8s\tremaining: 5m 16s\n",
      "36:\tlearn: 0.0265502\ttotal: 12.1s\tremaining: 5m 15s\n",
      "37:\tlearn: 0.0249076\ttotal: 12.5s\tremaining: 5m 15s\n",
      "38:\tlearn: 0.0238040\ttotal: 12.8s\tremaining: 5m 14s\n",
      "39:\tlearn: 0.0227097\ttotal: 13.1s\tremaining: 5m 14s\n",
      "40:\tlearn: 0.0217228\ttotal: 13.4s\tremaining: 5m 14s\n",
      "41:\tlearn: 0.0208010\ttotal: 13.8s\tremaining: 5m 13s\n",
      "42:\tlearn: 0.0197916\ttotal: 14.1s\tremaining: 5m 13s\n",
      "43:\tlearn: 0.0190632\ttotal: 14.4s\tremaining: 5m 13s\n",
      "44:\tlearn: 0.0183463\ttotal: 14.8s\tremaining: 5m 13s\n",
      "45:\tlearn: 0.0176698\ttotal: 15.1s\tremaining: 5m 12s\n",
      "46:\tlearn: 0.0171491\ttotal: 15.4s\tremaining: 5m 12s\n",
      "47:\tlearn: 0.0163251\ttotal: 15.7s\tremaining: 5m 12s\n",
      "48:\tlearn: 0.0157872\ttotal: 16.1s\tremaining: 5m 11s\n",
      "49:\tlearn: 0.0150836\ttotal: 16.4s\tremaining: 5m 11s\n",
      "50:\tlearn: 0.0146906\ttotal: 16.7s\tremaining: 5m 10s\n",
      "51:\tlearn: 0.0141460\ttotal: 17s\tremaining: 5m 10s\n",
      "52:\tlearn: 0.0136401\ttotal: 17.4s\tremaining: 5m 10s\n",
      "53:\tlearn: 0.0133051\ttotal: 17.7s\tremaining: 5m 9s\n",
      "54:\tlearn: 0.0128097\ttotal: 18s\tremaining: 5m 9s\n",
      "55:\tlearn: 0.0124088\ttotal: 18.3s\tremaining: 5m 9s\n",
      "56:\tlearn: 0.0120313\ttotal: 18.7s\tremaining: 5m 8s\n",
      "57:\tlearn: 0.0116271\ttotal: 19s\tremaining: 5m 8s\n",
      "58:\tlearn: 0.0114305\ttotal: 19.3s\tremaining: 5m 8s\n",
      "59:\tlearn: 0.0110643\ttotal: 19.7s\tremaining: 5m 8s\n",
      "60:\tlearn: 0.0106584\ttotal: 20s\tremaining: 5m 7s\n",
      "61:\tlearn: 0.0104539\ttotal: 20.3s\tremaining: 5m 7s\n",
      "62:\tlearn: 0.0102474\ttotal: 20.6s\tremaining: 5m 7s\n",
      "63:\tlearn: 0.0098478\ttotal: 21s\tremaining: 5m 6s\n",
      "64:\tlearn: 0.0095846\ttotal: 21.3s\tremaining: 5m 6s\n",
      "65:\tlearn: 0.0093976\ttotal: 21.6s\tremaining: 5m 5s\n",
      "66:\tlearn: 0.0090822\ttotal: 21.9s\tremaining: 5m 5s\n",
      "67:\tlearn: 0.0087809\ttotal: 22.3s\tremaining: 5m 5s\n",
      "68:\tlearn: 0.0086135\ttotal: 22.6s\tremaining: 5m 4s\n",
      "69:\tlearn: 0.0084311\ttotal: 22.9s\tremaining: 5m 4s\n",
      "70:\tlearn: 0.0083156\ttotal: 23.3s\tremaining: 5m 4s\n",
      "71:\tlearn: 0.0081097\ttotal: 23.6s\tremaining: 5m 3s\n",
      "72:\tlearn: 0.0079248\ttotal: 23.9s\tremaining: 5m 3s\n",
      "73:\tlearn: 0.0077994\ttotal: 24.2s\tremaining: 5m 3s\n",
      "74:\tlearn: 0.0076098\ttotal: 24.6s\tremaining: 5m 2s\n",
      "75:\tlearn: 0.0074634\ttotal: 24.9s\tremaining: 5m 2s\n",
      "76:\tlearn: 0.0073187\ttotal: 25.2s\tremaining: 5m 2s\n",
      "77:\tlearn: 0.0071596\ttotal: 25.5s\tremaining: 5m 1s\n",
      "78:\tlearn: 0.0070577\ttotal: 25.9s\tremaining: 5m 1s\n",
      "79:\tlearn: 0.0069399\ttotal: 26.2s\tremaining: 5m 1s\n",
      "80:\tlearn: 0.0068402\ttotal: 26.5s\tremaining: 5m\n",
      "81:\tlearn: 0.0067249\ttotal: 26.9s\tremaining: 5m\n",
      "82:\tlearn: 0.0066233\ttotal: 27.2s\tremaining: 5m\n",
      "83:\tlearn: 0.0065335\ttotal: 27.5s\tremaining: 5m\n",
      "84:\tlearn: 0.0063568\ttotal: 27.9s\tremaining: 4m 59s\n",
      "85:\tlearn: 0.0062785\ttotal: 28.2s\tremaining: 4m 59s\n",
      "86:\tlearn: 0.0061742\ttotal: 28.5s\tremaining: 4m 59s\n",
      "87:\tlearn: 0.0060435\ttotal: 28.8s\tremaining: 4m 58s\n",
      "88:\tlearn: 0.0059652\ttotal: 29.1s\tremaining: 4m 58s\n",
      "89:\tlearn: 0.0058973\ttotal: 29.5s\tremaining: 4m 58s\n",
      "90:\tlearn: 0.0058088\ttotal: 29.8s\tremaining: 4m 57s\n",
      "91:\tlearn: 0.0056913\ttotal: 30.1s\tremaining: 4m 57s\n",
      "92:\tlearn: 0.0056170\ttotal: 30.4s\tremaining: 4m 56s\n",
      "93:\tlearn: 0.0055478\ttotal: 30.8s\tremaining: 4m 56s\n",
      "94:\tlearn: 0.0054962\ttotal: 31.1s\tremaining: 4m 56s\n",
      "95:\tlearn: 0.0054233\ttotal: 31.4s\tremaining: 4m 55s\n",
      "96:\tlearn: 0.0053586\ttotal: 31.7s\tremaining: 4m 55s\n",
      "97:\tlearn: 0.0052284\ttotal: 32.1s\tremaining: 4m 55s\n",
      "98:\tlearn: 0.0051603\ttotal: 32.4s\tremaining: 4m 54s\n",
      "99:\tlearn: 0.0050862\ttotal: 32.7s\tremaining: 4m 54s\n",
      "100:\tlearn: 0.0050306\ttotal: 33.1s\tremaining: 4m 54s\n",
      "101:\tlearn: 0.0049253\ttotal: 33.4s\tremaining: 4m 53s\n",
      "102:\tlearn: 0.0048703\ttotal: 33.7s\tremaining: 4m 53s\n",
      "103:\tlearn: 0.0048120\ttotal: 34s\tremaining: 4m 53s\n",
      "104:\tlearn: 0.0047447\ttotal: 34.4s\tremaining: 4m 52s\n",
      "105:\tlearn: 0.0046896\ttotal: 34.7s\tremaining: 4m 52s\n",
      "106:\tlearn: 0.0046393\ttotal: 35s\tremaining: 4m 52s\n",
      "107:\tlearn: 0.0045812\ttotal: 35.3s\tremaining: 4m 51s\n",
      "108:\tlearn: 0.0045437\ttotal: 35.7s\tremaining: 4m 51s\n",
      "109:\tlearn: 0.0044630\ttotal: 36s\tremaining: 4m 51s\n",
      "110:\tlearn: 0.0044210\ttotal: 36.3s\tremaining: 4m 50s\n",
      "111:\tlearn: 0.0043581\ttotal: 36.6s\tremaining: 4m 50s\n",
      "112:\tlearn: 0.0043005\ttotal: 37s\tremaining: 4m 50s\n",
      "113:\tlearn: 0.0042705\ttotal: 37.3s\tremaining: 4m 49s\n",
      "114:\tlearn: 0.0042262\ttotal: 37.6s\tremaining: 4m 49s\n",
      "115:\tlearn: 0.0041878\ttotal: 37.9s\tremaining: 4m 49s\n",
      "116:\tlearn: 0.0041436\ttotal: 38.3s\tremaining: 4m 48s\n",
      "117:\tlearn: 0.0040905\ttotal: 38.6s\tremaining: 4m 48s\n",
      "118:\tlearn: 0.0040492\ttotal: 38.9s\tremaining: 4m 48s\n",
      "119:\tlearn: 0.0039909\ttotal: 39.3s\tremaining: 4m 47s\n",
      "120:\tlearn: 0.0039557\ttotal: 39.6s\tremaining: 4m 47s\n",
      "121:\tlearn: 0.0039304\ttotal: 39.9s\tremaining: 4m 47s\n",
      "122:\tlearn: 0.0038840\ttotal: 40.2s\tremaining: 4m 46s\n",
      "123:\tlearn: 0.0038582\ttotal: 40.6s\tremaining: 4m 46s\n",
      "124:\tlearn: 0.0038180\ttotal: 40.9s\tremaining: 4m 46s\n",
      "125:\tlearn: 0.0037890\ttotal: 41.2s\tremaining: 4m 46s\n",
      "126:\tlearn: 0.0037587\ttotal: 41.6s\tremaining: 4m 45s\n",
      "127:\tlearn: 0.0037116\ttotal: 41.9s\tremaining: 4m 45s\n",
      "128:\tlearn: 0.0036677\ttotal: 42.2s\tremaining: 4m 45s\n",
      "129:\tlearn: 0.0036371\ttotal: 42.6s\tremaining: 4m 44s\n",
      "130:\tlearn: 0.0035845\ttotal: 42.9s\tremaining: 4m 44s\n",
      "131:\tlearn: 0.0035455\ttotal: 43.2s\tremaining: 4m 44s\n",
      "132:\tlearn: 0.0035084\ttotal: 43.5s\tremaining: 4m 43s\n",
      "133:\tlearn: 0.0034799\ttotal: 43.9s\tremaining: 4m 43s\n",
      "134:\tlearn: 0.0034422\ttotal: 44.2s\tremaining: 4m 43s\n",
      "135:\tlearn: 0.0034033\ttotal: 44.5s\tremaining: 4m 42s\n",
      "136:\tlearn: 0.0033723\ttotal: 44.8s\tremaining: 4m 42s\n",
      "137:\tlearn: 0.0033356\ttotal: 45.2s\tremaining: 4m 42s\n",
      "138:\tlearn: 0.0033022\ttotal: 45.5s\tremaining: 4m 41s\n",
      "139:\tlearn: 0.0032737\ttotal: 45.8s\tremaining: 4m 41s\n",
      "140:\tlearn: 0.0032320\ttotal: 46.1s\tremaining: 4m 41s\n",
      "141:\tlearn: 0.0031667\ttotal: 46.5s\tremaining: 4m 40s\n",
      "142:\tlearn: 0.0031375\ttotal: 46.8s\tremaining: 4m 40s\n",
      "143:\tlearn: 0.0031173\ttotal: 47.1s\tremaining: 4m 40s\n",
      "144:\tlearn: 0.0030653\ttotal: 47.4s\tremaining: 4m 39s\n",
      "145:\tlearn: 0.0030439\ttotal: 47.8s\tremaining: 4m 39s\n",
      "146:\tlearn: 0.0030204\ttotal: 48.1s\tremaining: 4m 39s\n",
      "147:\tlearn: 0.0029975\ttotal: 48.4s\tremaining: 4m 38s\n",
      "148:\tlearn: 0.0029622\ttotal: 48.7s\tremaining: 4m 38s\n",
      "149:\tlearn: 0.0029360\ttotal: 49.1s\tremaining: 4m 38s\n",
      "150:\tlearn: 0.0029044\ttotal: 49.4s\tremaining: 4m 37s\n",
      "151:\tlearn: 0.0028700\ttotal: 49.7s\tremaining: 4m 37s\n",
      "152:\tlearn: 0.0028246\ttotal: 50s\tremaining: 4m 37s\n",
      "153:\tlearn: 0.0027851\ttotal: 50.4s\tremaining: 4m 36s\n",
      "154:\tlearn: 0.0027649\ttotal: 50.7s\tremaining: 4m 36s\n",
      "155:\tlearn: 0.0027514\ttotal: 51s\tremaining: 4m 36s\n",
      "156:\tlearn: 0.0027241\ttotal: 51.3s\tremaining: 4m 35s\n",
      "157:\tlearn: 0.0027102\ttotal: 51.7s\tremaining: 4m 35s\n",
      "158:\tlearn: 0.0026869\ttotal: 52s\tremaining: 4m 34s\n",
      "159:\tlearn: 0.0026679\ttotal: 52.3s\tremaining: 4m 34s\n",
      "160:\tlearn: 0.0026454\ttotal: 52.6s\tremaining: 4m 34s\n",
      "161:\tlearn: 0.0026170\ttotal: 53s\tremaining: 4m 33s\n",
      "162:\tlearn: 0.0026007\ttotal: 53.3s\tremaining: 4m 33s\n",
      "163:\tlearn: 0.0025845\ttotal: 53.6s\tremaining: 4m 33s\n",
      "164:\tlearn: 0.0025468\ttotal: 53.9s\tremaining: 4m 32s\n",
      "165:\tlearn: 0.0025273\ttotal: 54.3s\tremaining: 4m 32s\n",
      "166:\tlearn: 0.0025055\ttotal: 54.7s\tremaining: 4m 32s\n",
      "167:\tlearn: 0.0024917\ttotal: 55s\tremaining: 4m 32s\n",
      "168:\tlearn: 0.0024874\ttotal: 55.4s\tremaining: 4m 32s\n",
      "169:\tlearn: 0.0024527\ttotal: 55.7s\tremaining: 4m 31s\n",
      "170:\tlearn: 0.0024338\ttotal: 56s\tremaining: 4m 31s\n",
      "171:\tlearn: 0.0024156\ttotal: 56.3s\tremaining: 4m 31s\n",
      "172:\tlearn: 0.0023965\ttotal: 56.7s\tremaining: 4m 30s\n",
      "173:\tlearn: 0.0023822\ttotal: 57s\tremaining: 4m 30s\n",
      "174:\tlearn: 0.0023630\ttotal: 57.3s\tremaining: 4m 30s\n",
      "175:\tlearn: 0.0023461\ttotal: 57.6s\tremaining: 4m 29s\n",
      "176:\tlearn: 0.0023336\ttotal: 58s\tremaining: 4m 29s\n",
      "177:\tlearn: 0.0023220\ttotal: 58.3s\tremaining: 4m 29s\n",
      "178:\tlearn: 0.0023054\ttotal: 58.6s\tremaining: 4m 28s\n",
      "179:\tlearn: 0.0022959\ttotal: 58.9s\tremaining: 4m 28s\n",
      "180:\tlearn: 0.0022957\ttotal: 59.3s\tremaining: 4m 28s\n",
      "181:\tlearn: 0.0022852\ttotal: 59.6s\tremaining: 4m 27s\n",
      "182:\tlearn: 0.0022736\ttotal: 59.9s\tremaining: 4m 27s\n",
      "183:\tlearn: 0.0022602\ttotal: 1m\tremaining: 4m 27s\n",
      "184:\tlearn: 0.0022483\ttotal: 1m\tremaining: 4m 26s\n",
      "185:\tlearn: 0.0022385\ttotal: 1m\tremaining: 4m 26s\n",
      "186:\tlearn: 0.0022260\ttotal: 1m 1s\tremaining: 4m 26s\n",
      "187:\tlearn: 0.0022106\ttotal: 1m 1s\tremaining: 4m 25s\n",
      "188:\tlearn: 0.0021982\ttotal: 1m 1s\tremaining: 4m 25s\n",
      "189:\tlearn: 0.0021806\ttotal: 1m 2s\tremaining: 4m 25s\n",
      "190:\tlearn: 0.0021606\ttotal: 1m 2s\tremaining: 4m 24s\n",
      "191:\tlearn: 0.0021421\ttotal: 1m 2s\tremaining: 4m 24s\n",
      "192:\tlearn: 0.0021318\ttotal: 1m 3s\tremaining: 4m 24s\n",
      "193:\tlearn: 0.0021135\ttotal: 1m 3s\tremaining: 4m 23s\n",
      "194:\tlearn: 0.0021016\ttotal: 1m 3s\tremaining: 4m 23s\n",
      "195:\tlearn: 0.0020784\ttotal: 1m 4s\tremaining: 4m 23s\n",
      "196:\tlearn: 0.0020720\ttotal: 1m 4s\tremaining: 4m 22s\n",
      "197:\tlearn: 0.0020625\ttotal: 1m 4s\tremaining: 4m 22s\n",
      "198:\tlearn: 0.0020456\ttotal: 1m 5s\tremaining: 4m 22s\n",
      "199:\tlearn: 0.0020358\ttotal: 1m 5s\tremaining: 4m 21s\n",
      "200:\tlearn: 0.0020288\ttotal: 1m 5s\tremaining: 4m 21s\n",
      "201:\tlearn: 0.0020146\ttotal: 1m 6s\tremaining: 4m 21s\n",
      "202:\tlearn: 0.0020106\ttotal: 1m 6s\tremaining: 4m 20s\n",
      "203:\tlearn: 0.0019956\ttotal: 1m 6s\tremaining: 4m 20s\n",
      "204:\tlearn: 0.0019955\ttotal: 1m 7s\tremaining: 4m 20s\n",
      "205:\tlearn: 0.0019862\ttotal: 1m 7s\tremaining: 4m 19s\n",
      "206:\tlearn: 0.0019744\ttotal: 1m 7s\tremaining: 4m 19s\n",
      "207:\tlearn: 0.0019611\ttotal: 1m 8s\tremaining: 4m 19s\n",
      "208:\tlearn: 0.0019492\ttotal: 1m 8s\tremaining: 4m 18s\n",
      "209:\tlearn: 0.0019374\ttotal: 1m 8s\tremaining: 4m 18s\n",
      "210:\tlearn: 0.0019260\ttotal: 1m 9s\tremaining: 4m 18s\n",
      "211:\tlearn: 0.0019155\ttotal: 1m 9s\tremaining: 4m 17s\n",
      "212:\tlearn: 0.0019078\ttotal: 1m 9s\tremaining: 4m 17s\n",
      "213:\tlearn: 0.0018952\ttotal: 1m 10s\tremaining: 4m 17s\n",
      "214:\tlearn: 0.0018855\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "215:\tlearn: 0.0018785\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "216:\tlearn: 0.0018700\ttotal: 1m 10s\tremaining: 4m 16s\n",
      "217:\tlearn: 0.0018639\ttotal: 1m 11s\tremaining: 4m 15s\n",
      "218:\tlearn: 0.0018584\ttotal: 1m 11s\tremaining: 4m 15s\n",
      "219:\tlearn: 0.0018464\ttotal: 1m 11s\tremaining: 4m 15s\n",
      "220:\tlearn: 0.0018321\ttotal: 1m 12s\tremaining: 4m 14s\n",
      "221:\tlearn: 0.0018233\ttotal: 1m 12s\tremaining: 4m 14s\n",
      "222:\tlearn: 0.0018152\ttotal: 1m 12s\tremaining: 4m 14s\n",
      "223:\tlearn: 0.0018009\ttotal: 1m 13s\tremaining: 4m 13s\n",
      "224:\tlearn: 0.0018009\ttotal: 1m 13s\tremaining: 4m 13s\n",
      "225:\tlearn: 0.0017931\ttotal: 1m 13s\tremaining: 4m 13s\n",
      "226:\tlearn: 0.0017931\ttotal: 1m 14s\tremaining: 4m 12s\n",
      "227:\tlearn: 0.0017846\ttotal: 1m 14s\tremaining: 4m 12s\n",
      "228:\tlearn: 0.0017782\ttotal: 1m 14s\tremaining: 4m 12s\n",
      "229:\tlearn: 0.0017666\ttotal: 1m 15s\tremaining: 4m 12s\n",
      "230:\tlearn: 0.0017586\ttotal: 1m 15s\tremaining: 4m 11s\n",
      "231:\tlearn: 0.0017495\ttotal: 1m 15s\tremaining: 4m 11s\n",
      "232:\tlearn: 0.0017494\ttotal: 1m 16s\tremaining: 4m 11s\n",
      "233:\tlearn: 0.0017444\ttotal: 1m 16s\tremaining: 4m 10s\n",
      "234:\tlearn: 0.0017443\ttotal: 1m 16s\tremaining: 4m 10s\n",
      "235:\tlearn: 0.0017416\ttotal: 1m 17s\tremaining: 4m 10s\n",
      "236:\tlearn: 0.0017331\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "237:\tlearn: 0.0017228\ttotal: 1m 17s\tremaining: 4m 9s\n",
      "238:\tlearn: 0.0017185\ttotal: 1m 18s\tremaining: 4m 9s\n",
      "239:\tlearn: 0.0017103\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "240:\tlearn: 0.0017103\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "241:\tlearn: 0.0017020\ttotal: 1m 19s\tremaining: 4m 8s\n",
      "242:\tlearn: 0.0016946\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "243:\tlearn: 0.0016888\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "244:\tlearn: 0.0016887\ttotal: 1m 20s\tremaining: 4m 7s\n",
      "245:\tlearn: 0.0016810\ttotal: 1m 20s\tremaining: 4m 6s\n",
      "246:\tlearn: 0.0016658\ttotal: 1m 20s\tremaining: 4m 6s\n",
      "247:\tlearn: 0.0016658\ttotal: 1m 21s\tremaining: 4m 6s\n",
      "248:\tlearn: 0.0016573\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "249:\tlearn: 0.0016498\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "250:\tlearn: 0.0016498\ttotal: 1m 22s\tremaining: 4m 5s\n",
      "251:\tlearn: 0.0016436\ttotal: 1m 22s\tremaining: 4m 4s\n",
      "252:\tlearn: 0.0016401\ttotal: 1m 22s\tremaining: 4m 4s\n",
      "253:\tlearn: 0.0016358\ttotal: 1m 23s\tremaining: 4m 4s\n",
      "254:\tlearn: 0.0016297\ttotal: 1m 23s\tremaining: 4m 4s\n",
      "255:\tlearn: 0.0016150\ttotal: 1m 24s\tremaining: 4m 4s\n",
      "256:\tlearn: 0.0016079\ttotal: 1m 24s\tremaining: 4m 3s\n",
      "257:\tlearn: 0.0015993\ttotal: 1m 24s\tremaining: 4m 3s\n",
      "258:\tlearn: 0.0015990\ttotal: 1m 25s\tremaining: 4m 3s\n",
      "259:\tlearn: 0.0015968\ttotal: 1m 25s\tremaining: 4m 3s\n",
      "260:\tlearn: 0.0015862\ttotal: 1m 25s\tremaining: 4m 2s\n",
      "261:\tlearn: 0.0015775\ttotal: 1m 26s\tremaining: 4m 2s\n",
      "262:\tlearn: 0.0015725\ttotal: 1m 26s\tremaining: 4m 2s\n",
      "263:\tlearn: 0.0015607\ttotal: 1m 26s\tremaining: 4m 1s\n",
      "264:\tlearn: 0.0015607\ttotal: 1m 27s\tremaining: 4m 1s\n",
      "265:\tlearn: 0.0015583\ttotal: 1m 27s\tremaining: 4m 1s\n",
      "266:\tlearn: 0.0015535\ttotal: 1m 27s\tremaining: 4m\n",
      "267:\tlearn: 0.0015445\ttotal: 1m 28s\tremaining: 4m\n",
      "268:\tlearn: 0.0015445\ttotal: 1m 28s\tremaining: 4m\n",
      "269:\tlearn: 0.0015444\ttotal: 1m 28s\tremaining: 3m 59s\n",
      "270:\tlearn: 0.0015444\ttotal: 1m 28s\tremaining: 3m 59s\n",
      "271:\tlearn: 0.0015443\ttotal: 1m 29s\tremaining: 3m 59s\n",
      "272:\tlearn: 0.0015367\ttotal: 1m 29s\tremaining: 3m 58s\n",
      "273:\tlearn: 0.0015305\ttotal: 1m 29s\tremaining: 3m 58s\n",
      "274:\tlearn: 0.0015305\ttotal: 1m 30s\tremaining: 3m 58s\n",
      "275:\tlearn: 0.0015302\ttotal: 1m 30s\tremaining: 3m 57s\n",
      "276:\tlearn: 0.0015222\ttotal: 1m 30s\tremaining: 3m 57s\n",
      "277:\tlearn: 0.0015153\ttotal: 1m 31s\tremaining: 3m 57s\n",
      "278:\tlearn: 0.0015152\ttotal: 1m 31s\tremaining: 3m 56s\n",
      "279:\tlearn: 0.0015131\ttotal: 1m 31s\tremaining: 3m 56s\n",
      "280:\tlearn: 0.0015131\ttotal: 1m 32s\tremaining: 3m 56s\n",
      "281:\tlearn: 0.0015130\ttotal: 1m 32s\tremaining: 3m 55s\n",
      "282:\tlearn: 0.0015129\ttotal: 1m 32s\tremaining: 3m 55s\n",
      "283:\tlearn: 0.0015128\ttotal: 1m 33s\tremaining: 3m 55s\n",
      "284:\tlearn: 0.0015126\ttotal: 1m 33s\tremaining: 3m 54s\n",
      "285:\tlearn: 0.0015095\ttotal: 1m 33s\tremaining: 3m 54s\n",
      "286:\tlearn: 0.0015064\ttotal: 1m 34s\tremaining: 3m 54s\n",
      "287:\tlearn: 0.0015064\ttotal: 1m 34s\tremaining: 3m 53s\n",
      "288:\tlearn: 0.0015064\ttotal: 1m 34s\tremaining: 3m 53s\n",
      "289:\tlearn: 0.0015004\ttotal: 1m 35s\tremaining: 3m 53s\n",
      "290:\tlearn: 0.0015003\ttotal: 1m 35s\tremaining: 3m 52s\n",
      "291:\tlearn: 0.0015003\ttotal: 1m 35s\tremaining: 3m 52s\n",
      "292:\tlearn: 0.0014950\ttotal: 1m 36s\tremaining: 3m 52s\n",
      "293:\tlearn: 0.0014948\ttotal: 1m 36s\tremaining: 3m 51s\n",
      "294:\tlearn: 0.0014925\ttotal: 1m 36s\tremaining: 3m 51s\n",
      "295:\tlearn: 0.0014870\ttotal: 1m 37s\tremaining: 3m 51s\n",
      "296:\tlearn: 0.0014870\ttotal: 1m 37s\tremaining: 3m 50s\n",
      "297:\tlearn: 0.0014821\ttotal: 1m 37s\tremaining: 3m 50s\n",
      "298:\tlearn: 0.0014761\ttotal: 1m 38s\tremaining: 3m 50s\n",
      "299:\tlearn: 0.0014694\ttotal: 1m 38s\tremaining: 3m 49s\n",
      "300:\tlearn: 0.0014694\ttotal: 1m 38s\tremaining: 3m 49s\n",
      "301:\tlearn: 0.0014693\ttotal: 1m 39s\tremaining: 3m 49s\n",
      "302:\tlearn: 0.0014691\ttotal: 1m 39s\tremaining: 3m 48s\n",
      "303:\tlearn: 0.0014682\ttotal: 1m 39s\tremaining: 3m 48s\n",
      "304:\tlearn: 0.0014642\ttotal: 1m 40s\tremaining: 3m 48s\n",
      "305:\tlearn: 0.0014642\ttotal: 1m 40s\tremaining: 3m 47s\n",
      "306:\tlearn: 0.0014573\ttotal: 1m 40s\tremaining: 3m 47s\n",
      "307:\tlearn: 0.0014572\ttotal: 1m 41s\tremaining: 3m 47s\n",
      "308:\tlearn: 0.0014572\ttotal: 1m 41s\tremaining: 3m 46s\n",
      "309:\tlearn: 0.0014569\ttotal: 1m 41s\tremaining: 3m 46s\n",
      "310:\tlearn: 0.0014473\ttotal: 1m 42s\tremaining: 3m 46s\n",
      "311:\tlearn: 0.0014400\ttotal: 1m 42s\tremaining: 3m 45s\n",
      "312:\tlearn: 0.0014400\ttotal: 1m 42s\tremaining: 3m 45s\n",
      "313:\tlearn: 0.0014396\ttotal: 1m 43s\tremaining: 3m 45s\n",
      "314:\tlearn: 0.0014338\ttotal: 1m 43s\tremaining: 3m 44s\n",
      "315:\tlearn: 0.0014319\ttotal: 1m 43s\tremaining: 3m 44s\n",
      "316:\tlearn: 0.0014318\ttotal: 1m 44s\tremaining: 3m 44s\n",
      "317:\tlearn: 0.0014298\ttotal: 1m 44s\tremaining: 3m 43s\n",
      "318:\tlearn: 0.0014278\ttotal: 1m 44s\tremaining: 3m 43s\n",
      "319:\tlearn: 0.0014278\ttotal: 1m 45s\tremaining: 3m 43s\n",
      "320:\tlearn: 0.0014277\ttotal: 1m 45s\tremaining: 3m 42s\n",
      "321:\tlearn: 0.0014277\ttotal: 1m 45s\tremaining: 3m 42s\n",
      "322:\tlearn: 0.0014276\ttotal: 1m 46s\tremaining: 3m 42s\n",
      "323:\tlearn: 0.0014274\ttotal: 1m 46s\tremaining: 3m 41s\n",
      "324:\tlearn: 0.0014274\ttotal: 1m 46s\tremaining: 3m 41s\n",
      "325:\tlearn: 0.0014247\ttotal: 1m 47s\tremaining: 3m 41s\n",
      "326:\tlearn: 0.0014203\ttotal: 1m 47s\tremaining: 3m 40s\n",
      "327:\tlearn: 0.0014173\ttotal: 1m 47s\tremaining: 3m 40s\n",
      "328:\tlearn: 0.0014101\ttotal: 1m 47s\tremaining: 3m 40s\n",
      "329:\tlearn: 0.0014100\ttotal: 1m 48s\tremaining: 3m 39s\n",
      "330:\tlearn: 0.0014072\ttotal: 1m 48s\tremaining: 3m 39s\n",
      "331:\tlearn: 0.0014072\ttotal: 1m 48s\tremaining: 3m 39s\n",
      "332:\tlearn: 0.0014072\ttotal: 1m 49s\tremaining: 3m 38s\n",
      "333:\tlearn: 0.0014072\ttotal: 1m 49s\tremaining: 3m 38s\n",
      "334:\tlearn: 0.0014071\ttotal: 1m 49s\tremaining: 3m 38s\n",
      "335:\tlearn: 0.0014024\ttotal: 1m 50s\tremaining: 3m 37s\n",
      "336:\tlearn: 0.0014022\ttotal: 1m 50s\tremaining: 3m 37s\n",
      "337:\tlearn: 0.0014022\ttotal: 1m 50s\tremaining: 3m 37s\n",
      "338:\tlearn: 0.0014022\ttotal: 1m 51s\tremaining: 3m 36s\n",
      "339:\tlearn: 0.0014019\ttotal: 1m 51s\tremaining: 3m 36s\n",
      "340:\tlearn: 0.0014019\ttotal: 1m 51s\tremaining: 3m 36s\n",
      "341:\tlearn: 0.0014019\ttotal: 1m 52s\tremaining: 3m 35s\n",
      "342:\tlearn: 0.0014019\ttotal: 1m 52s\tremaining: 3m 35s\n",
      "343:\tlearn: 0.0014019\ttotal: 1m 52s\tremaining: 3m 35s\n",
      "344:\tlearn: 0.0014006\ttotal: 1m 53s\tremaining: 3m 34s\n",
      "345:\tlearn: 0.0013947\ttotal: 1m 53s\tremaining: 3m 34s\n",
      "346:\tlearn: 0.0013946\ttotal: 1m 53s\tremaining: 3m 34s\n",
      "347:\tlearn: 0.0013913\ttotal: 1m 54s\tremaining: 3m 33s\n",
      "348:\tlearn: 0.0013860\ttotal: 1m 54s\tremaining: 3m 33s\n",
      "349:\tlearn: 0.0013860\ttotal: 1m 54s\tremaining: 3m 33s\n",
      "350:\tlearn: 0.0013814\ttotal: 1m 55s\tremaining: 3m 32s\n",
      "351:\tlearn: 0.0013784\ttotal: 1m 55s\tremaining: 3m 32s\n",
      "352:\tlearn: 0.0013721\ttotal: 1m 55s\tremaining: 3m 32s\n",
      "353:\tlearn: 0.0013721\ttotal: 1m 56s\tremaining: 3m 31s\n",
      "354:\tlearn: 0.0013720\ttotal: 1m 56s\tremaining: 3m 31s\n",
      "355:\tlearn: 0.0013718\ttotal: 1m 56s\tremaining: 3m 31s\n",
      "356:\tlearn: 0.0013716\ttotal: 1m 57s\tremaining: 3m 30s\n",
      "357:\tlearn: 0.0013638\ttotal: 1m 57s\tremaining: 3m 30s\n",
      "358:\tlearn: 0.0013591\ttotal: 1m 57s\tremaining: 3m 30s\n",
      "359:\tlearn: 0.0013541\ttotal: 1m 58s\tremaining: 3m 29s\n",
      "360:\tlearn: 0.0013495\ttotal: 1m 58s\tremaining: 3m 29s\n",
      "361:\tlearn: 0.0013495\ttotal: 1m 58s\tremaining: 3m 29s\n",
      "362:\tlearn: 0.0013442\ttotal: 1m 59s\tremaining: 3m 28s\n",
      "363:\tlearn: 0.0013442\ttotal: 1m 59s\tremaining: 3m 28s\n",
      "364:\tlearn: 0.0013442\ttotal: 1m 59s\tremaining: 3m 28s\n",
      "365:\tlearn: 0.0013442\ttotal: 2m\tremaining: 3m 27s\n",
      "366:\tlearn: 0.0013419\ttotal: 2m\tremaining: 3m 27s\n",
      "367:\tlearn: 0.0013418\ttotal: 2m\tremaining: 3m 27s\n",
      "368:\tlearn: 0.0013417\ttotal: 2m 1s\tremaining: 3m 27s\n",
      "369:\tlearn: 0.0013414\ttotal: 2m 1s\tremaining: 3m 26s\n",
      "370:\tlearn: 0.0013414\ttotal: 2m 1s\tremaining: 3m 26s\n",
      "371:\tlearn: 0.0013390\ttotal: 2m 2s\tremaining: 3m 26s\n",
      "372:\tlearn: 0.0013390\ttotal: 2m 2s\tremaining: 3m 25s\n",
      "373:\tlearn: 0.0013390\ttotal: 2m 2s\tremaining: 3m 25s\n",
      "374:\tlearn: 0.0013390\ttotal: 2m 3s\tremaining: 3m 25s\n",
      "375:\tlearn: 0.0013385\ttotal: 2m 3s\tremaining: 3m 24s\n",
      "376:\tlearn: 0.0013383\ttotal: 2m 3s\tremaining: 3m 24s\n",
      "377:\tlearn: 0.0013335\ttotal: 2m 3s\tremaining: 3m 24s\n",
      "378:\tlearn: 0.0013318\ttotal: 2m 4s\tremaining: 3m 23s\n",
      "379:\tlearn: 0.0013316\ttotal: 2m 4s\tremaining: 3m 23s\n",
      "380:\tlearn: 0.0013316\ttotal: 2m 4s\tremaining: 3m 23s\n",
      "381:\tlearn: 0.0013316\ttotal: 2m 5s\tremaining: 3m 22s\n",
      "382:\tlearn: 0.0013316\ttotal: 2m 5s\tremaining: 3m 22s\n",
      "383:\tlearn: 0.0013316\ttotal: 2m 5s\tremaining: 3m 22s\n",
      "384:\tlearn: 0.0013316\ttotal: 2m 6s\tremaining: 3m 21s\n",
      "385:\tlearn: 0.0013316\ttotal: 2m 6s\tremaining: 3m 21s\n",
      "386:\tlearn: 0.0013316\ttotal: 2m 6s\tremaining: 3m 21s\n",
      "387:\tlearn: 0.0013315\ttotal: 2m 7s\tremaining: 3m 20s\n",
      "388:\tlearn: 0.0013315\ttotal: 2m 7s\tremaining: 3m 20s\n",
      "389:\tlearn: 0.0013273\ttotal: 2m 7s\tremaining: 3m 20s\n",
      "390:\tlearn: 0.0013248\ttotal: 2m 8s\tremaining: 3m 19s\n",
      "391:\tlearn: 0.0013248\ttotal: 2m 8s\tremaining: 3m 19s\n",
      "392:\tlearn: 0.0013224\ttotal: 2m 8s\tremaining: 3m 19s\n",
      "393:\tlearn: 0.0013224\ttotal: 2m 9s\tremaining: 3m 18s\n",
      "394:\tlearn: 0.0013224\ttotal: 2m 9s\tremaining: 3m 18s\n",
      "395:\tlearn: 0.0013224\ttotal: 2m 9s\tremaining: 3m 18s\n",
      "396:\tlearn: 0.0013222\ttotal: 2m 10s\tremaining: 3m 17s\n",
      "397:\tlearn: 0.0013222\ttotal: 2m 10s\tremaining: 3m 17s\n",
      "398:\tlearn: 0.0013222\ttotal: 2m 10s\tremaining: 3m 17s\n",
      "399:\tlearn: 0.0013221\ttotal: 2m 11s\tremaining: 3m 16s\n",
      "400:\tlearn: 0.0013191\ttotal: 2m 11s\tremaining: 3m 16s\n",
      "401:\tlearn: 0.0013191\ttotal: 2m 11s\tremaining: 3m 16s\n",
      "402:\tlearn: 0.0013191\ttotal: 2m 12s\tremaining: 3m 15s\n",
      "403:\tlearn: 0.0013191\ttotal: 2m 12s\tremaining: 3m 15s\n",
      "404:\tlearn: 0.0013140\ttotal: 2m 12s\tremaining: 3m 15s\n",
      "405:\tlearn: 0.0013139\ttotal: 2m 13s\tremaining: 3m 14s\n",
      "406:\tlearn: 0.0013138\ttotal: 2m 13s\tremaining: 3m 14s\n",
      "407:\tlearn: 0.0013137\ttotal: 2m 13s\tremaining: 3m 14s\n",
      "408:\tlearn: 0.0013137\ttotal: 2m 14s\tremaining: 3m 13s\n",
      "409:\tlearn: 0.0013094\ttotal: 2m 14s\tremaining: 3m 13s\n",
      "410:\tlearn: 0.0013044\ttotal: 2m 14s\tremaining: 3m 13s\n",
      "411:\tlearn: 0.0013044\ttotal: 2m 15s\tremaining: 3m 12s\n",
      "412:\tlearn: 0.0012996\ttotal: 2m 15s\tremaining: 3m 12s\n",
      "413:\tlearn: 0.0012995\ttotal: 2m 15s\tremaining: 3m 12s\n",
      "414:\tlearn: 0.0012995\ttotal: 2m 16s\tremaining: 3m 12s\n",
      "415:\tlearn: 0.0012995\ttotal: 2m 16s\tremaining: 3m 11s\n",
      "416:\tlearn: 0.0012945\ttotal: 2m 16s\tremaining: 3m 11s\n",
      "417:\tlearn: 0.0012944\ttotal: 2m 17s\tremaining: 3m 11s\n",
      "418:\tlearn: 0.0012944\ttotal: 2m 17s\tremaining: 3m 10s\n",
      "419:\tlearn: 0.0012908\ttotal: 2m 17s\tremaining: 3m 10s\n",
      "420:\tlearn: 0.0012903\ttotal: 2m 18s\tremaining: 3m 10s\n",
      "421:\tlearn: 0.0012903\ttotal: 2m 18s\tremaining: 3m 9s\n",
      "422:\tlearn: 0.0012903\ttotal: 2m 18s\tremaining: 3m 9s\n",
      "423:\tlearn: 0.0012899\ttotal: 2m 19s\tremaining: 3m 9s\n",
      "424:\tlearn: 0.0012897\ttotal: 2m 19s\tremaining: 3m 8s\n",
      "425:\tlearn: 0.0012889\ttotal: 2m 19s\tremaining: 3m 8s\n",
      "426:\tlearn: 0.0012862\ttotal: 2m 20s\tremaining: 3m 8s\n",
      "427:\tlearn: 0.0012862\ttotal: 2m 20s\tremaining: 3m 7s\n",
      "428:\tlearn: 0.0012786\ttotal: 2m 20s\tremaining: 3m 7s\n",
      "429:\tlearn: 0.0012785\ttotal: 2m 21s\tremaining: 3m 7s\n",
      "430:\tlearn: 0.0012785\ttotal: 2m 21s\tremaining: 3m 6s\n",
      "431:\tlearn: 0.0012757\ttotal: 2m 21s\tremaining: 3m 6s\n",
      "432:\tlearn: 0.0012757\ttotal: 2m 22s\tremaining: 3m 6s\n",
      "433:\tlearn: 0.0012757\ttotal: 2m 22s\tremaining: 3m 5s\n",
      "434:\tlearn: 0.0012757\ttotal: 2m 22s\tremaining: 3m 5s\n",
      "435:\tlearn: 0.0012757\ttotal: 2m 23s\tremaining: 3m 5s\n",
      "436:\tlearn: 0.0012757\ttotal: 2m 23s\tremaining: 3m 4s\n",
      "437:\tlearn: 0.0012757\ttotal: 2m 23s\tremaining: 3m 4s\n",
      "438:\tlearn: 0.0012756\ttotal: 2m 24s\tremaining: 3m 4s\n",
      "439:\tlearn: 0.0012755\ttotal: 2m 24s\tremaining: 3m 3s\n",
      "440:\tlearn: 0.0012718\ttotal: 2m 24s\tremaining: 3m 3s\n",
      "441:\tlearn: 0.0012718\ttotal: 2m 25s\tremaining: 3m 3s\n",
      "442:\tlearn: 0.0012718\ttotal: 2m 25s\tremaining: 3m 2s\n",
      "443:\tlearn: 0.0012683\ttotal: 2m 25s\tremaining: 3m 2s\n",
      "444:\tlearn: 0.0012681\ttotal: 2m 26s\tremaining: 3m 2s\n",
      "445:\tlearn: 0.0012680\ttotal: 2m 26s\tremaining: 3m 1s\n",
      "446:\tlearn: 0.0012680\ttotal: 2m 26s\tremaining: 3m 1s\n",
      "447:\tlearn: 0.0012680\ttotal: 2m 27s\tremaining: 3m 1s\n",
      "448:\tlearn: 0.0012679\ttotal: 2m 27s\tremaining: 3m\n",
      "449:\tlearn: 0.0012641\ttotal: 2m 27s\tremaining: 3m\n",
      "450:\tlearn: 0.0012641\ttotal: 2m 27s\tremaining: 3m\n",
      "451:\tlearn: 0.0012640\ttotal: 2m 28s\tremaining: 2m 59s\n",
      "452:\tlearn: 0.0012588\ttotal: 2m 28s\tremaining: 2m 59s\n",
      "453:\tlearn: 0.0012582\ttotal: 2m 28s\tremaining: 2m 59s\n",
      "454:\tlearn: 0.0012582\ttotal: 2m 29s\tremaining: 2m 58s\n",
      "455:\tlearn: 0.0012577\ttotal: 2m 29s\tremaining: 2m 58s\n",
      "456:\tlearn: 0.0012577\ttotal: 2m 29s\tremaining: 2m 58s\n",
      "457:\tlearn: 0.0012574\ttotal: 2m 30s\tremaining: 2m 57s\n",
      "458:\tlearn: 0.0012549\ttotal: 2m 30s\tremaining: 2m 57s\n",
      "459:\tlearn: 0.0012548\ttotal: 2m 30s\tremaining: 2m 57s\n",
      "460:\tlearn: 0.0012506\ttotal: 2m 31s\tremaining: 2m 56s\n",
      "461:\tlearn: 0.0012506\ttotal: 2m 31s\tremaining: 2m 56s\n",
      "462:\tlearn: 0.0012506\ttotal: 2m 31s\tremaining: 2m 56s\n",
      "463:\tlearn: 0.0012502\ttotal: 2m 32s\tremaining: 2m 55s\n",
      "464:\tlearn: 0.0012460\ttotal: 2m 32s\tremaining: 2m 55s\n",
      "465:\tlearn: 0.0012459\ttotal: 2m 32s\tremaining: 2m 55s\n",
      "466:\tlearn: 0.0012458\ttotal: 2m 33s\tremaining: 2m 54s\n",
      "467:\tlearn: 0.0012458\ttotal: 2m 33s\tremaining: 2m 54s\n",
      "468:\tlearn: 0.0012420\ttotal: 2m 33s\tremaining: 2m 54s\n",
      "469:\tlearn: 0.0012406\ttotal: 2m 34s\tremaining: 2m 53s\n",
      "470:\tlearn: 0.0012406\ttotal: 2m 34s\tremaining: 2m 53s\n",
      "471:\tlearn: 0.0012406\ttotal: 2m 34s\tremaining: 2m 53s\n",
      "472:\tlearn: 0.0012406\ttotal: 2m 35s\tremaining: 2m 52s\n",
      "473:\tlearn: 0.0012406\ttotal: 2m 35s\tremaining: 2m 52s\n",
      "474:\tlearn: 0.0012403\ttotal: 2m 35s\tremaining: 2m 52s\n",
      "475:\tlearn: 0.0012403\ttotal: 2m 36s\tremaining: 2m 51s\n",
      "476:\tlearn: 0.0012403\ttotal: 2m 36s\tremaining: 2m 51s\n",
      "477:\tlearn: 0.0012402\ttotal: 2m 36s\tremaining: 2m 51s\n",
      "478:\tlearn: 0.0012401\ttotal: 2m 37s\tremaining: 2m 51s\n",
      "479:\tlearn: 0.0012400\ttotal: 2m 37s\tremaining: 2m 50s\n",
      "480:\tlearn: 0.0012364\ttotal: 2m 37s\tremaining: 2m 50s\n",
      "481:\tlearn: 0.0012361\ttotal: 2m 38s\tremaining: 2m 50s\n",
      "482:\tlearn: 0.0012361\ttotal: 2m 38s\tremaining: 2m 49s\n",
      "483:\tlearn: 0.0012361\ttotal: 2m 38s\tremaining: 2m 49s\n",
      "484:\tlearn: 0.0012360\ttotal: 2m 39s\tremaining: 2m 49s\n",
      "485:\tlearn: 0.0012359\ttotal: 2m 39s\tremaining: 2m 48s\n",
      "486:\tlearn: 0.0012338\ttotal: 2m 39s\tremaining: 2m 48s\n",
      "487:\tlearn: 0.0012338\ttotal: 2m 40s\tremaining: 2m 48s\n",
      "488:\tlearn: 0.0012337\ttotal: 2m 40s\tremaining: 2m 47s\n",
      "489:\tlearn: 0.0012337\ttotal: 2m 40s\tremaining: 2m 47s\n",
      "490:\tlearn: 0.0012336\ttotal: 2m 41s\tremaining: 2m 47s\n",
      "491:\tlearn: 0.0012336\ttotal: 2m 41s\tremaining: 2m 46s\n",
      "492:\tlearn: 0.0012304\ttotal: 2m 41s\tremaining: 2m 46s\n",
      "493:\tlearn: 0.0012304\ttotal: 2m 42s\tremaining: 2m 46s\n",
      "494:\tlearn: 0.0012304\ttotal: 2m 42s\tremaining: 2m 45s\n",
      "495:\tlearn: 0.0012302\ttotal: 2m 42s\tremaining: 2m 45s\n",
      "496:\tlearn: 0.0012300\ttotal: 2m 43s\tremaining: 2m 45s\n",
      "497:\tlearn: 0.0012288\ttotal: 2m 43s\tremaining: 2m 44s\n",
      "498:\tlearn: 0.0012287\ttotal: 2m 43s\tremaining: 2m 44s\n",
      "499:\tlearn: 0.0012286\ttotal: 2m 44s\tremaining: 2m 44s\n",
      "500:\tlearn: 0.0012286\ttotal: 2m 44s\tremaining: 2m 43s\n",
      "501:\tlearn: 0.0012286\ttotal: 2m 44s\tremaining: 2m 43s\n",
      "502:\tlearn: 0.0012286\ttotal: 2m 45s\tremaining: 2m 43s\n",
      "503:\tlearn: 0.0012272\ttotal: 2m 45s\tremaining: 2m 42s\n",
      "504:\tlearn: 0.0012271\ttotal: 2m 45s\tremaining: 2m 42s\n",
      "505:\tlearn: 0.0012270\ttotal: 2m 46s\tremaining: 2m 42s\n",
      "506:\tlearn: 0.0012269\ttotal: 2m 46s\tremaining: 2m 41s\n",
      "507:\tlearn: 0.0012269\ttotal: 2m 46s\tremaining: 2m 41s\n",
      "508:\tlearn: 0.0012250\ttotal: 2m 47s\tremaining: 2m 41s\n",
      "509:\tlearn: 0.0012205\ttotal: 2m 47s\tremaining: 2m 40s\n",
      "510:\tlearn: 0.0012204\ttotal: 2m 47s\tremaining: 2m 40s\n",
      "511:\tlearn: 0.0012204\ttotal: 2m 48s\tremaining: 2m 40s\n",
      "512:\tlearn: 0.0012204\ttotal: 2m 48s\tremaining: 2m 39s\n",
      "513:\tlearn: 0.0012203\ttotal: 2m 48s\tremaining: 2m 39s\n",
      "514:\tlearn: 0.0012184\ttotal: 2m 49s\tremaining: 2m 39s\n",
      "515:\tlearn: 0.0012183\ttotal: 2m 49s\tremaining: 2m 38s\n",
      "516:\tlearn: 0.0012182\ttotal: 2m 49s\tremaining: 2m 38s\n",
      "517:\tlearn: 0.0012170\ttotal: 2m 49s\tremaining: 2m 38s\n",
      "518:\tlearn: 0.0012170\ttotal: 2m 50s\tremaining: 2m 37s\n",
      "519:\tlearn: 0.0012170\ttotal: 2m 50s\tremaining: 2m 37s\n",
      "520:\tlearn: 0.0012170\ttotal: 2m 50s\tremaining: 2m 37s\n",
      "521:\tlearn: 0.0012146\ttotal: 2m 51s\tremaining: 2m 36s\n",
      "522:\tlearn: 0.0012129\ttotal: 2m 51s\tremaining: 2m 36s\n",
      "523:\tlearn: 0.0012128\ttotal: 2m 51s\tremaining: 2m 36s\n",
      "524:\tlearn: 0.0012112\ttotal: 2m 52s\tremaining: 2m 35s\n",
      "525:\tlearn: 0.0012111\ttotal: 2m 52s\tremaining: 2m 35s\n",
      "526:\tlearn: 0.0012110\ttotal: 2m 52s\tremaining: 2m 35s\n",
      "527:\tlearn: 0.0012110\ttotal: 2m 53s\tremaining: 2m 34s\n",
      "528:\tlearn: 0.0012110\ttotal: 2m 53s\tremaining: 2m 34s\n",
      "529:\tlearn: 0.0012109\ttotal: 2m 53s\tremaining: 2m 34s\n",
      "530:\tlearn: 0.0012108\ttotal: 2m 54s\tremaining: 2m 33s\n",
      "531:\tlearn: 0.0012105\ttotal: 2m 54s\tremaining: 2m 33s\n",
      "532:\tlearn: 0.0012105\ttotal: 2m 54s\tremaining: 2m 33s\n",
      "533:\tlearn: 0.0012105\ttotal: 2m 55s\tremaining: 2m 32s\n",
      "534:\tlearn: 0.0012105\ttotal: 2m 55s\tremaining: 2m 32s\n",
      "535:\tlearn: 0.0012104\ttotal: 2m 55s\tremaining: 2m 32s\n",
      "536:\tlearn: 0.0012103\ttotal: 2m 56s\tremaining: 2m 31s\n",
      "537:\tlearn: 0.0012103\ttotal: 2m 56s\tremaining: 2m 31s\n",
      "538:\tlearn: 0.0012101\ttotal: 2m 56s\tremaining: 2m 31s\n",
      "539:\tlearn: 0.0012083\ttotal: 2m 57s\tremaining: 2m 30s\n",
      "540:\tlearn: 0.0012082\ttotal: 2m 57s\tremaining: 2m 30s\n",
      "541:\tlearn: 0.0012080\ttotal: 2m 57s\tremaining: 2m 30s\n",
      "542:\tlearn: 0.0012079\ttotal: 2m 58s\tremaining: 2m 29s\n",
      "543:\tlearn: 0.0012079\ttotal: 2m 58s\tremaining: 2m 29s\n",
      "544:\tlearn: 0.0012078\ttotal: 2m 58s\tremaining: 2m 29s\n",
      "545:\tlearn: 0.0012074\ttotal: 2m 59s\tremaining: 2m 28s\n",
      "546:\tlearn: 0.0012072\ttotal: 2m 59s\tremaining: 2m 28s\n",
      "547:\tlearn: 0.0012071\ttotal: 2m 59s\tremaining: 2m 28s\n",
      "548:\tlearn: 0.0012070\ttotal: 3m\tremaining: 2m 27s\n",
      "549:\tlearn: 0.0012070\ttotal: 3m\tremaining: 2m 27s\n",
      "550:\tlearn: 0.0012069\ttotal: 3m\tremaining: 2m 27s\n",
      "551:\tlearn: 0.0012068\ttotal: 3m 1s\tremaining: 2m 26s\n",
      "552:\tlearn: 0.0012066\ttotal: 3m 1s\tremaining: 2m 26s\n",
      "553:\tlearn: 0.0012066\ttotal: 3m 1s\tremaining: 2m 26s\n",
      "554:\tlearn: 0.0012065\ttotal: 3m 2s\tremaining: 2m 26s\n",
      "555:\tlearn: 0.0012064\ttotal: 3m 2s\tremaining: 2m 25s\n",
      "556:\tlearn: 0.0012063\ttotal: 3m 2s\tremaining: 2m 25s\n",
      "557:\tlearn: 0.0012063\ttotal: 3m 3s\tremaining: 2m 25s\n",
      "558:\tlearn: 0.0012063\ttotal: 3m 3s\tremaining: 2m 24s\n",
      "559:\tlearn: 0.0012063\ttotal: 3m 3s\tremaining: 2m 24s\n",
      "560:\tlearn: 0.0012061\ttotal: 3m 4s\tremaining: 2m 24s\n",
      "561:\tlearn: 0.0012061\ttotal: 3m 4s\tremaining: 2m 23s\n",
      "562:\tlearn: 0.0012059\ttotal: 3m 4s\tremaining: 2m 23s\n",
      "563:\tlearn: 0.0012059\ttotal: 3m 5s\tremaining: 2m 23s\n",
      "564:\tlearn: 0.0012059\ttotal: 3m 5s\tremaining: 2m 22s\n",
      "565:\tlearn: 0.0012035\ttotal: 3m 5s\tremaining: 2m 22s\n",
      "566:\tlearn: 0.0012034\ttotal: 3m 6s\tremaining: 2m 22s\n",
      "567:\tlearn: 0.0012034\ttotal: 3m 6s\tremaining: 2m 21s\n",
      "568:\tlearn: 0.0012033\ttotal: 3m 6s\tremaining: 2m 21s\n",
      "569:\tlearn: 0.0012001\ttotal: 3m 7s\tremaining: 2m 21s\n",
      "570:\tlearn: 0.0011976\ttotal: 3m 7s\tremaining: 2m 20s\n",
      "571:\tlearn: 0.0011975\ttotal: 3m 7s\tremaining: 2m 20s\n",
      "572:\tlearn: 0.0011961\ttotal: 3m 7s\tremaining: 2m 20s\n",
      "573:\tlearn: 0.0011961\ttotal: 3m 8s\tremaining: 2m 19s\n",
      "574:\tlearn: 0.0011961\ttotal: 3m 8s\tremaining: 2m 19s\n",
      "575:\tlearn: 0.0011961\ttotal: 3m 8s\tremaining: 2m 19s\n",
      "576:\tlearn: 0.0011960\ttotal: 3m 9s\tremaining: 2m 18s\n",
      "577:\tlearn: 0.0011957\ttotal: 3m 9s\tremaining: 2m 18s\n",
      "578:\tlearn: 0.0011932\ttotal: 3m 10s\tremaining: 2m 18s\n",
      "579:\tlearn: 0.0011931\ttotal: 3m 10s\tremaining: 2m 17s\n",
      "580:\tlearn: 0.0011931\ttotal: 3m 10s\tremaining: 2m 17s\n",
      "581:\tlearn: 0.0011915\ttotal: 3m 10s\tremaining: 2m 17s\n",
      "582:\tlearn: 0.0011914\ttotal: 3m 11s\tremaining: 2m 16s\n",
      "583:\tlearn: 0.0011843\ttotal: 3m 11s\tremaining: 2m 16s\n",
      "584:\tlearn: 0.0011843\ttotal: 3m 11s\tremaining: 2m 16s\n",
      "585:\tlearn: 0.0011842\ttotal: 3m 12s\tremaining: 2m 15s\n",
      "586:\tlearn: 0.0011842\ttotal: 3m 12s\tremaining: 2m 15s\n",
      "587:\tlearn: 0.0011840\ttotal: 3m 12s\tremaining: 2m 15s\n",
      "588:\tlearn: 0.0011840\ttotal: 3m 13s\tremaining: 2m 14s\n",
      "589:\tlearn: 0.0011839\ttotal: 3m 13s\tremaining: 2m 14s\n",
      "590:\tlearn: 0.0011839\ttotal: 3m 13s\tremaining: 2m 14s\n",
      "591:\tlearn: 0.0011838\ttotal: 3m 14s\tremaining: 2m 13s\n",
      "592:\tlearn: 0.0011813\ttotal: 3m 14s\tremaining: 2m 13s\n",
      "593:\tlearn: 0.0011796\ttotal: 3m 14s\tremaining: 2m 13s\n",
      "594:\tlearn: 0.0011772\ttotal: 3m 15s\tremaining: 2m 12s\n",
      "595:\tlearn: 0.0011771\ttotal: 3m 15s\tremaining: 2m 12s\n",
      "596:\tlearn: 0.0011770\ttotal: 3m 15s\tremaining: 2m 12s\n",
      "597:\tlearn: 0.0011767\ttotal: 3m 16s\tremaining: 2m 11s\n",
      "598:\tlearn: 0.0011767\ttotal: 3m 16s\tremaining: 2m 11s\n",
      "599:\tlearn: 0.0011764\ttotal: 3m 16s\tremaining: 2m 11s\n",
      "600:\tlearn: 0.0011764\ttotal: 3m 17s\tremaining: 2m 10s\n",
      "601:\tlearn: 0.0011764\ttotal: 3m 17s\tremaining: 2m 10s\n",
      "602:\tlearn: 0.0011764\ttotal: 3m 17s\tremaining: 2m 10s\n",
      "603:\tlearn: 0.0011764\ttotal: 3m 18s\tremaining: 2m 9s\n",
      "604:\tlearn: 0.0011763\ttotal: 3m 18s\tremaining: 2m 9s\n",
      "605:\tlearn: 0.0011763\ttotal: 3m 18s\tremaining: 2m 9s\n",
      "606:\tlearn: 0.0011760\ttotal: 3m 19s\tremaining: 2m 8s\n",
      "607:\tlearn: 0.0011760\ttotal: 3m 19s\tremaining: 2m 8s\n",
      "608:\tlearn: 0.0011759\ttotal: 3m 19s\tremaining: 2m 8s\n",
      "609:\tlearn: 0.0011759\ttotal: 3m 20s\tremaining: 2m 7s\n",
      "610:\tlearn: 0.0011756\ttotal: 3m 20s\tremaining: 2m 7s\n",
      "611:\tlearn: 0.0011756\ttotal: 3m 20s\tremaining: 2m 7s\n",
      "612:\tlearn: 0.0011755\ttotal: 3m 21s\tremaining: 2m 6s\n",
      "613:\tlearn: 0.0011740\ttotal: 3m 21s\tremaining: 2m 6s\n",
      "614:\tlearn: 0.0011739\ttotal: 3m 21s\tremaining: 2m 6s\n",
      "615:\tlearn: 0.0011738\ttotal: 3m 22s\tremaining: 2m 5s\n",
      "616:\tlearn: 0.0011737\ttotal: 3m 22s\tremaining: 2m 5s\n",
      "617:\tlearn: 0.0011723\ttotal: 3m 22s\tremaining: 2m 5s\n",
      "618:\tlearn: 0.0011723\ttotal: 3m 23s\tremaining: 2m 4s\n",
      "619:\tlearn: 0.0011722\ttotal: 3m 23s\tremaining: 2m 4s\n",
      "620:\tlearn: 0.0011720\ttotal: 3m 23s\tremaining: 2m 4s\n",
      "621:\tlearn: 0.0011720\ttotal: 3m 24s\tremaining: 2m 3s\n",
      "622:\tlearn: 0.0011719\ttotal: 3m 24s\tremaining: 2m 3s\n",
      "623:\tlearn: 0.0011718\ttotal: 3m 24s\tremaining: 2m 3s\n",
      "624:\tlearn: 0.0011716\ttotal: 3m 25s\tremaining: 2m 3s\n",
      "625:\tlearn: 0.0011716\ttotal: 3m 25s\tremaining: 2m 2s\n",
      "626:\tlearn: 0.0011716\ttotal: 3m 25s\tremaining: 2m 2s\n",
      "627:\tlearn: 0.0011716\ttotal: 3m 26s\tremaining: 2m 2s\n",
      "628:\tlearn: 0.0011716\ttotal: 3m 26s\tremaining: 2m 1s\n",
      "629:\tlearn: 0.0011716\ttotal: 3m 26s\tremaining: 2m 1s\n",
      "630:\tlearn: 0.0011679\ttotal: 3m 27s\tremaining: 2m 1s\n",
      "631:\tlearn: 0.0011679\ttotal: 3m 27s\tremaining: 2m\n",
      "632:\tlearn: 0.0011679\ttotal: 3m 27s\tremaining: 2m\n",
      "633:\tlearn: 0.0011678\ttotal: 3m 27s\tremaining: 2m\n",
      "634:\tlearn: 0.0011624\ttotal: 3m 28s\tremaining: 1m 59s\n",
      "635:\tlearn: 0.0011622\ttotal: 3m 28s\tremaining: 1m 59s\n",
      "636:\tlearn: 0.0011621\ttotal: 3m 28s\tremaining: 1m 59s\n",
      "637:\tlearn: 0.0011592\ttotal: 3m 29s\tremaining: 1m 58s\n",
      "638:\tlearn: 0.0011592\ttotal: 3m 29s\tremaining: 1m 58s\n",
      "639:\tlearn: 0.0011592\ttotal: 3m 29s\tremaining: 1m 58s\n",
      "640:\tlearn: 0.0011592\ttotal: 3m 30s\tremaining: 1m 57s\n",
      "641:\tlearn: 0.0011590\ttotal: 3m 30s\tremaining: 1m 57s\n",
      "642:\tlearn: 0.0011536\ttotal: 3m 30s\tremaining: 1m 57s\n",
      "643:\tlearn: 0.0011536\ttotal: 3m 31s\tremaining: 1m 56s\n",
      "644:\tlearn: 0.0011536\ttotal: 3m 31s\tremaining: 1m 56s\n",
      "645:\tlearn: 0.0011534\ttotal: 3m 31s\tremaining: 1m 56s\n",
      "646:\tlearn: 0.0011534\ttotal: 3m 32s\tremaining: 1m 55s\n",
      "647:\tlearn: 0.0011534\ttotal: 3m 32s\tremaining: 1m 55s\n",
      "648:\tlearn: 0.0011534\ttotal: 3m 32s\tremaining: 1m 55s\n",
      "649:\tlearn: 0.0011534\ttotal: 3m 33s\tremaining: 1m 54s\n",
      "650:\tlearn: 0.0011534\ttotal: 3m 33s\tremaining: 1m 54s\n",
      "651:\tlearn: 0.0011534\ttotal: 3m 33s\tremaining: 1m 54s\n",
      "652:\tlearn: 0.0011533\ttotal: 3m 34s\tremaining: 1m 53s\n",
      "653:\tlearn: 0.0011532\ttotal: 3m 34s\tremaining: 1m 53s\n",
      "654:\tlearn: 0.0011494\ttotal: 3m 34s\tremaining: 1m 53s\n",
      "655:\tlearn: 0.0011494\ttotal: 3m 35s\tremaining: 1m 52s\n",
      "656:\tlearn: 0.0011493\ttotal: 3m 35s\tremaining: 1m 52s\n",
      "657:\tlearn: 0.0011493\ttotal: 3m 35s\tremaining: 1m 52s\n",
      "658:\tlearn: 0.0011492\ttotal: 3m 36s\tremaining: 1m 51s\n",
      "659:\tlearn: 0.0011491\ttotal: 3m 36s\tremaining: 1m 51s\n",
      "660:\tlearn: 0.0011487\ttotal: 3m 36s\tremaining: 1m 51s\n",
      "661:\tlearn: 0.0011487\ttotal: 3m 37s\tremaining: 1m 50s\n",
      "662:\tlearn: 0.0011487\ttotal: 3m 37s\tremaining: 1m 50s\n",
      "663:\tlearn: 0.0011486\ttotal: 3m 37s\tremaining: 1m 50s\n",
      "664:\tlearn: 0.0011485\ttotal: 3m 38s\tremaining: 1m 49s\n",
      "665:\tlearn: 0.0011484\ttotal: 3m 38s\tremaining: 1m 49s\n",
      "666:\tlearn: 0.0011484\ttotal: 3m 38s\tremaining: 1m 49s\n",
      "667:\tlearn: 0.0011484\ttotal: 3m 39s\tremaining: 1m 48s\n",
      "668:\tlearn: 0.0011484\ttotal: 3m 39s\tremaining: 1m 48s\n",
      "669:\tlearn: 0.0011483\ttotal: 3m 39s\tremaining: 1m 48s\n",
      "670:\tlearn: 0.0011482\ttotal: 3m 40s\tremaining: 1m 47s\n",
      "671:\tlearn: 0.0011482\ttotal: 3m 40s\tremaining: 1m 47s\n",
      "672:\tlearn: 0.0011480\ttotal: 3m 40s\tremaining: 1m 47s\n",
      "673:\tlearn: 0.0011480\ttotal: 3m 41s\tremaining: 1m 46s\n",
      "674:\tlearn: 0.0011480\ttotal: 3m 41s\tremaining: 1m 46s\n",
      "675:\tlearn: 0.0011480\ttotal: 3m 41s\tremaining: 1m 46s\n",
      "676:\tlearn: 0.0011479\ttotal: 3m 41s\tremaining: 1m 45s\n",
      "677:\tlearn: 0.0011477\ttotal: 3m 42s\tremaining: 1m 45s\n",
      "678:\tlearn: 0.0011476\ttotal: 3m 42s\tremaining: 1m 45s\n",
      "679:\tlearn: 0.0011416\ttotal: 3m 42s\tremaining: 1m 44s\n",
      "680:\tlearn: 0.0011416\ttotal: 3m 43s\tremaining: 1m 44s\n",
      "681:\tlearn: 0.0011416\ttotal: 3m 43s\tremaining: 1m 44s\n",
      "682:\tlearn: 0.0011416\ttotal: 3m 43s\tremaining: 1m 43s\n",
      "683:\tlearn: 0.0011415\ttotal: 3m 44s\tremaining: 1m 43s\n",
      "684:\tlearn: 0.0011399\ttotal: 3m 44s\tremaining: 1m 43s\n",
      "685:\tlearn: 0.0011399\ttotal: 3m 44s\tremaining: 1m 42s\n",
      "686:\tlearn: 0.0011399\ttotal: 3m 45s\tremaining: 1m 42s\n",
      "687:\tlearn: 0.0011397\ttotal: 3m 45s\tremaining: 1m 42s\n",
      "688:\tlearn: 0.0011397\ttotal: 3m 45s\tremaining: 1m 41s\n",
      "689:\tlearn: 0.0011396\ttotal: 3m 46s\tremaining: 1m 41s\n",
      "690:\tlearn: 0.0011396\ttotal: 3m 46s\tremaining: 1m 41s\n",
      "691:\tlearn: 0.0011395\ttotal: 3m 46s\tremaining: 1m 40s\n",
      "692:\tlearn: 0.0011394\ttotal: 3m 47s\tremaining: 1m 40s\n",
      "693:\tlearn: 0.0011393\ttotal: 3m 47s\tremaining: 1m 40s\n",
      "694:\tlearn: 0.0011393\ttotal: 3m 47s\tremaining: 1m 40s\n",
      "695:\tlearn: 0.0011393\ttotal: 3m 48s\tremaining: 1m 39s\n",
      "696:\tlearn: 0.0011392\ttotal: 3m 48s\tremaining: 1m 39s\n",
      "697:\tlearn: 0.0011392\ttotal: 3m 48s\tremaining: 1m 39s\n",
      "698:\tlearn: 0.0011391\ttotal: 3m 49s\tremaining: 1m 38s\n",
      "699:\tlearn: 0.0011391\ttotal: 3m 49s\tremaining: 1m 38s\n",
      "700:\tlearn: 0.0011391\ttotal: 3m 49s\tremaining: 1m 38s\n",
      "701:\tlearn: 0.0011391\ttotal: 3m 50s\tremaining: 1m 37s\n",
      "702:\tlearn: 0.0011385\ttotal: 3m 50s\tremaining: 1m 37s\n",
      "703:\tlearn: 0.0011385\ttotal: 3m 50s\tremaining: 1m 37s\n",
      "704:\tlearn: 0.0011383\ttotal: 3m 51s\tremaining: 1m 36s\n",
      "705:\tlearn: 0.0011383\ttotal: 3m 51s\tremaining: 1m 36s\n",
      "706:\tlearn: 0.0011383\ttotal: 3m 51s\tremaining: 1m 36s\n",
      "707:\tlearn: 0.0011383\ttotal: 3m 52s\tremaining: 1m 35s\n",
      "708:\tlearn: 0.0011383\ttotal: 3m 52s\tremaining: 1m 35s\n",
      "709:\tlearn: 0.0011382\ttotal: 3m 52s\tremaining: 1m 35s\n",
      "710:\tlearn: 0.0011382\ttotal: 3m 53s\tremaining: 1m 34s\n",
      "711:\tlearn: 0.0011382\ttotal: 3m 53s\tremaining: 1m 34s\n",
      "712:\tlearn: 0.0011382\ttotal: 3m 53s\tremaining: 1m 34s\n",
      "713:\tlearn: 0.0011359\ttotal: 3m 54s\tremaining: 1m 33s\n",
      "714:\tlearn: 0.0011326\ttotal: 3m 54s\tremaining: 1m 33s\n",
      "715:\tlearn: 0.0011326\ttotal: 3m 54s\tremaining: 1m 33s\n",
      "716:\tlearn: 0.0011325\ttotal: 3m 55s\tremaining: 1m 32s\n",
      "717:\tlearn: 0.0011325\ttotal: 3m 55s\tremaining: 1m 32s\n",
      "718:\tlearn: 0.0011325\ttotal: 3m 55s\tremaining: 1m 32s\n",
      "719:\tlearn: 0.0011324\ttotal: 3m 56s\tremaining: 1m 31s\n",
      "720:\tlearn: 0.0011324\ttotal: 3m 56s\tremaining: 1m 31s\n",
      "721:\tlearn: 0.0011304\ttotal: 3m 56s\tremaining: 1m 31s\n",
      "722:\tlearn: 0.0011268\ttotal: 3m 57s\tremaining: 1m 30s\n",
      "723:\tlearn: 0.0011268\ttotal: 3m 57s\tremaining: 1m 30s\n",
      "724:\tlearn: 0.0011268\ttotal: 3m 57s\tremaining: 1m 30s\n",
      "725:\tlearn: 0.0011268\ttotal: 3m 57s\tremaining: 1m 29s\n",
      "726:\tlearn: 0.0011268\ttotal: 3m 58s\tremaining: 1m 29s\n",
      "727:\tlearn: 0.0011246\ttotal: 3m 58s\tremaining: 1m 29s\n",
      "728:\tlearn: 0.0011245\ttotal: 3m 58s\tremaining: 1m 28s\n",
      "729:\tlearn: 0.0011245\ttotal: 3m 59s\tremaining: 1m 28s\n",
      "730:\tlearn: 0.0011244\ttotal: 3m 59s\tremaining: 1m 28s\n",
      "731:\tlearn: 0.0011243\ttotal: 3m 59s\tremaining: 1m 27s\n",
      "732:\tlearn: 0.0011239\ttotal: 4m\tremaining: 1m 27s\n",
      "733:\tlearn: 0.0011237\ttotal: 4m\tremaining: 1m 27s\n",
      "734:\tlearn: 0.0011236\ttotal: 4m\tremaining: 1m 26s\n",
      "735:\tlearn: 0.0011214\ttotal: 4m 1s\tremaining: 1m 26s\n",
      "736:\tlearn: 0.0011214\ttotal: 4m 1s\tremaining: 1m 26s\n",
      "737:\tlearn: 0.0011214\ttotal: 4m 1s\tremaining: 1m 25s\n",
      "738:\tlearn: 0.0011201\ttotal: 4m 2s\tremaining: 1m 25s\n",
      "739:\tlearn: 0.0011178\ttotal: 4m 2s\tremaining: 1m 25s\n",
      "740:\tlearn: 0.0011177\ttotal: 4m 2s\tremaining: 1m 24s\n",
      "741:\tlearn: 0.0011176\ttotal: 4m 3s\tremaining: 1m 24s\n",
      "742:\tlearn: 0.0011176\ttotal: 4m 3s\tremaining: 1m 24s\n",
      "743:\tlearn: 0.0011150\ttotal: 4m 3s\tremaining: 1m 23s\n",
      "744:\tlearn: 0.0011150\ttotal: 4m 4s\tremaining: 1m 23s\n",
      "745:\tlearn: 0.0011150\ttotal: 4m 4s\tremaining: 1m 23s\n",
      "746:\tlearn: 0.0011150\ttotal: 4m 4s\tremaining: 1m 22s\n",
      "747:\tlearn: 0.0011149\ttotal: 4m 5s\tremaining: 1m 22s\n",
      "748:\tlearn: 0.0011149\ttotal: 4m 5s\tremaining: 1m 22s\n",
      "749:\tlearn: 0.0011110\ttotal: 4m 5s\tremaining: 1m 21s\n",
      "750:\tlearn: 0.0011110\ttotal: 4m 6s\tremaining: 1m 21s\n",
      "751:\tlearn: 0.0011110\ttotal: 4m 6s\tremaining: 1m 21s\n",
      "752:\tlearn: 0.0011110\ttotal: 4m 6s\tremaining: 1m 20s\n",
      "753:\tlearn: 0.0011110\ttotal: 4m 7s\tremaining: 1m 20s\n",
      "754:\tlearn: 0.0011109\ttotal: 4m 7s\tremaining: 1m 20s\n",
      "755:\tlearn: 0.0011108\ttotal: 4m 7s\tremaining: 1m 19s\n",
      "756:\tlearn: 0.0011108\ttotal: 4m 8s\tremaining: 1m 19s\n",
      "757:\tlearn: 0.0011108\ttotal: 4m 8s\tremaining: 1m 19s\n",
      "758:\tlearn: 0.0011106\ttotal: 4m 8s\tremaining: 1m 19s\n",
      "759:\tlearn: 0.0011105\ttotal: 4m 9s\tremaining: 1m 18s\n",
      "760:\tlearn: 0.0011105\ttotal: 4m 9s\tremaining: 1m 18s\n",
      "761:\tlearn: 0.0011104\ttotal: 4m 9s\tremaining: 1m 18s\n",
      "762:\tlearn: 0.0011079\ttotal: 4m 10s\tremaining: 1m 17s\n",
      "763:\tlearn: 0.0011079\ttotal: 4m 10s\tremaining: 1m 17s\n",
      "764:\tlearn: 0.0011079\ttotal: 4m 10s\tremaining: 1m 17s\n",
      "765:\tlearn: 0.0011079\ttotal: 4m 11s\tremaining: 1m 16s\n",
      "766:\tlearn: 0.0011077\ttotal: 4m 11s\tremaining: 1m 16s\n",
      "767:\tlearn: 0.0011077\ttotal: 4m 11s\tremaining: 1m 16s\n",
      "768:\tlearn: 0.0011077\ttotal: 4m 12s\tremaining: 1m 15s\n",
      "769:\tlearn: 0.0011077\ttotal: 4m 12s\tremaining: 1m 15s\n",
      "770:\tlearn: 0.0011077\ttotal: 4m 12s\tremaining: 1m 15s\n",
      "771:\tlearn: 0.0011043\ttotal: 4m 13s\tremaining: 1m 14s\n",
      "772:\tlearn: 0.0011043\ttotal: 4m 13s\tremaining: 1m 14s\n",
      "773:\tlearn: 0.0011041\ttotal: 4m 13s\tremaining: 1m 14s\n",
      "774:\tlearn: 0.0011040\ttotal: 4m 14s\tremaining: 1m 13s\n",
      "775:\tlearn: 0.0011039\ttotal: 4m 14s\tremaining: 1m 13s\n",
      "776:\tlearn: 0.0011038\ttotal: 4m 14s\tremaining: 1m 13s\n",
      "777:\tlearn: 0.0011037\ttotal: 4m 15s\tremaining: 1m 12s\n",
      "778:\tlearn: 0.0011037\ttotal: 4m 15s\tremaining: 1m 12s\n",
      "779:\tlearn: 0.0011037\ttotal: 4m 15s\tremaining: 1m 12s\n",
      "780:\tlearn: 0.0011037\ttotal: 4m 16s\tremaining: 1m 11s\n",
      "781:\tlearn: 0.0011036\ttotal: 4m 16s\tremaining: 1m 11s\n",
      "782:\tlearn: 0.0011001\ttotal: 4m 16s\tremaining: 1m 11s\n",
      "783:\tlearn: 0.0010997\ttotal: 4m 17s\tremaining: 1m 10s\n",
      "784:\tlearn: 0.0010997\ttotal: 4m 17s\tremaining: 1m 10s\n",
      "785:\tlearn: 0.0010996\ttotal: 4m 17s\tremaining: 1m 10s\n",
      "786:\tlearn: 0.0010995\ttotal: 4m 18s\tremaining: 1m 9s\n",
      "787:\tlearn: 0.0010995\ttotal: 4m 18s\tremaining: 1m 9s\n",
      "788:\tlearn: 0.0010995\ttotal: 4m 18s\tremaining: 1m 9s\n",
      "789:\tlearn: 0.0010995\ttotal: 4m 19s\tremaining: 1m 8s\n",
      "790:\tlearn: 0.0010993\ttotal: 4m 19s\tremaining: 1m 8s\n",
      "791:\tlearn: 0.0010993\ttotal: 4m 19s\tremaining: 1m 8s\n",
      "792:\tlearn: 0.0010981\ttotal: 4m 20s\tremaining: 1m 7s\n",
      "793:\tlearn: 0.0010958\ttotal: 4m 20s\tremaining: 1m 7s\n",
      "794:\tlearn: 0.0010958\ttotal: 4m 20s\tremaining: 1m 7s\n",
      "795:\tlearn: 0.0010957\ttotal: 4m 21s\tremaining: 1m 6s\n",
      "796:\tlearn: 0.0010956\ttotal: 4m 21s\tremaining: 1m 6s\n",
      "797:\tlearn: 0.0010956\ttotal: 4m 21s\tremaining: 1m 6s\n",
      "798:\tlearn: 0.0010956\ttotal: 4m 22s\tremaining: 1m 5s\n",
      "799:\tlearn: 0.0010929\ttotal: 4m 22s\tremaining: 1m 5s\n",
      "800:\tlearn: 0.0010923\ttotal: 4m 22s\tremaining: 1m 5s\n",
      "801:\tlearn: 0.0010918\ttotal: 4m 23s\tremaining: 1m 4s\n",
      "802:\tlearn: 0.0010918\ttotal: 4m 23s\tremaining: 1m 4s\n",
      "803:\tlearn: 0.0010918\ttotal: 4m 23s\tremaining: 1m 4s\n",
      "804:\tlearn: 0.0010916\ttotal: 4m 24s\tremaining: 1m 3s\n",
      "805:\tlearn: 0.0010892\ttotal: 4m 24s\tremaining: 1m 3s\n",
      "806:\tlearn: 0.0010892\ttotal: 4m 24s\tremaining: 1m 3s\n",
      "807:\tlearn: 0.0010890\ttotal: 4m 25s\tremaining: 1m 2s\n",
      "808:\tlearn: 0.0010883\ttotal: 4m 25s\tremaining: 1m 2s\n",
      "809:\tlearn: 0.0010882\ttotal: 4m 25s\tremaining: 1m 2s\n",
      "810:\tlearn: 0.0010882\ttotal: 4m 26s\tremaining: 1m 2s\n",
      "811:\tlearn: 0.0010881\ttotal: 4m 26s\tremaining: 1m 1s\n",
      "812:\tlearn: 0.0010881\ttotal: 4m 26s\tremaining: 1m 1s\n",
      "813:\tlearn: 0.0010881\ttotal: 4m 27s\tremaining: 1m 1s\n",
      "814:\tlearn: 0.0010881\ttotal: 4m 27s\tremaining: 1m\n",
      "815:\tlearn: 0.0010880\ttotal: 4m 27s\tremaining: 1m\n",
      "816:\tlearn: 0.0010880\ttotal: 4m 28s\tremaining: 1m\n",
      "817:\tlearn: 0.0010879\ttotal: 4m 28s\tremaining: 59.7s\n",
      "818:\tlearn: 0.0010878\ttotal: 4m 28s\tremaining: 59.4s\n",
      "819:\tlearn: 0.0010877\ttotal: 4m 29s\tremaining: 59.1s\n",
      "820:\tlearn: 0.0010877\ttotal: 4m 29s\tremaining: 58.7s\n",
      "821:\tlearn: 0.0010877\ttotal: 4m 29s\tremaining: 58.4s\n",
      "822:\tlearn: 0.0010877\ttotal: 4m 30s\tremaining: 58.1s\n",
      "823:\tlearn: 0.0010876\ttotal: 4m 30s\tremaining: 57.7s\n",
      "824:\tlearn: 0.0010876\ttotal: 4m 30s\tremaining: 57.4s\n",
      "825:\tlearn: 0.0010876\ttotal: 4m 30s\tremaining: 57.1s\n",
      "826:\tlearn: 0.0010874\ttotal: 4m 31s\tremaining: 56.8s\n",
      "827:\tlearn: 0.0010850\ttotal: 4m 31s\tremaining: 56.4s\n",
      "828:\tlearn: 0.0010850\ttotal: 4m 31s\tremaining: 56.1s\n",
      "829:\tlearn: 0.0010850\ttotal: 4m 32s\tremaining: 55.8s\n",
      "830:\tlearn: 0.0010849\ttotal: 4m 32s\tremaining: 55.4s\n",
      "831:\tlearn: 0.0010849\ttotal: 4m 32s\tremaining: 55.1s\n",
      "832:\tlearn: 0.0010849\ttotal: 4m 33s\tremaining: 54.8s\n",
      "833:\tlearn: 0.0010849\ttotal: 4m 33s\tremaining: 54.5s\n",
      "834:\tlearn: 0.0010849\ttotal: 4m 33s\tremaining: 54.1s\n",
      "835:\tlearn: 0.0010846\ttotal: 4m 34s\tremaining: 53.8s\n",
      "836:\tlearn: 0.0010817\ttotal: 4m 34s\tremaining: 53.5s\n",
      "837:\tlearn: 0.0010817\ttotal: 4m 34s\tremaining: 53.1s\n",
      "838:\tlearn: 0.0010815\ttotal: 4m 35s\tremaining: 52.8s\n",
      "839:\tlearn: 0.0010815\ttotal: 4m 35s\tremaining: 52.5s\n",
      "840:\tlearn: 0.0010815\ttotal: 4m 35s\tremaining: 52.2s\n",
      "841:\tlearn: 0.0010814\ttotal: 4m 36s\tremaining: 51.8s\n",
      "842:\tlearn: 0.0010812\ttotal: 4m 36s\tremaining: 51.5s\n",
      "843:\tlearn: 0.0010793\ttotal: 4m 36s\tremaining: 51.2s\n",
      "844:\tlearn: 0.0010791\ttotal: 4m 37s\tremaining: 50.8s\n",
      "845:\tlearn: 0.0010790\ttotal: 4m 37s\tremaining: 50.5s\n",
      "846:\tlearn: 0.0010787\ttotal: 4m 37s\tremaining: 50.2s\n",
      "847:\tlearn: 0.0010768\ttotal: 4m 38s\tremaining: 49.9s\n",
      "848:\tlearn: 0.0010767\ttotal: 4m 38s\tremaining: 49.5s\n",
      "849:\tlearn: 0.0010766\ttotal: 4m 38s\tremaining: 49.2s\n",
      "850:\tlearn: 0.0010761\ttotal: 4m 39s\tremaining: 48.9s\n",
      "851:\tlearn: 0.0010756\ttotal: 4m 39s\tremaining: 48.5s\n",
      "852:\tlearn: 0.0010756\ttotal: 4m 39s\tremaining: 48.2s\n",
      "853:\tlearn: 0.0010755\ttotal: 4m 40s\tremaining: 47.9s\n",
      "854:\tlearn: 0.0010755\ttotal: 4m 40s\tremaining: 47.6s\n",
      "855:\tlearn: 0.0010754\ttotal: 4m 40s\tremaining: 47.2s\n",
      "856:\tlearn: 0.0010752\ttotal: 4m 41s\tremaining: 46.9s\n",
      "857:\tlearn: 0.0010752\ttotal: 4m 41s\tremaining: 46.6s\n",
      "858:\tlearn: 0.0010752\ttotal: 4m 41s\tremaining: 46.2s\n",
      "859:\tlearn: 0.0010751\ttotal: 4m 42s\tremaining: 45.9s\n",
      "860:\tlearn: 0.0010750\ttotal: 4m 42s\tremaining: 45.6s\n",
      "861:\tlearn: 0.0010750\ttotal: 4m 42s\tremaining: 45.3s\n",
      "862:\tlearn: 0.0010749\ttotal: 4m 43s\tremaining: 44.9s\n",
      "863:\tlearn: 0.0010745\ttotal: 4m 43s\tremaining: 44.6s\n",
      "864:\tlearn: 0.0010744\ttotal: 4m 43s\tremaining: 44.3s\n",
      "865:\tlearn: 0.0010743\ttotal: 4m 43s\tremaining: 43.9s\n",
      "866:\tlearn: 0.0010742\ttotal: 4m 44s\tremaining: 43.6s\n",
      "867:\tlearn: 0.0010741\ttotal: 4m 44s\tremaining: 43.3s\n",
      "868:\tlearn: 0.0010741\ttotal: 4m 44s\tremaining: 43s\n",
      "869:\tlearn: 0.0010741\ttotal: 4m 45s\tremaining: 42.6s\n",
      "870:\tlearn: 0.0010708\ttotal: 4m 45s\tremaining: 42.3s\n",
      "871:\tlearn: 0.0010708\ttotal: 4m 45s\tremaining: 42s\n",
      "872:\tlearn: 0.0010708\ttotal: 4m 46s\tremaining: 41.6s\n",
      "873:\tlearn: 0.0010708\ttotal: 4m 46s\tremaining: 41.3s\n",
      "874:\tlearn: 0.0010708\ttotal: 4m 46s\tremaining: 41s\n",
      "875:\tlearn: 0.0010707\ttotal: 4m 47s\tremaining: 40.7s\n",
      "876:\tlearn: 0.0010707\ttotal: 4m 47s\tremaining: 40.3s\n",
      "877:\tlearn: 0.0010706\ttotal: 4m 47s\tremaining: 40s\n",
      "878:\tlearn: 0.0010706\ttotal: 4m 48s\tremaining: 39.7s\n",
      "879:\tlearn: 0.0010706\ttotal: 4m 48s\tremaining: 39.3s\n",
      "880:\tlearn: 0.0010705\ttotal: 4m 48s\tremaining: 39s\n",
      "881:\tlearn: 0.0010704\ttotal: 4m 49s\tremaining: 38.7s\n",
      "882:\tlearn: 0.0010704\ttotal: 4m 49s\tremaining: 38.4s\n",
      "883:\tlearn: 0.0010702\ttotal: 4m 49s\tremaining: 38s\n",
      "884:\tlearn: 0.0010702\ttotal: 4m 50s\tremaining: 37.7s\n",
      "885:\tlearn: 0.0010702\ttotal: 4m 50s\tremaining: 37.4s\n",
      "886:\tlearn: 0.0010702\ttotal: 4m 50s\tremaining: 37.1s\n",
      "887:\tlearn: 0.0010702\ttotal: 4m 51s\tremaining: 36.7s\n",
      "888:\tlearn: 0.0010702\ttotal: 4m 51s\tremaining: 36.4s\n",
      "889:\tlearn: 0.0010693\ttotal: 4m 51s\tremaining: 36.1s\n",
      "890:\tlearn: 0.0010692\ttotal: 4m 52s\tremaining: 35.7s\n",
      "891:\tlearn: 0.0010692\ttotal: 4m 52s\tremaining: 35.4s\n",
      "892:\tlearn: 0.0010692\ttotal: 4m 52s\tremaining: 35.1s\n",
      "893:\tlearn: 0.0010691\ttotal: 4m 53s\tremaining: 34.8s\n",
      "894:\tlearn: 0.0010691\ttotal: 4m 53s\tremaining: 34.4s\n",
      "895:\tlearn: 0.0010689\ttotal: 4m 53s\tremaining: 34.1s\n",
      "896:\tlearn: 0.0010689\ttotal: 4m 54s\tremaining: 33.8s\n",
      "897:\tlearn: 0.0010689\ttotal: 4m 54s\tremaining: 33.4s\n",
      "898:\tlearn: 0.0010689\ttotal: 4m 54s\tremaining: 33.1s\n",
      "899:\tlearn: 0.0010689\ttotal: 4m 55s\tremaining: 32.8s\n",
      "900:\tlearn: 0.0010689\ttotal: 4m 55s\tremaining: 32.5s\n",
      "901:\tlearn: 0.0010689\ttotal: 4m 55s\tremaining: 32.1s\n",
      "902:\tlearn: 0.0010689\ttotal: 4m 56s\tremaining: 31.8s\n",
      "903:\tlearn: 0.0010689\ttotal: 4m 56s\tremaining: 31.5s\n",
      "904:\tlearn: 0.0010688\ttotal: 4m 56s\tremaining: 31.1s\n",
      "905:\tlearn: 0.0010687\ttotal: 4m 57s\tremaining: 30.8s\n",
      "906:\tlearn: 0.0010687\ttotal: 4m 57s\tremaining: 30.5s\n",
      "907:\tlearn: 0.0010686\ttotal: 4m 57s\tremaining: 30.2s\n",
      "908:\tlearn: 0.0010686\ttotal: 4m 58s\tremaining: 29.8s\n",
      "909:\tlearn: 0.0010686\ttotal: 4m 58s\tremaining: 29.5s\n",
      "910:\tlearn: 0.0010686\ttotal: 4m 58s\tremaining: 29.2s\n",
      "911:\tlearn: 0.0010685\ttotal: 4m 59s\tremaining: 28.9s\n",
      "912:\tlearn: 0.0010677\ttotal: 4m 59s\tremaining: 28.5s\n",
      "913:\tlearn: 0.0010652\ttotal: 4m 59s\tremaining: 28.2s\n",
      "914:\tlearn: 0.0010651\ttotal: 4m 59s\tremaining: 27.9s\n",
      "915:\tlearn: 0.0010650\ttotal: 5m\tremaining: 27.5s\n",
      "916:\tlearn: 0.0010650\ttotal: 5m\tremaining: 27.2s\n",
      "917:\tlearn: 0.0010648\ttotal: 5m\tremaining: 26.9s\n",
      "918:\tlearn: 0.0010647\ttotal: 5m 1s\tremaining: 26.6s\n",
      "919:\tlearn: 0.0010647\ttotal: 5m 1s\tremaining: 26.2s\n",
      "920:\tlearn: 0.0010643\ttotal: 5m 1s\tremaining: 25.9s\n",
      "921:\tlearn: 0.0010643\ttotal: 5m 2s\tremaining: 25.6s\n",
      "922:\tlearn: 0.0010643\ttotal: 5m 2s\tremaining: 25.2s\n",
      "923:\tlearn: 0.0010639\ttotal: 5m 2s\tremaining: 24.9s\n",
      "924:\tlearn: 0.0010639\ttotal: 5m 3s\tremaining: 24.6s\n",
      "925:\tlearn: 0.0010639\ttotal: 5m 3s\tremaining: 24.3s\n",
      "926:\tlearn: 0.0010638\ttotal: 5m 3s\tremaining: 23.9s\n",
      "927:\tlearn: 0.0010636\ttotal: 5m 4s\tremaining: 23.6s\n",
      "928:\tlearn: 0.0010636\ttotal: 5m 4s\tremaining: 23.3s\n",
      "929:\tlearn: 0.0010636\ttotal: 5m 4s\tremaining: 22.9s\n",
      "930:\tlearn: 0.0010636\ttotal: 5m 5s\tremaining: 22.6s\n",
      "931:\tlearn: 0.0010616\ttotal: 5m 5s\tremaining: 22.3s\n",
      "932:\tlearn: 0.0010616\ttotal: 5m 5s\tremaining: 22s\n",
      "933:\tlearn: 0.0010616\ttotal: 5m 6s\tremaining: 21.6s\n",
      "934:\tlearn: 0.0010616\ttotal: 5m 6s\tremaining: 21.3s\n",
      "935:\tlearn: 0.0010616\ttotal: 5m 6s\tremaining: 21s\n",
      "936:\tlearn: 0.0010616\ttotal: 5m 7s\tremaining: 20.7s\n",
      "937:\tlearn: 0.0010614\ttotal: 5m 7s\tremaining: 20.3s\n",
      "938:\tlearn: 0.0010614\ttotal: 5m 7s\tremaining: 20s\n",
      "939:\tlearn: 0.0010614\ttotal: 5m 8s\tremaining: 19.7s\n",
      "940:\tlearn: 0.0010613\ttotal: 5m 8s\tremaining: 19.3s\n",
      "941:\tlearn: 0.0010613\ttotal: 5m 8s\tremaining: 19s\n",
      "942:\tlearn: 0.0010611\ttotal: 5m 9s\tremaining: 18.7s\n",
      "943:\tlearn: 0.0010610\ttotal: 5m 9s\tremaining: 18.4s\n",
      "944:\tlearn: 0.0010609\ttotal: 5m 9s\tremaining: 18s\n",
      "945:\tlearn: 0.0010608\ttotal: 5m 10s\tremaining: 17.7s\n",
      "946:\tlearn: 0.0010606\ttotal: 5m 10s\tremaining: 17.4s\n",
      "947:\tlearn: 0.0010604\ttotal: 5m 10s\tremaining: 17s\n",
      "948:\tlearn: 0.0010604\ttotal: 5m 11s\tremaining: 16.7s\n",
      "949:\tlearn: 0.0010604\ttotal: 5m 11s\tremaining: 16.4s\n",
      "950:\tlearn: 0.0010604\ttotal: 5m 11s\tremaining: 16.1s\n",
      "951:\tlearn: 0.0010596\ttotal: 5m 12s\tremaining: 15.7s\n",
      "952:\tlearn: 0.0010596\ttotal: 5m 12s\tremaining: 15.4s\n",
      "953:\tlearn: 0.0010595\ttotal: 5m 12s\tremaining: 15.1s\n",
      "954:\tlearn: 0.0010594\ttotal: 5m 13s\tremaining: 14.7s\n",
      "955:\tlearn: 0.0010594\ttotal: 5m 13s\tremaining: 14.4s\n",
      "956:\tlearn: 0.0010593\ttotal: 5m 13s\tremaining: 14.1s\n",
      "957:\tlearn: 0.0010569\ttotal: 5m 14s\tremaining: 13.8s\n",
      "958:\tlearn: 0.0010568\ttotal: 5m 14s\tremaining: 13.4s\n",
      "959:\tlearn: 0.0010553\ttotal: 5m 14s\tremaining: 13.1s\n",
      "960:\tlearn: 0.0010553\ttotal: 5m 14s\tremaining: 12.8s\n",
      "961:\tlearn: 0.0010553\ttotal: 5m 15s\tremaining: 12.5s\n",
      "962:\tlearn: 0.0010553\ttotal: 5m 15s\tremaining: 12.1s\n",
      "963:\tlearn: 0.0010553\ttotal: 5m 15s\tremaining: 11.8s\n",
      "964:\tlearn: 0.0010552\ttotal: 5m 16s\tremaining: 11.5s\n",
      "965:\tlearn: 0.0010552\ttotal: 5m 16s\tremaining: 11.1s\n",
      "966:\tlearn: 0.0010552\ttotal: 5m 16s\tremaining: 10.8s\n",
      "967:\tlearn: 0.0010552\ttotal: 5m 17s\tremaining: 10.5s\n",
      "968:\tlearn: 0.0010544\ttotal: 5m 17s\tremaining: 10.2s\n",
      "969:\tlearn: 0.0010543\ttotal: 5m 17s\tremaining: 9.83s\n",
      "970:\tlearn: 0.0010542\ttotal: 5m 18s\tremaining: 9.5s\n",
      "971:\tlearn: 0.0010541\ttotal: 5m 18s\tremaining: 9.18s\n",
      "972:\tlearn: 0.0010531\ttotal: 5m 18s\tremaining: 8.85s\n",
      "973:\tlearn: 0.0010531\ttotal: 5m 19s\tremaining: 8.52s\n",
      "974:\tlearn: 0.0010531\ttotal: 5m 19s\tremaining: 8.19s\n",
      "975:\tlearn: 0.0010522\ttotal: 5m 19s\tremaining: 7.87s\n",
      "976:\tlearn: 0.0010522\ttotal: 5m 20s\tremaining: 7.54s\n",
      "977:\tlearn: 0.0010522\ttotal: 5m 20s\tremaining: 7.21s\n",
      "978:\tlearn: 0.0010522\ttotal: 5m 20s\tremaining: 6.88s\n",
      "979:\tlearn: 0.0010521\ttotal: 5m 21s\tremaining: 6.55s\n",
      "980:\tlearn: 0.0010521\ttotal: 5m 21s\tremaining: 6.23s\n",
      "981:\tlearn: 0.0010520\ttotal: 5m 21s\tremaining: 5.9s\n",
      "982:\tlearn: 0.0010520\ttotal: 5m 22s\tremaining: 5.57s\n",
      "983:\tlearn: 0.0010520\ttotal: 5m 22s\tremaining: 5.24s\n",
      "984:\tlearn: 0.0010520\ttotal: 5m 22s\tremaining: 4.92s\n",
      "985:\tlearn: 0.0010520\ttotal: 5m 23s\tremaining: 4.59s\n",
      "986:\tlearn: 0.0010520\ttotal: 5m 23s\tremaining: 4.26s\n",
      "987:\tlearn: 0.0010520\ttotal: 5m 23s\tremaining: 3.93s\n",
      "988:\tlearn: 0.0010502\ttotal: 5m 24s\tremaining: 3.6s\n",
      "989:\tlearn: 0.0010499\ttotal: 5m 24s\tremaining: 3.28s\n",
      "990:\tlearn: 0.0010476\ttotal: 5m 24s\tremaining: 2.95s\n",
      "991:\tlearn: 0.0010476\ttotal: 5m 25s\tremaining: 2.62s\n",
      "992:\tlearn: 0.0010475\ttotal: 5m 25s\tremaining: 2.29s\n",
      "993:\tlearn: 0.0010475\ttotal: 5m 25s\tremaining: 1.97s\n",
      "994:\tlearn: 0.0010475\ttotal: 5m 26s\tremaining: 1.64s\n",
      "995:\tlearn: 0.0010475\ttotal: 5m 26s\tremaining: 1.31s\n",
      "996:\tlearn: 0.0010455\ttotal: 5m 26s\tremaining: 983ms\n",
      "997:\tlearn: 0.0010455\ttotal: 5m 27s\tremaining: 655ms\n",
      "998:\tlearn: 0.0010454\ttotal: 5m 27s\tremaining: 328ms\n",
      "999:\tlearn: 0.0010454\ttotal: 5m 27s\tremaining: 0us\n",
      "UPDRS: updrs_2\n",
      "Hyperparameters: {'learning_rate': 0.954836104194404, 'depth': 6, 'l2_leaf_reg': 5.757077697096695, 'bagging_temperature': 4.095757640680914, 'min_data_in_leaf': 8}\n",
      "\n",
      "\n",
      "0:\tlearn: 0.5457237\ttotal: 191ms\tremaining: 3m 10s\n",
      "1:\tlearn: 0.4334153\ttotal: 367ms\tremaining: 3m 3s\n",
      "2:\tlearn: 0.3450588\ttotal: 539ms\tremaining: 2m 59s\n",
      "3:\tlearn: 0.2997921\ttotal: 712ms\tremaining: 2m 57s\n",
      "4:\tlearn: 0.2090938\ttotal: 886ms\tremaining: 2m 56s\n",
      "5:\tlearn: 0.1897071\ttotal: 1.06s\tremaining: 2m 55s\n",
      "6:\tlearn: 0.1698073\ttotal: 1.24s\tremaining: 2m 55s\n",
      "7:\tlearn: 0.1254085\ttotal: 1.41s\tremaining: 2m 54s\n",
      "8:\tlearn: 0.0896322\ttotal: 1.58s\tremaining: 2m 54s\n",
      "9:\tlearn: 0.0731698\ttotal: 1.76s\tremaining: 2m 54s\n",
      "10:\tlearn: 0.0573534\ttotal: 1.93s\tremaining: 2m 53s\n",
      "11:\tlearn: 0.0469114\ttotal: 2.11s\tremaining: 2m 53s\n",
      "12:\tlearn: 0.0409710\ttotal: 2.29s\tremaining: 2m 53s\n",
      "13:\tlearn: 0.0368397\ttotal: 2.46s\tremaining: 2m 53s\n",
      "14:\tlearn: 0.0300840\ttotal: 2.64s\tremaining: 2m 53s\n",
      "15:\tlearn: 0.0252629\ttotal: 2.82s\tremaining: 2m 53s\n",
      "16:\tlearn: 0.0225146\ttotal: 2.99s\tremaining: 2m 53s\n",
      "17:\tlearn: 0.0199794\ttotal: 3.17s\tremaining: 2m 52s\n",
      "18:\tlearn: 0.0181759\ttotal: 3.35s\tremaining: 2m 52s\n",
      "19:\tlearn: 0.0159239\ttotal: 3.52s\tremaining: 2m 52s\n",
      "20:\tlearn: 0.0144436\ttotal: 3.7s\tremaining: 2m 52s\n",
      "21:\tlearn: 0.0129082\ttotal: 3.87s\tremaining: 2m 52s\n",
      "22:\tlearn: 0.0124248\ttotal: 4.05s\tremaining: 2m 52s\n",
      "23:\tlearn: 0.0113432\ttotal: 4.23s\tremaining: 2m 51s\n",
      "24:\tlearn: 0.0111122\ttotal: 4.4s\tremaining: 2m 51s\n",
      "25:\tlearn: 0.0102733\ttotal: 4.58s\tremaining: 2m 51s\n",
      "26:\tlearn: 0.0094922\ttotal: 4.75s\tremaining: 2m 51s\n",
      "27:\tlearn: 0.0092024\ttotal: 4.93s\tremaining: 2m 51s\n",
      "28:\tlearn: 0.0085815\ttotal: 5.11s\tremaining: 2m 51s\n",
      "29:\tlearn: 0.0081508\ttotal: 5.3s\tremaining: 2m 51s\n",
      "30:\tlearn: 0.0075704\ttotal: 5.47s\tremaining: 2m 51s\n",
      "31:\tlearn: 0.0069806\ttotal: 5.65s\tremaining: 2m 50s\n",
      "32:\tlearn: 0.0066704\ttotal: 5.83s\tremaining: 2m 50s\n",
      "33:\tlearn: 0.0064049\ttotal: 6s\tremaining: 2m 50s\n",
      "34:\tlearn: 0.0061237\ttotal: 6.18s\tremaining: 2m 50s\n",
      "35:\tlearn: 0.0060603\ttotal: 6.35s\tremaining: 2m 50s\n",
      "36:\tlearn: 0.0057682\ttotal: 6.52s\tremaining: 2m 49s\n",
      "37:\tlearn: 0.0054223\ttotal: 6.7s\tremaining: 2m 49s\n",
      "38:\tlearn: 0.0051581\ttotal: 6.87s\tremaining: 2m 49s\n",
      "39:\tlearn: 0.0050178\ttotal: 7.04s\tremaining: 2m 49s\n",
      "40:\tlearn: 0.0047694\ttotal: 7.21s\tremaining: 2m 48s\n",
      "41:\tlearn: 0.0045811\ttotal: 7.39s\tremaining: 2m 48s\n",
      "42:\tlearn: 0.0044530\ttotal: 7.56s\tremaining: 2m 48s\n",
      "43:\tlearn: 0.0042853\ttotal: 7.74s\tremaining: 2m 48s\n",
      "44:\tlearn: 0.0041724\ttotal: 7.91s\tremaining: 2m 47s\n",
      "45:\tlearn: 0.0040526\ttotal: 8.09s\tremaining: 2m 47s\n",
      "46:\tlearn: 0.0039420\ttotal: 8.26s\tremaining: 2m 47s\n",
      "47:\tlearn: 0.0037973\ttotal: 8.44s\tremaining: 2m 47s\n",
      "48:\tlearn: 0.0036659\ttotal: 8.61s\tremaining: 2m 47s\n",
      "49:\tlearn: 0.0036196\ttotal: 8.79s\tremaining: 2m 47s\n",
      "50:\tlearn: 0.0034789\ttotal: 8.97s\tremaining: 2m 46s\n",
      "51:\tlearn: 0.0033607\ttotal: 9.14s\tremaining: 2m 46s\n",
      "52:\tlearn: 0.0032713\ttotal: 9.31s\tremaining: 2m 46s\n",
      "53:\tlearn: 0.0031735\ttotal: 9.49s\tremaining: 2m 46s\n",
      "54:\tlearn: 0.0030874\ttotal: 9.66s\tremaining: 2m 46s\n",
      "55:\tlearn: 0.0030008\ttotal: 9.84s\tremaining: 2m 45s\n",
      "56:\tlearn: 0.0029219\ttotal: 10s\tremaining: 2m 45s\n",
      "57:\tlearn: 0.0028561\ttotal: 10.2s\tremaining: 2m 45s\n",
      "58:\tlearn: 0.0027657\ttotal: 10.4s\tremaining: 2m 45s\n",
      "59:\tlearn: 0.0027150\ttotal: 10.5s\tremaining: 2m 45s\n",
      "60:\tlearn: 0.0026298\ttotal: 10.7s\tremaining: 2m 45s\n",
      "61:\tlearn: 0.0025432\ttotal: 10.9s\tremaining: 2m 44s\n",
      "62:\tlearn: 0.0024595\ttotal: 11.1s\tremaining: 2m 44s\n",
      "63:\tlearn: 0.0024062\ttotal: 11.2s\tremaining: 2m 44s\n",
      "64:\tlearn: 0.0023562\ttotal: 11.4s\tremaining: 2m 44s\n",
      "65:\tlearn: 0.0023098\ttotal: 11.6s\tremaining: 2m 44s\n",
      "66:\tlearn: 0.0022610\ttotal: 11.8s\tremaining: 2m 43s\n",
      "67:\tlearn: 0.0022025\ttotal: 11.9s\tremaining: 2m 43s\n",
      "68:\tlearn: 0.0021364\ttotal: 12.1s\tremaining: 2m 43s\n",
      "69:\tlearn: 0.0020873\ttotal: 12.3s\tremaining: 2m 43s\n",
      "70:\tlearn: 0.0020536\ttotal: 12.5s\tremaining: 2m 43s\n",
      "71:\tlearn: 0.0020042\ttotal: 12.7s\tremaining: 2m 43s\n",
      "72:\tlearn: 0.0020041\ttotal: 12.8s\tremaining: 2m 42s\n",
      "73:\tlearn: 0.0019558\ttotal: 13s\tremaining: 2m 42s\n",
      "74:\tlearn: 0.0019216\ttotal: 13.2s\tremaining: 2m 42s\n",
      "75:\tlearn: 0.0019212\ttotal: 13.3s\tremaining: 2m 42s\n",
      "76:\tlearn: 0.0018899\ttotal: 13.5s\tremaining: 2m 42s\n",
      "77:\tlearn: 0.0018668\ttotal: 13.7s\tremaining: 2m 41s\n",
      "78:\tlearn: 0.0018440\ttotal: 13.9s\tremaining: 2m 41s\n",
      "79:\tlearn: 0.0018239\ttotal: 14s\tremaining: 2m 41s\n",
      "80:\tlearn: 0.0017941\ttotal: 14.2s\tremaining: 2m 41s\n",
      "81:\tlearn: 0.0017941\ttotal: 14.4s\tremaining: 2m 41s\n",
      "82:\tlearn: 0.0017768\ttotal: 14.6s\tremaining: 2m 40s\n",
      "83:\tlearn: 0.0017531\ttotal: 14.7s\tremaining: 2m 40s\n",
      "84:\tlearn: 0.0017374\ttotal: 14.9s\tremaining: 2m 40s\n",
      "85:\tlearn: 0.0017192\ttotal: 15.1s\tremaining: 2m 40s\n",
      "86:\tlearn: 0.0016997\ttotal: 15.3s\tremaining: 2m 40s\n",
      "87:\tlearn: 0.0016989\ttotal: 15.4s\tremaining: 2m 40s\n",
      "88:\tlearn: 0.0016985\ttotal: 15.6s\tremaining: 2m 39s\n",
      "89:\tlearn: 0.0016728\ttotal: 15.8s\tremaining: 2m 39s\n",
      "90:\tlearn: 0.0016419\ttotal: 16s\tremaining: 2m 39s\n",
      "91:\tlearn: 0.0016240\ttotal: 16.1s\tremaining: 2m 39s\n",
      "92:\tlearn: 0.0016080\ttotal: 16.3s\tremaining: 2m 39s\n",
      "93:\tlearn: 0.0015817\ttotal: 16.5s\tremaining: 2m 38s\n",
      "94:\tlearn: 0.0015815\ttotal: 16.7s\tremaining: 2m 38s\n",
      "95:\tlearn: 0.0015815\ttotal: 16.8s\tremaining: 2m 38s\n",
      "96:\tlearn: 0.0015474\ttotal: 17s\tremaining: 2m 38s\n",
      "97:\tlearn: 0.0015474\ttotal: 17.2s\tremaining: 2m 38s\n",
      "98:\tlearn: 0.0015233\ttotal: 17.4s\tremaining: 2m 37s\n",
      "99:\tlearn: 0.0015233\ttotal: 17.5s\tremaining: 2m 37s\n",
      "100:\tlearn: 0.0015232\ttotal: 17.7s\tremaining: 2m 37s\n",
      "101:\tlearn: 0.0015232\ttotal: 17.9s\tremaining: 2m 37s\n",
      "102:\tlearn: 0.0015232\ttotal: 18.1s\tremaining: 2m 37s\n",
      "103:\tlearn: 0.0015231\ttotal: 18.2s\tremaining: 2m 37s\n",
      "104:\tlearn: 0.0015227\ttotal: 18.4s\tremaining: 2m 36s\n",
      "105:\tlearn: 0.0015227\ttotal: 18.6s\tremaining: 2m 36s\n",
      "106:\tlearn: 0.0015223\ttotal: 18.8s\tremaining: 2m 36s\n",
      "107:\tlearn: 0.0015222\ttotal: 18.9s\tremaining: 2m 36s\n",
      "108:\tlearn: 0.0015020\ttotal: 19.1s\tremaining: 2m 36s\n",
      "109:\tlearn: 0.0014833\ttotal: 19.3s\tremaining: 2m 36s\n",
      "110:\tlearn: 0.0014833\ttotal: 19.5s\tremaining: 2m 35s\n",
      "111:\tlearn: 0.0014679\ttotal: 19.6s\tremaining: 2m 35s\n",
      "112:\tlearn: 0.0014536\ttotal: 19.8s\tremaining: 2m 35s\n",
      "113:\tlearn: 0.0014378\ttotal: 20s\tremaining: 2m 35s\n",
      "114:\tlearn: 0.0014231\ttotal: 20.2s\tremaining: 2m 35s\n",
      "115:\tlearn: 0.0014230\ttotal: 20.3s\tremaining: 2m 34s\n",
      "116:\tlearn: 0.0014229\ttotal: 20.5s\tremaining: 2m 34s\n",
      "117:\tlearn: 0.0014229\ttotal: 20.7s\tremaining: 2m 34s\n",
      "118:\tlearn: 0.0014224\ttotal: 20.9s\tremaining: 2m 34s\n",
      "119:\tlearn: 0.0014224\ttotal: 21s\tremaining: 2m 34s\n",
      "120:\tlearn: 0.0014108\ttotal: 21.2s\tremaining: 2m 34s\n",
      "121:\tlearn: 0.0013986\ttotal: 21.4s\tremaining: 2m 33s\n",
      "122:\tlearn: 0.0013986\ttotal: 21.6s\tremaining: 2m 33s\n",
      "123:\tlearn: 0.0013985\ttotal: 21.7s\tremaining: 2m 33s\n",
      "124:\tlearn: 0.0013984\ttotal: 21.9s\tremaining: 2m 33s\n",
      "125:\tlearn: 0.0013984\ttotal: 22.1s\tremaining: 2m 33s\n",
      "126:\tlearn: 0.0013983\ttotal: 22.2s\tremaining: 2m 32s\n",
      "127:\tlearn: 0.0013981\ttotal: 22.4s\tremaining: 2m 32s\n",
      "128:\tlearn: 0.0013888\ttotal: 22.6s\tremaining: 2m 32s\n",
      "129:\tlearn: 0.0013668\ttotal: 22.8s\tremaining: 2m 32s\n",
      "130:\tlearn: 0.0013534\ttotal: 22.9s\tremaining: 2m 32s\n",
      "131:\tlearn: 0.0013534\ttotal: 23.1s\tremaining: 2m 31s\n",
      "132:\tlearn: 0.0013370\ttotal: 23.3s\tremaining: 2m 31s\n",
      "133:\tlearn: 0.0013152\ttotal: 23.5s\tremaining: 2m 31s\n",
      "134:\tlearn: 0.0013151\ttotal: 23.6s\tremaining: 2m 31s\n",
      "135:\tlearn: 0.0013151\ttotal: 23.9s\tremaining: 2m 31s\n",
      "136:\tlearn: 0.0013149\ttotal: 24.1s\tremaining: 2m 31s\n",
      "137:\tlearn: 0.0013148\ttotal: 24.3s\tremaining: 2m 31s\n",
      "138:\tlearn: 0.0013148\ttotal: 24.4s\tremaining: 2m 31s\n",
      "139:\tlearn: 0.0013148\ttotal: 24.6s\tremaining: 2m 31s\n",
      "140:\tlearn: 0.0013148\ttotal: 24.8s\tremaining: 2m 31s\n",
      "141:\tlearn: 0.0013146\ttotal: 25s\tremaining: 2m 30s\n",
      "142:\tlearn: 0.0013146\ttotal: 25.2s\tremaining: 2m 30s\n",
      "143:\tlearn: 0.0013145\ttotal: 25.3s\tremaining: 2m 30s\n",
      "144:\tlearn: 0.0013144\ttotal: 25.5s\tremaining: 2m 30s\n",
      "145:\tlearn: 0.0013144\ttotal: 25.7s\tremaining: 2m 30s\n",
      "146:\tlearn: 0.0013143\ttotal: 25.9s\tremaining: 2m 30s\n",
      "147:\tlearn: 0.0013011\ttotal: 26s\tremaining: 2m 29s\n",
      "148:\tlearn: 0.0012874\ttotal: 26.2s\tremaining: 2m 29s\n",
      "149:\tlearn: 0.0012870\ttotal: 26.4s\tremaining: 2m 29s\n",
      "150:\tlearn: 0.0012619\ttotal: 26.6s\tremaining: 2m 29s\n",
      "151:\tlearn: 0.0012507\ttotal: 26.7s\tremaining: 2m 29s\n",
      "152:\tlearn: 0.0012507\ttotal: 26.9s\tremaining: 2m 28s\n",
      "153:\tlearn: 0.0012374\ttotal: 27.1s\tremaining: 2m 28s\n",
      "154:\tlearn: 0.0012374\ttotal: 27.3s\tremaining: 2m 28s\n",
      "155:\tlearn: 0.0012342\ttotal: 27.4s\tremaining: 2m 28s\n",
      "156:\tlearn: 0.0012168\ttotal: 27.6s\tremaining: 2m 28s\n",
      "157:\tlearn: 0.0012061\ttotal: 27.8s\tremaining: 2m 28s\n",
      "158:\tlearn: 0.0011975\ttotal: 28s\tremaining: 2m 27s\n",
      "159:\tlearn: 0.0011975\ttotal: 28.1s\tremaining: 2m 27s\n",
      "160:\tlearn: 0.0011975\ttotal: 28.3s\tremaining: 2m 27s\n",
      "161:\tlearn: 0.0011974\ttotal: 28.5s\tremaining: 2m 27s\n",
      "162:\tlearn: 0.0011970\ttotal: 28.7s\tremaining: 2m 27s\n",
      "163:\tlearn: 0.0011849\ttotal: 28.8s\tremaining: 2m 27s\n",
      "164:\tlearn: 0.0011848\ttotal: 29s\tremaining: 2m 26s\n",
      "165:\tlearn: 0.0011712\ttotal: 29.2s\tremaining: 2m 26s\n",
      "166:\tlearn: 0.0011712\ttotal: 29.4s\tremaining: 2m 26s\n",
      "167:\tlearn: 0.0011711\ttotal: 29.5s\tremaining: 2m 26s\n",
      "168:\tlearn: 0.0011710\ttotal: 29.7s\tremaining: 2m 26s\n",
      "169:\tlearn: 0.0011708\ttotal: 29.9s\tremaining: 2m 25s\n",
      "170:\tlearn: 0.0011619\ttotal: 30.1s\tremaining: 2m 25s\n",
      "171:\tlearn: 0.0011453\ttotal: 30.2s\tremaining: 2m 25s\n",
      "172:\tlearn: 0.0011412\ttotal: 30.4s\tremaining: 2m 25s\n",
      "173:\tlearn: 0.0011376\ttotal: 30.6s\tremaining: 2m 25s\n",
      "174:\tlearn: 0.0011319\ttotal: 30.8s\tremaining: 2m 24s\n",
      "175:\tlearn: 0.0011318\ttotal: 30.9s\tremaining: 2m 24s\n",
      "176:\tlearn: 0.0011316\ttotal: 31.1s\tremaining: 2m 24s\n",
      "177:\tlearn: 0.0011184\ttotal: 31.3s\tremaining: 2m 24s\n",
      "178:\tlearn: 0.0011111\ttotal: 31.5s\tremaining: 2m 24s\n",
      "179:\tlearn: 0.0010985\ttotal: 31.6s\tremaining: 2m 24s\n",
      "180:\tlearn: 0.0010867\ttotal: 31.8s\tremaining: 2m 23s\n",
      "181:\tlearn: 0.0010867\ttotal: 32s\tremaining: 2m 23s\n",
      "182:\tlearn: 0.0010865\ttotal: 32.2s\tremaining: 2m 23s\n",
      "183:\tlearn: 0.0010864\ttotal: 32.4s\tremaining: 2m 23s\n",
      "184:\tlearn: 0.0010863\ttotal: 32.5s\tremaining: 2m 23s\n",
      "185:\tlearn: 0.0010860\ttotal: 32.7s\tremaining: 2m 23s\n",
      "186:\tlearn: 0.0010860\ttotal: 32.9s\tremaining: 2m 22s\n",
      "187:\tlearn: 0.0010665\ttotal: 33s\tremaining: 2m 22s\n",
      "188:\tlearn: 0.0010665\ttotal: 33.2s\tremaining: 2m 22s\n",
      "189:\tlearn: 0.0010664\ttotal: 33.4s\tremaining: 2m 22s\n",
      "190:\tlearn: 0.0010664\ttotal: 33.6s\tremaining: 2m 22s\n",
      "191:\tlearn: 0.0010650\ttotal: 33.7s\tremaining: 2m 21s\n",
      "192:\tlearn: 0.0010648\ttotal: 33.9s\tremaining: 2m 21s\n",
      "193:\tlearn: 0.0010646\ttotal: 34.1s\tremaining: 2m 21s\n",
      "194:\tlearn: 0.0010588\ttotal: 34.3s\tremaining: 2m 21s\n",
      "195:\tlearn: 0.0010509\ttotal: 34.4s\tremaining: 2m 21s\n",
      "196:\tlearn: 0.0010509\ttotal: 34.6s\tremaining: 2m 21s\n",
      "197:\tlearn: 0.0010508\ttotal: 34.8s\tremaining: 2m 20s\n",
      "198:\tlearn: 0.0010508\ttotal: 35s\tremaining: 2m 20s\n",
      "199:\tlearn: 0.0010506\ttotal: 35.1s\tremaining: 2m 20s\n",
      "200:\tlearn: 0.0010430\ttotal: 35.3s\tremaining: 2m 20s\n",
      "201:\tlearn: 0.0010339\ttotal: 35.5s\tremaining: 2m 20s\n",
      "202:\tlearn: 0.0010339\ttotal: 35.7s\tremaining: 2m 20s\n",
      "203:\tlearn: 0.0010336\ttotal: 35.8s\tremaining: 2m 19s\n",
      "204:\tlearn: 0.0010336\ttotal: 36s\tremaining: 2m 19s\n",
      "205:\tlearn: 0.0010336\ttotal: 36.2s\tremaining: 2m 19s\n",
      "206:\tlearn: 0.0010336\ttotal: 36.4s\tremaining: 2m 19s\n",
      "207:\tlearn: 0.0010333\ttotal: 36.5s\tremaining: 2m 19s\n",
      "208:\tlearn: 0.0010332\ttotal: 36.7s\tremaining: 2m 18s\n",
      "209:\tlearn: 0.0010332\ttotal: 36.9s\tremaining: 2m 18s\n",
      "210:\tlearn: 0.0010322\ttotal: 37.1s\tremaining: 2m 18s\n",
      "211:\tlearn: 0.0010322\ttotal: 37.2s\tremaining: 2m 18s\n",
      "212:\tlearn: 0.0010288\ttotal: 37.4s\tremaining: 2m 18s\n",
      "213:\tlearn: 0.0010287\ttotal: 37.6s\tremaining: 2m 18s\n",
      "214:\tlearn: 0.0010276\ttotal: 37.8s\tremaining: 2m 17s\n",
      "215:\tlearn: 0.0010275\ttotal: 37.9s\tremaining: 2m 17s\n",
      "216:\tlearn: 0.0010274\ttotal: 38.1s\tremaining: 2m 17s\n",
      "217:\tlearn: 0.0010169\ttotal: 38.3s\tremaining: 2m 17s\n",
      "218:\tlearn: 0.0010169\ttotal: 38.5s\tremaining: 2m 17s\n",
      "219:\tlearn: 0.0010169\ttotal: 38.6s\tremaining: 2m 16s\n",
      "220:\tlearn: 0.0010169\ttotal: 38.8s\tremaining: 2m 16s\n",
      "221:\tlearn: 0.0010169\ttotal: 39s\tremaining: 2m 16s\n",
      "222:\tlearn: 0.0010166\ttotal: 39.2s\tremaining: 2m 16s\n",
      "223:\tlearn: 0.0010166\ttotal: 39.4s\tremaining: 2m 16s\n",
      "224:\tlearn: 0.0010165\ttotal: 39.5s\tremaining: 2m 16s\n",
      "225:\tlearn: 0.0010108\ttotal: 39.7s\tremaining: 2m 15s\n",
      "226:\tlearn: 0.0010018\ttotal: 39.9s\tremaining: 2m 15s\n",
      "227:\tlearn: 0.0010014\ttotal: 40.1s\tremaining: 2m 15s\n",
      "228:\tlearn: 0.0010013\ttotal: 40.2s\tremaining: 2m 15s\n",
      "229:\tlearn: 0.0010010\ttotal: 40.4s\tremaining: 2m 15s\n",
      "230:\tlearn: 0.0010010\ttotal: 40.6s\tremaining: 2m 15s\n",
      "231:\tlearn: 0.0009934\ttotal: 40.8s\tremaining: 2m 14s\n",
      "232:\tlearn: 0.0009934\ttotal: 40.9s\tremaining: 2m 14s\n",
      "233:\tlearn: 0.0009933\ttotal: 41.1s\tremaining: 2m 14s\n",
      "234:\tlearn: 0.0009932\ttotal: 41.3s\tremaining: 2m 14s\n",
      "235:\tlearn: 0.0009931\ttotal: 41.5s\tremaining: 2m 14s\n",
      "236:\tlearn: 0.0009931\ttotal: 41.6s\tremaining: 2m 13s\n",
      "237:\tlearn: 0.0009928\ttotal: 41.8s\tremaining: 2m 13s\n",
      "238:\tlearn: 0.0009928\ttotal: 42s\tremaining: 2m 13s\n",
      "239:\tlearn: 0.0009928\ttotal: 42.1s\tremaining: 2m 13s\n",
      "240:\tlearn: 0.0009927\ttotal: 42.3s\tremaining: 2m 13s\n",
      "241:\tlearn: 0.0009919\ttotal: 42.5s\tremaining: 2m 13s\n",
      "242:\tlearn: 0.0009918\ttotal: 42.7s\tremaining: 2m 12s\n",
      "243:\tlearn: 0.0009917\ttotal: 42.8s\tremaining: 2m 12s\n",
      "244:\tlearn: 0.0009915\ttotal: 43s\tremaining: 2m 12s\n",
      "245:\tlearn: 0.0009915\ttotal: 43.2s\tremaining: 2m 12s\n",
      "246:\tlearn: 0.0009915\ttotal: 43.4s\tremaining: 2m 12s\n",
      "247:\tlearn: 0.0009859\ttotal: 43.5s\tremaining: 2m 12s\n",
      "248:\tlearn: 0.0009858\ttotal: 43.7s\tremaining: 2m 11s\n",
      "249:\tlearn: 0.0009857\ttotal: 43.9s\tremaining: 2m 11s\n",
      "250:\tlearn: 0.0009857\ttotal: 44.1s\tremaining: 2m 11s\n",
      "251:\tlearn: 0.0009758\ttotal: 44.2s\tremaining: 2m 11s\n",
      "252:\tlearn: 0.0009758\ttotal: 44.4s\tremaining: 2m 11s\n",
      "253:\tlearn: 0.0009757\ttotal: 44.6s\tremaining: 2m 10s\n",
      "254:\tlearn: 0.0009757\ttotal: 44.8s\tremaining: 2m 10s\n",
      "255:\tlearn: 0.0009681\ttotal: 44.9s\tremaining: 2m 10s\n",
      "256:\tlearn: 0.0009681\ttotal: 45.1s\tremaining: 2m 10s\n",
      "257:\tlearn: 0.0009678\ttotal: 45.3s\tremaining: 2m 10s\n",
      "258:\tlearn: 0.0009629\ttotal: 45.4s\tremaining: 2m 10s\n",
      "259:\tlearn: 0.0009590\ttotal: 45.6s\tremaining: 2m 9s\n",
      "260:\tlearn: 0.0009589\ttotal: 45.8s\tremaining: 2m 9s\n",
      "261:\tlearn: 0.0009577\ttotal: 46s\tremaining: 2m 9s\n",
      "262:\tlearn: 0.0009572\ttotal: 46.1s\tremaining: 2m 9s\n",
      "263:\tlearn: 0.0009572\ttotal: 46.3s\tremaining: 2m 9s\n",
      "264:\tlearn: 0.0009572\ttotal: 46.5s\tremaining: 2m 8s\n",
      "265:\tlearn: 0.0009568\ttotal: 46.7s\tremaining: 2m 8s\n",
      "266:\tlearn: 0.0009567\ttotal: 46.8s\tremaining: 2m 8s\n",
      "267:\tlearn: 0.0009566\ttotal: 47s\tremaining: 2m 8s\n",
      "268:\tlearn: 0.0009566\ttotal: 47.2s\tremaining: 2m 8s\n",
      "269:\tlearn: 0.0009566\ttotal: 47.4s\tremaining: 2m 8s\n",
      "270:\tlearn: 0.0009566\ttotal: 47.5s\tremaining: 2m 7s\n",
      "271:\tlearn: 0.0009565\ttotal: 47.7s\tremaining: 2m 7s\n",
      "272:\tlearn: 0.0009550\ttotal: 47.9s\tremaining: 2m 7s\n",
      "273:\tlearn: 0.0009549\ttotal: 48.1s\tremaining: 2m 7s\n",
      "274:\tlearn: 0.0009545\ttotal: 48.2s\tremaining: 2m 7s\n",
      "275:\tlearn: 0.0009544\ttotal: 48.4s\tremaining: 2m 7s\n",
      "276:\tlearn: 0.0009543\ttotal: 48.6s\tremaining: 2m 6s\n",
      "277:\tlearn: 0.0009541\ttotal: 48.8s\tremaining: 2m 6s\n",
      "278:\tlearn: 0.0009540\ttotal: 48.9s\tremaining: 2m 6s\n",
      "279:\tlearn: 0.0009540\ttotal: 49.1s\tremaining: 2m 6s\n",
      "280:\tlearn: 0.0009538\ttotal: 49.3s\tremaining: 2m 6s\n",
      "281:\tlearn: 0.0009538\ttotal: 49.5s\tremaining: 2m 5s\n",
      "282:\tlearn: 0.0009536\ttotal: 49.6s\tremaining: 2m 5s\n",
      "283:\tlearn: 0.0009536\ttotal: 49.8s\tremaining: 2m 5s\n",
      "284:\tlearn: 0.0009535\ttotal: 50s\tremaining: 2m 5s\n",
      "285:\tlearn: 0.0009531\ttotal: 50.2s\tremaining: 2m 5s\n",
      "286:\tlearn: 0.0009530\ttotal: 50.4s\tremaining: 2m 5s\n",
      "287:\tlearn: 0.0009530\ttotal: 50.5s\tremaining: 2m 4s\n",
      "288:\tlearn: 0.0009527\ttotal: 50.7s\tremaining: 2m 4s\n",
      "289:\tlearn: 0.0009527\ttotal: 50.9s\tremaining: 2m 4s\n",
      "290:\tlearn: 0.0009527\ttotal: 51s\tremaining: 2m 4s\n",
      "291:\tlearn: 0.0009470\ttotal: 51.2s\tremaining: 2m 4s\n",
      "292:\tlearn: 0.0009439\ttotal: 51.4s\tremaining: 2m 4s\n",
      "293:\tlearn: 0.0009433\ttotal: 51.6s\tremaining: 2m 3s\n",
      "294:\tlearn: 0.0009429\ttotal: 51.7s\tremaining: 2m 3s\n",
      "295:\tlearn: 0.0009429\ttotal: 51.9s\tremaining: 2m 3s\n",
      "296:\tlearn: 0.0009428\ttotal: 52.1s\tremaining: 2m 3s\n",
      "297:\tlearn: 0.0009426\ttotal: 52.3s\tremaining: 2m 3s\n",
      "298:\tlearn: 0.0009426\ttotal: 52.4s\tremaining: 2m 2s\n",
      "299:\tlearn: 0.0009426\ttotal: 52.6s\tremaining: 2m 2s\n",
      "300:\tlearn: 0.0009425\ttotal: 52.8s\tremaining: 2m 2s\n",
      "301:\tlearn: 0.0009423\ttotal: 53s\tremaining: 2m 2s\n",
      "302:\tlearn: 0.0009423\ttotal: 53.1s\tremaining: 2m 2s\n",
      "303:\tlearn: 0.0009423\ttotal: 53.3s\tremaining: 2m 2s\n",
      "304:\tlearn: 0.0009421\ttotal: 53.5s\tremaining: 2m 1s\n",
      "305:\tlearn: 0.0009420\ttotal: 53.7s\tremaining: 2m 1s\n",
      "306:\tlearn: 0.0009420\ttotal: 53.8s\tremaining: 2m 1s\n",
      "307:\tlearn: 0.0009415\ttotal: 54s\tremaining: 2m 1s\n",
      "308:\tlearn: 0.0009414\ttotal: 54.2s\tremaining: 2m 1s\n",
      "309:\tlearn: 0.0009414\ttotal: 54.4s\tremaining: 2m 1s\n",
      "310:\tlearn: 0.0009414\ttotal: 54.5s\tremaining: 2m\n",
      "311:\tlearn: 0.0009413\ttotal: 54.7s\tremaining: 2m\n",
      "312:\tlearn: 0.0009412\ttotal: 54.9s\tremaining: 2m\n",
      "313:\tlearn: 0.0009409\ttotal: 55.1s\tremaining: 2m\n",
      "314:\tlearn: 0.0009407\ttotal: 55.2s\tremaining: 2m\n",
      "315:\tlearn: 0.0009357\ttotal: 55.4s\tremaining: 1m 59s\n",
      "316:\tlearn: 0.0009357\ttotal: 55.6s\tremaining: 1m 59s\n",
      "317:\tlearn: 0.0009355\ttotal: 55.8s\tremaining: 1m 59s\n",
      "318:\tlearn: 0.0009345\ttotal: 55.9s\tremaining: 1m 59s\n",
      "319:\tlearn: 0.0009345\ttotal: 56.1s\tremaining: 1m 59s\n",
      "320:\tlearn: 0.0009345\ttotal: 56.3s\tremaining: 1m 59s\n",
      "321:\tlearn: 0.0009345\ttotal: 56.5s\tremaining: 1m 58s\n",
      "322:\tlearn: 0.0009344\ttotal: 56.6s\tremaining: 1m 58s\n",
      "323:\tlearn: 0.0009344\ttotal: 56.8s\tremaining: 1m 58s\n",
      "324:\tlearn: 0.0009344\ttotal: 57s\tremaining: 1m 58s\n",
      "325:\tlearn: 0.0009340\ttotal: 57.2s\tremaining: 1m 58s\n",
      "326:\tlearn: 0.0009340\ttotal: 57.3s\tremaining: 1m 58s\n",
      "327:\tlearn: 0.0009337\ttotal: 57.5s\tremaining: 1m 57s\n",
      "328:\tlearn: 0.0009337\ttotal: 57.7s\tremaining: 1m 57s\n",
      "329:\tlearn: 0.0009336\ttotal: 57.9s\tremaining: 1m 57s\n",
      "330:\tlearn: 0.0009335\ttotal: 58s\tremaining: 1m 57s\n",
      "331:\tlearn: 0.0009335\ttotal: 58.2s\tremaining: 1m 57s\n",
      "332:\tlearn: 0.0009334\ttotal: 58.4s\tremaining: 1m 56s\n",
      "333:\tlearn: 0.0009334\ttotal: 58.6s\tremaining: 1m 56s\n",
      "334:\tlearn: 0.0009334\ttotal: 58.7s\tremaining: 1m 56s\n",
      "335:\tlearn: 0.0009334\ttotal: 58.9s\tremaining: 1m 56s\n",
      "336:\tlearn: 0.0009334\ttotal: 59.1s\tremaining: 1m 56s\n",
      "337:\tlearn: 0.0009333\ttotal: 59.3s\tremaining: 1m 56s\n",
      "338:\tlearn: 0.0009333\ttotal: 59.4s\tremaining: 1m 55s\n",
      "339:\tlearn: 0.0009333\ttotal: 59.6s\tremaining: 1m 55s\n",
      "340:\tlearn: 0.0009333\ttotal: 59.8s\tremaining: 1m 55s\n",
      "341:\tlearn: 0.0009333\ttotal: 60s\tremaining: 1m 55s\n",
      "342:\tlearn: 0.0009331\ttotal: 1m\tremaining: 1m 55s\n",
      "343:\tlearn: 0.0009331\ttotal: 1m\tremaining: 1m 55s\n",
      "344:\tlearn: 0.0009328\ttotal: 1m\tremaining: 1m 54s\n",
      "345:\tlearn: 0.0009261\ttotal: 1m\tremaining: 1m 54s\n",
      "346:\tlearn: 0.0009260\ttotal: 1m\tremaining: 1m 54s\n",
      "347:\tlearn: 0.0009260\ttotal: 1m 1s\tremaining: 1m 54s\n",
      "348:\tlearn: 0.0009256\ttotal: 1m 1s\tremaining: 1m 54s\n",
      "349:\tlearn: 0.0009256\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "350:\tlearn: 0.0009202\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "351:\tlearn: 0.0009202\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "352:\tlearn: 0.0009201\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "353:\tlearn: 0.0009201\ttotal: 1m 2s\tremaining: 1m 53s\n",
      "354:\tlearn: 0.0009201\ttotal: 1m 2s\tremaining: 1m 53s\n",
      "355:\tlearn: 0.0009201\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "356:\tlearn: 0.0009198\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "357:\tlearn: 0.0009198\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "358:\tlearn: 0.0009197\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "359:\tlearn: 0.0009197\ttotal: 1m 3s\tremaining: 1m 52s\n",
      "360:\tlearn: 0.0009197\ttotal: 1m 3s\tremaining: 1m 52s\n",
      "361:\tlearn: 0.0009103\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "362:\tlearn: 0.0009103\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "363:\tlearn: 0.0009063\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "364:\tlearn: 0.0009060\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "365:\tlearn: 0.0009060\ttotal: 1m 4s\tremaining: 1m 51s\n",
      "366:\tlearn: 0.0009059\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "367:\tlearn: 0.0009059\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "368:\tlearn: 0.0009058\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "369:\tlearn: 0.0009058\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "370:\tlearn: 0.0009057\ttotal: 1m 5s\tremaining: 1m 50s\n",
      "371:\tlearn: 0.0009056\ttotal: 1m 5s\tremaining: 1m 50s\n",
      "372:\tlearn: 0.0008996\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "373:\tlearn: 0.0008991\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "374:\tlearn: 0.0008991\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "375:\tlearn: 0.0008991\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "376:\tlearn: 0.0008987\ttotal: 1m 6s\tremaining: 1m 49s\n",
      "377:\tlearn: 0.0008987\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "378:\tlearn: 0.0008970\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "379:\tlearn: 0.0008938\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "380:\tlearn: 0.0008938\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "381:\tlearn: 0.0008937\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "382:\tlearn: 0.0008937\ttotal: 1m 7s\tremaining: 1m 48s\n",
      "383:\tlearn: 0.0008937\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "384:\tlearn: 0.0008937\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "385:\tlearn: 0.0008932\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "386:\tlearn: 0.0008931\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "387:\tlearn: 0.0008930\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "388:\tlearn: 0.0008849\ttotal: 1m 8s\tremaining: 1m 47s\n",
      "389:\tlearn: 0.0008849\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "390:\tlearn: 0.0008849\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "391:\tlearn: 0.0008849\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "392:\tlearn: 0.0008848\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "393:\tlearn: 0.0008845\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "394:\tlearn: 0.0008845\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "395:\tlearn: 0.0008845\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "396:\tlearn: 0.0008844\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "397:\tlearn: 0.0008844\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "398:\tlearn: 0.0008844\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "399:\tlearn: 0.0008844\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "400:\tlearn: 0.0008844\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "401:\tlearn: 0.0008843\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "402:\tlearn: 0.0008834\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "403:\tlearn: 0.0008833\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "404:\tlearn: 0.0008831\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "405:\tlearn: 0.0008831\ttotal: 1m 11s\tremaining: 1m 44s\n",
      "406:\tlearn: 0.0008829\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "407:\tlearn: 0.0008824\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "408:\tlearn: 0.0008823\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "409:\tlearn: 0.0008823\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "410:\tlearn: 0.0008823\ttotal: 1m 12s\tremaining: 1m 43s\n",
      "411:\tlearn: 0.0008821\ttotal: 1m 12s\tremaining: 1m 43s\n",
      "412:\tlearn: 0.0008821\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "413:\tlearn: 0.0008821\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "414:\tlearn: 0.0008820\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "415:\tlearn: 0.0008819\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "416:\tlearn: 0.0008819\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "417:\tlearn: 0.0008818\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "418:\tlearn: 0.0008818\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "419:\tlearn: 0.0008818\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "420:\tlearn: 0.0008814\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "421:\tlearn: 0.0008813\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "422:\tlearn: 0.0008813\ttotal: 1m 14s\tremaining: 1m 41s\n",
      "423:\tlearn: 0.0008812\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "424:\tlearn: 0.0008809\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "425:\tlearn: 0.0008809\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "426:\tlearn: 0.0008809\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "427:\tlearn: 0.0008809\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "428:\tlearn: 0.0008809\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "429:\tlearn: 0.0008809\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "430:\tlearn: 0.0008809\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "431:\tlearn: 0.0008808\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "432:\tlearn: 0.0008808\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "433:\tlearn: 0.0008808\ttotal: 1m 16s\tremaining: 1m 39s\n",
      "434:\tlearn: 0.0008808\ttotal: 1m 16s\tremaining: 1m 39s\n",
      "435:\tlearn: 0.0008808\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "436:\tlearn: 0.0008808\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "437:\tlearn: 0.0008808\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "438:\tlearn: 0.0008808\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "439:\tlearn: 0.0008770\ttotal: 1m 17s\tremaining: 1m 38s\n",
      "440:\tlearn: 0.0008769\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "441:\tlearn: 0.0008766\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "442:\tlearn: 0.0008765\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "443:\tlearn: 0.0008711\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "444:\tlearn: 0.0008711\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "445:\tlearn: 0.0008644\ttotal: 1m 18s\tremaining: 1m 37s\n",
      "446:\tlearn: 0.0008576\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "447:\tlearn: 0.0008575\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "448:\tlearn: 0.0008524\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "449:\tlearn: 0.0008521\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "450:\tlearn: 0.0008517\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "451:\tlearn: 0.0008517\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "452:\tlearn: 0.0008517\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "453:\tlearn: 0.0008516\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "454:\tlearn: 0.0008514\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "455:\tlearn: 0.0008511\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "456:\tlearn: 0.0008510\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "457:\tlearn: 0.0008510\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "458:\tlearn: 0.0008510\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "459:\tlearn: 0.0008510\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "460:\tlearn: 0.0008505\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "461:\tlearn: 0.0008502\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "462:\tlearn: 0.0008502\ttotal: 1m 21s\tremaining: 1m 34s\n",
      "463:\tlearn: 0.0008501\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "464:\tlearn: 0.0008495\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "465:\tlearn: 0.0008493\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "466:\tlearn: 0.0008492\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "467:\tlearn: 0.0008492\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "468:\tlearn: 0.0008491\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "469:\tlearn: 0.0008491\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "470:\tlearn: 0.0008490\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "471:\tlearn: 0.0008401\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "472:\tlearn: 0.0008399\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "473:\tlearn: 0.0008396\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "474:\tlearn: 0.0008392\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "475:\tlearn: 0.0008392\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "476:\tlearn: 0.0008391\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "477:\tlearn: 0.0008390\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "478:\tlearn: 0.0008389\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "479:\tlearn: 0.0008389\ttotal: 1m 24s\tremaining: 1m 31s\n",
      "480:\tlearn: 0.0008388\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "481:\tlearn: 0.0008388\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "482:\tlearn: 0.0008387\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "483:\tlearn: 0.0008352\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "484:\tlearn: 0.0008351\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "485:\tlearn: 0.0008351\ttotal: 1m 25s\tremaining: 1m 30s\n",
      "486:\tlearn: 0.0008349\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "487:\tlearn: 0.0008349\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "488:\tlearn: 0.0008349\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "489:\tlearn: 0.0008345\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "490:\tlearn: 0.0008343\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "491:\tlearn: 0.0008342\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "492:\tlearn: 0.0008341\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "493:\tlearn: 0.0008340\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "494:\tlearn: 0.0008340\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "495:\tlearn: 0.0008340\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "496:\tlearn: 0.0008339\ttotal: 1m 27s\tremaining: 1m 28s\n",
      "497:\tlearn: 0.0008339\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "498:\tlearn: 0.0008339\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "499:\tlearn: 0.0008339\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "500:\tlearn: 0.0008338\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "501:\tlearn: 0.0008335\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "502:\tlearn: 0.0008334\ttotal: 1m 28s\tremaining: 1m 27s\n",
      "503:\tlearn: 0.0008333\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "504:\tlearn: 0.0008333\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "505:\tlearn: 0.0008332\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "506:\tlearn: 0.0008332\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "507:\tlearn: 0.0008331\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "508:\tlearn: 0.0008331\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "509:\tlearn: 0.0008329\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "510:\tlearn: 0.0008329\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "511:\tlearn: 0.0008329\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "512:\tlearn: 0.0008329\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "513:\tlearn: 0.0008328\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "514:\tlearn: 0.0008321\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "515:\tlearn: 0.0008320\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "516:\tlearn: 0.0008319\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "517:\tlearn: 0.0008318\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "518:\tlearn: 0.0008318\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "519:\tlearn: 0.0008318\ttotal: 1m 31s\tremaining: 1m 24s\n",
      "520:\tlearn: 0.0008318\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "521:\tlearn: 0.0008318\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "522:\tlearn: 0.0008318\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "523:\tlearn: 0.0008267\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "524:\tlearn: 0.0008267\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "525:\tlearn: 0.0008260\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "526:\tlearn: 0.0008259\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "527:\tlearn: 0.0008258\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "528:\tlearn: 0.0008258\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "529:\tlearn: 0.0008257\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "530:\tlearn: 0.0008252\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "531:\tlearn: 0.0008245\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "532:\tlearn: 0.0008245\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "533:\tlearn: 0.0008245\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "534:\tlearn: 0.0008241\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "535:\tlearn: 0.0008241\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "536:\tlearn: 0.0008239\ttotal: 1m 34s\tremaining: 1m 21s\n",
      "537:\tlearn: 0.0008238\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "538:\tlearn: 0.0008236\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "539:\tlearn: 0.0008235\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "540:\tlearn: 0.0008224\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "541:\tlearn: 0.0008224\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "542:\tlearn: 0.0008220\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "543:\tlearn: 0.0008220\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "544:\tlearn: 0.0008218\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "545:\tlearn: 0.0008217\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "546:\tlearn: 0.0008215\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "547:\tlearn: 0.0008214\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "548:\tlearn: 0.0008213\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "549:\tlearn: 0.0008212\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "550:\tlearn: 0.0008193\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "551:\tlearn: 0.0008193\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "552:\tlearn: 0.0008193\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "553:\tlearn: 0.0008193\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "554:\tlearn: 0.0008193\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "555:\tlearn: 0.0008192\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "556:\tlearn: 0.0008192\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "557:\tlearn: 0.0008191\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "558:\tlearn: 0.0008191\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "559:\tlearn: 0.0008132\ttotal: 1m 37s\tremaining: 1m 16s\n",
      "560:\tlearn: 0.0008132\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "561:\tlearn: 0.0008132\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "562:\tlearn: 0.0008129\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "563:\tlearn: 0.0008128\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "564:\tlearn: 0.0008120\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "565:\tlearn: 0.0008119\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "566:\tlearn: 0.0008070\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "567:\tlearn: 0.0008068\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "568:\tlearn: 0.0008068\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "569:\tlearn: 0.0008068\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "570:\tlearn: 0.0008068\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "571:\tlearn: 0.0008066\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "572:\tlearn: 0.0008028\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "573:\tlearn: 0.0008027\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "574:\tlearn: 0.0008023\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "575:\tlearn: 0.0008003\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "576:\tlearn: 0.0007960\ttotal: 1m 41s\tremaining: 1m 14s\n",
      "577:\tlearn: 0.0007960\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "578:\tlearn: 0.0007959\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "579:\tlearn: 0.0007958\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "580:\tlearn: 0.0007956\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "581:\tlearn: 0.0007956\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "582:\tlearn: 0.0007956\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "583:\tlearn: 0.0007954\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "584:\tlearn: 0.0007954\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "585:\tlearn: 0.0007949\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "586:\tlearn: 0.0007920\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "587:\tlearn: 0.0007920\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "588:\tlearn: 0.0007920\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "589:\tlearn: 0.0007919\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "590:\tlearn: 0.0007919\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "591:\tlearn: 0.0007886\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "592:\tlearn: 0.0007886\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "593:\tlearn: 0.0007857\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "594:\tlearn: 0.0007853\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "595:\tlearn: 0.0007853\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "596:\tlearn: 0.0007853\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "597:\tlearn: 0.0007852\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "598:\tlearn: 0.0007852\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "599:\tlearn: 0.0007852\ttotal: 1m 44s\tremaining: 1m 9s\n",
      "600:\tlearn: 0.0007852\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "601:\tlearn: 0.0007852\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "602:\tlearn: 0.0007850\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "603:\tlearn: 0.0007850\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "604:\tlearn: 0.0007850\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "605:\tlearn: 0.0007849\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "606:\tlearn: 0.0007849\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "607:\tlearn: 0.0007847\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "608:\tlearn: 0.0007847\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "609:\tlearn: 0.0007846\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "610:\tlearn: 0.0007846\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "611:\tlearn: 0.0007844\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "612:\tlearn: 0.0007844\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "613:\tlearn: 0.0007843\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "614:\tlearn: 0.0007842\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "615:\tlearn: 0.0007811\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "616:\tlearn: 0.0007810\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "617:\tlearn: 0.0007810\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "618:\tlearn: 0.0007809\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "619:\tlearn: 0.0007809\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "620:\tlearn: 0.0007809\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "621:\tlearn: 0.0007809\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "622:\tlearn: 0.0007809\ttotal: 1m 48s\tremaining: 1m 5s\n",
      "623:\tlearn: 0.0007808\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "624:\tlearn: 0.0007808\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "625:\tlearn: 0.0007807\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "626:\tlearn: 0.0007795\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "627:\tlearn: 0.0007795\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "628:\tlearn: 0.0007793\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "629:\tlearn: 0.0007792\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "630:\tlearn: 0.0007788\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "631:\tlearn: 0.0007787\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "632:\tlearn: 0.0007787\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "633:\tlearn: 0.0007787\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "634:\tlearn: 0.0007787\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "635:\tlearn: 0.0007785\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "636:\tlearn: 0.0007785\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "637:\tlearn: 0.0007785\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "638:\tlearn: 0.0007785\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "639:\tlearn: 0.0007784\ttotal: 1m 51s\tremaining: 1m 2s\n",
      "640:\tlearn: 0.0007781\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "641:\tlearn: 0.0007781\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "642:\tlearn: 0.0007781\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "643:\tlearn: 0.0007779\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "644:\tlearn: 0.0007778\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "645:\tlearn: 0.0007778\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "646:\tlearn: 0.0007778\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "647:\tlearn: 0.0007778\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "648:\tlearn: 0.0007778\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "649:\tlearn: 0.0007778\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "650:\tlearn: 0.0007778\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "651:\tlearn: 0.0007777\ttotal: 1m 54s\tremaining: 1m\n",
      "652:\tlearn: 0.0007777\ttotal: 1m 54s\tremaining: 1m\n",
      "653:\tlearn: 0.0007777\ttotal: 1m 54s\tremaining: 1m\n",
      "654:\tlearn: 0.0007777\ttotal: 1m 54s\tremaining: 1m\n",
      "655:\tlearn: 0.0007776\ttotal: 1m 54s\tremaining: 1m\n",
      "656:\tlearn: 0.0007774\ttotal: 1m 54s\tremaining: 60s\n",
      "657:\tlearn: 0.0007773\ttotal: 1m 55s\tremaining: 59.8s\n",
      "658:\tlearn: 0.0007773\ttotal: 1m 55s\tremaining: 59.6s\n",
      "659:\tlearn: 0.0007772\ttotal: 1m 55s\tremaining: 59.5s\n",
      "660:\tlearn: 0.0007771\ttotal: 1m 55s\tremaining: 59.3s\n",
      "661:\tlearn: 0.0007771\ttotal: 1m 55s\tremaining: 59.1s\n",
      "662:\tlearn: 0.0007770\ttotal: 1m 55s\tremaining: 58.9s\n",
      "663:\tlearn: 0.0007770\ttotal: 1m 56s\tremaining: 58.8s\n",
      "664:\tlearn: 0.0007770\ttotal: 1m 56s\tremaining: 58.6s\n",
      "665:\tlearn: 0.0007769\ttotal: 1m 56s\tremaining: 58.4s\n",
      "666:\tlearn: 0.0007768\ttotal: 1m 56s\tremaining: 58.3s\n",
      "667:\tlearn: 0.0007768\ttotal: 1m 56s\tremaining: 58.1s\n",
      "668:\tlearn: 0.0007768\ttotal: 1m 57s\tremaining: 57.9s\n",
      "669:\tlearn: 0.0007768\ttotal: 1m 57s\tremaining: 57.7s\n",
      "670:\tlearn: 0.0007768\ttotal: 1m 57s\tremaining: 57.6s\n",
      "671:\tlearn: 0.0007767\ttotal: 1m 57s\tremaining: 57.4s\n",
      "672:\tlearn: 0.0007767\ttotal: 1m 57s\tremaining: 57.2s\n",
      "673:\tlearn: 0.0007765\ttotal: 1m 57s\tremaining: 57s\n",
      "674:\tlearn: 0.0007765\ttotal: 1m 58s\tremaining: 56.9s\n",
      "675:\tlearn: 0.0007750\ttotal: 1m 58s\tremaining: 56.7s\n",
      "676:\tlearn: 0.0007749\ttotal: 1m 58s\tremaining: 56.5s\n",
      "677:\tlearn: 0.0007746\ttotal: 1m 58s\tremaining: 56.3s\n",
      "678:\tlearn: 0.0007746\ttotal: 1m 58s\tremaining: 56.2s\n",
      "679:\tlearn: 0.0007745\ttotal: 1m 58s\tremaining: 56s\n",
      "680:\tlearn: 0.0007706\ttotal: 1m 59s\tremaining: 55.8s\n",
      "681:\tlearn: 0.0007705\ttotal: 1m 59s\tremaining: 55.6s\n",
      "682:\tlearn: 0.0007705\ttotal: 1m 59s\tremaining: 55.5s\n",
      "683:\tlearn: 0.0007705\ttotal: 1m 59s\tremaining: 55.3s\n",
      "684:\tlearn: 0.0007705\ttotal: 1m 59s\tremaining: 55.1s\n",
      "685:\tlearn: 0.0007705\ttotal: 2m\tremaining: 54.9s\n",
      "686:\tlearn: 0.0007697\ttotal: 2m\tremaining: 54.8s\n",
      "687:\tlearn: 0.0007696\ttotal: 2m\tremaining: 54.6s\n",
      "688:\tlearn: 0.0007694\ttotal: 2m\tremaining: 54.4s\n",
      "689:\tlearn: 0.0007694\ttotal: 2m\tremaining: 54.2s\n",
      "690:\tlearn: 0.0007666\ttotal: 2m\tremaining: 54.1s\n",
      "691:\tlearn: 0.0007666\ttotal: 2m 1s\tremaining: 53.9s\n",
      "692:\tlearn: 0.0007665\ttotal: 2m 1s\tremaining: 53.7s\n",
      "693:\tlearn: 0.0007665\ttotal: 2m 1s\tremaining: 53.5s\n",
      "694:\tlearn: 0.0007665\ttotal: 2m 1s\tremaining: 53.4s\n",
      "695:\tlearn: 0.0007665\ttotal: 2m 1s\tremaining: 53.2s\n",
      "696:\tlearn: 0.0007634\ttotal: 2m 1s\tremaining: 53s\n",
      "697:\tlearn: 0.0007634\ttotal: 2m 2s\tremaining: 52.8s\n",
      "698:\tlearn: 0.0007633\ttotal: 2m 2s\tremaining: 52.7s\n",
      "699:\tlearn: 0.0007632\ttotal: 2m 2s\tremaining: 52.5s\n",
      "700:\tlearn: 0.0007630\ttotal: 2m 2s\tremaining: 52.3s\n",
      "701:\tlearn: 0.0007630\ttotal: 2m 2s\tremaining: 52.1s\n",
      "702:\tlearn: 0.0007623\ttotal: 2m 2s\tremaining: 52s\n",
      "703:\tlearn: 0.0007623\ttotal: 2m 3s\tremaining: 51.8s\n",
      "704:\tlearn: 0.0007623\ttotal: 2m 3s\tremaining: 51.6s\n",
      "705:\tlearn: 0.0007622\ttotal: 2m 3s\tremaining: 51.4s\n",
      "706:\tlearn: 0.0007622\ttotal: 2m 3s\tremaining: 51.3s\n",
      "707:\tlearn: 0.0007621\ttotal: 2m 3s\tremaining: 51.1s\n",
      "708:\tlearn: 0.0007620\ttotal: 2m 4s\tremaining: 50.9s\n",
      "709:\tlearn: 0.0007615\ttotal: 2m 4s\tremaining: 50.7s\n",
      "710:\tlearn: 0.0007611\ttotal: 2m 4s\tremaining: 50.6s\n",
      "711:\tlearn: 0.0007607\ttotal: 2m 4s\tremaining: 50.4s\n",
      "712:\tlearn: 0.0007604\ttotal: 2m 4s\tremaining: 50.2s\n",
      "713:\tlearn: 0.0007603\ttotal: 2m 4s\tremaining: 50s\n",
      "714:\tlearn: 0.0007603\ttotal: 2m 5s\tremaining: 49.9s\n",
      "715:\tlearn: 0.0007602\ttotal: 2m 5s\tremaining: 49.7s\n",
      "716:\tlearn: 0.0007602\ttotal: 2m 5s\tremaining: 49.5s\n",
      "717:\tlearn: 0.0007602\ttotal: 2m 5s\tremaining: 49.3s\n",
      "718:\tlearn: 0.0007602\ttotal: 2m 5s\tremaining: 49.2s\n",
      "719:\tlearn: 0.0007601\ttotal: 2m 5s\tremaining: 49s\n",
      "720:\tlearn: 0.0007598\ttotal: 2m 6s\tremaining: 48.8s\n",
      "721:\tlearn: 0.0007597\ttotal: 2m 6s\tremaining: 48.6s\n",
      "722:\tlearn: 0.0007597\ttotal: 2m 6s\tremaining: 48.5s\n",
      "723:\tlearn: 0.0007597\ttotal: 2m 6s\tremaining: 48.3s\n",
      "724:\tlearn: 0.0007597\ttotal: 2m 6s\tremaining: 48.1s\n",
      "725:\tlearn: 0.0007596\ttotal: 2m 7s\tremaining: 47.9s\n",
      "726:\tlearn: 0.0007596\ttotal: 2m 7s\tremaining: 47.8s\n",
      "727:\tlearn: 0.0007595\ttotal: 2m 7s\tremaining: 47.6s\n",
      "728:\tlearn: 0.0007542\ttotal: 2m 7s\tremaining: 47.4s\n",
      "729:\tlearn: 0.0007538\ttotal: 2m 7s\tremaining: 47.2s\n",
      "730:\tlearn: 0.0007535\ttotal: 2m 7s\tremaining: 47.1s\n",
      "731:\tlearn: 0.0007535\ttotal: 2m 8s\tremaining: 46.9s\n",
      "732:\tlearn: 0.0007534\ttotal: 2m 8s\tremaining: 46.7s\n",
      "733:\tlearn: 0.0007524\ttotal: 2m 8s\tremaining: 46.5s\n",
      "734:\tlearn: 0.0007524\ttotal: 2m 8s\tremaining: 46.4s\n",
      "735:\tlearn: 0.0007523\ttotal: 2m 8s\tremaining: 46.2s\n",
      "736:\tlearn: 0.0007523\ttotal: 2m 8s\tremaining: 46s\n",
      "737:\tlearn: 0.0007523\ttotal: 2m 9s\tremaining: 45.8s\n",
      "738:\tlearn: 0.0007520\ttotal: 2m 9s\tremaining: 45.7s\n",
      "739:\tlearn: 0.0007519\ttotal: 2m 9s\tremaining: 45.5s\n",
      "740:\tlearn: 0.0007514\ttotal: 2m 9s\tremaining: 45.3s\n",
      "741:\tlearn: 0.0007512\ttotal: 2m 9s\tremaining: 45.1s\n",
      "742:\tlearn: 0.0007512\ttotal: 2m 10s\tremaining: 45s\n",
      "743:\tlearn: 0.0007511\ttotal: 2m 10s\tremaining: 44.8s\n",
      "744:\tlearn: 0.0007511\ttotal: 2m 10s\tremaining: 44.6s\n",
      "745:\tlearn: 0.0007511\ttotal: 2m 10s\tremaining: 44.5s\n",
      "746:\tlearn: 0.0007511\ttotal: 2m 10s\tremaining: 44.3s\n",
      "747:\tlearn: 0.0007459\ttotal: 2m 11s\tremaining: 44.1s\n",
      "748:\tlearn: 0.0007458\ttotal: 2m 11s\tremaining: 44s\n",
      "749:\tlearn: 0.0007458\ttotal: 2m 11s\tremaining: 43.8s\n",
      "750:\tlearn: 0.0007456\ttotal: 2m 11s\tremaining: 43.6s\n",
      "751:\tlearn: 0.0007456\ttotal: 2m 11s\tremaining: 43.5s\n",
      "752:\tlearn: 0.0007456\ttotal: 2m 12s\tremaining: 43.3s\n",
      "753:\tlearn: 0.0007455\ttotal: 2m 12s\tremaining: 43.1s\n",
      "754:\tlearn: 0.0007453\ttotal: 2m 12s\tremaining: 43s\n",
      "755:\tlearn: 0.0007453\ttotal: 2m 12s\tremaining: 42.8s\n",
      "756:\tlearn: 0.0007453\ttotal: 2m 12s\tremaining: 42.6s\n",
      "757:\tlearn: 0.0007453\ttotal: 2m 13s\tremaining: 42.5s\n",
      "758:\tlearn: 0.0007453\ttotal: 2m 13s\tremaining: 42.3s\n",
      "759:\tlearn: 0.0007417\ttotal: 2m 13s\tremaining: 42.1s\n",
      "760:\tlearn: 0.0007415\ttotal: 2m 13s\tremaining: 42s\n",
      "761:\tlearn: 0.0007413\ttotal: 2m 13s\tremaining: 41.8s\n",
      "762:\tlearn: 0.0007413\ttotal: 2m 13s\tremaining: 41.6s\n",
      "763:\tlearn: 0.0007413\ttotal: 2m 14s\tremaining: 41.5s\n",
      "764:\tlearn: 0.0007413\ttotal: 2m 14s\tremaining: 41.3s\n",
      "765:\tlearn: 0.0007412\ttotal: 2m 14s\tremaining: 41.1s\n",
      "766:\tlearn: 0.0007412\ttotal: 2m 14s\tremaining: 40.9s\n",
      "767:\tlearn: 0.0007409\ttotal: 2m 14s\tremaining: 40.8s\n",
      "768:\tlearn: 0.0007409\ttotal: 2m 15s\tremaining: 40.6s\n",
      "769:\tlearn: 0.0007409\ttotal: 2m 15s\tremaining: 40.4s\n",
      "770:\tlearn: 0.0007407\ttotal: 2m 15s\tremaining: 40.3s\n",
      "771:\tlearn: 0.0007406\ttotal: 2m 15s\tremaining: 40.1s\n",
      "772:\tlearn: 0.0007405\ttotal: 2m 15s\tremaining: 39.9s\n",
      "773:\tlearn: 0.0007404\ttotal: 2m 16s\tremaining: 39.8s\n",
      "774:\tlearn: 0.0007402\ttotal: 2m 16s\tremaining: 39.6s\n",
      "775:\tlearn: 0.0007402\ttotal: 2m 16s\tremaining: 39.4s\n",
      "776:\tlearn: 0.0007401\ttotal: 2m 16s\tremaining: 39.3s\n",
      "777:\tlearn: 0.0007400\ttotal: 2m 16s\tremaining: 39.1s\n",
      "778:\tlearn: 0.0007399\ttotal: 2m 17s\tremaining: 38.9s\n",
      "779:\tlearn: 0.0007396\ttotal: 2m 17s\tremaining: 38.7s\n",
      "780:\tlearn: 0.0007395\ttotal: 2m 17s\tremaining: 38.6s\n",
      "781:\tlearn: 0.0007395\ttotal: 2m 17s\tremaining: 38.4s\n",
      "782:\tlearn: 0.0007394\ttotal: 2m 17s\tremaining: 38.2s\n",
      "783:\tlearn: 0.0007392\ttotal: 2m 18s\tremaining: 38s\n",
      "784:\tlearn: 0.0007391\ttotal: 2m 18s\tremaining: 37.9s\n",
      "785:\tlearn: 0.0007390\ttotal: 2m 18s\tremaining: 37.7s\n",
      "786:\tlearn: 0.0007388\ttotal: 2m 18s\tremaining: 37.5s\n",
      "787:\tlearn: 0.0007388\ttotal: 2m 18s\tremaining: 37.3s\n",
      "788:\tlearn: 0.0007387\ttotal: 2m 18s\tremaining: 37.2s\n",
      "789:\tlearn: 0.0007387\ttotal: 2m 19s\tremaining: 37s\n",
      "790:\tlearn: 0.0007387\ttotal: 2m 19s\tremaining: 36.8s\n",
      "791:\tlearn: 0.0007386\ttotal: 2m 19s\tremaining: 36.6s\n",
      "792:\tlearn: 0.0007386\ttotal: 2m 19s\tremaining: 36.4s\n",
      "793:\tlearn: 0.0007386\ttotal: 2m 19s\tremaining: 36.3s\n",
      "794:\tlearn: 0.0007386\ttotal: 2m 19s\tremaining: 36.1s\n",
      "795:\tlearn: 0.0007382\ttotal: 2m 20s\tremaining: 35.9s\n",
      "796:\tlearn: 0.0007382\ttotal: 2m 20s\tremaining: 35.7s\n",
      "797:\tlearn: 0.0007378\ttotal: 2m 20s\tremaining: 35.6s\n",
      "798:\tlearn: 0.0007378\ttotal: 2m 20s\tremaining: 35.4s\n",
      "799:\tlearn: 0.0007373\ttotal: 2m 20s\tremaining: 35.2s\n",
      "800:\tlearn: 0.0007373\ttotal: 2m 21s\tremaining: 35s\n",
      "801:\tlearn: 0.0007373\ttotal: 2m 21s\tremaining: 34.9s\n",
      "802:\tlearn: 0.0007372\ttotal: 2m 21s\tremaining: 34.7s\n",
      "803:\tlearn: 0.0007371\ttotal: 2m 21s\tremaining: 34.5s\n",
      "804:\tlearn: 0.0007370\ttotal: 2m 21s\tremaining: 34.3s\n",
      "805:\tlearn: 0.0007368\ttotal: 2m 21s\tremaining: 34.2s\n",
      "806:\tlearn: 0.0007367\ttotal: 2m 22s\tremaining: 34s\n",
      "807:\tlearn: 0.0007366\ttotal: 2m 22s\tremaining: 33.8s\n",
      "808:\tlearn: 0.0007364\ttotal: 2m 22s\tremaining: 33.6s\n",
      "809:\tlearn: 0.0007363\ttotal: 2m 22s\tremaining: 33.4s\n",
      "810:\tlearn: 0.0007363\ttotal: 2m 22s\tremaining: 33.3s\n",
      "811:\tlearn: 0.0007362\ttotal: 2m 22s\tremaining: 33.1s\n",
      "812:\tlearn: 0.0007327\ttotal: 2m 23s\tremaining: 32.9s\n",
      "813:\tlearn: 0.0007327\ttotal: 2m 23s\tremaining: 32.7s\n",
      "814:\tlearn: 0.0007327\ttotal: 2m 23s\tremaining: 32.6s\n",
      "815:\tlearn: 0.0007327\ttotal: 2m 23s\tremaining: 32.4s\n",
      "816:\tlearn: 0.0007326\ttotal: 2m 23s\tremaining: 32.2s\n",
      "817:\tlearn: 0.0007324\ttotal: 2m 24s\tremaining: 32s\n",
      "818:\tlearn: 0.0007320\ttotal: 2m 24s\tremaining: 31.9s\n",
      "819:\tlearn: 0.0007319\ttotal: 2m 24s\tremaining: 31.7s\n",
      "820:\tlearn: 0.0007317\ttotal: 2m 24s\tremaining: 31.5s\n",
      "821:\tlearn: 0.0007317\ttotal: 2m 24s\tremaining: 31.3s\n",
      "822:\tlearn: 0.0007317\ttotal: 2m 24s\tremaining: 31.2s\n",
      "823:\tlearn: 0.0007317\ttotal: 2m 25s\tremaining: 31s\n",
      "824:\tlearn: 0.0007317\ttotal: 2m 25s\tremaining: 30.8s\n",
      "825:\tlearn: 0.0007317\ttotal: 2m 25s\tremaining: 30.6s\n",
      "826:\tlearn: 0.0007316\ttotal: 2m 25s\tremaining: 30.5s\n",
      "827:\tlearn: 0.0007294\ttotal: 2m 25s\tremaining: 30.3s\n",
      "828:\tlearn: 0.0007292\ttotal: 2m 25s\tremaining: 30.1s\n",
      "829:\tlearn: 0.0007291\ttotal: 2m 26s\tremaining: 29.9s\n",
      "830:\tlearn: 0.0007288\ttotal: 2m 26s\tremaining: 29.8s\n",
      "831:\tlearn: 0.0007287\ttotal: 2m 26s\tremaining: 29.6s\n",
      "832:\tlearn: 0.0007286\ttotal: 2m 26s\tremaining: 29.4s\n",
      "833:\tlearn: 0.0007286\ttotal: 2m 26s\tremaining: 29.2s\n",
      "834:\tlearn: 0.0007286\ttotal: 2m 26s\tremaining: 29s\n",
      "835:\tlearn: 0.0007286\ttotal: 2m 27s\tremaining: 28.9s\n",
      "836:\tlearn: 0.0007285\ttotal: 2m 27s\tremaining: 28.7s\n",
      "837:\tlearn: 0.0007284\ttotal: 2m 27s\tremaining: 28.5s\n",
      "838:\tlearn: 0.0007282\ttotal: 2m 27s\tremaining: 28.3s\n",
      "839:\tlearn: 0.0007282\ttotal: 2m 27s\tremaining: 28.2s\n",
      "840:\tlearn: 0.0007282\ttotal: 2m 28s\tremaining: 28s\n",
      "841:\tlearn: 0.0007282\ttotal: 2m 28s\tremaining: 27.8s\n",
      "842:\tlearn: 0.0007224\ttotal: 2m 28s\tremaining: 27.6s\n",
      "843:\tlearn: 0.0007222\ttotal: 2m 28s\tremaining: 27.5s\n",
      "844:\tlearn: 0.0007222\ttotal: 2m 28s\tremaining: 27.3s\n",
      "845:\tlearn: 0.0007222\ttotal: 2m 28s\tremaining: 27.1s\n",
      "846:\tlearn: 0.0007222\ttotal: 2m 29s\tremaining: 26.9s\n",
      "847:\tlearn: 0.0007220\ttotal: 2m 29s\tremaining: 26.8s\n",
      "848:\tlearn: 0.0007220\ttotal: 2m 29s\tremaining: 26.6s\n",
      "849:\tlearn: 0.0007220\ttotal: 2m 29s\tremaining: 26.4s\n",
      "850:\tlearn: 0.0007220\ttotal: 2m 29s\tremaining: 26.2s\n",
      "851:\tlearn: 0.0007219\ttotal: 2m 29s\tremaining: 26s\n",
      "852:\tlearn: 0.0007219\ttotal: 2m 30s\tremaining: 25.9s\n",
      "853:\tlearn: 0.0007218\ttotal: 2m 30s\tremaining: 25.7s\n",
      "854:\tlearn: 0.0007216\ttotal: 2m 30s\tremaining: 25.5s\n",
      "855:\tlearn: 0.0007216\ttotal: 2m 30s\tremaining: 25.3s\n",
      "856:\tlearn: 0.0007216\ttotal: 2m 30s\tremaining: 25.2s\n",
      "857:\tlearn: 0.0007216\ttotal: 2m 31s\tremaining: 25s\n",
      "858:\tlearn: 0.0007215\ttotal: 2m 31s\tremaining: 24.8s\n",
      "859:\tlearn: 0.0007215\ttotal: 2m 31s\tremaining: 24.6s\n",
      "860:\tlearn: 0.0007213\ttotal: 2m 31s\tremaining: 24.5s\n",
      "861:\tlearn: 0.0007212\ttotal: 2m 31s\tremaining: 24.3s\n",
      "862:\tlearn: 0.0007212\ttotal: 2m 31s\tremaining: 24.1s\n",
      "863:\tlearn: 0.0007211\ttotal: 2m 32s\tremaining: 23.9s\n",
      "864:\tlearn: 0.0007209\ttotal: 2m 32s\tremaining: 23.8s\n",
      "865:\tlearn: 0.0007170\ttotal: 2m 32s\tremaining: 23.6s\n",
      "866:\tlearn: 0.0007170\ttotal: 2m 32s\tremaining: 23.4s\n",
      "867:\tlearn: 0.0007170\ttotal: 2m 32s\tremaining: 23.2s\n",
      "868:\tlearn: 0.0007169\ttotal: 2m 32s\tremaining: 23.1s\n",
      "869:\tlearn: 0.0007164\ttotal: 2m 33s\tremaining: 22.9s\n",
      "870:\tlearn: 0.0007142\ttotal: 2m 33s\tremaining: 22.7s\n",
      "871:\tlearn: 0.0007142\ttotal: 2m 33s\tremaining: 22.5s\n",
      "872:\tlearn: 0.0007140\ttotal: 2m 33s\tremaining: 22.3s\n",
      "873:\tlearn: 0.0007139\ttotal: 2m 33s\tremaining: 22.2s\n",
      "874:\tlearn: 0.0007136\ttotal: 2m 34s\tremaining: 22s\n",
      "875:\tlearn: 0.0007136\ttotal: 2m 34s\tremaining: 21.8s\n",
      "876:\tlearn: 0.0007134\ttotal: 2m 34s\tremaining: 21.7s\n",
      "877:\tlearn: 0.0007134\ttotal: 2m 34s\tremaining: 21.5s\n",
      "878:\tlearn: 0.0007134\ttotal: 2m 34s\tremaining: 21.3s\n",
      "879:\tlearn: 0.0007133\ttotal: 2m 34s\tremaining: 21.1s\n",
      "880:\tlearn: 0.0007133\ttotal: 2m 35s\tremaining: 21s\n",
      "881:\tlearn: 0.0007133\ttotal: 2m 35s\tremaining: 20.8s\n",
      "882:\tlearn: 0.0007132\ttotal: 2m 35s\tremaining: 20.6s\n",
      "883:\tlearn: 0.0007132\ttotal: 2m 35s\tremaining: 20.4s\n",
      "884:\tlearn: 0.0007131\ttotal: 2m 35s\tremaining: 20.2s\n",
      "885:\tlearn: 0.0007131\ttotal: 2m 35s\tremaining: 20.1s\n",
      "886:\tlearn: 0.0007130\ttotal: 2m 36s\tremaining: 19.9s\n",
      "887:\tlearn: 0.0007129\ttotal: 2m 36s\tremaining: 19.7s\n",
      "888:\tlearn: 0.0007105\ttotal: 2m 36s\tremaining: 19.5s\n",
      "889:\tlearn: 0.0007105\ttotal: 2m 36s\tremaining: 19.4s\n",
      "890:\tlearn: 0.0007105\ttotal: 2m 36s\tremaining: 19.2s\n",
      "891:\tlearn: 0.0007104\ttotal: 2m 37s\tremaining: 19s\n",
      "892:\tlearn: 0.0007104\ttotal: 2m 37s\tremaining: 18.8s\n",
      "893:\tlearn: 0.0007104\ttotal: 2m 37s\tremaining: 18.7s\n",
      "894:\tlearn: 0.0007064\ttotal: 2m 37s\tremaining: 18.5s\n",
      "895:\tlearn: 0.0007064\ttotal: 2m 37s\tremaining: 18.3s\n",
      "896:\tlearn: 0.0007064\ttotal: 2m 37s\tremaining: 18.1s\n",
      "897:\tlearn: 0.0007064\ttotal: 2m 38s\tremaining: 18s\n",
      "898:\tlearn: 0.0007063\ttotal: 2m 38s\tremaining: 17.8s\n",
      "899:\tlearn: 0.0007063\ttotal: 2m 38s\tremaining: 17.6s\n",
      "900:\tlearn: 0.0007062\ttotal: 2m 38s\tremaining: 17.4s\n",
      "901:\tlearn: 0.0007062\ttotal: 2m 38s\tremaining: 17.3s\n",
      "902:\tlearn: 0.0007055\ttotal: 2m 38s\tremaining: 17.1s\n",
      "903:\tlearn: 0.0007054\ttotal: 2m 39s\tremaining: 16.9s\n",
      "904:\tlearn: 0.0007054\ttotal: 2m 39s\tremaining: 16.7s\n",
      "905:\tlearn: 0.0007054\ttotal: 2m 39s\tremaining: 16.5s\n",
      "906:\tlearn: 0.0007053\ttotal: 2m 39s\tremaining: 16.4s\n",
      "907:\tlearn: 0.0007053\ttotal: 2m 39s\tremaining: 16.2s\n",
      "908:\tlearn: 0.0007053\ttotal: 2m 40s\tremaining: 16s\n",
      "909:\tlearn: 0.0007053\ttotal: 2m 40s\tremaining: 15.8s\n",
      "910:\tlearn: 0.0007053\ttotal: 2m 40s\tremaining: 15.7s\n",
      "911:\tlearn: 0.0007050\ttotal: 2m 40s\tremaining: 15.5s\n",
      "912:\tlearn: 0.0007050\ttotal: 2m 40s\tremaining: 15.3s\n",
      "913:\tlearn: 0.0007050\ttotal: 2m 40s\tremaining: 15.1s\n",
      "914:\tlearn: 0.0007050\ttotal: 2m 41s\tremaining: 15s\n",
      "915:\tlearn: 0.0007046\ttotal: 2m 41s\tremaining: 14.8s\n",
      "916:\tlearn: 0.0007046\ttotal: 2m 41s\tremaining: 14.6s\n",
      "917:\tlearn: 0.0007044\ttotal: 2m 41s\tremaining: 14.4s\n",
      "918:\tlearn: 0.0007044\ttotal: 2m 41s\tremaining: 14.3s\n",
      "919:\tlearn: 0.0007038\ttotal: 2m 41s\tremaining: 14.1s\n",
      "920:\tlearn: 0.0007038\ttotal: 2m 42s\tremaining: 13.9s\n",
      "921:\tlearn: 0.0007036\ttotal: 2m 42s\tremaining: 13.7s\n",
      "922:\tlearn: 0.0007035\ttotal: 2m 42s\tremaining: 13.6s\n",
      "923:\tlearn: 0.0007034\ttotal: 2m 42s\tremaining: 13.4s\n",
      "924:\tlearn: 0.0007034\ttotal: 2m 42s\tremaining: 13.2s\n",
      "925:\tlearn: 0.0007034\ttotal: 2m 42s\tremaining: 13s\n",
      "926:\tlearn: 0.0007002\ttotal: 2m 43s\tremaining: 12.8s\n",
      "927:\tlearn: 0.0006999\ttotal: 2m 43s\tremaining: 12.7s\n",
      "928:\tlearn: 0.0006998\ttotal: 2m 43s\tremaining: 12.5s\n",
      "929:\tlearn: 0.0006998\ttotal: 2m 43s\tremaining: 12.3s\n",
      "930:\tlearn: 0.0006996\ttotal: 2m 43s\tremaining: 12.1s\n",
      "931:\tlearn: 0.0006996\ttotal: 2m 44s\tremaining: 12s\n",
      "932:\tlearn: 0.0006995\ttotal: 2m 44s\tremaining: 11.8s\n",
      "933:\tlearn: 0.0006995\ttotal: 2m 44s\tremaining: 11.6s\n",
      "934:\tlearn: 0.0006995\ttotal: 2m 44s\tremaining: 11.4s\n",
      "935:\tlearn: 0.0006992\ttotal: 2m 44s\tremaining: 11.3s\n",
      "936:\tlearn: 0.0006992\ttotal: 2m 44s\tremaining: 11.1s\n",
      "937:\tlearn: 0.0006991\ttotal: 2m 45s\tremaining: 10.9s\n",
      "938:\tlearn: 0.0006991\ttotal: 2m 45s\tremaining: 10.7s\n",
      "939:\tlearn: 0.0006989\ttotal: 2m 45s\tremaining: 10.6s\n",
      "940:\tlearn: 0.0006989\ttotal: 2m 45s\tremaining: 10.4s\n",
      "941:\tlearn: 0.0006988\ttotal: 2m 45s\tremaining: 10.2s\n",
      "942:\tlearn: 0.0006988\ttotal: 2m 45s\tremaining: 10s\n",
      "943:\tlearn: 0.0006988\ttotal: 2m 46s\tremaining: 9.85s\n",
      "944:\tlearn: 0.0006986\ttotal: 2m 46s\tremaining: 9.68s\n",
      "945:\tlearn: 0.0006986\ttotal: 2m 46s\tremaining: 9.5s\n",
      "946:\tlearn: 0.0006985\ttotal: 2m 46s\tremaining: 9.32s\n",
      "947:\tlearn: 0.0006985\ttotal: 2m 46s\tremaining: 9.15s\n",
      "948:\tlearn: 0.0006985\ttotal: 2m 46s\tremaining: 8.97s\n",
      "949:\tlearn: 0.0006985\ttotal: 2m 47s\tremaining: 8.8s\n",
      "950:\tlearn: 0.0006985\ttotal: 2m 47s\tremaining: 8.62s\n",
      "951:\tlearn: 0.0006985\ttotal: 2m 47s\tremaining: 8.44s\n",
      "952:\tlearn: 0.0006985\ttotal: 2m 47s\tremaining: 8.27s\n",
      "953:\tlearn: 0.0006985\ttotal: 2m 47s\tremaining: 8.09s\n",
      "954:\tlearn: 0.0006985\ttotal: 2m 48s\tremaining: 7.92s\n",
      "955:\tlearn: 0.0006983\ttotal: 2m 48s\tremaining: 7.74s\n",
      "956:\tlearn: 0.0006982\ttotal: 2m 48s\tremaining: 7.56s\n",
      "957:\tlearn: 0.0006981\ttotal: 2m 48s\tremaining: 7.39s\n",
      "958:\tlearn: 0.0006981\ttotal: 2m 48s\tremaining: 7.21s\n",
      "959:\tlearn: 0.0006981\ttotal: 2m 48s\tremaining: 7.04s\n",
      "960:\tlearn: 0.0006981\ttotal: 2m 49s\tremaining: 6.86s\n",
      "961:\tlearn: 0.0006980\ttotal: 2m 49s\tremaining: 6.68s\n",
      "962:\tlearn: 0.0006980\ttotal: 2m 49s\tremaining: 6.51s\n",
      "963:\tlearn: 0.0006979\ttotal: 2m 49s\tremaining: 6.33s\n",
      "964:\tlearn: 0.0006978\ttotal: 2m 49s\tremaining: 6.16s\n",
      "965:\tlearn: 0.0006978\ttotal: 2m 49s\tremaining: 5.98s\n",
      "966:\tlearn: 0.0006978\ttotal: 2m 50s\tremaining: 5.8s\n",
      "967:\tlearn: 0.0006978\ttotal: 2m 50s\tremaining: 5.63s\n",
      "968:\tlearn: 0.0006977\ttotal: 2m 50s\tremaining: 5.45s\n",
      "969:\tlearn: 0.0006977\ttotal: 2m 50s\tremaining: 5.28s\n",
      "970:\tlearn: 0.0006977\ttotal: 2m 50s\tremaining: 5.1s\n",
      "971:\tlearn: 0.0006976\ttotal: 2m 50s\tremaining: 4.92s\n",
      "972:\tlearn: 0.0006976\ttotal: 2m 51s\tremaining: 4.75s\n",
      "973:\tlearn: 0.0006976\ttotal: 2m 51s\tremaining: 4.57s\n",
      "974:\tlearn: 0.0006976\ttotal: 2m 51s\tremaining: 4.4s\n",
      "975:\tlearn: 0.0006973\ttotal: 2m 51s\tremaining: 4.22s\n",
      "976:\tlearn: 0.0006973\ttotal: 2m 51s\tremaining: 4.04s\n",
      "977:\tlearn: 0.0006970\ttotal: 2m 52s\tremaining: 3.87s\n",
      "978:\tlearn: 0.0006969\ttotal: 2m 52s\tremaining: 3.69s\n",
      "979:\tlearn: 0.0006969\ttotal: 2m 52s\tremaining: 3.52s\n",
      "980:\tlearn: 0.0006969\ttotal: 2m 52s\tremaining: 3.34s\n",
      "981:\tlearn: 0.0006968\ttotal: 2m 52s\tremaining: 3.17s\n",
      "982:\tlearn: 0.0006968\ttotal: 2m 52s\tremaining: 2.99s\n",
      "983:\tlearn: 0.0006967\ttotal: 2m 53s\tremaining: 2.81s\n",
      "984:\tlearn: 0.0006967\ttotal: 2m 53s\tremaining: 2.64s\n",
      "985:\tlearn: 0.0006966\ttotal: 2m 53s\tremaining: 2.46s\n",
      "986:\tlearn: 0.0006966\ttotal: 2m 53s\tremaining: 2.29s\n",
      "987:\tlearn: 0.0006961\ttotal: 2m 53s\tremaining: 2.11s\n",
      "988:\tlearn: 0.0006961\ttotal: 2m 54s\tremaining: 1.94s\n",
      "989:\tlearn: 0.0006960\ttotal: 2m 54s\tremaining: 1.76s\n",
      "990:\tlearn: 0.0006958\ttotal: 2m 54s\tremaining: 1.58s\n",
      "991:\tlearn: 0.0006957\ttotal: 2m 54s\tremaining: 1.41s\n",
      "992:\tlearn: 0.0006956\ttotal: 2m 54s\tremaining: 1.23s\n",
      "993:\tlearn: 0.0006956\ttotal: 2m 54s\tremaining: 1.05s\n",
      "994:\tlearn: 0.0006933\ttotal: 2m 55s\tremaining: 880ms\n",
      "995:\tlearn: 0.0006932\ttotal: 2m 55s\tremaining: 704ms\n",
      "996:\tlearn: 0.0006931\ttotal: 2m 55s\tremaining: 528ms\n",
      "997:\tlearn: 0.0006929\ttotal: 2m 55s\tremaining: 352ms\n",
      "998:\tlearn: 0.0006929\ttotal: 2m 55s\tremaining: 176ms\n",
      "999:\tlearn: 0.0006929\ttotal: 2m 55s\tremaining: 0us\n",
      "0:\tlearn: 0.5361128\ttotal: 207ms\tremaining: 3m 27s\n",
      "1:\tlearn: 0.4013558\ttotal: 381ms\tremaining: 3m 10s\n",
      "2:\tlearn: 0.3223873\ttotal: 560ms\tremaining: 3m 6s\n",
      "3:\tlearn: 0.2744144\ttotal: 733ms\tremaining: 3m 2s\n",
      "4:\tlearn: 0.2060232\ttotal: 908ms\tremaining: 3m\n",
      "5:\tlearn: 0.1711624\ttotal: 1.08s\tremaining: 2m 59s\n",
      "6:\tlearn: 0.1392475\ttotal: 1.27s\tremaining: 2m 59s\n",
      "7:\tlearn: 0.1361894\ttotal: 1.44s\tremaining: 2m 58s\n",
      "8:\tlearn: 0.0978466\ttotal: 1.61s\tremaining: 2m 57s\n",
      "9:\tlearn: 0.0738132\ttotal: 1.79s\tremaining: 2m 57s\n",
      "10:\tlearn: 0.0697898\ttotal: 1.96s\tremaining: 2m 56s\n",
      "11:\tlearn: 0.0520417\ttotal: 2.14s\tremaining: 2m 56s\n",
      "12:\tlearn: 0.0421592\ttotal: 2.31s\tremaining: 2m 55s\n",
      "13:\tlearn: 0.0355017\ttotal: 2.49s\tremaining: 2m 55s\n",
      "14:\tlearn: 0.0290764\ttotal: 2.66s\tremaining: 2m 54s\n",
      "15:\tlearn: 0.0236121\ttotal: 2.83s\tremaining: 2m 54s\n",
      "16:\tlearn: 0.0212190\ttotal: 3.02s\tremaining: 2m 54s\n",
      "17:\tlearn: 0.0184976\ttotal: 3.19s\tremaining: 2m 54s\n",
      "18:\tlearn: 0.0168595\ttotal: 3.37s\tremaining: 2m 53s\n",
      "19:\tlearn: 0.0150562\ttotal: 3.54s\tremaining: 2m 53s\n",
      "20:\tlearn: 0.0133888\ttotal: 3.72s\tremaining: 2m 53s\n",
      "21:\tlearn: 0.0123617\ttotal: 3.89s\tremaining: 2m 53s\n",
      "22:\tlearn: 0.0114412\ttotal: 4.06s\tremaining: 2m 52s\n",
      "23:\tlearn: 0.0106339\ttotal: 4.24s\tremaining: 2m 52s\n",
      "24:\tlearn: 0.0097909\ttotal: 4.41s\tremaining: 2m 52s\n",
      "25:\tlearn: 0.0090725\ttotal: 4.59s\tremaining: 2m 51s\n",
      "26:\tlearn: 0.0085441\ttotal: 4.76s\tremaining: 2m 51s\n",
      "27:\tlearn: 0.0080068\ttotal: 4.93s\tremaining: 2m 51s\n",
      "28:\tlearn: 0.0076145\ttotal: 5.11s\tremaining: 2m 51s\n",
      "29:\tlearn: 0.0073764\ttotal: 5.29s\tremaining: 2m 51s\n",
      "30:\tlearn: 0.0068329\ttotal: 5.46s\tremaining: 2m 50s\n",
      "31:\tlearn: 0.0064840\ttotal: 5.64s\tremaining: 2m 50s\n",
      "32:\tlearn: 0.0061307\ttotal: 5.82s\tremaining: 2m 50s\n",
      "33:\tlearn: 0.0058506\ttotal: 5.99s\tremaining: 2m 50s\n",
      "34:\tlearn: 0.0056035\ttotal: 6.17s\tremaining: 2m 50s\n",
      "35:\tlearn: 0.0053597\ttotal: 6.34s\tremaining: 2m 49s\n",
      "36:\tlearn: 0.0051523\ttotal: 6.52s\tremaining: 2m 49s\n",
      "37:\tlearn: 0.0049157\ttotal: 6.69s\tremaining: 2m 49s\n",
      "38:\tlearn: 0.0047057\ttotal: 6.87s\tremaining: 2m 49s\n",
      "39:\tlearn: 0.0045600\ttotal: 7.05s\tremaining: 2m 49s\n",
      "40:\tlearn: 0.0043596\ttotal: 7.22s\tremaining: 2m 48s\n",
      "41:\tlearn: 0.0041990\ttotal: 7.4s\tremaining: 2m 48s\n",
      "42:\tlearn: 0.0040824\ttotal: 7.57s\tremaining: 2m 48s\n",
      "43:\tlearn: 0.0039095\ttotal: 7.75s\tremaining: 2m 48s\n",
      "44:\tlearn: 0.0037651\ttotal: 7.92s\tremaining: 2m 48s\n",
      "45:\tlearn: 0.0037009\ttotal: 8.1s\tremaining: 2m 47s\n",
      "46:\tlearn: 0.0035276\ttotal: 8.27s\tremaining: 2m 47s\n",
      "47:\tlearn: 0.0034335\ttotal: 8.45s\tremaining: 2m 47s\n",
      "48:\tlearn: 0.0033693\ttotal: 8.62s\tremaining: 2m 47s\n",
      "49:\tlearn: 0.0032606\ttotal: 8.79s\tremaining: 2m 47s\n",
      "50:\tlearn: 0.0031750\ttotal: 8.96s\tremaining: 2m 46s\n",
      "51:\tlearn: 0.0030501\ttotal: 9.13s\tremaining: 2m 46s\n",
      "52:\tlearn: 0.0029338\ttotal: 9.31s\tremaining: 2m 46s\n",
      "53:\tlearn: 0.0028686\ttotal: 9.48s\tremaining: 2m 46s\n",
      "54:\tlearn: 0.0028089\ttotal: 9.66s\tremaining: 2m 45s\n",
      "55:\tlearn: 0.0026407\ttotal: 9.83s\tremaining: 2m 45s\n",
      "56:\tlearn: 0.0026006\ttotal: 10s\tremaining: 2m 45s\n",
      "57:\tlearn: 0.0025484\ttotal: 10.2s\tremaining: 2m 45s\n",
      "58:\tlearn: 0.0024731\ttotal: 10.4s\tremaining: 2m 45s\n",
      "59:\tlearn: 0.0024133\ttotal: 10.5s\tremaining: 2m 45s\n",
      "60:\tlearn: 0.0023652\ttotal: 10.7s\tremaining: 2m 44s\n",
      "61:\tlearn: 0.0023131\ttotal: 10.9s\tremaining: 2m 44s\n",
      "62:\tlearn: 0.0022622\ttotal: 11.1s\tremaining: 2m 44s\n",
      "63:\tlearn: 0.0022328\ttotal: 11.2s\tremaining: 2m 44s\n",
      "64:\tlearn: 0.0022011\ttotal: 11.4s\tremaining: 2m 44s\n",
      "65:\tlearn: 0.0021309\ttotal: 11.6s\tremaining: 2m 44s\n",
      "66:\tlearn: 0.0021309\ttotal: 11.8s\tremaining: 2m 44s\n",
      "67:\tlearn: 0.0020741\ttotal: 12s\tremaining: 2m 43s\n",
      "68:\tlearn: 0.0020327\ttotal: 12.1s\tremaining: 2m 43s\n",
      "69:\tlearn: 0.0019757\ttotal: 12.3s\tremaining: 2m 43s\n",
      "70:\tlearn: 0.0019514\ttotal: 12.5s\tremaining: 2m 43s\n",
      "71:\tlearn: 0.0019288\ttotal: 12.7s\tremaining: 2m 43s\n",
      "72:\tlearn: 0.0018865\ttotal: 12.8s\tremaining: 2m 43s\n",
      "73:\tlearn: 0.0018647\ttotal: 13s\tremaining: 2m 42s\n",
      "74:\tlearn: 0.0018365\ttotal: 13.2s\tremaining: 2m 42s\n",
      "75:\tlearn: 0.0018103\ttotal: 13.4s\tremaining: 2m 42s\n",
      "76:\tlearn: 0.0017944\ttotal: 13.5s\tremaining: 2m 42s\n",
      "77:\tlearn: 0.0017403\ttotal: 13.7s\tremaining: 2m 42s\n",
      "78:\tlearn: 0.0017402\ttotal: 13.9s\tremaining: 2m 41s\n",
      "79:\tlearn: 0.0017129\ttotal: 14.1s\tremaining: 2m 41s\n",
      "80:\tlearn: 0.0016924\ttotal: 14.2s\tremaining: 2m 41s\n",
      "81:\tlearn: 0.0016586\ttotal: 14.4s\tremaining: 2m 41s\n",
      "82:\tlearn: 0.0016442\ttotal: 14.6s\tremaining: 2m 41s\n",
      "83:\tlearn: 0.0016113\ttotal: 14.8s\tremaining: 2m 40s\n",
      "84:\tlearn: 0.0015888\ttotal: 14.9s\tremaining: 2m 40s\n",
      "85:\tlearn: 0.0015669\ttotal: 15.1s\tremaining: 2m 40s\n",
      "86:\tlearn: 0.0015474\ttotal: 15.3s\tremaining: 2m 40s\n",
      "87:\tlearn: 0.0015473\ttotal: 15.5s\tremaining: 2m 40s\n",
      "88:\tlearn: 0.0015243\ttotal: 15.6s\tremaining: 2m 40s\n",
      "89:\tlearn: 0.0015066\ttotal: 15.8s\tremaining: 2m 39s\n",
      "90:\tlearn: 0.0014912\ttotal: 16s\tremaining: 2m 39s\n",
      "91:\tlearn: 0.0014782\ttotal: 16.2s\tremaining: 2m 39s\n",
      "92:\tlearn: 0.0014577\ttotal: 16.3s\tremaining: 2m 39s\n",
      "93:\tlearn: 0.0014354\ttotal: 16.5s\tremaining: 2m 39s\n",
      "94:\tlearn: 0.0014186\ttotal: 16.7s\tremaining: 2m 38s\n",
      "95:\tlearn: 0.0013982\ttotal: 16.9s\tremaining: 2m 38s\n",
      "96:\tlearn: 0.0013982\ttotal: 17s\tremaining: 2m 38s\n",
      "97:\tlearn: 0.0013982\ttotal: 17.2s\tremaining: 2m 38s\n",
      "98:\tlearn: 0.0013850\ttotal: 17.4s\tremaining: 2m 38s\n",
      "99:\tlearn: 0.0013706\ttotal: 17.6s\tremaining: 2m 38s\n",
      "100:\tlearn: 0.0013569\ttotal: 17.7s\tremaining: 2m 37s\n",
      "101:\tlearn: 0.0013567\ttotal: 17.9s\tremaining: 2m 37s\n",
      "102:\tlearn: 0.0013565\ttotal: 18.1s\tremaining: 2m 37s\n",
      "103:\tlearn: 0.0013562\ttotal: 18.3s\tremaining: 2m 37s\n",
      "104:\tlearn: 0.0013248\ttotal: 18.4s\tremaining: 2m 37s\n",
      "105:\tlearn: 0.0013111\ttotal: 18.6s\tremaining: 2m 36s\n",
      "106:\tlearn: 0.0013002\ttotal: 18.8s\tremaining: 2m 36s\n",
      "107:\tlearn: 0.0012998\ttotal: 19s\tremaining: 2m 36s\n",
      "108:\tlearn: 0.0012994\ttotal: 19.1s\tremaining: 2m 36s\n",
      "109:\tlearn: 0.0012994\ttotal: 19.3s\tremaining: 2m 36s\n",
      "110:\tlearn: 0.0012811\ttotal: 19.5s\tremaining: 2m 36s\n",
      "111:\tlearn: 0.0012692\ttotal: 19.7s\tremaining: 2m 35s\n",
      "112:\tlearn: 0.0012691\ttotal: 19.8s\tremaining: 2m 35s\n",
      "113:\tlearn: 0.0012614\ttotal: 20s\tremaining: 2m 35s\n",
      "114:\tlearn: 0.0012614\ttotal: 20.2s\tremaining: 2m 35s\n",
      "115:\tlearn: 0.0012465\ttotal: 20.4s\tremaining: 2m 35s\n",
      "116:\tlearn: 0.0012383\ttotal: 20.5s\tremaining: 2m 35s\n",
      "117:\tlearn: 0.0012273\ttotal: 20.7s\tremaining: 2m 34s\n",
      "118:\tlearn: 0.0012153\ttotal: 20.9s\tremaining: 2m 34s\n",
      "119:\tlearn: 0.0012153\ttotal: 21.1s\tremaining: 2m 34s\n",
      "120:\tlearn: 0.0012053\ttotal: 21.2s\tremaining: 2m 34s\n",
      "121:\tlearn: 0.0012034\ttotal: 21.4s\tremaining: 2m 34s\n",
      "122:\tlearn: 0.0012034\ttotal: 21.6s\tremaining: 2m 33s\n",
      "123:\tlearn: 0.0012032\ttotal: 21.8s\tremaining: 2m 33s\n",
      "124:\tlearn: 0.0012032\ttotal: 21.9s\tremaining: 2m 33s\n",
      "125:\tlearn: 0.0011867\ttotal: 22.1s\tremaining: 2m 33s\n",
      "126:\tlearn: 0.0011864\ttotal: 22.3s\tremaining: 2m 33s\n",
      "127:\tlearn: 0.0011860\ttotal: 22.5s\tremaining: 2m 33s\n",
      "128:\tlearn: 0.0011772\ttotal: 22.7s\tremaining: 2m 32s\n",
      "129:\tlearn: 0.0011771\ttotal: 22.8s\tremaining: 2m 32s\n",
      "130:\tlearn: 0.0011728\ttotal: 23s\tremaining: 2m 32s\n",
      "131:\tlearn: 0.0011727\ttotal: 23.2s\tremaining: 2m 32s\n",
      "132:\tlearn: 0.0011656\ttotal: 23.3s\tremaining: 2m 32s\n",
      "133:\tlearn: 0.0011523\ttotal: 23.5s\tremaining: 2m 32s\n",
      "134:\tlearn: 0.0011523\ttotal: 23.7s\tremaining: 2m 31s\n",
      "135:\tlearn: 0.0011523\ttotal: 23.9s\tremaining: 2m 31s\n",
      "136:\tlearn: 0.0011523\ttotal: 24.1s\tremaining: 2m 31s\n",
      "137:\tlearn: 0.0011523\ttotal: 24.2s\tremaining: 2m 31s\n",
      "138:\tlearn: 0.0011518\ttotal: 24.4s\tremaining: 2m 31s\n",
      "139:\tlearn: 0.0011395\ttotal: 24.6s\tremaining: 2m 30s\n",
      "140:\tlearn: 0.0011298\ttotal: 24.8s\tremaining: 2m 30s\n",
      "141:\tlearn: 0.0011282\ttotal: 24.9s\tremaining: 2m 30s\n",
      "142:\tlearn: 0.0011281\ttotal: 25.1s\tremaining: 2m 30s\n",
      "143:\tlearn: 0.0011169\ttotal: 25.3s\tremaining: 2m 30s\n",
      "144:\tlearn: 0.0011020\ttotal: 25.5s\tremaining: 2m 30s\n",
      "145:\tlearn: 0.0011020\ttotal: 25.6s\tremaining: 2m 29s\n",
      "146:\tlearn: 0.0011013\ttotal: 25.8s\tremaining: 2m 29s\n",
      "147:\tlearn: 0.0011005\ttotal: 26s\tremaining: 2m 29s\n",
      "148:\tlearn: 0.0011002\ttotal: 26.2s\tremaining: 2m 29s\n",
      "149:\tlearn: 0.0010945\ttotal: 26.3s\tremaining: 2m 29s\n",
      "150:\tlearn: 0.0010945\ttotal: 26.5s\tremaining: 2m 29s\n",
      "151:\tlearn: 0.0010881\ttotal: 26.7s\tremaining: 2m 28s\n",
      "152:\tlearn: 0.0010803\ttotal: 26.9s\tremaining: 2m 28s\n",
      "153:\tlearn: 0.0010725\ttotal: 27s\tremaining: 2m 28s\n",
      "154:\tlearn: 0.0010724\ttotal: 27.2s\tremaining: 2m 28s\n",
      "155:\tlearn: 0.0010724\ttotal: 27.4s\tremaining: 2m 28s\n",
      "156:\tlearn: 0.0010722\ttotal: 27.6s\tremaining: 2m 27s\n",
      "157:\tlearn: 0.0010721\ttotal: 27.7s\tremaining: 2m 27s\n",
      "158:\tlearn: 0.0010720\ttotal: 27.9s\tremaining: 2m 27s\n",
      "159:\tlearn: 0.0010717\ttotal: 28.1s\tremaining: 2m 27s\n",
      "160:\tlearn: 0.0010717\ttotal: 28.3s\tremaining: 2m 27s\n",
      "161:\tlearn: 0.0010716\ttotal: 28.4s\tremaining: 2m 27s\n",
      "162:\tlearn: 0.0010715\ttotal: 28.6s\tremaining: 2m 26s\n",
      "163:\tlearn: 0.0010714\ttotal: 28.8s\tremaining: 2m 26s\n",
      "164:\tlearn: 0.0010598\ttotal: 29s\tremaining: 2m 26s\n",
      "165:\tlearn: 0.0010598\ttotal: 29.1s\tremaining: 2m 26s\n",
      "166:\tlearn: 0.0010598\ttotal: 29.3s\tremaining: 2m 26s\n",
      "167:\tlearn: 0.0010585\ttotal: 29.5s\tremaining: 2m 25s\n",
      "168:\tlearn: 0.0010582\ttotal: 29.7s\tremaining: 2m 25s\n",
      "169:\tlearn: 0.0010582\ttotal: 29.8s\tremaining: 2m 25s\n",
      "170:\tlearn: 0.0010529\ttotal: 30s\tremaining: 2m 25s\n",
      "171:\tlearn: 0.0010529\ttotal: 30.2s\tremaining: 2m 25s\n",
      "172:\tlearn: 0.0010525\ttotal: 30.4s\tremaining: 2m 25s\n",
      "173:\tlearn: 0.0010382\ttotal: 30.5s\tremaining: 2m 24s\n",
      "174:\tlearn: 0.0010382\ttotal: 30.7s\tremaining: 2m 24s\n",
      "175:\tlearn: 0.0010376\ttotal: 30.9s\tremaining: 2m 24s\n",
      "176:\tlearn: 0.0010373\ttotal: 31.1s\tremaining: 2m 24s\n",
      "177:\tlearn: 0.0010363\ttotal: 31.2s\tremaining: 2m 24s\n",
      "178:\tlearn: 0.0010361\ttotal: 31.4s\tremaining: 2m 24s\n",
      "179:\tlearn: 0.0010351\ttotal: 31.6s\tremaining: 2m 23s\n",
      "180:\tlearn: 0.0010350\ttotal: 31.8s\tremaining: 2m 23s\n",
      "181:\tlearn: 0.0010350\ttotal: 31.9s\tremaining: 2m 23s\n",
      "182:\tlearn: 0.0010346\ttotal: 32.1s\tremaining: 2m 23s\n",
      "183:\tlearn: 0.0010345\ttotal: 32.3s\tremaining: 2m 23s\n",
      "184:\tlearn: 0.0010342\ttotal: 32.5s\tremaining: 2m 23s\n",
      "185:\tlearn: 0.0010341\ttotal: 32.6s\tremaining: 2m 22s\n",
      "186:\tlearn: 0.0010341\ttotal: 32.8s\tremaining: 2m 22s\n",
      "187:\tlearn: 0.0010339\ttotal: 33s\tremaining: 2m 22s\n",
      "188:\tlearn: 0.0010339\ttotal: 33.2s\tremaining: 2m 22s\n",
      "189:\tlearn: 0.0010338\ttotal: 33.3s\tremaining: 2m 22s\n",
      "190:\tlearn: 0.0010337\ttotal: 33.5s\tremaining: 2m 21s\n",
      "191:\tlearn: 0.0010334\ttotal: 33.7s\tremaining: 2m 21s\n",
      "192:\tlearn: 0.0010332\ttotal: 33.9s\tremaining: 2m 21s\n",
      "193:\tlearn: 0.0010332\ttotal: 34s\tremaining: 2m 21s\n",
      "194:\tlearn: 0.0010330\ttotal: 34.2s\tremaining: 2m 21s\n",
      "195:\tlearn: 0.0010330\ttotal: 34.4s\tremaining: 2m 21s\n",
      "196:\tlearn: 0.0010320\ttotal: 34.6s\tremaining: 2m 20s\n",
      "197:\tlearn: 0.0010320\ttotal: 34.7s\tremaining: 2m 20s\n",
      "198:\tlearn: 0.0010320\ttotal: 34.9s\tremaining: 2m 20s\n",
      "199:\tlearn: 0.0010319\ttotal: 35.1s\tremaining: 2m 20s\n",
      "200:\tlearn: 0.0010319\ttotal: 35.3s\tremaining: 2m 20s\n",
      "201:\tlearn: 0.0010318\ttotal: 35.4s\tremaining: 2m 20s\n",
      "202:\tlearn: 0.0010317\ttotal: 35.6s\tremaining: 2m 19s\n",
      "203:\tlearn: 0.0010316\ttotal: 35.8s\tremaining: 2m 19s\n",
      "204:\tlearn: 0.0010316\ttotal: 36s\tremaining: 2m 19s\n",
      "205:\tlearn: 0.0010251\ttotal: 36.1s\tremaining: 2m 19s\n",
      "206:\tlearn: 0.0010140\ttotal: 36.3s\tremaining: 2m 19s\n",
      "207:\tlearn: 0.0010106\ttotal: 36.5s\tremaining: 2m 18s\n",
      "208:\tlearn: 0.0010011\ttotal: 36.7s\tremaining: 2m 18s\n",
      "209:\tlearn: 0.0009932\ttotal: 36.8s\tremaining: 2m 18s\n",
      "210:\tlearn: 0.0009931\ttotal: 37s\tremaining: 2m 18s\n",
      "211:\tlearn: 0.0009931\ttotal: 37.2s\tremaining: 2m 18s\n",
      "212:\tlearn: 0.0009930\ttotal: 37.4s\tremaining: 2m 18s\n",
      "213:\tlearn: 0.0009843\ttotal: 37.5s\tremaining: 2m 17s\n",
      "214:\tlearn: 0.0009841\ttotal: 37.7s\tremaining: 2m 17s\n",
      "215:\tlearn: 0.0009841\ttotal: 37.9s\tremaining: 2m 17s\n",
      "216:\tlearn: 0.0009839\ttotal: 38.1s\tremaining: 2m 17s\n",
      "217:\tlearn: 0.0009836\ttotal: 38.2s\tremaining: 2m 17s\n",
      "218:\tlearn: 0.0009834\ttotal: 38.4s\tremaining: 2m 17s\n",
      "219:\tlearn: 0.0009834\ttotal: 38.6s\tremaining: 2m 16s\n",
      "220:\tlearn: 0.0009832\ttotal: 38.8s\tremaining: 2m 16s\n",
      "221:\tlearn: 0.0009831\ttotal: 38.9s\tremaining: 2m 16s\n",
      "222:\tlearn: 0.0009830\ttotal: 39.1s\tremaining: 2m 16s\n",
      "223:\tlearn: 0.0009830\ttotal: 39.3s\tremaining: 2m 16s\n",
      "224:\tlearn: 0.0009821\ttotal: 39.5s\tremaining: 2m 15s\n",
      "225:\tlearn: 0.0009821\ttotal: 39.6s\tremaining: 2m 15s\n",
      "226:\tlearn: 0.0009821\ttotal: 39.8s\tremaining: 2m 15s\n",
      "227:\tlearn: 0.0009815\ttotal: 40s\tremaining: 2m 15s\n",
      "228:\tlearn: 0.0009815\ttotal: 40.2s\tremaining: 2m 15s\n",
      "229:\tlearn: 0.0009814\ttotal: 40.3s\tremaining: 2m 15s\n",
      "230:\tlearn: 0.0009811\ttotal: 40.5s\tremaining: 2m 14s\n",
      "231:\tlearn: 0.0009810\ttotal: 40.7s\tremaining: 2m 14s\n",
      "232:\tlearn: 0.0009808\ttotal: 40.9s\tremaining: 2m 14s\n",
      "233:\tlearn: 0.0009805\ttotal: 41s\tremaining: 2m 14s\n",
      "234:\tlearn: 0.0009803\ttotal: 41.2s\tremaining: 2m 14s\n",
      "235:\tlearn: 0.0009801\ttotal: 41.4s\tremaining: 2m 13s\n",
      "236:\tlearn: 0.0009801\ttotal: 41.6s\tremaining: 2m 13s\n",
      "237:\tlearn: 0.0009801\ttotal: 41.7s\tremaining: 2m 13s\n",
      "238:\tlearn: 0.0009767\ttotal: 41.9s\tremaining: 2m 13s\n",
      "239:\tlearn: 0.0009766\ttotal: 42.1s\tremaining: 2m 13s\n",
      "240:\tlearn: 0.0009765\ttotal: 42.3s\tremaining: 2m 13s\n",
      "241:\tlearn: 0.0009638\ttotal: 42.4s\tremaining: 2m 12s\n",
      "242:\tlearn: 0.0009637\ttotal: 42.6s\tremaining: 2m 12s\n",
      "243:\tlearn: 0.0009637\ttotal: 42.8s\tremaining: 2m 12s\n",
      "244:\tlearn: 0.0009637\ttotal: 43s\tremaining: 2m 12s\n",
      "245:\tlearn: 0.0009634\ttotal: 43.1s\tremaining: 2m 12s\n",
      "246:\tlearn: 0.0009633\ttotal: 43.3s\tremaining: 2m 12s\n",
      "247:\tlearn: 0.0009548\ttotal: 43.5s\tremaining: 2m 11s\n",
      "248:\tlearn: 0.0009545\ttotal: 43.7s\tremaining: 2m 11s\n",
      "249:\tlearn: 0.0009545\ttotal: 43.8s\tremaining: 2m 11s\n",
      "250:\tlearn: 0.0009535\ttotal: 44s\tremaining: 2m 11s\n",
      "251:\tlearn: 0.0009535\ttotal: 44.2s\tremaining: 2m 11s\n",
      "252:\tlearn: 0.0009534\ttotal: 44.4s\tremaining: 2m 10s\n",
      "253:\tlearn: 0.0009534\ttotal: 44.5s\tremaining: 2m 10s\n",
      "254:\tlearn: 0.0009532\ttotal: 44.7s\tremaining: 2m 10s\n",
      "255:\tlearn: 0.0009526\ttotal: 44.9s\tremaining: 2m 10s\n",
      "256:\tlearn: 0.0009526\ttotal: 45s\tremaining: 2m 10s\n",
      "257:\tlearn: 0.0009525\ttotal: 45.2s\tremaining: 2m 10s\n",
      "258:\tlearn: 0.0009454\ttotal: 45.4s\tremaining: 2m 9s\n",
      "259:\tlearn: 0.0009453\ttotal: 45.6s\tremaining: 2m 9s\n",
      "260:\tlearn: 0.0009453\ttotal: 45.7s\tremaining: 2m 9s\n",
      "261:\tlearn: 0.0009418\ttotal: 45.9s\tremaining: 2m 9s\n",
      "262:\tlearn: 0.0009418\ttotal: 46.1s\tremaining: 2m 9s\n",
      "263:\tlearn: 0.0009373\ttotal: 46.3s\tremaining: 2m 8s\n",
      "264:\tlearn: 0.0009372\ttotal: 46.4s\tremaining: 2m 8s\n",
      "265:\tlearn: 0.0009372\ttotal: 46.6s\tremaining: 2m 8s\n",
      "266:\tlearn: 0.0009371\ttotal: 46.8s\tremaining: 2m 8s\n",
      "267:\tlearn: 0.0009366\ttotal: 47s\tremaining: 2m 8s\n",
      "268:\tlearn: 0.0009366\ttotal: 47.1s\tremaining: 2m 8s\n",
      "269:\tlearn: 0.0009365\ttotal: 47.3s\tremaining: 2m 7s\n",
      "270:\tlearn: 0.0009365\ttotal: 47.5s\tremaining: 2m 7s\n",
      "271:\tlearn: 0.0009361\ttotal: 47.7s\tremaining: 2m 7s\n",
      "272:\tlearn: 0.0009357\ttotal: 47.8s\tremaining: 2m 7s\n",
      "273:\tlearn: 0.0009354\ttotal: 48s\tremaining: 2m 7s\n",
      "274:\tlearn: 0.0009354\ttotal: 48.2s\tremaining: 2m 7s\n",
      "275:\tlearn: 0.0009353\ttotal: 48.4s\tremaining: 2m 6s\n",
      "276:\tlearn: 0.0009353\ttotal: 48.5s\tremaining: 2m 6s\n",
      "277:\tlearn: 0.0009352\ttotal: 48.7s\tremaining: 2m 6s\n",
      "278:\tlearn: 0.0009352\ttotal: 48.9s\tremaining: 2m 6s\n",
      "279:\tlearn: 0.0009351\ttotal: 49.1s\tremaining: 2m 6s\n",
      "280:\tlearn: 0.0009334\ttotal: 49.2s\tremaining: 2m 6s\n",
      "281:\tlearn: 0.0009334\ttotal: 49.4s\tremaining: 2m 5s\n",
      "282:\tlearn: 0.0009334\ttotal: 49.6s\tremaining: 2m 5s\n",
      "283:\tlearn: 0.0009311\ttotal: 49.8s\tremaining: 2m 5s\n",
      "284:\tlearn: 0.0009310\ttotal: 50s\tremaining: 2m 5s\n",
      "285:\tlearn: 0.0009309\ttotal: 50.1s\tremaining: 2m 5s\n",
      "286:\tlearn: 0.0009309\ttotal: 50.3s\tremaining: 2m 4s\n",
      "287:\tlearn: 0.0009309\ttotal: 50.5s\tremaining: 2m 4s\n",
      "288:\tlearn: 0.0009309\ttotal: 50.7s\tremaining: 2m 4s\n",
      "289:\tlearn: 0.0009309\ttotal: 50.8s\tremaining: 2m 4s\n",
      "290:\tlearn: 0.0009309\ttotal: 51s\tremaining: 2m 4s\n",
      "291:\tlearn: 0.0009308\ttotal: 51.2s\tremaining: 2m 4s\n",
      "292:\tlearn: 0.0009307\ttotal: 51.4s\tremaining: 2m 3s\n",
      "293:\tlearn: 0.0009307\ttotal: 51.5s\tremaining: 2m 3s\n",
      "294:\tlearn: 0.0009304\ttotal: 51.7s\tremaining: 2m 3s\n",
      "295:\tlearn: 0.0009304\ttotal: 51.9s\tremaining: 2m 3s\n",
      "296:\tlearn: 0.0009301\ttotal: 52.1s\tremaining: 2m 3s\n",
      "297:\tlearn: 0.0009301\ttotal: 52.2s\tremaining: 2m 3s\n",
      "298:\tlearn: 0.0009301\ttotal: 52.4s\tremaining: 2m 2s\n",
      "299:\tlearn: 0.0009300\ttotal: 52.6s\tremaining: 2m 2s\n",
      "300:\tlearn: 0.0009298\ttotal: 52.8s\tremaining: 2m 2s\n",
      "301:\tlearn: 0.0009297\ttotal: 52.9s\tremaining: 2m 2s\n",
      "302:\tlearn: 0.0009296\ttotal: 53.1s\tremaining: 2m 2s\n",
      "303:\tlearn: 0.0009294\ttotal: 53.3s\tremaining: 2m 1s\n",
      "304:\tlearn: 0.0009293\ttotal: 53.4s\tremaining: 2m 1s\n",
      "305:\tlearn: 0.0009290\ttotal: 53.6s\tremaining: 2m 1s\n",
      "306:\tlearn: 0.0009289\ttotal: 53.8s\tremaining: 2m 1s\n",
      "307:\tlearn: 0.0009289\ttotal: 54s\tremaining: 2m 1s\n",
      "308:\tlearn: 0.0009289\ttotal: 54.1s\tremaining: 2m 1s\n",
      "309:\tlearn: 0.0009287\ttotal: 54.3s\tremaining: 2m\n",
      "310:\tlearn: 0.0009286\ttotal: 54.5s\tremaining: 2m\n",
      "311:\tlearn: 0.0009285\ttotal: 54.7s\tremaining: 2m\n",
      "312:\tlearn: 0.0009285\ttotal: 54.8s\tremaining: 2m\n",
      "313:\tlearn: 0.0009284\ttotal: 55s\tremaining: 2m\n",
      "314:\tlearn: 0.0009282\ttotal: 55.2s\tremaining: 2m\n",
      "315:\tlearn: 0.0009282\ttotal: 55.4s\tremaining: 1m 59s\n",
      "316:\tlearn: 0.0009281\ttotal: 55.5s\tremaining: 1m 59s\n",
      "317:\tlearn: 0.0009281\ttotal: 55.7s\tremaining: 1m 59s\n",
      "318:\tlearn: 0.0009281\ttotal: 55.9s\tremaining: 1m 59s\n",
      "319:\tlearn: 0.0009280\ttotal: 56.1s\tremaining: 1m 59s\n",
      "320:\tlearn: 0.0009276\ttotal: 56.2s\tremaining: 1m 58s\n",
      "321:\tlearn: 0.0009275\ttotal: 56.4s\tremaining: 1m 58s\n",
      "322:\tlearn: 0.0009225\ttotal: 56.6s\tremaining: 1m 58s\n",
      "323:\tlearn: 0.0009212\ttotal: 56.8s\tremaining: 1m 58s\n",
      "324:\tlearn: 0.0009209\ttotal: 56.9s\tremaining: 1m 58s\n",
      "325:\tlearn: 0.0009156\ttotal: 57.1s\tremaining: 1m 58s\n",
      "326:\tlearn: 0.0009155\ttotal: 57.3s\tremaining: 1m 57s\n",
      "327:\tlearn: 0.0009155\ttotal: 57.5s\tremaining: 1m 57s\n",
      "328:\tlearn: 0.0009154\ttotal: 57.6s\tremaining: 1m 57s\n",
      "329:\tlearn: 0.0009104\ttotal: 57.8s\tremaining: 1m 57s\n",
      "330:\tlearn: 0.0009092\ttotal: 58s\tremaining: 1m 57s\n",
      "331:\tlearn: 0.0009091\ttotal: 58.2s\tremaining: 1m 57s\n",
      "332:\tlearn: 0.0009090\ttotal: 58.4s\tremaining: 1m 56s\n",
      "333:\tlearn: 0.0009088\ttotal: 58.5s\tremaining: 1m 56s\n",
      "334:\tlearn: 0.0009088\ttotal: 58.7s\tremaining: 1m 56s\n",
      "335:\tlearn: 0.0009088\ttotal: 58.9s\tremaining: 1m 56s\n",
      "336:\tlearn: 0.0009088\ttotal: 59.1s\tremaining: 1m 56s\n",
      "337:\tlearn: 0.0009087\ttotal: 59.2s\tremaining: 1m 56s\n",
      "338:\tlearn: 0.0009086\ttotal: 59.4s\tremaining: 1m 55s\n",
      "339:\tlearn: 0.0009085\ttotal: 59.6s\tremaining: 1m 55s\n",
      "340:\tlearn: 0.0009084\ttotal: 59.8s\tremaining: 1m 55s\n",
      "341:\tlearn: 0.0009084\ttotal: 59.9s\tremaining: 1m 55s\n",
      "342:\tlearn: 0.0009084\ttotal: 1m\tremaining: 1m 55s\n",
      "343:\tlearn: 0.0009084\ttotal: 1m\tremaining: 1m 54s\n",
      "344:\tlearn: 0.0009084\ttotal: 1m\tremaining: 1m 54s\n",
      "345:\tlearn: 0.0009084\ttotal: 1m\tremaining: 1m 54s\n",
      "346:\tlearn: 0.0009062\ttotal: 1m\tremaining: 1m 54s\n",
      "347:\tlearn: 0.0009062\ttotal: 1m\tremaining: 1m 54s\n",
      "348:\tlearn: 0.0009062\ttotal: 1m 1s\tremaining: 1m 54s\n",
      "349:\tlearn: 0.0009061\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "350:\tlearn: 0.0009061\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "351:\tlearn: 0.0009012\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "352:\tlearn: 0.0009011\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "353:\tlearn: 0.0009011\ttotal: 1m 2s\tremaining: 1m 53s\n",
      "354:\tlearn: 0.0009009\ttotal: 1m 2s\tremaining: 1m 53s\n",
      "355:\tlearn: 0.0009009\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "356:\tlearn: 0.0009007\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "357:\tlearn: 0.0009003\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "358:\tlearn: 0.0009002\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "359:\tlearn: 0.0009000\ttotal: 1m 3s\tremaining: 1m 52s\n",
      "360:\tlearn: 0.0008924\ttotal: 1m 3s\tremaining: 1m 52s\n",
      "361:\tlearn: 0.0008922\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "362:\tlearn: 0.0008920\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "363:\tlearn: 0.0008918\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "364:\tlearn: 0.0008918\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "365:\tlearn: 0.0008917\ttotal: 1m 4s\tremaining: 1m 51s\n",
      "366:\tlearn: 0.0008917\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "367:\tlearn: 0.0008915\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "368:\tlearn: 0.0008915\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "369:\tlearn: 0.0008907\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "370:\tlearn: 0.0008906\ttotal: 1m 5s\tremaining: 1m 50s\n",
      "371:\tlearn: 0.0008906\ttotal: 1m 5s\tremaining: 1m 50s\n",
      "372:\tlearn: 0.0008905\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "373:\tlearn: 0.0008904\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "374:\tlearn: 0.0008904\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "375:\tlearn: 0.0008904\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "376:\tlearn: 0.0008903\ttotal: 1m 6s\tremaining: 1m 49s\n",
      "377:\tlearn: 0.0008903\ttotal: 1m 6s\tremaining: 1m 49s\n",
      "378:\tlearn: 0.0008899\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "379:\tlearn: 0.0008898\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "380:\tlearn: 0.0008898\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "381:\tlearn: 0.0008897\ttotal: 1m 7s\tremaining: 1m 48s\n",
      "382:\tlearn: 0.0008897\ttotal: 1m 7s\tremaining: 1m 48s\n",
      "383:\tlearn: 0.0008897\ttotal: 1m 7s\tremaining: 1m 48s\n",
      "384:\tlearn: 0.0008895\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "385:\tlearn: 0.0008891\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "386:\tlearn: 0.0008891\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "387:\tlearn: 0.0008888\ttotal: 1m 8s\tremaining: 1m 47s\n",
      "388:\tlearn: 0.0008887\ttotal: 1m 8s\tremaining: 1m 47s\n",
      "389:\tlearn: 0.0008879\ttotal: 1m 8s\tremaining: 1m 47s\n",
      "390:\tlearn: 0.0008877\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "391:\tlearn: 0.0008825\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "392:\tlearn: 0.0008825\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "393:\tlearn: 0.0008825\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "394:\tlearn: 0.0008824\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "395:\tlearn: 0.0008823\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "396:\tlearn: 0.0008820\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "397:\tlearn: 0.0008819\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "398:\tlearn: 0.0008815\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "399:\tlearn: 0.0008810\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "400:\tlearn: 0.0008807\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "401:\tlearn: 0.0008807\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "402:\tlearn: 0.0008806\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "403:\tlearn: 0.0008704\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "404:\tlearn: 0.0008702\ttotal: 1m 11s\tremaining: 1m 44s\n",
      "405:\tlearn: 0.0008702\ttotal: 1m 11s\tremaining: 1m 44s\n",
      "406:\tlearn: 0.0008702\ttotal: 1m 11s\tremaining: 1m 44s\n",
      "407:\tlearn: 0.0008700\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "408:\tlearn: 0.0008692\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "409:\tlearn: 0.0008692\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "410:\tlearn: 0.0008692\ttotal: 1m 12s\tremaining: 1m 43s\n",
      "411:\tlearn: 0.0008626\ttotal: 1m 12s\tremaining: 1m 43s\n",
      "412:\tlearn: 0.0008585\ttotal: 1m 12s\tremaining: 1m 43s\n",
      "413:\tlearn: 0.0008585\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "414:\tlearn: 0.0008585\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "415:\tlearn: 0.0008583\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "416:\tlearn: 0.0008580\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "417:\tlearn: 0.0008579\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "418:\tlearn: 0.0008579\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "419:\tlearn: 0.0008579\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "420:\tlearn: 0.0008578\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "421:\tlearn: 0.0008578\ttotal: 1m 14s\tremaining: 1m 41s\n",
      "422:\tlearn: 0.0008577\ttotal: 1m 14s\tremaining: 1m 41s\n",
      "423:\tlearn: 0.0008576\ttotal: 1m 14s\tremaining: 1m 41s\n",
      "424:\tlearn: 0.0008572\ttotal: 1m 14s\tremaining: 1m 41s\n",
      "425:\tlearn: 0.0008567\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "426:\tlearn: 0.0008564\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "427:\tlearn: 0.0008563\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "428:\tlearn: 0.0008563\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "429:\tlearn: 0.0008560\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "430:\tlearn: 0.0008560\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "431:\tlearn: 0.0008560\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "432:\tlearn: 0.0008560\ttotal: 1m 16s\tremaining: 1m 39s\n",
      "433:\tlearn: 0.0008547\ttotal: 1m 16s\tremaining: 1m 39s\n",
      "434:\tlearn: 0.0008546\ttotal: 1m 16s\tremaining: 1m 39s\n",
      "435:\tlearn: 0.0008546\ttotal: 1m 16s\tremaining: 1m 39s\n",
      "436:\tlearn: 0.0008546\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "437:\tlearn: 0.0008545\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "438:\tlearn: 0.0008512\ttotal: 1m 17s\tremaining: 1m 38s\n",
      "439:\tlearn: 0.0008512\ttotal: 1m 17s\tremaining: 1m 38s\n",
      "440:\tlearn: 0.0008512\ttotal: 1m 17s\tremaining: 1m 38s\n",
      "441:\tlearn: 0.0008512\ttotal: 1m 17s\tremaining: 1m 38s\n",
      "442:\tlearn: 0.0008511\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "443:\tlearn: 0.0008511\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "444:\tlearn: 0.0008480\ttotal: 1m 18s\tremaining: 1m 37s\n",
      "445:\tlearn: 0.0008480\ttotal: 1m 18s\tremaining: 1m 37s\n",
      "446:\tlearn: 0.0008480\ttotal: 1m 18s\tremaining: 1m 37s\n",
      "447:\tlearn: 0.0008479\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "448:\tlearn: 0.0008479\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "449:\tlearn: 0.0008479\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "450:\tlearn: 0.0008478\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "451:\tlearn: 0.0008476\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "452:\tlearn: 0.0008476\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "453:\tlearn: 0.0008476\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "454:\tlearn: 0.0008474\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "455:\tlearn: 0.0008473\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "456:\tlearn: 0.0008472\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "457:\tlearn: 0.0008471\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "458:\tlearn: 0.0008471\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "459:\tlearn: 0.0008463\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "460:\tlearn: 0.0008463\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "461:\tlearn: 0.0008463\ttotal: 1m 21s\tremaining: 1m 34s\n",
      "462:\tlearn: 0.0008461\ttotal: 1m 21s\tremaining: 1m 34s\n",
      "463:\tlearn: 0.0008461\ttotal: 1m 21s\tremaining: 1m 34s\n",
      "464:\tlearn: 0.0008461\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "465:\tlearn: 0.0008461\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "466:\tlearn: 0.0008458\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "467:\tlearn: 0.0008458\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "468:\tlearn: 0.0008457\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "469:\tlearn: 0.0008457\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "470:\tlearn: 0.0008456\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "471:\tlearn: 0.0008455\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "472:\tlearn: 0.0008452\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "473:\tlearn: 0.0008452\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "474:\tlearn: 0.0008452\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "475:\tlearn: 0.0008450\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "476:\tlearn: 0.0008448\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "477:\tlearn: 0.0008447\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "478:\tlearn: 0.0008445\ttotal: 1m 24s\tremaining: 1m 31s\n",
      "479:\tlearn: 0.0008437\ttotal: 1m 24s\tremaining: 1m 31s\n",
      "480:\tlearn: 0.0008437\ttotal: 1m 24s\tremaining: 1m 31s\n",
      "481:\tlearn: 0.0008436\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "482:\tlearn: 0.0008436\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "483:\tlearn: 0.0008435\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "484:\tlearn: 0.0008432\ttotal: 1m 25s\tremaining: 1m 30s\n",
      "485:\tlearn: 0.0008432\ttotal: 1m 25s\tremaining: 1m 30s\n",
      "486:\tlearn: 0.0008429\ttotal: 1m 25s\tremaining: 1m 30s\n",
      "487:\tlearn: 0.0008429\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "488:\tlearn: 0.0008428\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "489:\tlearn: 0.0008428\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "490:\tlearn: 0.0008428\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "491:\tlearn: 0.0008425\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "492:\tlearn: 0.0008419\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "493:\tlearn: 0.0008419\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "494:\tlearn: 0.0008418\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "495:\tlearn: 0.0008417\ttotal: 1m 27s\tremaining: 1m 28s\n",
      "496:\tlearn: 0.0008416\ttotal: 1m 27s\tremaining: 1m 28s\n",
      "497:\tlearn: 0.0008416\ttotal: 1m 27s\tremaining: 1m 28s\n",
      "498:\tlearn: 0.0008415\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "499:\tlearn: 0.0008383\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "500:\tlearn: 0.0008383\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "501:\tlearn: 0.0008381\ttotal: 1m 28s\tremaining: 1m 27s\n",
      "502:\tlearn: 0.0008381\ttotal: 1m 28s\tremaining: 1m 27s\n",
      "503:\tlearn: 0.0008381\ttotal: 1m 28s\tremaining: 1m 27s\n",
      "504:\tlearn: 0.0008380\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "505:\tlearn: 0.0008379\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "506:\tlearn: 0.0008379\ttotal: 1m 29s\tremaining: 1m 26s\n",
      "507:\tlearn: 0.0008378\ttotal: 1m 29s\tremaining: 1m 26s\n",
      "508:\tlearn: 0.0008378\ttotal: 1m 29s\tremaining: 1m 26s\n",
      "509:\tlearn: 0.0008377\ttotal: 1m 29s\tremaining: 1m 26s\n",
      "510:\tlearn: 0.0008372\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "511:\tlearn: 0.0008371\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "512:\tlearn: 0.0008371\ttotal: 1m 30s\tremaining: 1m 25s\n",
      "513:\tlearn: 0.0008371\ttotal: 1m 30s\tremaining: 1m 25s\n",
      "514:\tlearn: 0.0008371\ttotal: 1m 30s\tremaining: 1m 25s\n",
      "515:\tlearn: 0.0008368\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "516:\tlearn: 0.0008367\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "517:\tlearn: 0.0008323\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "518:\tlearn: 0.0008323\ttotal: 1m 31s\tremaining: 1m 24s\n",
      "519:\tlearn: 0.0008321\ttotal: 1m 31s\tremaining: 1m 24s\n",
      "520:\tlearn: 0.0008317\ttotal: 1m 31s\tremaining: 1m 24s\n",
      "521:\tlearn: 0.0008317\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "522:\tlearn: 0.0008280\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "523:\tlearn: 0.0008279\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "524:\tlearn: 0.0008279\ttotal: 1m 32s\tremaining: 1m 23s\n",
      "525:\tlearn: 0.0008276\ttotal: 1m 32s\tremaining: 1m 23s\n",
      "526:\tlearn: 0.0008270\ttotal: 1m 32s\tremaining: 1m 23s\n",
      "527:\tlearn: 0.0008270\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "528:\tlearn: 0.0008270\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "529:\tlearn: 0.0008270\ttotal: 1m 33s\tremaining: 1m 22s\n",
      "530:\tlearn: 0.0008270\ttotal: 1m 33s\tremaining: 1m 22s\n",
      "531:\tlearn: 0.0008269\ttotal: 1m 33s\tremaining: 1m 22s\n",
      "532:\tlearn: 0.0008269\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "533:\tlearn: 0.0008265\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "534:\tlearn: 0.0008264\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "535:\tlearn: 0.0008261\ttotal: 1m 34s\tremaining: 1m 21s\n",
      "536:\tlearn: 0.0008204\ttotal: 1m 34s\tremaining: 1m 21s\n",
      "537:\tlearn: 0.0008204\ttotal: 1m 34s\tremaining: 1m 21s\n",
      "538:\tlearn: 0.0008203\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "539:\tlearn: 0.0008203\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "540:\tlearn: 0.0008203\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "541:\tlearn: 0.0008203\ttotal: 1m 35s\tremaining: 1m 20s\n",
      "542:\tlearn: 0.0008201\ttotal: 1m 35s\tremaining: 1m 20s\n",
      "543:\tlearn: 0.0008201\ttotal: 1m 35s\tremaining: 1m 20s\n",
      "544:\tlearn: 0.0008197\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "545:\tlearn: 0.0008197\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "546:\tlearn: 0.0008196\ttotal: 1m 36s\tremaining: 1m 19s\n",
      "547:\tlearn: 0.0008196\ttotal: 1m 36s\tremaining: 1m 19s\n",
      "548:\tlearn: 0.0008196\ttotal: 1m 36s\tremaining: 1m 19s\n",
      "549:\tlearn: 0.0008194\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "550:\tlearn: 0.0008194\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "551:\tlearn: 0.0008190\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "552:\tlearn: 0.0008188\ttotal: 1m 37s\tremaining: 1m 18s\n",
      "553:\tlearn: 0.0008160\ttotal: 1m 37s\tremaining: 1m 18s\n",
      "554:\tlearn: 0.0008160\ttotal: 1m 37s\tremaining: 1m 18s\n",
      "555:\tlearn: 0.0008160\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "556:\tlearn: 0.0008160\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "557:\tlearn: 0.0008158\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "558:\tlearn: 0.0008118\ttotal: 1m 38s\tremaining: 1m 17s\n",
      "559:\tlearn: 0.0008117\ttotal: 1m 38s\tremaining: 1m 17s\n",
      "560:\tlearn: 0.0008116\ttotal: 1m 38s\tremaining: 1m 17s\n",
      "561:\tlearn: 0.0008116\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "562:\tlearn: 0.0008114\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "563:\tlearn: 0.0008067\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "564:\tlearn: 0.0008066\ttotal: 1m 39s\tremaining: 1m 16s\n",
      "565:\tlearn: 0.0008064\ttotal: 1m 39s\tremaining: 1m 16s\n",
      "566:\tlearn: 0.0008064\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "567:\tlearn: 0.0008063\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "568:\tlearn: 0.0008062\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "569:\tlearn: 0.0008056\ttotal: 1m 40s\tremaining: 1m 15s\n",
      "570:\tlearn: 0.0008055\ttotal: 1m 40s\tremaining: 1m 15s\n",
      "571:\tlearn: 0.0008055\ttotal: 1m 40s\tremaining: 1m 15s\n",
      "572:\tlearn: 0.0008054\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "573:\tlearn: 0.0008054\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "574:\tlearn: 0.0008054\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "575:\tlearn: 0.0008054\ttotal: 1m 41s\tremaining: 1m 14s\n",
      "576:\tlearn: 0.0008054\ttotal: 1m 41s\tremaining: 1m 14s\n",
      "577:\tlearn: 0.0008052\ttotal: 1m 41s\tremaining: 1m 14s\n",
      "578:\tlearn: 0.0008051\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "579:\tlearn: 0.0008050\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "580:\tlearn: 0.0008049\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "581:\tlearn: 0.0008009\ttotal: 1m 42s\tremaining: 1m 13s\n",
      "582:\tlearn: 0.0008008\ttotal: 1m 42s\tremaining: 1m 13s\n",
      "583:\tlearn: 0.0008007\ttotal: 1m 42s\tremaining: 1m 13s\n",
      "584:\tlearn: 0.0008007\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "585:\tlearn: 0.0008006\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "586:\tlearn: 0.0008004\ttotal: 1m 43s\tremaining: 1m 12s\n",
      "587:\tlearn: 0.0007997\ttotal: 1m 43s\tremaining: 1m 12s\n",
      "588:\tlearn: 0.0007997\ttotal: 1m 43s\tremaining: 1m 12s\n",
      "589:\tlearn: 0.0007996\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "590:\tlearn: 0.0007993\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "591:\tlearn: 0.0007991\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "592:\tlearn: 0.0007951\ttotal: 1m 44s\tremaining: 1m 11s\n",
      "593:\tlearn: 0.0007951\ttotal: 1m 44s\tremaining: 1m 11s\n",
      "594:\tlearn: 0.0007950\ttotal: 1m 44s\tremaining: 1m 11s\n",
      "595:\tlearn: 0.0007948\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "596:\tlearn: 0.0007934\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "597:\tlearn: 0.0007933\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "598:\tlearn: 0.0007929\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "599:\tlearn: 0.0007928\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "600:\tlearn: 0.0007927\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "601:\tlearn: 0.0007926\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "602:\tlearn: 0.0007926\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "603:\tlearn: 0.0007925\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "604:\tlearn: 0.0007925\ttotal: 1m 46s\tremaining: 1m 9s\n",
      "605:\tlearn: 0.0007925\ttotal: 1m 46s\tremaining: 1m 9s\n",
      "606:\tlearn: 0.0007925\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "607:\tlearn: 0.0007925\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "608:\tlearn: 0.0007923\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "609:\tlearn: 0.0007923\ttotal: 1m 47s\tremaining: 1m 8s\n",
      "610:\tlearn: 0.0007898\ttotal: 1m 47s\tremaining: 1m 8s\n",
      "611:\tlearn: 0.0007894\ttotal: 1m 47s\tremaining: 1m 8s\n",
      "612:\tlearn: 0.0007894\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "613:\tlearn: 0.0007894\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "614:\tlearn: 0.0007894\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "615:\tlearn: 0.0007894\ttotal: 1m 48s\tremaining: 1m 7s\n",
      "616:\tlearn: 0.0007893\ttotal: 1m 48s\tremaining: 1m 7s\n",
      "617:\tlearn: 0.0007892\ttotal: 1m 48s\tremaining: 1m 7s\n",
      "618:\tlearn: 0.0007892\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "619:\tlearn: 0.0007892\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "620:\tlearn: 0.0007891\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "621:\tlearn: 0.0007891\ttotal: 1m 49s\tremaining: 1m 6s\n",
      "622:\tlearn: 0.0007891\ttotal: 1m 49s\tremaining: 1m 6s\n",
      "623:\tlearn: 0.0007890\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "624:\tlearn: 0.0007889\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "625:\tlearn: 0.0007889\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "626:\tlearn: 0.0007889\ttotal: 1m 50s\tremaining: 1m 5s\n",
      "627:\tlearn: 0.0007886\ttotal: 1m 50s\tremaining: 1m 5s\n",
      "628:\tlearn: 0.0007885\ttotal: 1m 50s\tremaining: 1m 5s\n",
      "629:\tlearn: 0.0007885\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "630:\tlearn: 0.0007880\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "631:\tlearn: 0.0007879\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "632:\tlearn: 0.0007876\ttotal: 1m 51s\tremaining: 1m 4s\n",
      "633:\tlearn: 0.0007872\ttotal: 1m 51s\tremaining: 1m 4s\n",
      "634:\tlearn: 0.0007872\ttotal: 1m 51s\tremaining: 1m 4s\n",
      "635:\tlearn: 0.0007871\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "636:\tlearn: 0.0007870\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "637:\tlearn: 0.0007870\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "638:\tlearn: 0.0007869\ttotal: 1m 52s\tremaining: 1m 3s\n",
      "639:\tlearn: 0.0007869\ttotal: 1m 52s\tremaining: 1m 3s\n",
      "640:\tlearn: 0.0007869\ttotal: 1m 52s\tremaining: 1m 3s\n",
      "641:\tlearn: 0.0007867\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "642:\tlearn: 0.0007867\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "643:\tlearn: 0.0007867\ttotal: 1m 53s\tremaining: 1m 2s\n",
      "644:\tlearn: 0.0007865\ttotal: 1m 53s\tremaining: 1m 2s\n",
      "645:\tlearn: 0.0007865\ttotal: 1m 53s\tremaining: 1m 2s\n",
      "646:\tlearn: 0.0007861\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "647:\tlearn: 0.0007815\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "648:\tlearn: 0.0007815\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "649:\tlearn: 0.0007814\ttotal: 1m 54s\tremaining: 1m 1s\n",
      "650:\tlearn: 0.0007814\ttotal: 1m 54s\tremaining: 1m 1s\n",
      "651:\tlearn: 0.0007813\ttotal: 1m 54s\tremaining: 1m 1s\n",
      "652:\tlearn: 0.0007812\ttotal: 1m 54s\tremaining: 1m\n",
      "653:\tlearn: 0.0007811\ttotal: 1m 54s\tremaining: 1m\n",
      "654:\tlearn: 0.0007811\ttotal: 1m 54s\tremaining: 1m\n",
      "655:\tlearn: 0.0007810\ttotal: 1m 55s\tremaining: 1m\n",
      "656:\tlearn: 0.0007810\ttotal: 1m 55s\tremaining: 1m\n",
      "657:\tlearn: 0.0007810\ttotal: 1m 55s\tremaining: 1m\n",
      "658:\tlearn: 0.0007810\ttotal: 1m 55s\tremaining: 59.8s\n",
      "659:\tlearn: 0.0007810\ttotal: 1m 55s\tremaining: 59.7s\n",
      "660:\tlearn: 0.0007810\ttotal: 1m 56s\tremaining: 59.5s\n",
      "661:\tlearn: 0.0007810\ttotal: 1m 56s\tremaining: 59.3s\n",
      "662:\tlearn: 0.0007810\ttotal: 1m 56s\tremaining: 59.1s\n",
      "663:\tlearn: 0.0007808\ttotal: 1m 56s\tremaining: 59s\n",
      "664:\tlearn: 0.0007808\ttotal: 1m 56s\tremaining: 58.8s\n",
      "665:\tlearn: 0.0007807\ttotal: 1m 56s\tremaining: 58.6s\n",
      "666:\tlearn: 0.0007797\ttotal: 1m 57s\tremaining: 58.4s\n",
      "667:\tlearn: 0.0007797\ttotal: 1m 57s\tremaining: 58.3s\n",
      "668:\tlearn: 0.0007797\ttotal: 1m 57s\tremaining: 58.1s\n",
      "669:\tlearn: 0.0007795\ttotal: 1m 57s\tremaining: 57.9s\n",
      "670:\tlearn: 0.0007794\ttotal: 1m 57s\tremaining: 57.7s\n",
      "671:\tlearn: 0.0007794\ttotal: 1m 57s\tremaining: 57.6s\n",
      "672:\tlearn: 0.0007791\ttotal: 1m 58s\tremaining: 57.4s\n",
      "673:\tlearn: 0.0007791\ttotal: 1m 58s\tremaining: 57.2s\n",
      "674:\tlearn: 0.0007791\ttotal: 1m 58s\tremaining: 57s\n",
      "675:\tlearn: 0.0007788\ttotal: 1m 58s\tremaining: 56.9s\n",
      "676:\tlearn: 0.0007788\ttotal: 1m 58s\tremaining: 56.7s\n",
      "677:\tlearn: 0.0007788\ttotal: 1m 58s\tremaining: 56.5s\n",
      "678:\tlearn: 0.0007786\ttotal: 1m 59s\tremaining: 56.3s\n",
      "679:\tlearn: 0.0007785\ttotal: 1m 59s\tremaining: 56.1s\n",
      "680:\tlearn: 0.0007784\ttotal: 1m 59s\tremaining: 56s\n",
      "681:\tlearn: 0.0007782\ttotal: 1m 59s\tremaining: 55.8s\n",
      "682:\tlearn: 0.0007781\ttotal: 1m 59s\tremaining: 55.6s\n",
      "683:\tlearn: 0.0007780\ttotal: 2m\tremaining: 55.4s\n",
      "684:\tlearn: 0.0007780\ttotal: 2m\tremaining: 55.3s\n",
      "685:\tlearn: 0.0007779\ttotal: 2m\tremaining: 55.1s\n",
      "686:\tlearn: 0.0007778\ttotal: 2m\tremaining: 54.9s\n",
      "687:\tlearn: 0.0007777\ttotal: 2m\tremaining: 54.7s\n",
      "688:\tlearn: 0.0007777\ttotal: 2m\tremaining: 54.6s\n",
      "689:\tlearn: 0.0007777\ttotal: 2m 1s\tremaining: 54.4s\n",
      "690:\tlearn: 0.0007777\ttotal: 2m 1s\tremaining: 54.2s\n",
      "691:\tlearn: 0.0007777\ttotal: 2m 1s\tremaining: 54s\n",
      "692:\tlearn: 0.0007776\ttotal: 2m 1s\tremaining: 53.9s\n",
      "693:\tlearn: 0.0007772\ttotal: 2m 1s\tremaining: 53.7s\n",
      "694:\tlearn: 0.0007745\ttotal: 2m 1s\tremaining: 53.5s\n",
      "695:\tlearn: 0.0007735\ttotal: 2m 2s\tremaining: 53.3s\n",
      "696:\tlearn: 0.0007735\ttotal: 2m 2s\tremaining: 53.2s\n",
      "697:\tlearn: 0.0007693\ttotal: 2m 2s\tremaining: 53s\n",
      "698:\tlearn: 0.0007689\ttotal: 2m 2s\tremaining: 52.8s\n",
      "699:\tlearn: 0.0007689\ttotal: 2m 2s\tremaining: 52.6s\n",
      "700:\tlearn: 0.0007689\ttotal: 2m 2s\tremaining: 52.5s\n",
      "701:\tlearn: 0.0007689\ttotal: 2m 3s\tremaining: 52.3s\n",
      "702:\tlearn: 0.0007689\ttotal: 2m 3s\tremaining: 52.1s\n",
      "703:\tlearn: 0.0007689\ttotal: 2m 3s\tremaining: 51.9s\n",
      "704:\tlearn: 0.0007687\ttotal: 2m 3s\tremaining: 51.8s\n",
      "705:\tlearn: 0.0007687\ttotal: 2m 3s\tremaining: 51.6s\n",
      "706:\tlearn: 0.0007687\ttotal: 2m 4s\tremaining: 51.4s\n",
      "707:\tlearn: 0.0007687\ttotal: 2m 4s\tremaining: 51.2s\n",
      "708:\tlearn: 0.0007687\ttotal: 2m 4s\tremaining: 51.1s\n",
      "709:\tlearn: 0.0007686\ttotal: 2m 4s\tremaining: 50.9s\n",
      "710:\tlearn: 0.0007686\ttotal: 2m 4s\tremaining: 50.7s\n",
      "711:\tlearn: 0.0007655\ttotal: 2m 4s\tremaining: 50.5s\n",
      "712:\tlearn: 0.0007653\ttotal: 2m 5s\tremaining: 50.4s\n",
      "713:\tlearn: 0.0007648\ttotal: 2m 5s\tremaining: 50.2s\n",
      "714:\tlearn: 0.0007647\ttotal: 2m 5s\tremaining: 50s\n",
      "715:\tlearn: 0.0007647\ttotal: 2m 5s\tremaining: 49.8s\n",
      "716:\tlearn: 0.0007647\ttotal: 2m 5s\tremaining: 49.7s\n",
      "717:\tlearn: 0.0007646\ttotal: 2m 5s\tremaining: 49.5s\n",
      "718:\tlearn: 0.0007638\ttotal: 2m 6s\tremaining: 49.3s\n",
      "719:\tlearn: 0.0007636\ttotal: 2m 6s\tremaining: 49.1s\n",
      "720:\tlearn: 0.0007597\ttotal: 2m 6s\tremaining: 49s\n",
      "721:\tlearn: 0.0007596\ttotal: 2m 6s\tremaining: 48.8s\n",
      "722:\tlearn: 0.0007596\ttotal: 2m 6s\tremaining: 48.6s\n",
      "723:\tlearn: 0.0007590\ttotal: 2m 7s\tremaining: 48.4s\n",
      "724:\tlearn: 0.0007590\ttotal: 2m 7s\tremaining: 48.2s\n",
      "725:\tlearn: 0.0007590\ttotal: 2m 7s\tremaining: 48.1s\n",
      "726:\tlearn: 0.0007589\ttotal: 2m 7s\tremaining: 47.9s\n",
      "727:\tlearn: 0.0007589\ttotal: 2m 7s\tremaining: 47.7s\n",
      "728:\tlearn: 0.0007571\ttotal: 2m 7s\tremaining: 47.5s\n",
      "729:\tlearn: 0.0007571\ttotal: 2m 8s\tremaining: 47.4s\n",
      "730:\tlearn: 0.0007570\ttotal: 2m 8s\tremaining: 47.2s\n",
      "731:\tlearn: 0.0007570\ttotal: 2m 8s\tremaining: 47s\n",
      "732:\tlearn: 0.0007566\ttotal: 2m 8s\tremaining: 46.8s\n",
      "733:\tlearn: 0.0007566\ttotal: 2m 8s\tremaining: 46.7s\n",
      "734:\tlearn: 0.0007565\ttotal: 2m 8s\tremaining: 46.5s\n",
      "735:\tlearn: 0.0007565\ttotal: 2m 9s\tremaining: 46.3s\n",
      "736:\tlearn: 0.0007565\ttotal: 2m 9s\tremaining: 46.1s\n",
      "737:\tlearn: 0.0007565\ttotal: 2m 9s\tremaining: 46s\n",
      "738:\tlearn: 0.0007565\ttotal: 2m 9s\tremaining: 45.8s\n",
      "739:\tlearn: 0.0007564\ttotal: 2m 9s\tremaining: 45.6s\n",
      "740:\tlearn: 0.0007564\ttotal: 2m 9s\tremaining: 45.4s\n",
      "741:\tlearn: 0.0007562\ttotal: 2m 10s\tremaining: 45.3s\n",
      "742:\tlearn: 0.0007562\ttotal: 2m 10s\tremaining: 45.1s\n",
      "743:\tlearn: 0.0007561\ttotal: 2m 10s\tremaining: 44.9s\n",
      "744:\tlearn: 0.0007561\ttotal: 2m 10s\tremaining: 44.8s\n",
      "745:\tlearn: 0.0007561\ttotal: 2m 10s\tremaining: 44.6s\n",
      "746:\tlearn: 0.0007559\ttotal: 2m 11s\tremaining: 44.4s\n",
      "747:\tlearn: 0.0007559\ttotal: 2m 11s\tremaining: 44.3s\n",
      "748:\tlearn: 0.0007558\ttotal: 2m 11s\tremaining: 44.1s\n",
      "749:\tlearn: 0.0007554\ttotal: 2m 11s\tremaining: 43.9s\n",
      "750:\tlearn: 0.0007553\ttotal: 2m 11s\tremaining: 43.8s\n",
      "751:\tlearn: 0.0007553\ttotal: 2m 12s\tremaining: 43.6s\n",
      "752:\tlearn: 0.0007553\ttotal: 2m 12s\tremaining: 43.4s\n",
      "753:\tlearn: 0.0007523\ttotal: 2m 12s\tremaining: 43.3s\n",
      "754:\tlearn: 0.0007522\ttotal: 2m 12s\tremaining: 43.1s\n",
      "755:\tlearn: 0.0007519\ttotal: 2m 12s\tremaining: 42.9s\n",
      "756:\tlearn: 0.0007517\ttotal: 2m 13s\tremaining: 42.8s\n",
      "757:\tlearn: 0.0007517\ttotal: 2m 13s\tremaining: 42.6s\n",
      "758:\tlearn: 0.0007517\ttotal: 2m 13s\tremaining: 42.4s\n",
      "759:\tlearn: 0.0007516\ttotal: 2m 13s\tremaining: 42.2s\n",
      "760:\tlearn: 0.0007516\ttotal: 2m 13s\tremaining: 42.1s\n",
      "761:\tlearn: 0.0007515\ttotal: 2m 14s\tremaining: 41.9s\n",
      "762:\tlearn: 0.0007515\ttotal: 2m 14s\tremaining: 41.7s\n",
      "763:\tlearn: 0.0007499\ttotal: 2m 14s\tremaining: 41.6s\n",
      "764:\tlearn: 0.0007499\ttotal: 2m 14s\tremaining: 41.4s\n",
      "765:\tlearn: 0.0007499\ttotal: 2m 14s\tremaining: 41.2s\n",
      "766:\tlearn: 0.0007498\ttotal: 2m 15s\tremaining: 41.1s\n",
      "767:\tlearn: 0.0007468\ttotal: 2m 15s\tremaining: 40.9s\n",
      "768:\tlearn: 0.0007467\ttotal: 2m 15s\tremaining: 40.7s\n",
      "769:\tlearn: 0.0007466\ttotal: 2m 15s\tremaining: 40.6s\n",
      "770:\tlearn: 0.0007449\ttotal: 2m 15s\tremaining: 40.4s\n",
      "771:\tlearn: 0.0007446\ttotal: 2m 16s\tremaining: 40.2s\n",
      "772:\tlearn: 0.0007441\ttotal: 2m 16s\tremaining: 40s\n",
      "773:\tlearn: 0.0007441\ttotal: 2m 16s\tremaining: 39.9s\n",
      "774:\tlearn: 0.0007437\ttotal: 2m 16s\tremaining: 39.7s\n",
      "775:\tlearn: 0.0007437\ttotal: 2m 16s\tremaining: 39.5s\n",
      "776:\tlearn: 0.0007437\ttotal: 2m 17s\tremaining: 39.4s\n",
      "777:\tlearn: 0.0007435\ttotal: 2m 17s\tremaining: 39.2s\n",
      "778:\tlearn: 0.0007435\ttotal: 2m 17s\tremaining: 39s\n",
      "779:\tlearn: 0.0007435\ttotal: 2m 17s\tremaining: 38.9s\n",
      "780:\tlearn: 0.0007434\ttotal: 2m 17s\tremaining: 38.7s\n",
      "781:\tlearn: 0.0007434\ttotal: 2m 18s\tremaining: 38.5s\n",
      "782:\tlearn: 0.0007432\ttotal: 2m 18s\tremaining: 38.3s\n",
      "783:\tlearn: 0.0007432\ttotal: 2m 18s\tremaining: 38.2s\n",
      "784:\tlearn: 0.0007432\ttotal: 2m 18s\tremaining: 38s\n",
      "785:\tlearn: 0.0007431\ttotal: 2m 18s\tremaining: 37.8s\n",
      "786:\tlearn: 0.0007431\ttotal: 2m 19s\tremaining: 37.6s\n",
      "787:\tlearn: 0.0007429\ttotal: 2m 19s\tremaining: 37.4s\n",
      "788:\tlearn: 0.0007428\ttotal: 2m 19s\tremaining: 37.3s\n",
      "789:\tlearn: 0.0007428\ttotal: 2m 19s\tremaining: 37.1s\n",
      "790:\tlearn: 0.0007427\ttotal: 2m 19s\tremaining: 36.9s\n",
      "791:\tlearn: 0.0007427\ttotal: 2m 19s\tremaining: 36.7s\n",
      "792:\tlearn: 0.0007408\ttotal: 2m 20s\tremaining: 36.6s\n",
      "793:\tlearn: 0.0007381\ttotal: 2m 20s\tremaining: 36.4s\n",
      "794:\tlearn: 0.0007380\ttotal: 2m 20s\tremaining: 36.2s\n",
      "795:\tlearn: 0.0007379\ttotal: 2m 20s\tremaining: 36s\n",
      "796:\tlearn: 0.0007379\ttotal: 2m 20s\tremaining: 35.9s\n",
      "797:\tlearn: 0.0007379\ttotal: 2m 20s\tremaining: 35.7s\n",
      "798:\tlearn: 0.0007379\ttotal: 2m 21s\tremaining: 35.5s\n",
      "799:\tlearn: 0.0007374\ttotal: 2m 21s\tremaining: 35.3s\n",
      "800:\tlearn: 0.0007374\ttotal: 2m 21s\tremaining: 35.1s\n",
      "801:\tlearn: 0.0007373\ttotal: 2m 21s\tremaining: 35s\n",
      "802:\tlearn: 0.0007372\ttotal: 2m 21s\tremaining: 34.8s\n",
      "803:\tlearn: 0.0007372\ttotal: 2m 21s\tremaining: 34.6s\n",
      "804:\tlearn: 0.0007370\ttotal: 2m 22s\tremaining: 34.4s\n",
      "805:\tlearn: 0.0007365\ttotal: 2m 22s\tremaining: 34.3s\n",
      "806:\tlearn: 0.0007364\ttotal: 2m 22s\tremaining: 34.1s\n",
      "807:\tlearn: 0.0007363\ttotal: 2m 22s\tremaining: 33.9s\n",
      "808:\tlearn: 0.0007363\ttotal: 2m 22s\tremaining: 33.7s\n",
      "809:\tlearn: 0.0007363\ttotal: 2m 23s\tremaining: 33.6s\n",
      "810:\tlearn: 0.0007363\ttotal: 2m 23s\tremaining: 33.4s\n",
      "811:\tlearn: 0.0007362\ttotal: 2m 23s\tremaining: 33.2s\n",
      "812:\tlearn: 0.0007362\ttotal: 2m 23s\tremaining: 33s\n",
      "813:\tlearn: 0.0007362\ttotal: 2m 23s\tremaining: 32.8s\n",
      "814:\tlearn: 0.0007362\ttotal: 2m 23s\tremaining: 32.7s\n",
      "815:\tlearn: 0.0007361\ttotal: 2m 24s\tremaining: 32.5s\n",
      "816:\tlearn: 0.0007361\ttotal: 2m 24s\tremaining: 32.3s\n",
      "817:\tlearn: 0.0007358\ttotal: 2m 24s\tremaining: 32.1s\n",
      "818:\tlearn: 0.0007358\ttotal: 2m 24s\tremaining: 32s\n",
      "819:\tlearn: 0.0007358\ttotal: 2m 24s\tremaining: 31.8s\n",
      "820:\tlearn: 0.0007358\ttotal: 2m 24s\tremaining: 31.6s\n",
      "821:\tlearn: 0.0007357\ttotal: 2m 25s\tremaining: 31.4s\n",
      "822:\tlearn: 0.0007357\ttotal: 2m 25s\tremaining: 31.3s\n",
      "823:\tlearn: 0.0007357\ttotal: 2m 25s\tremaining: 31.1s\n",
      "824:\tlearn: 0.0007350\ttotal: 2m 25s\tremaining: 30.9s\n",
      "825:\tlearn: 0.0007349\ttotal: 2m 25s\tremaining: 30.7s\n",
      "826:\tlearn: 0.0007349\ttotal: 2m 26s\tremaining: 30.5s\n",
      "827:\tlearn: 0.0007349\ttotal: 2m 26s\tremaining: 30.4s\n",
      "828:\tlearn: 0.0007348\ttotal: 2m 26s\tremaining: 30.2s\n",
      "829:\tlearn: 0.0007346\ttotal: 2m 26s\tremaining: 30s\n",
      "830:\tlearn: 0.0007330\ttotal: 2m 26s\tremaining: 29.8s\n",
      "831:\tlearn: 0.0007330\ttotal: 2m 26s\tremaining: 29.7s\n",
      "832:\tlearn: 0.0007329\ttotal: 2m 27s\tremaining: 29.5s\n",
      "833:\tlearn: 0.0007328\ttotal: 2m 27s\tremaining: 29.3s\n",
      "834:\tlearn: 0.0007326\ttotal: 2m 27s\tremaining: 29.1s\n",
      "835:\tlearn: 0.0007325\ttotal: 2m 27s\tremaining: 29s\n",
      "836:\tlearn: 0.0007325\ttotal: 2m 27s\tremaining: 28.8s\n",
      "837:\tlearn: 0.0007325\ttotal: 2m 27s\tremaining: 28.6s\n",
      "838:\tlearn: 0.0007324\ttotal: 2m 28s\tremaining: 28.4s\n",
      "839:\tlearn: 0.0007324\ttotal: 2m 28s\tremaining: 28.2s\n",
      "840:\tlearn: 0.0007324\ttotal: 2m 28s\tremaining: 28.1s\n",
      "841:\tlearn: 0.0007324\ttotal: 2m 28s\tremaining: 27.9s\n",
      "842:\tlearn: 0.0007323\ttotal: 2m 28s\tremaining: 27.7s\n",
      "843:\tlearn: 0.0007314\ttotal: 2m 28s\tremaining: 27.5s\n",
      "844:\tlearn: 0.0007313\ttotal: 2m 29s\tremaining: 27.4s\n",
      "845:\tlearn: 0.0007313\ttotal: 2m 29s\tremaining: 27.2s\n",
      "846:\tlearn: 0.0007312\ttotal: 2m 29s\tremaining: 27s\n",
      "847:\tlearn: 0.0007312\ttotal: 2m 29s\tremaining: 26.8s\n",
      "848:\tlearn: 0.0007311\ttotal: 2m 29s\tremaining: 26.7s\n",
      "849:\tlearn: 0.0007310\ttotal: 2m 30s\tremaining: 26.5s\n",
      "850:\tlearn: 0.0007309\ttotal: 2m 30s\tremaining: 26.3s\n",
      "851:\tlearn: 0.0007308\ttotal: 2m 30s\tremaining: 26.1s\n",
      "852:\tlearn: 0.0007308\ttotal: 2m 30s\tremaining: 25.9s\n",
      "853:\tlearn: 0.0007308\ttotal: 2m 30s\tremaining: 25.8s\n",
      "854:\tlearn: 0.0007308\ttotal: 2m 30s\tremaining: 25.6s\n",
      "855:\tlearn: 0.0007306\ttotal: 2m 31s\tremaining: 25.4s\n",
      "856:\tlearn: 0.0007305\ttotal: 2m 31s\tremaining: 25.2s\n",
      "857:\tlearn: 0.0007304\ttotal: 2m 31s\tremaining: 25.1s\n",
      "858:\tlearn: 0.0007304\ttotal: 2m 31s\tremaining: 24.9s\n",
      "859:\tlearn: 0.0007304\ttotal: 2m 31s\tremaining: 24.7s\n",
      "860:\tlearn: 0.0007300\ttotal: 2m 31s\tremaining: 24.5s\n",
      "861:\tlearn: 0.0007265\ttotal: 2m 32s\tremaining: 24.4s\n",
      "862:\tlearn: 0.0007265\ttotal: 2m 32s\tremaining: 24.2s\n",
      "863:\tlearn: 0.0007263\ttotal: 2m 32s\tremaining: 24s\n",
      "864:\tlearn: 0.0007263\ttotal: 2m 32s\tremaining: 23.8s\n",
      "865:\tlearn: 0.0007263\ttotal: 2m 32s\tremaining: 23.7s\n",
      "866:\tlearn: 0.0007263\ttotal: 2m 33s\tremaining: 23.5s\n",
      "867:\tlearn: 0.0007262\ttotal: 2m 33s\tremaining: 23.3s\n",
      "868:\tlearn: 0.0007262\ttotal: 2m 33s\tremaining: 23.1s\n",
      "869:\tlearn: 0.0007262\ttotal: 2m 33s\tremaining: 22.9s\n",
      "870:\tlearn: 0.0007262\ttotal: 2m 33s\tremaining: 22.8s\n",
      "871:\tlearn: 0.0007262\ttotal: 2m 33s\tremaining: 22.6s\n",
      "872:\tlearn: 0.0007262\ttotal: 2m 34s\tremaining: 22.4s\n",
      "873:\tlearn: 0.0007261\ttotal: 2m 34s\tremaining: 22.2s\n",
      "874:\tlearn: 0.0007259\ttotal: 2m 34s\tremaining: 22.1s\n",
      "875:\tlearn: 0.0007246\ttotal: 2m 34s\tremaining: 21.9s\n",
      "876:\tlearn: 0.0007246\ttotal: 2m 34s\tremaining: 21.7s\n",
      "877:\tlearn: 0.0007246\ttotal: 2m 34s\tremaining: 21.5s\n",
      "878:\tlearn: 0.0007246\ttotal: 2m 35s\tremaining: 21.4s\n",
      "879:\tlearn: 0.0007246\ttotal: 2m 35s\tremaining: 21.2s\n",
      "880:\tlearn: 0.0007245\ttotal: 2m 35s\tremaining: 21s\n",
      "881:\tlearn: 0.0007245\ttotal: 2m 35s\tremaining: 20.8s\n",
      "882:\tlearn: 0.0007244\ttotal: 2m 35s\tremaining: 20.6s\n",
      "883:\tlearn: 0.0007238\ttotal: 2m 36s\tremaining: 20.5s\n",
      "884:\tlearn: 0.0007238\ttotal: 2m 36s\tremaining: 20.3s\n",
      "885:\tlearn: 0.0007237\ttotal: 2m 36s\tremaining: 20.1s\n",
      "886:\tlearn: 0.0007235\ttotal: 2m 36s\tremaining: 19.9s\n",
      "887:\tlearn: 0.0007235\ttotal: 2m 36s\tremaining: 19.8s\n",
      "888:\tlearn: 0.0007232\ttotal: 2m 36s\tremaining: 19.6s\n",
      "889:\tlearn: 0.0007230\ttotal: 2m 37s\tremaining: 19.4s\n",
      "890:\tlearn: 0.0007230\ttotal: 2m 37s\tremaining: 19.2s\n",
      "891:\tlearn: 0.0007230\ttotal: 2m 37s\tremaining: 19.1s\n",
      "892:\tlearn: 0.0007229\ttotal: 2m 37s\tremaining: 18.9s\n",
      "893:\tlearn: 0.0007228\ttotal: 2m 37s\tremaining: 18.7s\n",
      "894:\tlearn: 0.0007228\ttotal: 2m 37s\tremaining: 18.5s\n",
      "895:\tlearn: 0.0007227\ttotal: 2m 38s\tremaining: 18.4s\n",
      "896:\tlearn: 0.0007224\ttotal: 2m 38s\tremaining: 18.2s\n",
      "897:\tlearn: 0.0007224\ttotal: 2m 38s\tremaining: 18s\n",
      "898:\tlearn: 0.0007205\ttotal: 2m 38s\tremaining: 17.8s\n",
      "899:\tlearn: 0.0007205\ttotal: 2m 38s\tremaining: 17.6s\n",
      "900:\tlearn: 0.0007202\ttotal: 2m 38s\tremaining: 17.5s\n",
      "901:\tlearn: 0.0007201\ttotal: 2m 39s\tremaining: 17.3s\n",
      "902:\tlearn: 0.0007201\ttotal: 2m 39s\tremaining: 17.1s\n",
      "903:\tlearn: 0.0007200\ttotal: 2m 39s\tremaining: 16.9s\n",
      "904:\tlearn: 0.0007200\ttotal: 2m 39s\tremaining: 16.8s\n",
      "905:\tlearn: 0.0007200\ttotal: 2m 39s\tremaining: 16.6s\n",
      "906:\tlearn: 0.0007200\ttotal: 2m 40s\tremaining: 16.4s\n",
      "907:\tlearn: 0.0007200\ttotal: 2m 40s\tremaining: 16.2s\n",
      "908:\tlearn: 0.0007199\ttotal: 2m 40s\tremaining: 16.1s\n",
      "909:\tlearn: 0.0007199\ttotal: 2m 40s\tremaining: 15.9s\n",
      "910:\tlearn: 0.0007199\ttotal: 2m 40s\tremaining: 15.7s\n",
      "911:\tlearn: 0.0007199\ttotal: 2m 40s\tremaining: 15.5s\n",
      "912:\tlearn: 0.0007199\ttotal: 2m 41s\tremaining: 15.4s\n",
      "913:\tlearn: 0.0007199\ttotal: 2m 41s\tremaining: 15.2s\n",
      "914:\tlearn: 0.0007192\ttotal: 2m 41s\tremaining: 15s\n",
      "915:\tlearn: 0.0007192\ttotal: 2m 41s\tremaining: 14.8s\n",
      "916:\tlearn: 0.0007192\ttotal: 2m 41s\tremaining: 14.6s\n",
      "917:\tlearn: 0.0007192\ttotal: 2m 41s\tremaining: 14.5s\n",
      "918:\tlearn: 0.0007191\ttotal: 2m 42s\tremaining: 14.3s\n",
      "919:\tlearn: 0.0007191\ttotal: 2m 42s\tremaining: 14.1s\n",
      "920:\tlearn: 0.0007191\ttotal: 2m 42s\tremaining: 13.9s\n",
      "921:\tlearn: 0.0007190\ttotal: 2m 42s\tremaining: 13.8s\n",
      "922:\tlearn: 0.0007190\ttotal: 2m 42s\tremaining: 13.6s\n",
      "923:\tlearn: 0.0007190\ttotal: 2m 43s\tremaining: 13.4s\n",
      "924:\tlearn: 0.0007190\ttotal: 2m 43s\tremaining: 13.2s\n",
      "925:\tlearn: 0.0007190\ttotal: 2m 43s\tremaining: 13.1s\n",
      "926:\tlearn: 0.0007187\ttotal: 2m 43s\tremaining: 12.9s\n",
      "927:\tlearn: 0.0007185\ttotal: 2m 43s\tremaining: 12.7s\n",
      "928:\tlearn: 0.0007185\ttotal: 2m 43s\tremaining: 12.5s\n",
      "929:\tlearn: 0.0007184\ttotal: 2m 44s\tremaining: 12.3s\n",
      "930:\tlearn: 0.0007184\ttotal: 2m 44s\tremaining: 12.2s\n",
      "931:\tlearn: 0.0007184\ttotal: 2m 44s\tremaining: 12s\n",
      "932:\tlearn: 0.0007184\ttotal: 2m 44s\tremaining: 11.8s\n",
      "933:\tlearn: 0.0007182\ttotal: 2m 44s\tremaining: 11.6s\n",
      "934:\tlearn: 0.0007180\ttotal: 2m 44s\tremaining: 11.5s\n",
      "935:\tlearn: 0.0007180\ttotal: 2m 45s\tremaining: 11.3s\n",
      "936:\tlearn: 0.0007179\ttotal: 2m 45s\tremaining: 11.1s\n",
      "937:\tlearn: 0.0007178\ttotal: 2m 45s\tremaining: 10.9s\n",
      "938:\tlearn: 0.0007178\ttotal: 2m 45s\tremaining: 10.8s\n",
      "939:\tlearn: 0.0007178\ttotal: 2m 45s\tremaining: 10.6s\n",
      "940:\tlearn: 0.0007178\ttotal: 2m 45s\tremaining: 10.4s\n",
      "941:\tlearn: 0.0007177\ttotal: 2m 46s\tremaining: 10.2s\n",
      "942:\tlearn: 0.0007175\ttotal: 2m 46s\tremaining: 10.1s\n",
      "943:\tlearn: 0.0007173\ttotal: 2m 46s\tremaining: 9.88s\n",
      "944:\tlearn: 0.0007173\ttotal: 2m 46s\tremaining: 9.7s\n",
      "945:\tlearn: 0.0007173\ttotal: 2m 46s\tremaining: 9.53s\n",
      "946:\tlearn: 0.0007172\ttotal: 2m 47s\tremaining: 9.35s\n",
      "947:\tlearn: 0.0007172\ttotal: 2m 47s\tremaining: 9.17s\n",
      "948:\tlearn: 0.0007172\ttotal: 2m 47s\tremaining: 9s\n",
      "949:\tlearn: 0.0007172\ttotal: 2m 47s\tremaining: 8.82s\n",
      "950:\tlearn: 0.0007170\ttotal: 2m 47s\tremaining: 8.64s\n",
      "951:\tlearn: 0.0007169\ttotal: 2m 47s\tremaining: 8.47s\n",
      "952:\tlearn: 0.0007168\ttotal: 2m 48s\tremaining: 8.29s\n",
      "953:\tlearn: 0.0007167\ttotal: 2m 48s\tremaining: 8.12s\n",
      "954:\tlearn: 0.0007164\ttotal: 2m 48s\tremaining: 7.94s\n",
      "955:\tlearn: 0.0007162\ttotal: 2m 48s\tremaining: 7.76s\n",
      "956:\tlearn: 0.0007162\ttotal: 2m 48s\tremaining: 7.59s\n",
      "957:\tlearn: 0.0007162\ttotal: 2m 49s\tremaining: 7.41s\n",
      "958:\tlearn: 0.0007162\ttotal: 2m 49s\tremaining: 7.23s\n",
      "959:\tlearn: 0.0007161\ttotal: 2m 49s\tremaining: 7.06s\n",
      "960:\tlearn: 0.0007161\ttotal: 2m 49s\tremaining: 6.88s\n",
      "961:\tlearn: 0.0007161\ttotal: 2m 49s\tremaining: 6.7s\n",
      "962:\tlearn: 0.0007161\ttotal: 2m 49s\tremaining: 6.53s\n",
      "963:\tlearn: 0.0007161\ttotal: 2m 50s\tremaining: 6.35s\n",
      "964:\tlearn: 0.0007161\ttotal: 2m 50s\tremaining: 6.17s\n",
      "965:\tlearn: 0.0007161\ttotal: 2m 50s\tremaining: 6s\n",
      "966:\tlearn: 0.0007160\ttotal: 2m 50s\tremaining: 5.82s\n",
      "967:\tlearn: 0.0007151\ttotal: 2m 50s\tremaining: 5.64s\n",
      "968:\tlearn: 0.0007151\ttotal: 2m 50s\tremaining: 5.47s\n",
      "969:\tlearn: 0.0007151\ttotal: 2m 51s\tremaining: 5.29s\n",
      "970:\tlearn: 0.0007150\ttotal: 2m 51s\tremaining: 5.12s\n",
      "971:\tlearn: 0.0007150\ttotal: 2m 51s\tremaining: 4.94s\n",
      "972:\tlearn: 0.0007147\ttotal: 2m 51s\tremaining: 4.76s\n",
      "973:\tlearn: 0.0007146\ttotal: 2m 51s\tremaining: 4.59s\n",
      "974:\tlearn: 0.0007145\ttotal: 2m 52s\tremaining: 4.41s\n",
      "975:\tlearn: 0.0007145\ttotal: 2m 52s\tremaining: 4.23s\n",
      "976:\tlearn: 0.0007145\ttotal: 2m 52s\tremaining: 4.06s\n",
      "977:\tlearn: 0.0007144\ttotal: 2m 52s\tremaining: 3.88s\n",
      "978:\tlearn: 0.0007125\ttotal: 2m 52s\tremaining: 3.7s\n",
      "979:\tlearn: 0.0007123\ttotal: 2m 52s\tremaining: 3.53s\n",
      "980:\tlearn: 0.0007123\ttotal: 2m 53s\tremaining: 3.35s\n",
      "981:\tlearn: 0.0007122\ttotal: 2m 53s\tremaining: 3.17s\n",
      "982:\tlearn: 0.0007121\ttotal: 2m 53s\tremaining: 3s\n",
      "983:\tlearn: 0.0007121\ttotal: 2m 53s\tremaining: 2.82s\n",
      "984:\tlearn: 0.0007121\ttotal: 2m 53s\tremaining: 2.65s\n",
      "985:\tlearn: 0.0007121\ttotal: 2m 53s\tremaining: 2.47s\n",
      "986:\tlearn: 0.0007118\ttotal: 2m 54s\tremaining: 2.29s\n",
      "987:\tlearn: 0.0007117\ttotal: 2m 54s\tremaining: 2.12s\n",
      "988:\tlearn: 0.0007115\ttotal: 2m 54s\tremaining: 1.94s\n",
      "989:\tlearn: 0.0007112\ttotal: 2m 54s\tremaining: 1.76s\n",
      "990:\tlearn: 0.0007112\ttotal: 2m 54s\tremaining: 1.59s\n",
      "991:\tlearn: 0.0007112\ttotal: 2m 55s\tremaining: 1.41s\n",
      "992:\tlearn: 0.0007110\ttotal: 2m 55s\tremaining: 1.23s\n",
      "993:\tlearn: 0.0007110\ttotal: 2m 55s\tremaining: 1.06s\n",
      "994:\tlearn: 0.0007110\ttotal: 2m 55s\tremaining: 882ms\n",
      "995:\tlearn: 0.0007109\ttotal: 2m 55s\tremaining: 706ms\n",
      "996:\tlearn: 0.0007109\ttotal: 2m 55s\tremaining: 529ms\n",
      "997:\tlearn: 0.0007109\ttotal: 2m 56s\tremaining: 353ms\n",
      "998:\tlearn: 0.0007109\ttotal: 2m 56s\tremaining: 176ms\n",
      "999:\tlearn: 0.0007108\ttotal: 2m 56s\tremaining: 0us\n",
      "0:\tlearn: 0.5158290\ttotal: 255ms\tremaining: 4m 14s\n",
      "1:\tlearn: 0.4134066\ttotal: 456ms\tremaining: 3m 47s\n",
      "2:\tlearn: 0.3163830\ttotal: 700ms\tremaining: 3m 52s\n",
      "3:\tlearn: 0.2612109\ttotal: 948ms\tremaining: 3m 56s\n",
      "4:\tlearn: 0.1984294\ttotal: 1.19s\tremaining: 3m 57s\n",
      "5:\tlearn: 0.1651298\ttotal: 1.38s\tremaining: 3m 49s\n",
      "6:\tlearn: 0.1310679\ttotal: 1.59s\tremaining: 3m 45s\n",
      "7:\tlearn: 0.1097393\ttotal: 1.78s\tremaining: 3m 41s\n",
      "8:\tlearn: 0.1061142\ttotal: 1.98s\tremaining: 3m 38s\n",
      "9:\tlearn: 0.0845040\ttotal: 2.19s\tremaining: 3m 36s\n",
      "10:\tlearn: 0.0762285\ttotal: 2.4s\tremaining: 3m 35s\n",
      "11:\tlearn: 0.0553796\ttotal: 2.6s\tremaining: 3m 34s\n",
      "12:\tlearn: 0.0418496\ttotal: 2.81s\tremaining: 3m 33s\n",
      "13:\tlearn: 0.0360681\ttotal: 3.01s\tremaining: 3m 31s\n",
      "14:\tlearn: 0.0349221\ttotal: 3.2s\tremaining: 3m 30s\n",
      "15:\tlearn: 0.0282167\ttotal: 3.42s\tremaining: 3m 30s\n",
      "16:\tlearn: 0.0247500\ttotal: 3.63s\tremaining: 3m 29s\n",
      "17:\tlearn: 0.0215612\ttotal: 3.84s\tremaining: 3m 29s\n",
      "18:\tlearn: 0.0191713\ttotal: 4.05s\tremaining: 3m 29s\n",
      "19:\tlearn: 0.0168424\ttotal: 4.26s\tremaining: 3m 28s\n",
      "20:\tlearn: 0.0151898\ttotal: 4.46s\tremaining: 3m 27s\n",
      "21:\tlearn: 0.0138920\ttotal: 4.66s\tremaining: 3m 27s\n",
      "22:\tlearn: 0.0131052\ttotal: 4.86s\tremaining: 3m 26s\n",
      "23:\tlearn: 0.0119168\ttotal: 5.05s\tremaining: 3m 25s\n",
      "24:\tlearn: 0.0108684\ttotal: 5.25s\tremaining: 3m 24s\n",
      "25:\tlearn: 0.0101103\ttotal: 5.45s\tremaining: 3m 24s\n",
      "26:\tlearn: 0.0094599\ttotal: 5.65s\tremaining: 3m 23s\n",
      "27:\tlearn: 0.0088409\ttotal: 5.86s\tremaining: 3m 23s\n",
      "28:\tlearn: 0.0084026\ttotal: 6.06s\tremaining: 3m 22s\n",
      "29:\tlearn: 0.0079059\ttotal: 6.27s\tremaining: 3m 22s\n",
      "30:\tlearn: 0.0075975\ttotal: 6.47s\tremaining: 3m 22s\n",
      "31:\tlearn: 0.0073518\ttotal: 6.67s\tremaining: 3m 21s\n",
      "32:\tlearn: 0.0069664\ttotal: 6.87s\tremaining: 3m 21s\n",
      "33:\tlearn: 0.0066002\ttotal: 7.09s\tremaining: 3m 21s\n",
      "34:\tlearn: 0.0062940\ttotal: 7.29s\tremaining: 3m 20s\n",
      "35:\tlearn: 0.0060478\ttotal: 7.47s\tremaining: 3m 20s\n",
      "36:\tlearn: 0.0057629\ttotal: 7.67s\tremaining: 3m 19s\n",
      "37:\tlearn: 0.0055614\ttotal: 7.87s\tremaining: 3m 19s\n",
      "38:\tlearn: 0.0053665\ttotal: 8.07s\tremaining: 3m 18s\n",
      "39:\tlearn: 0.0051114\ttotal: 8.28s\tremaining: 3m 18s\n",
      "40:\tlearn: 0.0049368\ttotal: 8.48s\tremaining: 3m 18s\n",
      "41:\tlearn: 0.0046489\ttotal: 8.7s\tremaining: 3m 18s\n",
      "42:\tlearn: 0.0044797\ttotal: 8.91s\tremaining: 3m 18s\n",
      "43:\tlearn: 0.0042957\ttotal: 9.1s\tremaining: 3m 17s\n",
      "44:\tlearn: 0.0040320\ttotal: 9.29s\tremaining: 3m 17s\n",
      "45:\tlearn: 0.0038713\ttotal: 9.47s\tremaining: 3m 16s\n",
      "46:\tlearn: 0.0037869\ttotal: 9.66s\tremaining: 3m 15s\n",
      "47:\tlearn: 0.0036464\ttotal: 9.85s\tremaining: 3m 15s\n",
      "48:\tlearn: 0.0035470\ttotal: 10.1s\tremaining: 3m 15s\n",
      "49:\tlearn: 0.0034317\ttotal: 10.3s\tremaining: 3m 15s\n",
      "50:\tlearn: 0.0033461\ttotal: 10.5s\tremaining: 3m 14s\n",
      "51:\tlearn: 0.0032612\ttotal: 10.7s\tremaining: 3m 14s\n",
      "52:\tlearn: 0.0031883\ttotal: 10.8s\tremaining: 3m 13s\n",
      "53:\tlearn: 0.0030993\ttotal: 11s\tremaining: 3m 13s\n",
      "54:\tlearn: 0.0030099\ttotal: 11.2s\tremaining: 3m 12s\n",
      "55:\tlearn: 0.0028502\ttotal: 11.4s\tremaining: 3m 12s\n",
      "56:\tlearn: 0.0027882\ttotal: 11.6s\tremaining: 3m 12s\n",
      "57:\tlearn: 0.0027317\ttotal: 11.8s\tremaining: 3m 11s\n",
      "58:\tlearn: 0.0026782\ttotal: 12s\tremaining: 3m 11s\n",
      "59:\tlearn: 0.0026135\ttotal: 12.2s\tremaining: 3m 10s\n",
      "60:\tlearn: 0.0025868\ttotal: 12.4s\tremaining: 3m 10s\n",
      "61:\tlearn: 0.0025410\ttotal: 12.6s\tremaining: 3m 10s\n",
      "62:\tlearn: 0.0024918\ttotal: 12.8s\tremaining: 3m 10s\n",
      "63:\tlearn: 0.0024119\ttotal: 13s\tremaining: 3m 9s\n",
      "64:\tlearn: 0.0023724\ttotal: 13.2s\tremaining: 3m 9s\n",
      "65:\tlearn: 0.0023245\ttotal: 13.4s\tremaining: 3m 9s\n",
      "66:\tlearn: 0.0022701\ttotal: 13.6s\tremaining: 3m 9s\n",
      "67:\tlearn: 0.0022697\ttotal: 13.8s\tremaining: 3m 9s\n",
      "68:\tlearn: 0.0022577\ttotal: 14s\tremaining: 3m 8s\n",
      "69:\tlearn: 0.0022144\ttotal: 14.2s\tremaining: 3m 8s\n",
      "70:\tlearn: 0.0021878\ttotal: 14.4s\tremaining: 3m 8s\n",
      "71:\tlearn: 0.0021527\ttotal: 14.6s\tremaining: 3m 8s\n",
      "72:\tlearn: 0.0021214\ttotal: 14.8s\tremaining: 3m 8s\n",
      "73:\tlearn: 0.0020941\ttotal: 15s\tremaining: 3m 7s\n",
      "74:\tlearn: 0.0020479\ttotal: 15.2s\tremaining: 3m 7s\n",
      "75:\tlearn: 0.0020329\ttotal: 15.4s\tremaining: 3m 7s\n",
      "76:\tlearn: 0.0020057\ttotal: 15.6s\tremaining: 3m 6s\n",
      "77:\tlearn: 0.0019720\ttotal: 15.8s\tremaining: 3m 6s\n",
      "78:\tlearn: 0.0019430\ttotal: 16s\tremaining: 3m 6s\n",
      "79:\tlearn: 0.0018975\ttotal: 16.2s\tremaining: 3m 6s\n",
      "80:\tlearn: 0.0018680\ttotal: 16.4s\tremaining: 3m 6s\n",
      "81:\tlearn: 0.0018298\ttotal: 16.6s\tremaining: 3m 5s\n",
      "82:\tlearn: 0.0017934\ttotal: 16.8s\tremaining: 3m 5s\n",
      "83:\tlearn: 0.0017664\ttotal: 17s\tremaining: 3m 5s\n",
      "84:\tlearn: 0.0017661\ttotal: 17.2s\tremaining: 3m 4s\n",
      "85:\tlearn: 0.0017124\ttotal: 17.4s\tremaining: 3m 4s\n",
      "86:\tlearn: 0.0016857\ttotal: 17.6s\tremaining: 3m 4s\n",
      "87:\tlearn: 0.0016602\ttotal: 17.7s\tremaining: 3m 3s\n",
      "88:\tlearn: 0.0016368\ttotal: 17.9s\tremaining: 3m 3s\n",
      "89:\tlearn: 0.0016366\ttotal: 18.1s\tremaining: 3m 3s\n",
      "90:\tlearn: 0.0016288\ttotal: 18.3s\tremaining: 3m 2s\n",
      "91:\tlearn: 0.0016025\ttotal: 18.5s\tremaining: 3m 2s\n",
      "92:\tlearn: 0.0015803\ttotal: 18.7s\tremaining: 3m 2s\n",
      "93:\tlearn: 0.0015614\ttotal: 18.9s\tremaining: 3m 2s\n",
      "94:\tlearn: 0.0015392\ttotal: 19.1s\tremaining: 3m 1s\n",
      "95:\tlearn: 0.0015149\ttotal: 19.3s\tremaining: 3m 1s\n",
      "96:\tlearn: 0.0014831\ttotal: 19.5s\tremaining: 3m 1s\n",
      "97:\tlearn: 0.0014610\ttotal: 19.7s\tremaining: 3m 1s\n",
      "98:\tlearn: 0.0014398\ttotal: 19.9s\tremaining: 3m 1s\n",
      "99:\tlearn: 0.0014255\ttotal: 20.1s\tremaining: 3m 1s\n",
      "100:\tlearn: 0.0014084\ttotal: 20.3s\tremaining: 3m\n",
      "101:\tlearn: 0.0014084\ttotal: 20.5s\tremaining: 3m\n",
      "102:\tlearn: 0.0013849\ttotal: 20.7s\tremaining: 3m\n",
      "103:\tlearn: 0.0013743\ttotal: 20.9s\tremaining: 3m\n",
      "104:\tlearn: 0.0013521\ttotal: 21.1s\tremaining: 3m\n",
      "105:\tlearn: 0.0013521\ttotal: 21.3s\tremaining: 2m 59s\n",
      "106:\tlearn: 0.0013382\ttotal: 21.5s\tremaining: 2m 59s\n",
      "107:\tlearn: 0.0013299\ttotal: 21.7s\tremaining: 2m 59s\n",
      "108:\tlearn: 0.0013121\ttotal: 21.9s\tremaining: 2m 58s\n",
      "109:\tlearn: 0.0013120\ttotal: 22.1s\tremaining: 2m 58s\n",
      "110:\tlearn: 0.0013120\ttotal: 22.2s\tremaining: 2m 58s\n",
      "111:\tlearn: 0.0013001\ttotal: 22.4s\tremaining: 2m 57s\n",
      "112:\tlearn: 0.0012863\ttotal: 22.6s\tremaining: 2m 57s\n",
      "113:\tlearn: 0.0012722\ttotal: 22.8s\tremaining: 2m 56s\n",
      "114:\tlearn: 0.0012720\ttotal: 22.9s\tremaining: 2m 56s\n",
      "115:\tlearn: 0.0012626\ttotal: 23.1s\tremaining: 2m 56s\n",
      "116:\tlearn: 0.0012517\ttotal: 23.3s\tremaining: 2m 55s\n",
      "117:\tlearn: 0.0012516\ttotal: 23.5s\tremaining: 2m 55s\n",
      "118:\tlearn: 0.0012515\ttotal: 23.6s\tremaining: 2m 54s\n",
      "119:\tlearn: 0.0012404\ttotal: 23.8s\tremaining: 2m 54s\n",
      "120:\tlearn: 0.0012278\ttotal: 24s\tremaining: 2m 54s\n",
      "121:\tlearn: 0.0012149\ttotal: 24.2s\tremaining: 2m 53s\n",
      "122:\tlearn: 0.0012038\ttotal: 24.3s\tremaining: 2m 53s\n",
      "123:\tlearn: 0.0011914\ttotal: 24.5s\tremaining: 2m 53s\n",
      "124:\tlearn: 0.0011912\ttotal: 24.7s\tremaining: 2m 52s\n",
      "125:\tlearn: 0.0011912\ttotal: 24.9s\tremaining: 2m 52s\n",
      "126:\tlearn: 0.0011911\ttotal: 25s\tremaining: 2m 52s\n",
      "127:\tlearn: 0.0011909\ttotal: 25.2s\tremaining: 2m 51s\n",
      "128:\tlearn: 0.0011909\ttotal: 25.4s\tremaining: 2m 51s\n",
      "129:\tlearn: 0.0011811\ttotal: 25.5s\tremaining: 2m 50s\n",
      "130:\tlearn: 0.0011716\ttotal: 25.7s\tremaining: 2m 50s\n",
      "131:\tlearn: 0.0011590\ttotal: 25.9s\tremaining: 2m 50s\n",
      "132:\tlearn: 0.0011552\ttotal: 26.1s\tremaining: 2m 49s\n",
      "133:\tlearn: 0.0011552\ttotal: 26.3s\tremaining: 2m 49s\n",
      "134:\tlearn: 0.0011551\ttotal: 26.4s\tremaining: 2m 49s\n",
      "135:\tlearn: 0.0011550\ttotal: 26.6s\tremaining: 2m 49s\n",
      "136:\tlearn: 0.0011550\ttotal: 26.8s\tremaining: 2m 48s\n",
      "137:\tlearn: 0.0011545\ttotal: 27s\tremaining: 2m 48s\n",
      "138:\tlearn: 0.0011545\ttotal: 27.1s\tremaining: 2m 48s\n",
      "139:\tlearn: 0.0011545\ttotal: 27.3s\tremaining: 2m 47s\n",
      "140:\tlearn: 0.0011544\ttotal: 27.5s\tremaining: 2m 47s\n",
      "141:\tlearn: 0.0011411\ttotal: 27.7s\tremaining: 2m 47s\n",
      "142:\tlearn: 0.0011325\ttotal: 27.8s\tremaining: 2m 46s\n",
      "143:\tlearn: 0.0011325\ttotal: 28s\tremaining: 2m 46s\n",
      "144:\tlearn: 0.0011325\ttotal: 28.2s\tremaining: 2m 46s\n",
      "145:\tlearn: 0.0011323\ttotal: 28.4s\tremaining: 2m 45s\n",
      "146:\tlearn: 0.0011323\ttotal: 28.5s\tremaining: 2m 45s\n",
      "147:\tlearn: 0.0011323\ttotal: 28.7s\tremaining: 2m 45s\n",
      "148:\tlearn: 0.0011318\ttotal: 28.9s\tremaining: 2m 45s\n",
      "149:\tlearn: 0.0011318\ttotal: 29.1s\tremaining: 2m 44s\n",
      "150:\tlearn: 0.0011317\ttotal: 29.2s\tremaining: 2m 44s\n",
      "151:\tlearn: 0.0011316\ttotal: 29.4s\tremaining: 2m 44s\n",
      "152:\tlearn: 0.0011315\ttotal: 29.6s\tremaining: 2m 43s\n",
      "153:\tlearn: 0.0011146\ttotal: 29.8s\tremaining: 2m 43s\n",
      "154:\tlearn: 0.0011143\ttotal: 29.9s\tremaining: 2m 43s\n",
      "155:\tlearn: 0.0011142\ttotal: 30.1s\tremaining: 2m 42s\n",
      "156:\tlearn: 0.0011142\ttotal: 30.3s\tremaining: 2m 42s\n",
      "157:\tlearn: 0.0011142\ttotal: 30.5s\tremaining: 2m 42s\n",
      "158:\tlearn: 0.0011139\ttotal: 30.6s\tremaining: 2m 42s\n",
      "159:\tlearn: 0.0011137\ttotal: 30.8s\tremaining: 2m 41s\n",
      "160:\tlearn: 0.0011135\ttotal: 31s\tremaining: 2m 41s\n",
      "161:\tlearn: 0.0011099\ttotal: 31.2s\tremaining: 2m 41s\n",
      "162:\tlearn: 0.0011096\ttotal: 31.3s\tremaining: 2m 40s\n",
      "163:\tlearn: 0.0011094\ttotal: 31.5s\tremaining: 2m 40s\n",
      "164:\tlearn: 0.0011094\ttotal: 31.7s\tremaining: 2m 40s\n",
      "165:\tlearn: 0.0011092\ttotal: 31.9s\tremaining: 2m 40s\n",
      "166:\tlearn: 0.0011089\ttotal: 32s\tremaining: 2m 39s\n",
      "167:\tlearn: 0.0010987\ttotal: 32.2s\tremaining: 2m 39s\n",
      "168:\tlearn: 0.0010985\ttotal: 32.4s\tremaining: 2m 39s\n",
      "169:\tlearn: 0.0010980\ttotal: 32.6s\tremaining: 2m 39s\n",
      "170:\tlearn: 0.0010980\ttotal: 32.8s\tremaining: 2m 38s\n",
      "171:\tlearn: 0.0010976\ttotal: 32.9s\tremaining: 2m 38s\n",
      "172:\tlearn: 0.0010976\ttotal: 33.1s\tremaining: 2m 38s\n",
      "173:\tlearn: 0.0010973\ttotal: 33.3s\tremaining: 2m 37s\n",
      "174:\tlearn: 0.0010972\ttotal: 33.5s\tremaining: 2m 37s\n",
      "175:\tlearn: 0.0010971\ttotal: 33.6s\tremaining: 2m 37s\n",
      "176:\tlearn: 0.0010942\ttotal: 33.8s\tremaining: 2m 37s\n",
      "177:\tlearn: 0.0010936\ttotal: 34s\tremaining: 2m 36s\n",
      "178:\tlearn: 0.0010834\ttotal: 34.2s\tremaining: 2m 36s\n",
      "179:\tlearn: 0.0010834\ttotal: 34.3s\tremaining: 2m 36s\n",
      "180:\tlearn: 0.0010832\ttotal: 34.5s\tremaining: 2m 36s\n",
      "181:\tlearn: 0.0010831\ttotal: 34.7s\tremaining: 2m 35s\n",
      "182:\tlearn: 0.0010831\ttotal: 34.9s\tremaining: 2m 35s\n",
      "183:\tlearn: 0.0010831\ttotal: 35s\tremaining: 2m 35s\n",
      "184:\tlearn: 0.0010831\ttotal: 35.2s\tremaining: 2m 35s\n",
      "185:\tlearn: 0.0010791\ttotal: 35.4s\tremaining: 2m 34s\n",
      "186:\tlearn: 0.0010704\ttotal: 35.6s\tremaining: 2m 34s\n",
      "187:\tlearn: 0.0010640\ttotal: 35.7s\tremaining: 2m 34s\n",
      "188:\tlearn: 0.0010517\ttotal: 35.9s\tremaining: 2m 34s\n",
      "189:\tlearn: 0.0010514\ttotal: 36.1s\tremaining: 2m 33s\n",
      "190:\tlearn: 0.0010457\ttotal: 36.3s\tremaining: 2m 33s\n",
      "191:\tlearn: 0.0010456\ttotal: 36.4s\tremaining: 2m 33s\n",
      "192:\tlearn: 0.0010456\ttotal: 36.6s\tremaining: 2m 33s\n",
      "193:\tlearn: 0.0010456\ttotal: 36.8s\tremaining: 2m 32s\n",
      "194:\tlearn: 0.0010453\ttotal: 37s\tremaining: 2m 32s\n",
      "195:\tlearn: 0.0010452\ttotal: 37.1s\tremaining: 2m 32s\n",
      "196:\tlearn: 0.0010451\ttotal: 37.3s\tremaining: 2m 32s\n",
      "197:\tlearn: 0.0010391\ttotal: 37.5s\tremaining: 2m 31s\n",
      "198:\tlearn: 0.0010273\ttotal: 37.7s\tremaining: 2m 31s\n",
      "199:\tlearn: 0.0010271\ttotal: 37.8s\tremaining: 2m 31s\n",
      "200:\tlearn: 0.0010270\ttotal: 38s\tremaining: 2m 31s\n",
      "201:\tlearn: 0.0010262\ttotal: 38.2s\tremaining: 2m 30s\n",
      "202:\tlearn: 0.0010261\ttotal: 38.4s\tremaining: 2m 30s\n",
      "203:\tlearn: 0.0010259\ttotal: 38.5s\tremaining: 2m 30s\n",
      "204:\tlearn: 0.0010259\ttotal: 38.7s\tremaining: 2m 30s\n",
      "205:\tlearn: 0.0010259\ttotal: 38.9s\tremaining: 2m 29s\n",
      "206:\tlearn: 0.0010258\ttotal: 39.1s\tremaining: 2m 29s\n",
      "207:\tlearn: 0.0010258\ttotal: 39.2s\tremaining: 2m 29s\n",
      "208:\tlearn: 0.0010230\ttotal: 39.4s\tremaining: 2m 29s\n",
      "209:\tlearn: 0.0010224\ttotal: 39.6s\tremaining: 2m 28s\n",
      "210:\tlearn: 0.0010222\ttotal: 39.8s\tremaining: 2m 28s\n",
      "211:\tlearn: 0.0010222\ttotal: 39.9s\tremaining: 2m 28s\n",
      "212:\tlearn: 0.0010222\ttotal: 40.1s\tremaining: 2m 28s\n",
      "213:\tlearn: 0.0010222\ttotal: 40.3s\tremaining: 2m 27s\n",
      "214:\tlearn: 0.0010222\ttotal: 40.5s\tremaining: 2m 27s\n",
      "215:\tlearn: 0.0010222\ttotal: 40.6s\tremaining: 2m 27s\n",
      "216:\tlearn: 0.0010216\ttotal: 40.8s\tremaining: 2m 27s\n",
      "217:\tlearn: 0.0010214\ttotal: 41s\tremaining: 2m 27s\n",
      "218:\tlearn: 0.0010189\ttotal: 41.2s\tremaining: 2m 26s\n",
      "219:\tlearn: 0.0010186\ttotal: 41.3s\tremaining: 2m 26s\n",
      "220:\tlearn: 0.0010186\ttotal: 41.5s\tremaining: 2m 26s\n",
      "221:\tlearn: 0.0010185\ttotal: 41.7s\tremaining: 2m 26s\n",
      "222:\tlearn: 0.0010184\ttotal: 41.9s\tremaining: 2m 25s\n",
      "223:\tlearn: 0.0010174\ttotal: 42s\tremaining: 2m 25s\n",
      "224:\tlearn: 0.0010169\ttotal: 42.2s\tremaining: 2m 25s\n",
      "225:\tlearn: 0.0010102\ttotal: 42.4s\tremaining: 2m 25s\n",
      "226:\tlearn: 0.0010028\ttotal: 42.6s\tremaining: 2m 24s\n",
      "227:\tlearn: 0.0010027\ttotal: 42.7s\tremaining: 2m 24s\n",
      "228:\tlearn: 0.0010027\ttotal: 42.9s\tremaining: 2m 24s\n",
      "229:\tlearn: 0.0010026\ttotal: 43.1s\tremaining: 2m 24s\n",
      "230:\tlearn: 0.0010026\ttotal: 43.3s\tremaining: 2m 24s\n",
      "231:\tlearn: 0.0010025\ttotal: 43.4s\tremaining: 2m 23s\n",
      "232:\tlearn: 0.0010024\ttotal: 43.6s\tremaining: 2m 23s\n",
      "233:\tlearn: 0.0010024\ttotal: 43.8s\tremaining: 2m 23s\n",
      "234:\tlearn: 0.0010024\ttotal: 44s\tremaining: 2m 23s\n",
      "235:\tlearn: 0.0010024\ttotal: 44.2s\tremaining: 2m 22s\n",
      "236:\tlearn: 0.0010024\ttotal: 44.3s\tremaining: 2m 22s\n",
      "237:\tlearn: 0.0010023\ttotal: 44.5s\tremaining: 2m 22s\n",
      "238:\tlearn: 0.0010022\ttotal: 44.7s\tremaining: 2m 22s\n",
      "239:\tlearn: 0.0010017\ttotal: 44.9s\tremaining: 2m 22s\n",
      "240:\tlearn: 0.0009997\ttotal: 45s\tremaining: 2m 21s\n",
      "241:\tlearn: 0.0009997\ttotal: 45.2s\tremaining: 2m 21s\n",
      "242:\tlearn: 0.0009997\ttotal: 45.4s\tremaining: 2m 21s\n",
      "243:\tlearn: 0.0009995\ttotal: 45.6s\tremaining: 2m 21s\n",
      "244:\tlearn: 0.0009994\ttotal: 45.7s\tremaining: 2m 20s\n",
      "245:\tlearn: 0.0009994\ttotal: 45.9s\tremaining: 2m 20s\n",
      "246:\tlearn: 0.0009994\ttotal: 46.1s\tremaining: 2m 20s\n",
      "247:\tlearn: 0.0009993\ttotal: 46.3s\tremaining: 2m 20s\n",
      "248:\tlearn: 0.0009935\ttotal: 46.4s\tremaining: 2m 20s\n",
      "249:\tlearn: 0.0009934\ttotal: 46.6s\tremaining: 2m 19s\n",
      "250:\tlearn: 0.0009931\ttotal: 46.8s\tremaining: 2m 19s\n",
      "251:\tlearn: 0.0009816\ttotal: 47s\tremaining: 2m 19s\n",
      "252:\tlearn: 0.0009816\ttotal: 47.1s\tremaining: 2m 19s\n",
      "253:\tlearn: 0.0009816\ttotal: 47.3s\tremaining: 2m 18s\n",
      "254:\tlearn: 0.0009816\ttotal: 47.5s\tremaining: 2m 18s\n",
      "255:\tlearn: 0.0009816\ttotal: 47.7s\tremaining: 2m 18s\n",
      "256:\tlearn: 0.0009815\ttotal: 47.8s\tremaining: 2m 18s\n",
      "257:\tlearn: 0.0009814\ttotal: 48s\tremaining: 2m 18s\n",
      "258:\tlearn: 0.0009813\ttotal: 48.2s\tremaining: 2m 17s\n",
      "259:\tlearn: 0.0009812\ttotal: 48.4s\tremaining: 2m 17s\n",
      "260:\tlearn: 0.0009812\ttotal: 48.5s\tremaining: 2m 17s\n",
      "261:\tlearn: 0.0009780\ttotal: 48.7s\tremaining: 2m 17s\n",
      "262:\tlearn: 0.0009779\ttotal: 48.9s\tremaining: 2m 17s\n",
      "263:\tlearn: 0.0009778\ttotal: 49.1s\tremaining: 2m 16s\n",
      "264:\tlearn: 0.0009723\ttotal: 49.2s\tremaining: 2m 16s\n",
      "265:\tlearn: 0.0009651\ttotal: 49.4s\tremaining: 2m 16s\n",
      "266:\tlearn: 0.0009651\ttotal: 49.6s\tremaining: 2m 16s\n",
      "267:\tlearn: 0.0009651\ttotal: 49.8s\tremaining: 2m 15s\n",
      "268:\tlearn: 0.0009488\ttotal: 49.9s\tremaining: 2m 15s\n",
      "269:\tlearn: 0.0009456\ttotal: 50.1s\tremaining: 2m 15s\n",
      "270:\tlearn: 0.0009454\ttotal: 50.3s\tremaining: 2m 15s\n",
      "271:\tlearn: 0.0009454\ttotal: 50.5s\tremaining: 2m 15s\n",
      "272:\tlearn: 0.0009453\ttotal: 50.6s\tremaining: 2m 14s\n",
      "273:\tlearn: 0.0009453\ttotal: 50.8s\tremaining: 2m 14s\n",
      "274:\tlearn: 0.0009452\ttotal: 51s\tremaining: 2m 14s\n",
      "275:\tlearn: 0.0009452\ttotal: 51.2s\tremaining: 2m 14s\n",
      "276:\tlearn: 0.0009414\ttotal: 51.3s\tremaining: 2m 14s\n",
      "277:\tlearn: 0.0009413\ttotal: 51.5s\tremaining: 2m 13s\n",
      "278:\tlearn: 0.0009412\ttotal: 51.7s\tremaining: 2m 13s\n",
      "279:\tlearn: 0.0009411\ttotal: 51.9s\tremaining: 2m 13s\n",
      "280:\tlearn: 0.0009411\ttotal: 52s\tremaining: 2m 13s\n",
      "281:\tlearn: 0.0009409\ttotal: 52.2s\tremaining: 2m 12s\n",
      "282:\tlearn: 0.0009409\ttotal: 52.4s\tremaining: 2m 12s\n",
      "283:\tlearn: 0.0009409\ttotal: 52.6s\tremaining: 2m 12s\n",
      "284:\tlearn: 0.0009407\ttotal: 52.7s\tremaining: 2m 12s\n",
      "285:\tlearn: 0.0009406\ttotal: 52.9s\tremaining: 2m 12s\n",
      "286:\tlearn: 0.0009406\ttotal: 53.1s\tremaining: 2m 11s\n",
      "287:\tlearn: 0.0009406\ttotal: 53.3s\tremaining: 2m 11s\n",
      "288:\tlearn: 0.0009404\ttotal: 53.4s\tremaining: 2m 11s\n",
      "289:\tlearn: 0.0009403\ttotal: 53.6s\tremaining: 2m 11s\n",
      "290:\tlearn: 0.0009379\ttotal: 53.8s\tremaining: 2m 11s\n",
      "291:\tlearn: 0.0009313\ttotal: 54s\tremaining: 2m 10s\n",
      "292:\tlearn: 0.0009311\ttotal: 54.1s\tremaining: 2m 10s\n",
      "293:\tlearn: 0.0009310\ttotal: 54.3s\tremaining: 2m 10s\n",
      "294:\tlearn: 0.0009310\ttotal: 54.5s\tremaining: 2m 10s\n",
      "295:\tlearn: 0.0009309\ttotal: 54.6s\tremaining: 2m 9s\n",
      "296:\tlearn: 0.0009309\ttotal: 54.8s\tremaining: 2m 9s\n",
      "297:\tlearn: 0.0009309\ttotal: 55s\tremaining: 2m 9s\n",
      "298:\tlearn: 0.0009309\ttotal: 55.2s\tremaining: 2m 9s\n",
      "299:\tlearn: 0.0009308\ttotal: 55.3s\tremaining: 2m 9s\n",
      "300:\tlearn: 0.0009307\ttotal: 55.5s\tremaining: 2m 8s\n",
      "301:\tlearn: 0.0009301\ttotal: 55.7s\tremaining: 2m 8s\n",
      "302:\tlearn: 0.0009300\ttotal: 55.9s\tremaining: 2m 8s\n",
      "303:\tlearn: 0.0009300\ttotal: 56.1s\tremaining: 2m 8s\n",
      "304:\tlearn: 0.0009299\ttotal: 56.2s\tremaining: 2m 8s\n",
      "305:\tlearn: 0.0009299\ttotal: 56.4s\tremaining: 2m 7s\n",
      "306:\tlearn: 0.0009299\ttotal: 56.6s\tremaining: 2m 7s\n",
      "307:\tlearn: 0.0009298\ttotal: 56.7s\tremaining: 2m 7s\n",
      "308:\tlearn: 0.0009298\ttotal: 56.9s\tremaining: 2m 7s\n",
      "309:\tlearn: 0.0009297\ttotal: 57.1s\tremaining: 2m 7s\n",
      "310:\tlearn: 0.0009297\ttotal: 57.3s\tremaining: 2m 6s\n",
      "311:\tlearn: 0.0009297\ttotal: 57.4s\tremaining: 2m 6s\n",
      "312:\tlearn: 0.0009297\ttotal: 57.6s\tremaining: 2m 6s\n",
      "313:\tlearn: 0.0009259\ttotal: 57.8s\tremaining: 2m 6s\n",
      "314:\tlearn: 0.0009203\ttotal: 58s\tremaining: 2m 6s\n",
      "315:\tlearn: 0.0009124\ttotal: 58.1s\tremaining: 2m 5s\n",
      "316:\tlearn: 0.0009124\ttotal: 58.3s\tremaining: 2m 5s\n",
      "317:\tlearn: 0.0009120\ttotal: 58.5s\tremaining: 2m 5s\n",
      "318:\tlearn: 0.0009120\ttotal: 58.7s\tremaining: 2m 5s\n",
      "319:\tlearn: 0.0009120\ttotal: 58.8s\tremaining: 2m 5s\n",
      "320:\tlearn: 0.0009119\ttotal: 59s\tremaining: 2m 4s\n",
      "321:\tlearn: 0.0009119\ttotal: 59.2s\tremaining: 2m 4s\n",
      "322:\tlearn: 0.0009119\ttotal: 59.4s\tremaining: 2m 4s\n",
      "323:\tlearn: 0.0009117\ttotal: 59.5s\tremaining: 2m 4s\n",
      "324:\tlearn: 0.0009116\ttotal: 59.7s\tremaining: 2m 4s\n",
      "325:\tlearn: 0.0009115\ttotal: 59.9s\tremaining: 2m 3s\n",
      "326:\tlearn: 0.0009069\ttotal: 1m\tremaining: 2m 3s\n",
      "327:\tlearn: 0.0009068\ttotal: 1m\tremaining: 2m 3s\n",
      "328:\tlearn: 0.0009068\ttotal: 1m\tremaining: 2m 3s\n",
      "329:\tlearn: 0.0009068\ttotal: 1m\tremaining: 2m 3s\n",
      "330:\tlearn: 0.0009068\ttotal: 1m\tremaining: 2m 2s\n",
      "331:\tlearn: 0.0009067\ttotal: 1m\tremaining: 2m 2s\n",
      "332:\tlearn: 0.0008995\ttotal: 1m 1s\tremaining: 2m 2s\n",
      "333:\tlearn: 0.0008995\ttotal: 1m 1s\tremaining: 2m 2s\n",
      "334:\tlearn: 0.0008995\ttotal: 1m 1s\tremaining: 2m 1s\n",
      "335:\tlearn: 0.0008995\ttotal: 1m 1s\tremaining: 2m 1s\n",
      "336:\tlearn: 0.0008993\ttotal: 1m 1s\tremaining: 2m 1s\n",
      "337:\tlearn: 0.0008944\ttotal: 1m 1s\tremaining: 2m 1s\n",
      "338:\tlearn: 0.0008943\ttotal: 1m 2s\tremaining: 2m 1s\n",
      "339:\tlearn: 0.0008943\ttotal: 1m 2s\tremaining: 2m\n",
      "340:\tlearn: 0.0008942\ttotal: 1m 2s\tremaining: 2m\n",
      "341:\tlearn: 0.0008942\ttotal: 1m 2s\tremaining: 2m\n",
      "342:\tlearn: 0.0008940\ttotal: 1m 2s\tremaining: 2m\n",
      "343:\tlearn: 0.0008937\ttotal: 1m 3s\tremaining: 2m\n",
      "344:\tlearn: 0.0008910\ttotal: 1m 3s\tremaining: 1m 59s\n",
      "345:\tlearn: 0.0008907\ttotal: 1m 3s\tremaining: 1m 59s\n",
      "346:\tlearn: 0.0008907\ttotal: 1m 3s\tremaining: 1m 59s\n",
      "347:\tlearn: 0.0008906\ttotal: 1m 3s\tremaining: 1m 59s\n",
      "348:\tlearn: 0.0008906\ttotal: 1m 3s\tremaining: 1m 59s\n",
      "349:\tlearn: 0.0008906\ttotal: 1m 4s\tremaining: 1m 58s\n",
      "350:\tlearn: 0.0008906\ttotal: 1m 4s\tremaining: 1m 58s\n",
      "351:\tlearn: 0.0008906\ttotal: 1m 4s\tremaining: 1m 58s\n",
      "352:\tlearn: 0.0008904\ttotal: 1m 4s\tremaining: 1m 58s\n",
      "353:\tlearn: 0.0008826\ttotal: 1m 4s\tremaining: 1m 58s\n",
      "354:\tlearn: 0.0008825\ttotal: 1m 4s\tremaining: 1m 58s\n",
      "355:\tlearn: 0.0008825\ttotal: 1m 5s\tremaining: 1m 57s\n",
      "356:\tlearn: 0.0008825\ttotal: 1m 5s\tremaining: 1m 57s\n",
      "357:\tlearn: 0.0008824\ttotal: 1m 5s\tremaining: 1m 57s\n",
      "358:\tlearn: 0.0008823\ttotal: 1m 5s\tremaining: 1m 57s\n",
      "359:\tlearn: 0.0008823\ttotal: 1m 5s\tremaining: 1m 57s\n",
      "360:\tlearn: 0.0008823\ttotal: 1m 5s\tremaining: 1m 56s\n",
      "361:\tlearn: 0.0008823\ttotal: 1m 6s\tremaining: 1m 56s\n",
      "362:\tlearn: 0.0008823\ttotal: 1m 6s\tremaining: 1m 56s\n",
      "363:\tlearn: 0.0008821\ttotal: 1m 6s\tremaining: 1m 56s\n",
      "364:\tlearn: 0.0008818\ttotal: 1m 6s\tremaining: 1m 56s\n",
      "365:\tlearn: 0.0008818\ttotal: 1m 6s\tremaining: 1m 55s\n",
      "366:\tlearn: 0.0008817\ttotal: 1m 7s\tremaining: 1m 55s\n",
      "367:\tlearn: 0.0008817\ttotal: 1m 7s\tremaining: 1m 55s\n",
      "368:\tlearn: 0.0008816\ttotal: 1m 7s\tremaining: 1m 55s\n",
      "369:\tlearn: 0.0008765\ttotal: 1m 7s\tremaining: 1m 55s\n",
      "370:\tlearn: 0.0008764\ttotal: 1m 7s\tremaining: 1m 54s\n",
      "371:\tlearn: 0.0008764\ttotal: 1m 7s\tremaining: 1m 54s\n",
      "372:\tlearn: 0.0008763\ttotal: 1m 8s\tremaining: 1m 54s\n",
      "373:\tlearn: 0.0008763\ttotal: 1m 8s\tremaining: 1m 54s\n",
      "374:\tlearn: 0.0008763\ttotal: 1m 8s\tremaining: 1m 54s\n",
      "375:\tlearn: 0.0008762\ttotal: 1m 8s\tremaining: 1m 53s\n",
      "376:\tlearn: 0.0008762\ttotal: 1m 8s\tremaining: 1m 53s\n",
      "377:\tlearn: 0.0008762\ttotal: 1m 8s\tremaining: 1m 53s\n",
      "378:\tlearn: 0.0008760\ttotal: 1m 9s\tremaining: 1m 53s\n",
      "379:\tlearn: 0.0008757\ttotal: 1m 9s\tremaining: 1m 53s\n",
      "380:\tlearn: 0.0008757\ttotal: 1m 9s\tremaining: 1m 52s\n",
      "381:\tlearn: 0.0008756\ttotal: 1m 9s\tremaining: 1m 52s\n",
      "382:\tlearn: 0.0008755\ttotal: 1m 9s\tremaining: 1m 52s\n",
      "383:\tlearn: 0.0008755\ttotal: 1m 10s\tremaining: 1m 52s\n",
      "384:\tlearn: 0.0008755\ttotal: 1m 10s\tremaining: 1m 52s\n",
      "385:\tlearn: 0.0008755\ttotal: 1m 10s\tremaining: 1m 51s\n",
      "386:\tlearn: 0.0008755\ttotal: 1m 10s\tremaining: 1m 51s\n",
      "387:\tlearn: 0.0008754\ttotal: 1m 10s\tremaining: 1m 51s\n",
      "388:\tlearn: 0.0008754\ttotal: 1m 10s\tremaining: 1m 51s\n",
      "389:\tlearn: 0.0008752\ttotal: 1m 11s\tremaining: 1m 51s\n",
      "390:\tlearn: 0.0008752\ttotal: 1m 11s\tremaining: 1m 50s\n",
      "391:\tlearn: 0.0008728\ttotal: 1m 11s\tremaining: 1m 50s\n",
      "392:\tlearn: 0.0008688\ttotal: 1m 11s\tremaining: 1m 50s\n",
      "393:\tlearn: 0.0008687\ttotal: 1m 11s\tremaining: 1m 50s\n",
      "394:\tlearn: 0.0008687\ttotal: 1m 11s\tremaining: 1m 50s\n",
      "395:\tlearn: 0.0008687\ttotal: 1m 12s\tremaining: 1m 49s\n",
      "396:\tlearn: 0.0008686\ttotal: 1m 12s\tremaining: 1m 49s\n",
      "397:\tlearn: 0.0008685\ttotal: 1m 12s\tremaining: 1m 49s\n",
      "398:\tlearn: 0.0008685\ttotal: 1m 12s\tremaining: 1m 49s\n",
      "399:\tlearn: 0.0008684\ttotal: 1m 12s\tremaining: 1m 49s\n",
      "400:\tlearn: 0.0008682\ttotal: 1m 12s\tremaining: 1m 49s\n",
      "401:\tlearn: 0.0008636\ttotal: 1m 13s\tremaining: 1m 48s\n",
      "402:\tlearn: 0.0008635\ttotal: 1m 13s\tremaining: 1m 48s\n",
      "403:\tlearn: 0.0008635\ttotal: 1m 13s\tremaining: 1m 48s\n",
      "404:\tlearn: 0.0008635\ttotal: 1m 13s\tremaining: 1m 48s\n",
      "405:\tlearn: 0.0008634\ttotal: 1m 13s\tremaining: 1m 48s\n",
      "406:\tlearn: 0.0008634\ttotal: 1m 14s\tremaining: 1m 47s\n",
      "407:\tlearn: 0.0008622\ttotal: 1m 14s\tremaining: 1m 47s\n",
      "408:\tlearn: 0.0008584\ttotal: 1m 14s\tremaining: 1m 47s\n",
      "409:\tlearn: 0.0008584\ttotal: 1m 14s\tremaining: 1m 47s\n",
      "410:\tlearn: 0.0008584\ttotal: 1m 14s\tremaining: 1m 47s\n",
      "411:\tlearn: 0.0008583\ttotal: 1m 14s\tremaining: 1m 46s\n",
      "412:\tlearn: 0.0008582\ttotal: 1m 15s\tremaining: 1m 46s\n",
      "413:\tlearn: 0.0008582\ttotal: 1m 15s\tremaining: 1m 46s\n",
      "414:\tlearn: 0.0008581\ttotal: 1m 15s\tremaining: 1m 46s\n",
      "415:\tlearn: 0.0008581\ttotal: 1m 15s\tremaining: 1m 46s\n",
      "416:\tlearn: 0.0008581\ttotal: 1m 15s\tremaining: 1m 45s\n",
      "417:\tlearn: 0.0008576\ttotal: 1m 15s\tremaining: 1m 45s\n",
      "418:\tlearn: 0.0008576\ttotal: 1m 16s\tremaining: 1m 45s\n",
      "419:\tlearn: 0.0008575\ttotal: 1m 16s\tremaining: 1m 45s\n",
      "420:\tlearn: 0.0008575\ttotal: 1m 16s\tremaining: 1m 45s\n",
      "421:\tlearn: 0.0008575\ttotal: 1m 16s\tremaining: 1m 45s\n",
      "422:\tlearn: 0.0008574\ttotal: 1m 16s\tremaining: 1m 44s\n",
      "423:\tlearn: 0.0008574\ttotal: 1m 17s\tremaining: 1m 44s\n",
      "424:\tlearn: 0.0008573\ttotal: 1m 17s\tremaining: 1m 44s\n",
      "425:\tlearn: 0.0008573\ttotal: 1m 17s\tremaining: 1m 44s\n",
      "426:\tlearn: 0.0008532\ttotal: 1m 17s\tremaining: 1m 44s\n",
      "427:\tlearn: 0.0008531\ttotal: 1m 17s\tremaining: 1m 43s\n",
      "428:\tlearn: 0.0008530\ttotal: 1m 17s\tremaining: 1m 43s\n",
      "429:\tlearn: 0.0008530\ttotal: 1m 18s\tremaining: 1m 43s\n",
      "430:\tlearn: 0.0008529\ttotal: 1m 18s\tremaining: 1m 43s\n",
      "431:\tlearn: 0.0008529\ttotal: 1m 18s\tremaining: 1m 43s\n",
      "432:\tlearn: 0.0008529\ttotal: 1m 18s\tremaining: 1m 42s\n",
      "433:\tlearn: 0.0008528\ttotal: 1m 18s\tremaining: 1m 42s\n",
      "434:\tlearn: 0.0008528\ttotal: 1m 18s\tremaining: 1m 42s\n",
      "435:\tlearn: 0.0008528\ttotal: 1m 19s\tremaining: 1m 42s\n",
      "436:\tlearn: 0.0008527\ttotal: 1m 19s\tremaining: 1m 42s\n",
      "437:\tlearn: 0.0008527\ttotal: 1m 19s\tremaining: 1m 41s\n",
      "438:\tlearn: 0.0008513\ttotal: 1m 19s\tremaining: 1m 41s\n",
      "439:\tlearn: 0.0008512\ttotal: 1m 19s\tremaining: 1m 41s\n",
      "440:\tlearn: 0.0008512\ttotal: 1m 20s\tremaining: 1m 41s\n",
      "441:\tlearn: 0.0008512\ttotal: 1m 20s\tremaining: 1m 41s\n",
      "442:\tlearn: 0.0008509\ttotal: 1m 20s\tremaining: 1m 41s\n",
      "443:\tlearn: 0.0008509\ttotal: 1m 20s\tremaining: 1m 40s\n",
      "444:\tlearn: 0.0008508\ttotal: 1m 20s\tremaining: 1m 40s\n",
      "445:\tlearn: 0.0008478\ttotal: 1m 20s\tremaining: 1m 40s\n",
      "446:\tlearn: 0.0008478\ttotal: 1m 21s\tremaining: 1m 40s\n",
      "447:\tlearn: 0.0008477\ttotal: 1m 21s\tremaining: 1m 40s\n",
      "448:\tlearn: 0.0008477\ttotal: 1m 21s\tremaining: 1m 39s\n",
      "449:\tlearn: 0.0008476\ttotal: 1m 21s\tremaining: 1m 39s\n",
      "450:\tlearn: 0.0008471\ttotal: 1m 21s\tremaining: 1m 39s\n",
      "451:\tlearn: 0.0008471\ttotal: 1m 21s\tremaining: 1m 39s\n",
      "452:\tlearn: 0.0008470\ttotal: 1m 22s\tremaining: 1m 39s\n",
      "453:\tlearn: 0.0008466\ttotal: 1m 22s\tremaining: 1m 38s\n",
      "454:\tlearn: 0.0008439\ttotal: 1m 22s\tremaining: 1m 38s\n",
      "455:\tlearn: 0.0008439\ttotal: 1m 22s\tremaining: 1m 38s\n",
      "456:\tlearn: 0.0008439\ttotal: 1m 22s\tremaining: 1m 38s\n",
      "457:\tlearn: 0.0008439\ttotal: 1m 22s\tremaining: 1m 38s\n",
      "458:\tlearn: 0.0008438\ttotal: 1m 23s\tremaining: 1m 38s\n",
      "459:\tlearn: 0.0008438\ttotal: 1m 23s\tremaining: 1m 37s\n",
      "460:\tlearn: 0.0008438\ttotal: 1m 23s\tremaining: 1m 37s\n",
      "461:\tlearn: 0.0008438\ttotal: 1m 23s\tremaining: 1m 37s\n",
      "462:\tlearn: 0.0008437\ttotal: 1m 23s\tremaining: 1m 37s\n",
      "463:\tlearn: 0.0008436\ttotal: 1m 24s\tremaining: 1m 37s\n",
      "464:\tlearn: 0.0008435\ttotal: 1m 24s\tremaining: 1m 36s\n",
      "465:\tlearn: 0.0008435\ttotal: 1m 24s\tremaining: 1m 36s\n",
      "466:\tlearn: 0.0008435\ttotal: 1m 24s\tremaining: 1m 36s\n",
      "467:\tlearn: 0.0008434\ttotal: 1m 24s\tremaining: 1m 36s\n",
      "468:\tlearn: 0.0008434\ttotal: 1m 24s\tremaining: 1m 36s\n",
      "469:\tlearn: 0.0008434\ttotal: 1m 25s\tremaining: 1m 35s\n",
      "470:\tlearn: 0.0008434\ttotal: 1m 25s\tremaining: 1m 35s\n",
      "471:\tlearn: 0.0008434\ttotal: 1m 25s\tremaining: 1m 35s\n",
      "472:\tlearn: 0.0008434\ttotal: 1m 25s\tremaining: 1m 35s\n",
      "473:\tlearn: 0.0008433\ttotal: 1m 25s\tremaining: 1m 35s\n",
      "474:\tlearn: 0.0008432\ttotal: 1m 25s\tremaining: 1m 35s\n",
      "475:\tlearn: 0.0008431\ttotal: 1m 26s\tremaining: 1m 34s\n",
      "476:\tlearn: 0.0008431\ttotal: 1m 26s\tremaining: 1m 34s\n",
      "477:\tlearn: 0.0008430\ttotal: 1m 26s\tremaining: 1m 34s\n",
      "478:\tlearn: 0.0008429\ttotal: 1m 26s\tremaining: 1m 34s\n",
      "479:\tlearn: 0.0008427\ttotal: 1m 26s\tremaining: 1m 34s\n",
      "480:\tlearn: 0.0008426\ttotal: 1m 27s\tremaining: 1m 33s\n",
      "481:\tlearn: 0.0008390\ttotal: 1m 27s\tremaining: 1m 33s\n",
      "482:\tlearn: 0.0008389\ttotal: 1m 27s\tremaining: 1m 33s\n",
      "483:\tlearn: 0.0008389\ttotal: 1m 27s\tremaining: 1m 33s\n",
      "484:\tlearn: 0.0008389\ttotal: 1m 27s\tremaining: 1m 33s\n",
      "485:\tlearn: 0.0008324\ttotal: 1m 27s\tremaining: 1m 32s\n",
      "486:\tlearn: 0.0008324\ttotal: 1m 28s\tremaining: 1m 32s\n",
      "487:\tlearn: 0.0008323\ttotal: 1m 28s\tremaining: 1m 32s\n",
      "488:\tlearn: 0.0008323\ttotal: 1m 28s\tremaining: 1m 32s\n",
      "489:\tlearn: 0.0008323\ttotal: 1m 28s\tremaining: 1m 32s\n",
      "490:\tlearn: 0.0008322\ttotal: 1m 28s\tremaining: 1m 32s\n",
      "491:\tlearn: 0.0008322\ttotal: 1m 28s\tremaining: 1m 31s\n",
      "492:\tlearn: 0.0008321\ttotal: 1m 29s\tremaining: 1m 31s\n",
      "493:\tlearn: 0.0008321\ttotal: 1m 29s\tremaining: 1m 31s\n",
      "494:\tlearn: 0.0008321\ttotal: 1m 29s\tremaining: 1m 31s\n",
      "495:\tlearn: 0.0008320\ttotal: 1m 29s\tremaining: 1m 31s\n",
      "496:\tlearn: 0.0008319\ttotal: 1m 29s\tremaining: 1m 30s\n",
      "497:\tlearn: 0.0008318\ttotal: 1m 29s\tremaining: 1m 30s\n",
      "498:\tlearn: 0.0008317\ttotal: 1m 30s\tremaining: 1m 30s\n",
      "499:\tlearn: 0.0008315\ttotal: 1m 30s\tremaining: 1m 30s\n",
      "500:\tlearn: 0.0008286\ttotal: 1m 30s\tremaining: 1m 30s\n",
      "501:\tlearn: 0.0008286\ttotal: 1m 30s\tremaining: 1m 29s\n",
      "502:\tlearn: 0.0008277\ttotal: 1m 30s\tremaining: 1m 29s\n",
      "503:\tlearn: 0.0008277\ttotal: 1m 31s\tremaining: 1m 29s\n",
      "504:\tlearn: 0.0008277\ttotal: 1m 31s\tremaining: 1m 29s\n",
      "505:\tlearn: 0.0008276\ttotal: 1m 31s\tremaining: 1m 29s\n",
      "506:\tlearn: 0.0008276\ttotal: 1m 31s\tremaining: 1m 29s\n",
      "507:\tlearn: 0.0008276\ttotal: 1m 31s\tremaining: 1m 28s\n",
      "508:\tlearn: 0.0008275\ttotal: 1m 31s\tremaining: 1m 28s\n",
      "509:\tlearn: 0.0008271\ttotal: 1m 32s\tremaining: 1m 28s\n",
      "510:\tlearn: 0.0008271\ttotal: 1m 32s\tremaining: 1m 28s\n",
      "511:\tlearn: 0.0008269\ttotal: 1m 32s\tremaining: 1m 28s\n",
      "512:\tlearn: 0.0008268\ttotal: 1m 32s\tremaining: 1m 27s\n",
      "513:\tlearn: 0.0008268\ttotal: 1m 32s\tremaining: 1m 27s\n",
      "514:\tlearn: 0.0008267\ttotal: 1m 32s\tremaining: 1m 27s\n",
      "515:\tlearn: 0.0008267\ttotal: 1m 33s\tremaining: 1m 27s\n",
      "516:\tlearn: 0.0008255\ttotal: 1m 33s\tremaining: 1m 27s\n",
      "517:\tlearn: 0.0008252\ttotal: 1m 33s\tremaining: 1m 26s\n",
      "518:\tlearn: 0.0008252\ttotal: 1m 33s\tremaining: 1m 26s\n",
      "519:\tlearn: 0.0008252\ttotal: 1m 33s\tremaining: 1m 26s\n",
      "520:\tlearn: 0.0008230\ttotal: 1m 33s\tremaining: 1m 26s\n",
      "521:\tlearn: 0.0008230\ttotal: 1m 34s\tremaining: 1m 26s\n",
      "522:\tlearn: 0.0008230\ttotal: 1m 34s\tremaining: 1m 26s\n",
      "523:\tlearn: 0.0008230\ttotal: 1m 34s\tremaining: 1m 25s\n",
      "524:\tlearn: 0.0008229\ttotal: 1m 34s\tremaining: 1m 25s\n",
      "525:\tlearn: 0.0008228\ttotal: 1m 34s\tremaining: 1m 25s\n",
      "526:\tlearn: 0.0008228\ttotal: 1m 35s\tremaining: 1m 25s\n",
      "527:\tlearn: 0.0008228\ttotal: 1m 35s\tremaining: 1m 25s\n",
      "528:\tlearn: 0.0008220\ttotal: 1m 35s\tremaining: 1m 24s\n",
      "529:\tlearn: 0.0008217\ttotal: 1m 35s\tremaining: 1m 24s\n",
      "530:\tlearn: 0.0008217\ttotal: 1m 35s\tremaining: 1m 24s\n",
      "531:\tlearn: 0.0008216\ttotal: 1m 35s\tremaining: 1m 24s\n",
      "532:\tlearn: 0.0008215\ttotal: 1m 36s\tremaining: 1m 24s\n",
      "533:\tlearn: 0.0008215\ttotal: 1m 36s\tremaining: 1m 24s\n",
      "534:\tlearn: 0.0008215\ttotal: 1m 36s\tremaining: 1m 23s\n",
      "535:\tlearn: 0.0008177\ttotal: 1m 36s\tremaining: 1m 23s\n",
      "536:\tlearn: 0.0008177\ttotal: 1m 36s\tremaining: 1m 23s\n",
      "537:\tlearn: 0.0008177\ttotal: 1m 36s\tremaining: 1m 23s\n",
      "538:\tlearn: 0.0008177\ttotal: 1m 37s\tremaining: 1m 23s\n",
      "539:\tlearn: 0.0008177\ttotal: 1m 37s\tremaining: 1m 22s\n",
      "540:\tlearn: 0.0008177\ttotal: 1m 37s\tremaining: 1m 22s\n",
      "541:\tlearn: 0.0008177\ttotal: 1m 37s\tremaining: 1m 22s\n",
      "542:\tlearn: 0.0008176\ttotal: 1m 37s\tremaining: 1m 22s\n",
      "543:\tlearn: 0.0008176\ttotal: 1m 38s\tremaining: 1m 22s\n",
      "544:\tlearn: 0.0008172\ttotal: 1m 38s\tremaining: 1m 21s\n",
      "545:\tlearn: 0.0008172\ttotal: 1m 38s\tremaining: 1m 21s\n",
      "546:\tlearn: 0.0008171\ttotal: 1m 38s\tremaining: 1m 21s\n",
      "547:\tlearn: 0.0008132\ttotal: 1m 38s\tremaining: 1m 21s\n",
      "548:\tlearn: 0.0008132\ttotal: 1m 38s\tremaining: 1m 21s\n",
      "549:\tlearn: 0.0008132\ttotal: 1m 39s\tremaining: 1m 21s\n",
      "550:\tlearn: 0.0008131\ttotal: 1m 39s\tremaining: 1m 20s\n",
      "551:\tlearn: 0.0008130\ttotal: 1m 39s\tremaining: 1m 20s\n",
      "552:\tlearn: 0.0008130\ttotal: 1m 39s\tremaining: 1m 20s\n",
      "553:\tlearn: 0.0008129\ttotal: 1m 39s\tremaining: 1m 20s\n",
      "554:\tlearn: 0.0008129\ttotal: 1m 39s\tremaining: 1m 20s\n",
      "555:\tlearn: 0.0008126\ttotal: 1m 40s\tremaining: 1m 19s\n",
      "556:\tlearn: 0.0008125\ttotal: 1m 40s\tremaining: 1m 19s\n",
      "557:\tlearn: 0.0008123\ttotal: 1m 40s\tremaining: 1m 19s\n",
      "558:\tlearn: 0.0008122\ttotal: 1m 40s\tremaining: 1m 19s\n",
      "559:\tlearn: 0.0008122\ttotal: 1m 40s\tremaining: 1m 19s\n",
      "560:\tlearn: 0.0008122\ttotal: 1m 41s\tremaining: 1m 19s\n",
      "561:\tlearn: 0.0008121\ttotal: 1m 41s\tremaining: 1m 18s\n",
      "562:\tlearn: 0.0008121\ttotal: 1m 41s\tremaining: 1m 18s\n",
      "563:\tlearn: 0.0008120\ttotal: 1m 41s\tremaining: 1m 18s\n",
      "564:\tlearn: 0.0008120\ttotal: 1m 41s\tremaining: 1m 18s\n",
      "565:\tlearn: 0.0008120\ttotal: 1m 41s\tremaining: 1m 18s\n",
      "566:\tlearn: 0.0008116\ttotal: 1m 42s\tremaining: 1m 18s\n",
      "567:\tlearn: 0.0008116\ttotal: 1m 42s\tremaining: 1m 17s\n",
      "568:\tlearn: 0.0008115\ttotal: 1m 42s\tremaining: 1m 17s\n",
      "569:\tlearn: 0.0008115\ttotal: 1m 42s\tremaining: 1m 17s\n",
      "570:\tlearn: 0.0008114\ttotal: 1m 42s\tremaining: 1m 17s\n",
      "571:\tlearn: 0.0008113\ttotal: 1m 43s\tremaining: 1m 17s\n",
      "572:\tlearn: 0.0008113\ttotal: 1m 43s\tremaining: 1m 16s\n",
      "573:\tlearn: 0.0008112\ttotal: 1m 43s\tremaining: 1m 16s\n",
      "574:\tlearn: 0.0008112\ttotal: 1m 43s\tremaining: 1m 16s\n",
      "575:\tlearn: 0.0008112\ttotal: 1m 43s\tremaining: 1m 16s\n",
      "576:\tlearn: 0.0008110\ttotal: 1m 43s\tremaining: 1m 16s\n",
      "577:\tlearn: 0.0008110\ttotal: 1m 44s\tremaining: 1m 15s\n",
      "578:\tlearn: 0.0008110\ttotal: 1m 44s\tremaining: 1m 15s\n",
      "579:\tlearn: 0.0008109\ttotal: 1m 44s\tremaining: 1m 15s\n",
      "580:\tlearn: 0.0008108\ttotal: 1m 44s\tremaining: 1m 15s\n",
      "581:\tlearn: 0.0008108\ttotal: 1m 44s\tremaining: 1m 15s\n",
      "582:\tlearn: 0.0008108\ttotal: 1m 44s\tremaining: 1m 15s\n",
      "583:\tlearn: 0.0008108\ttotal: 1m 45s\tremaining: 1m 14s\n",
      "584:\tlearn: 0.0008107\ttotal: 1m 45s\tremaining: 1m 14s\n",
      "585:\tlearn: 0.0008106\ttotal: 1m 45s\tremaining: 1m 14s\n",
      "586:\tlearn: 0.0008106\ttotal: 1m 45s\tremaining: 1m 14s\n",
      "587:\tlearn: 0.0008106\ttotal: 1m 45s\tremaining: 1m 14s\n",
      "588:\tlearn: 0.0008104\ttotal: 1m 46s\tremaining: 1m 13s\n",
      "589:\tlearn: 0.0008073\ttotal: 1m 46s\tremaining: 1m 13s\n",
      "590:\tlearn: 0.0008073\ttotal: 1m 46s\tremaining: 1m 13s\n",
      "591:\tlearn: 0.0008073\ttotal: 1m 46s\tremaining: 1m 13s\n",
      "592:\tlearn: 0.0008072\ttotal: 1m 46s\tremaining: 1m 13s\n",
      "593:\tlearn: 0.0008006\ttotal: 1m 46s\tremaining: 1m 13s\n",
      "594:\tlearn: 0.0008005\ttotal: 1m 47s\tremaining: 1m 12s\n",
      "595:\tlearn: 0.0008005\ttotal: 1m 47s\tremaining: 1m 12s\n",
      "596:\tlearn: 0.0007990\ttotal: 1m 47s\tremaining: 1m 12s\n",
      "597:\tlearn: 0.0007935\ttotal: 1m 47s\tremaining: 1m 12s\n",
      "598:\tlearn: 0.0007935\ttotal: 1m 47s\tremaining: 1m 12s\n",
      "599:\tlearn: 0.0007934\ttotal: 1m 47s\tremaining: 1m 11s\n",
      "600:\tlearn: 0.0007932\ttotal: 1m 48s\tremaining: 1m 11s\n",
      "601:\tlearn: 0.0007932\ttotal: 1m 48s\tremaining: 1m 11s\n",
      "602:\tlearn: 0.0007930\ttotal: 1m 48s\tremaining: 1m 11s\n",
      "603:\tlearn: 0.0007926\ttotal: 1m 48s\tremaining: 1m 11s\n",
      "604:\tlearn: 0.0007922\ttotal: 1m 48s\tremaining: 1m 11s\n",
      "605:\tlearn: 0.0007922\ttotal: 1m 48s\tremaining: 1m 10s\n",
      "606:\tlearn: 0.0007922\ttotal: 1m 49s\tremaining: 1m 10s\n",
      "607:\tlearn: 0.0007921\ttotal: 1m 49s\tremaining: 1m 10s\n",
      "608:\tlearn: 0.0007921\ttotal: 1m 49s\tremaining: 1m 10s\n",
      "609:\tlearn: 0.0007919\ttotal: 1m 49s\tremaining: 1m 10s\n",
      "610:\tlearn: 0.0007918\ttotal: 1m 49s\tremaining: 1m 9s\n",
      "611:\tlearn: 0.0007915\ttotal: 1m 50s\tremaining: 1m 9s\n",
      "612:\tlearn: 0.0007914\ttotal: 1m 50s\tremaining: 1m 9s\n",
      "613:\tlearn: 0.0007913\ttotal: 1m 50s\tremaining: 1m 9s\n",
      "614:\tlearn: 0.0007912\ttotal: 1m 50s\tremaining: 1m 9s\n",
      "615:\tlearn: 0.0007911\ttotal: 1m 50s\tremaining: 1m 9s\n",
      "616:\tlearn: 0.0007911\ttotal: 1m 50s\tremaining: 1m 8s\n",
      "617:\tlearn: 0.0007910\ttotal: 1m 51s\tremaining: 1m 8s\n",
      "618:\tlearn: 0.0007909\ttotal: 1m 51s\tremaining: 1m 8s\n",
      "619:\tlearn: 0.0007909\ttotal: 1m 51s\tremaining: 1m 8s\n",
      "620:\tlearn: 0.0007904\ttotal: 1m 51s\tremaining: 1m 8s\n",
      "621:\tlearn: 0.0007904\ttotal: 1m 51s\tremaining: 1m 7s\n",
      "622:\tlearn: 0.0007904\ttotal: 1m 51s\tremaining: 1m 7s\n",
      "623:\tlearn: 0.0007903\ttotal: 1m 52s\tremaining: 1m 7s\n",
      "624:\tlearn: 0.0007902\ttotal: 1m 52s\tremaining: 1m 7s\n",
      "625:\tlearn: 0.0007900\ttotal: 1m 52s\tremaining: 1m 7s\n",
      "626:\tlearn: 0.0007896\ttotal: 1m 52s\tremaining: 1m 7s\n",
      "627:\tlearn: 0.0007894\ttotal: 1m 52s\tremaining: 1m 6s\n",
      "628:\tlearn: 0.0007893\ttotal: 1m 53s\tremaining: 1m 6s\n",
      "629:\tlearn: 0.0007891\ttotal: 1m 53s\tremaining: 1m 6s\n",
      "630:\tlearn: 0.0007891\ttotal: 1m 53s\tremaining: 1m 6s\n",
      "631:\tlearn: 0.0007891\ttotal: 1m 53s\tremaining: 1m 6s\n",
      "632:\tlearn: 0.0007891\ttotal: 1m 53s\tremaining: 1m 5s\n",
      "633:\tlearn: 0.0007890\ttotal: 1m 53s\tremaining: 1m 5s\n",
      "634:\tlearn: 0.0007890\ttotal: 1m 54s\tremaining: 1m 5s\n",
      "635:\tlearn: 0.0007889\ttotal: 1m 54s\tremaining: 1m 5s\n",
      "636:\tlearn: 0.0007889\ttotal: 1m 54s\tremaining: 1m 5s\n",
      "637:\tlearn: 0.0007889\ttotal: 1m 54s\tremaining: 1m 5s\n",
      "638:\tlearn: 0.0007889\ttotal: 1m 54s\tremaining: 1m 4s\n",
      "639:\tlearn: 0.0007887\ttotal: 1m 54s\tremaining: 1m 4s\n",
      "640:\tlearn: 0.0007887\ttotal: 1m 55s\tremaining: 1m 4s\n",
      "641:\tlearn: 0.0007885\ttotal: 1m 55s\tremaining: 1m 4s\n",
      "642:\tlearn: 0.0007880\ttotal: 1m 55s\tremaining: 1m 4s\n",
      "643:\tlearn: 0.0007880\ttotal: 1m 55s\tremaining: 1m 3s\n",
      "644:\tlearn: 0.0007879\ttotal: 1m 55s\tremaining: 1m 3s\n",
      "645:\tlearn: 0.0007878\ttotal: 1m 55s\tremaining: 1m 3s\n",
      "646:\tlearn: 0.0007878\ttotal: 1m 56s\tremaining: 1m 3s\n",
      "647:\tlearn: 0.0007876\ttotal: 1m 56s\tremaining: 1m 3s\n",
      "648:\tlearn: 0.0007854\ttotal: 1m 56s\tremaining: 1m 3s\n",
      "649:\tlearn: 0.0007849\ttotal: 1m 56s\tremaining: 1m 2s\n",
      "650:\tlearn: 0.0007847\ttotal: 1m 56s\tremaining: 1m 2s\n",
      "651:\tlearn: 0.0007847\ttotal: 1m 57s\tremaining: 1m 2s\n",
      "652:\tlearn: 0.0007846\ttotal: 1m 57s\tremaining: 1m 2s\n",
      "653:\tlearn: 0.0007844\ttotal: 1m 57s\tremaining: 1m 2s\n",
      "654:\tlearn: 0.0007844\ttotal: 1m 57s\tremaining: 1m 1s\n",
      "655:\tlearn: 0.0007844\ttotal: 1m 57s\tremaining: 1m 1s\n",
      "656:\tlearn: 0.0007843\ttotal: 1m 57s\tremaining: 1m 1s\n",
      "657:\tlearn: 0.0007843\ttotal: 1m 58s\tremaining: 1m 1s\n",
      "658:\tlearn: 0.0007843\ttotal: 1m 58s\tremaining: 1m 1s\n",
      "659:\tlearn: 0.0007843\ttotal: 1m 58s\tremaining: 1m 1s\n",
      "660:\tlearn: 0.0007842\ttotal: 1m 58s\tremaining: 1m\n",
      "661:\tlearn: 0.0007842\ttotal: 1m 58s\tremaining: 1m\n",
      "662:\tlearn: 0.0007840\ttotal: 1m 58s\tremaining: 1m\n",
      "663:\tlearn: 0.0007838\ttotal: 1m 59s\tremaining: 1m\n",
      "664:\tlearn: 0.0007838\ttotal: 1m 59s\tremaining: 1m\n",
      "665:\tlearn: 0.0007837\ttotal: 1m 59s\tremaining: 59.9s\n",
      "666:\tlearn: 0.0007836\ttotal: 1m 59s\tremaining: 59.7s\n",
      "667:\tlearn: 0.0007808\ttotal: 1m 59s\tremaining: 59.6s\n",
      "668:\tlearn: 0.0007806\ttotal: 2m\tremaining: 59.4s\n",
      "669:\tlearn: 0.0007806\ttotal: 2m\tremaining: 59.2s\n",
      "670:\tlearn: 0.0007804\ttotal: 2m\tremaining: 59s\n",
      "671:\tlearn: 0.0007796\ttotal: 2m\tremaining: 58.8s\n",
      "672:\tlearn: 0.0007795\ttotal: 2m\tremaining: 58.6s\n",
      "673:\tlearn: 0.0007794\ttotal: 2m\tremaining: 58.5s\n",
      "674:\tlearn: 0.0007792\ttotal: 2m 1s\tremaining: 58.3s\n",
      "675:\tlearn: 0.0007792\ttotal: 2m 1s\tremaining: 58.1s\n",
      "676:\tlearn: 0.0007792\ttotal: 2m 1s\tremaining: 57.9s\n",
      "677:\tlearn: 0.0007749\ttotal: 2m 1s\tremaining: 57.7s\n",
      "678:\tlearn: 0.0007749\ttotal: 2m 1s\tremaining: 57.6s\n",
      "679:\tlearn: 0.0007745\ttotal: 2m 1s\tremaining: 57.4s\n",
      "680:\tlearn: 0.0007710\ttotal: 2m 2s\tremaining: 57.2s\n",
      "681:\tlearn: 0.0007709\ttotal: 2m 2s\tremaining: 57s\n",
      "682:\tlearn: 0.0007709\ttotal: 2m 2s\tremaining: 56.8s\n",
      "683:\tlearn: 0.0007709\ttotal: 2m 2s\tremaining: 56.7s\n",
      "684:\tlearn: 0.0007709\ttotal: 2m 2s\tremaining: 56.5s\n",
      "685:\tlearn: 0.0007706\ttotal: 2m 2s\tremaining: 56.3s\n",
      "686:\tlearn: 0.0007706\ttotal: 2m 3s\tremaining: 56.1s\n",
      "687:\tlearn: 0.0007705\ttotal: 2m 3s\tremaining: 55.9s\n",
      "688:\tlearn: 0.0007705\ttotal: 2m 3s\tremaining: 55.7s\n",
      "689:\tlearn: 0.0007686\ttotal: 2m 3s\tremaining: 55.6s\n",
      "690:\tlearn: 0.0007686\ttotal: 2m 3s\tremaining: 55.4s\n",
      "691:\tlearn: 0.0007686\ttotal: 2m 4s\tremaining: 55.2s\n",
      "692:\tlearn: 0.0007685\ttotal: 2m 4s\tremaining: 55s\n",
      "693:\tlearn: 0.0007685\ttotal: 2m 4s\tremaining: 54.9s\n",
      "694:\tlearn: 0.0007685\ttotal: 2m 4s\tremaining: 54.7s\n",
      "695:\tlearn: 0.0007683\ttotal: 2m 4s\tremaining: 54.5s\n",
      "696:\tlearn: 0.0007683\ttotal: 2m 4s\tremaining: 54.3s\n",
      "697:\tlearn: 0.0007683\ttotal: 2m 5s\tremaining: 54.1s\n",
      "698:\tlearn: 0.0007682\ttotal: 2m 5s\tremaining: 53.9s\n",
      "699:\tlearn: 0.0007680\ttotal: 2m 5s\tremaining: 53.8s\n",
      "700:\tlearn: 0.0007680\ttotal: 2m 5s\tremaining: 53.6s\n",
      "701:\tlearn: 0.0007680\ttotal: 2m 5s\tremaining: 53.4s\n",
      "702:\tlearn: 0.0007680\ttotal: 2m 5s\tremaining: 53.2s\n",
      "703:\tlearn: 0.0007680\ttotal: 2m 6s\tremaining: 53s\n",
      "704:\tlearn: 0.0007678\ttotal: 2m 6s\tremaining: 52.9s\n",
      "705:\tlearn: 0.0007678\ttotal: 2m 6s\tremaining: 52.7s\n",
      "706:\tlearn: 0.0007677\ttotal: 2m 6s\tremaining: 52.5s\n",
      "707:\tlearn: 0.0007648\ttotal: 2m 6s\tremaining: 52.3s\n",
      "708:\tlearn: 0.0007647\ttotal: 2m 7s\tremaining: 52.1s\n",
      "709:\tlearn: 0.0007643\ttotal: 2m 7s\tremaining: 51.9s\n",
      "710:\tlearn: 0.0007642\ttotal: 2m 7s\tremaining: 51.8s\n",
      "711:\tlearn: 0.0007635\ttotal: 2m 7s\tremaining: 51.6s\n",
      "712:\tlearn: 0.0007635\ttotal: 2m 7s\tremaining: 51.4s\n",
      "713:\tlearn: 0.0007634\ttotal: 2m 7s\tremaining: 51.2s\n",
      "714:\tlearn: 0.0007634\ttotal: 2m 8s\tremaining: 51s\n",
      "715:\tlearn: 0.0007606\ttotal: 2m 8s\tremaining: 50.9s\n",
      "716:\tlearn: 0.0007605\ttotal: 2m 8s\tremaining: 50.7s\n",
      "717:\tlearn: 0.0007605\ttotal: 2m 8s\tremaining: 50.5s\n",
      "718:\tlearn: 0.0007603\ttotal: 2m 8s\tremaining: 50.3s\n",
      "719:\tlearn: 0.0007603\ttotal: 2m 8s\tremaining: 50.1s\n",
      "720:\tlearn: 0.0007603\ttotal: 2m 9s\tremaining: 50s\n",
      "721:\tlearn: 0.0007553\ttotal: 2m 9s\tremaining: 49.8s\n",
      "722:\tlearn: 0.0007553\ttotal: 2m 9s\tremaining: 49.6s\n",
      "723:\tlearn: 0.0007547\ttotal: 2m 9s\tremaining: 49.4s\n",
      "724:\tlearn: 0.0007547\ttotal: 2m 9s\tremaining: 49.2s\n",
      "725:\tlearn: 0.0007542\ttotal: 2m 9s\tremaining: 49.1s\n",
      "726:\tlearn: 0.0007540\ttotal: 2m 10s\tremaining: 48.9s\n",
      "727:\tlearn: 0.0007539\ttotal: 2m 10s\tremaining: 48.7s\n",
      "728:\tlearn: 0.0007538\ttotal: 2m 10s\tremaining: 48.5s\n",
      "729:\tlearn: 0.0007538\ttotal: 2m 10s\tremaining: 48.3s\n",
      "730:\tlearn: 0.0007538\ttotal: 2m 10s\tremaining: 48.1s\n",
      "731:\tlearn: 0.0007536\ttotal: 2m 11s\tremaining: 48s\n",
      "732:\tlearn: 0.0007536\ttotal: 2m 11s\tremaining: 47.8s\n",
      "733:\tlearn: 0.0007535\ttotal: 2m 11s\tremaining: 47.6s\n",
      "734:\tlearn: 0.0007535\ttotal: 2m 11s\tremaining: 47.4s\n",
      "735:\tlearn: 0.0007535\ttotal: 2m 11s\tremaining: 47.2s\n",
      "736:\tlearn: 0.0007535\ttotal: 2m 11s\tremaining: 47.1s\n",
      "737:\tlearn: 0.0007535\ttotal: 2m 12s\tremaining: 46.9s\n",
      "738:\tlearn: 0.0007534\ttotal: 2m 12s\tremaining: 46.7s\n",
      "739:\tlearn: 0.0007534\ttotal: 2m 12s\tremaining: 46.5s\n",
      "740:\tlearn: 0.0007533\ttotal: 2m 12s\tremaining: 46.3s\n",
      "741:\tlearn: 0.0007530\ttotal: 2m 12s\tremaining: 46.2s\n",
      "742:\tlearn: 0.0007530\ttotal: 2m 13s\tremaining: 46s\n",
      "743:\tlearn: 0.0007527\ttotal: 2m 13s\tremaining: 45.8s\n",
      "744:\tlearn: 0.0007527\ttotal: 2m 13s\tremaining: 45.7s\n",
      "745:\tlearn: 0.0007527\ttotal: 2m 13s\tremaining: 45.5s\n",
      "746:\tlearn: 0.0007526\ttotal: 2m 13s\tremaining: 45.3s\n",
      "747:\tlearn: 0.0007526\ttotal: 2m 14s\tremaining: 45.1s\n",
      "748:\tlearn: 0.0007525\ttotal: 2m 14s\tremaining: 45s\n",
      "749:\tlearn: 0.0007525\ttotal: 2m 14s\tremaining: 44.8s\n",
      "750:\tlearn: 0.0007525\ttotal: 2m 14s\tremaining: 44.6s\n",
      "751:\tlearn: 0.0007517\ttotal: 2m 14s\tremaining: 44.5s\n",
      "752:\tlearn: 0.0007517\ttotal: 2m 15s\tremaining: 44.3s\n",
      "753:\tlearn: 0.0007515\ttotal: 2m 15s\tremaining: 44.1s\n",
      "754:\tlearn: 0.0007514\ttotal: 2m 15s\tremaining: 43.9s\n",
      "755:\tlearn: 0.0007514\ttotal: 2m 15s\tremaining: 43.8s\n",
      "756:\tlearn: 0.0007512\ttotal: 2m 15s\tremaining: 43.6s\n",
      "757:\tlearn: 0.0007511\ttotal: 2m 16s\tremaining: 43.4s\n",
      "758:\tlearn: 0.0007509\ttotal: 2m 16s\tremaining: 43.2s\n",
      "759:\tlearn: 0.0007509\ttotal: 2m 16s\tremaining: 43.1s\n",
      "760:\tlearn: 0.0007506\ttotal: 2m 16s\tremaining: 42.9s\n",
      "761:\tlearn: 0.0007506\ttotal: 2m 16s\tremaining: 42.7s\n",
      "762:\tlearn: 0.0007502\ttotal: 2m 16s\tremaining: 42.6s\n",
      "763:\tlearn: 0.0007498\ttotal: 2m 17s\tremaining: 42.4s\n",
      "764:\tlearn: 0.0007462\ttotal: 2m 17s\tremaining: 42.2s\n",
      "765:\tlearn: 0.0007460\ttotal: 2m 17s\tremaining: 42s\n",
      "766:\tlearn: 0.0007460\ttotal: 2m 17s\tremaining: 41.9s\n",
      "767:\tlearn: 0.0007459\ttotal: 2m 17s\tremaining: 41.7s\n",
      "768:\tlearn: 0.0007458\ttotal: 2m 18s\tremaining: 41.5s\n",
      "769:\tlearn: 0.0007458\ttotal: 2m 18s\tremaining: 41.3s\n",
      "770:\tlearn: 0.0007458\ttotal: 2m 18s\tremaining: 41.2s\n",
      "771:\tlearn: 0.0007458\ttotal: 2m 18s\tremaining: 41s\n",
      "772:\tlearn: 0.0007458\ttotal: 2m 18s\tremaining: 40.8s\n",
      "773:\tlearn: 0.0007457\ttotal: 2m 19s\tremaining: 40.6s\n",
      "774:\tlearn: 0.0007457\ttotal: 2m 19s\tremaining: 40.5s\n",
      "775:\tlearn: 0.0007454\ttotal: 2m 19s\tremaining: 40.3s\n",
      "776:\tlearn: 0.0007425\ttotal: 2m 19s\tremaining: 40.1s\n",
      "777:\tlearn: 0.0007424\ttotal: 2m 19s\tremaining: 39.9s\n",
      "778:\tlearn: 0.0007402\ttotal: 2m 20s\tremaining: 39.8s\n",
      "779:\tlearn: 0.0007402\ttotal: 2m 20s\tremaining: 39.6s\n",
      "780:\tlearn: 0.0007395\ttotal: 2m 20s\tremaining: 39.4s\n",
      "781:\tlearn: 0.0007395\ttotal: 2m 20s\tremaining: 39.2s\n",
      "782:\tlearn: 0.0007395\ttotal: 2m 20s\tremaining: 39s\n",
      "783:\tlearn: 0.0007391\ttotal: 2m 21s\tremaining: 38.9s\n",
      "784:\tlearn: 0.0007391\ttotal: 2m 21s\tremaining: 38.7s\n",
      "785:\tlearn: 0.0007390\ttotal: 2m 21s\tremaining: 38.5s\n",
      "786:\tlearn: 0.0007390\ttotal: 2m 21s\tremaining: 38.3s\n",
      "787:\tlearn: 0.0007390\ttotal: 2m 21s\tremaining: 38.1s\n",
      "788:\tlearn: 0.0007357\ttotal: 2m 21s\tremaining: 38s\n",
      "789:\tlearn: 0.0007351\ttotal: 2m 22s\tremaining: 37.8s\n",
      "790:\tlearn: 0.0007351\ttotal: 2m 22s\tremaining: 37.6s\n",
      "791:\tlearn: 0.0007351\ttotal: 2m 22s\tremaining: 37.4s\n",
      "792:\tlearn: 0.0007350\ttotal: 2m 22s\tremaining: 37.2s\n",
      "793:\tlearn: 0.0007349\ttotal: 2m 22s\tremaining: 37.1s\n",
      "794:\tlearn: 0.0007349\ttotal: 2m 22s\tremaining: 36.9s\n",
      "795:\tlearn: 0.0007349\ttotal: 2m 23s\tremaining: 36.7s\n",
      "796:\tlearn: 0.0007348\ttotal: 2m 23s\tremaining: 36.5s\n",
      "797:\tlearn: 0.0007348\ttotal: 2m 23s\tremaining: 36.3s\n",
      "798:\tlearn: 0.0007348\ttotal: 2m 23s\tremaining: 36.1s\n",
      "799:\tlearn: 0.0007348\ttotal: 2m 23s\tremaining: 36s\n",
      "800:\tlearn: 0.0007347\ttotal: 2m 24s\tremaining: 35.8s\n",
      "801:\tlearn: 0.0007346\ttotal: 2m 24s\tremaining: 35.6s\n",
      "802:\tlearn: 0.0007345\ttotal: 2m 24s\tremaining: 35.4s\n",
      "803:\tlearn: 0.0007343\ttotal: 2m 24s\tremaining: 35.2s\n",
      "804:\tlearn: 0.0007343\ttotal: 2m 24s\tremaining: 35.1s\n",
      "805:\tlearn: 0.0007343\ttotal: 2m 24s\tremaining: 34.9s\n",
      "806:\tlearn: 0.0007343\ttotal: 2m 25s\tremaining: 34.7s\n",
      "807:\tlearn: 0.0007343\ttotal: 2m 25s\tremaining: 34.5s\n",
      "808:\tlearn: 0.0007342\ttotal: 2m 25s\tremaining: 34.3s\n",
      "809:\tlearn: 0.0007342\ttotal: 2m 25s\tremaining: 34.2s\n",
      "810:\tlearn: 0.0007341\ttotal: 2m 25s\tremaining: 34s\n",
      "811:\tlearn: 0.0007340\ttotal: 2m 25s\tremaining: 33.8s\n",
      "812:\tlearn: 0.0007340\ttotal: 2m 26s\tremaining: 33.6s\n",
      "813:\tlearn: 0.0007340\ttotal: 2m 26s\tremaining: 33.4s\n",
      "814:\tlearn: 0.0007340\ttotal: 2m 26s\tremaining: 33.3s\n",
      "815:\tlearn: 0.0007339\ttotal: 2m 26s\tremaining: 33.1s\n",
      "816:\tlearn: 0.0007338\ttotal: 2m 26s\tremaining: 32.9s\n",
      "817:\tlearn: 0.0007327\ttotal: 2m 27s\tremaining: 32.7s\n",
      "818:\tlearn: 0.0007326\ttotal: 2m 27s\tremaining: 32.5s\n",
      "819:\tlearn: 0.0007324\ttotal: 2m 27s\tremaining: 32.4s\n",
      "820:\tlearn: 0.0007323\ttotal: 2m 27s\tremaining: 32.2s\n",
      "821:\tlearn: 0.0007323\ttotal: 2m 27s\tremaining: 32s\n",
      "822:\tlearn: 0.0007323\ttotal: 2m 27s\tremaining: 31.8s\n",
      "823:\tlearn: 0.0007321\ttotal: 2m 28s\tremaining: 31.6s\n",
      "824:\tlearn: 0.0007321\ttotal: 2m 28s\tremaining: 31.4s\n",
      "825:\tlearn: 0.0007320\ttotal: 2m 28s\tremaining: 31.3s\n",
      "826:\tlearn: 0.0007319\ttotal: 2m 28s\tremaining: 31.1s\n",
      "827:\tlearn: 0.0007319\ttotal: 2m 28s\tremaining: 30.9s\n",
      "828:\tlearn: 0.0007319\ttotal: 2m 28s\tremaining: 30.7s\n",
      "829:\tlearn: 0.0007319\ttotal: 2m 29s\tremaining: 30.5s\n",
      "830:\tlearn: 0.0007319\ttotal: 2m 29s\tremaining: 30.4s\n",
      "831:\tlearn: 0.0007318\ttotal: 2m 29s\tremaining: 30.2s\n",
      "832:\tlearn: 0.0007317\ttotal: 2m 29s\tremaining: 30s\n",
      "833:\tlearn: 0.0007316\ttotal: 2m 29s\tremaining: 29.8s\n",
      "834:\tlearn: 0.0007315\ttotal: 2m 30s\tremaining: 29.6s\n",
      "835:\tlearn: 0.0007315\ttotal: 2m 30s\tremaining: 29.5s\n",
      "836:\tlearn: 0.0007313\ttotal: 2m 30s\tremaining: 29.3s\n",
      "837:\tlearn: 0.0007313\ttotal: 2m 30s\tremaining: 29.1s\n",
      "838:\tlearn: 0.0007313\ttotal: 2m 30s\tremaining: 28.9s\n",
      "839:\tlearn: 0.0007311\ttotal: 2m 30s\tremaining: 28.7s\n",
      "840:\tlearn: 0.0007297\ttotal: 2m 31s\tremaining: 28.6s\n",
      "841:\tlearn: 0.0007288\ttotal: 2m 31s\tremaining: 28.4s\n",
      "842:\tlearn: 0.0007288\ttotal: 2m 31s\tremaining: 28.2s\n",
      "843:\tlearn: 0.0007288\ttotal: 2m 31s\tremaining: 28s\n",
      "844:\tlearn: 0.0007287\ttotal: 2m 31s\tremaining: 27.8s\n",
      "845:\tlearn: 0.0007287\ttotal: 2m 31s\tremaining: 27.7s\n",
      "846:\tlearn: 0.0007287\ttotal: 2m 32s\tremaining: 27.5s\n",
      "847:\tlearn: 0.0007287\ttotal: 2m 32s\tremaining: 27.3s\n",
      "848:\tlearn: 0.0007286\ttotal: 2m 32s\tremaining: 27.1s\n",
      "849:\tlearn: 0.0007283\ttotal: 2m 32s\tremaining: 26.9s\n",
      "850:\tlearn: 0.0007282\ttotal: 2m 32s\tremaining: 26.8s\n",
      "851:\tlearn: 0.0007239\ttotal: 2m 32s\tremaining: 26.6s\n",
      "852:\tlearn: 0.0007236\ttotal: 2m 33s\tremaining: 26.4s\n",
      "853:\tlearn: 0.0007234\ttotal: 2m 33s\tremaining: 26.2s\n",
      "854:\tlearn: 0.0007233\ttotal: 2m 33s\tremaining: 26s\n",
      "855:\tlearn: 0.0007232\ttotal: 2m 33s\tremaining: 25.9s\n",
      "856:\tlearn: 0.0007232\ttotal: 2m 33s\tremaining: 25.7s\n",
      "857:\tlearn: 0.0007232\ttotal: 2m 34s\tremaining: 25.5s\n",
      "858:\tlearn: 0.0007232\ttotal: 2m 34s\tremaining: 25.3s\n",
      "859:\tlearn: 0.0007232\ttotal: 2m 34s\tremaining: 25.1s\n",
      "860:\tlearn: 0.0007232\ttotal: 2m 34s\tremaining: 24.9s\n",
      "861:\tlearn: 0.0007232\ttotal: 2m 34s\tremaining: 24.8s\n",
      "862:\tlearn: 0.0007232\ttotal: 2m 34s\tremaining: 24.6s\n",
      "863:\tlearn: 0.0007232\ttotal: 2m 35s\tremaining: 24.4s\n",
      "864:\tlearn: 0.0007232\ttotal: 2m 35s\tremaining: 24.2s\n",
      "865:\tlearn: 0.0007232\ttotal: 2m 35s\tremaining: 24s\n",
      "866:\tlearn: 0.0007232\ttotal: 2m 35s\tremaining: 23.9s\n",
      "867:\tlearn: 0.0007232\ttotal: 2m 35s\tremaining: 23.7s\n",
      "868:\tlearn: 0.0007231\ttotal: 2m 35s\tremaining: 23.5s\n",
      "869:\tlearn: 0.0007230\ttotal: 2m 36s\tremaining: 23.3s\n",
      "870:\tlearn: 0.0007230\ttotal: 2m 36s\tremaining: 23.1s\n",
      "871:\tlearn: 0.0007229\ttotal: 2m 36s\tremaining: 23s\n",
      "872:\tlearn: 0.0007227\ttotal: 2m 36s\tremaining: 22.8s\n",
      "873:\tlearn: 0.0007227\ttotal: 2m 36s\tremaining: 22.6s\n",
      "874:\tlearn: 0.0007227\ttotal: 2m 37s\tremaining: 22.4s\n",
      "875:\tlearn: 0.0007226\ttotal: 2m 37s\tremaining: 22.2s\n",
      "876:\tlearn: 0.0007226\ttotal: 2m 37s\tremaining: 22.1s\n",
      "877:\tlearn: 0.0007225\ttotal: 2m 37s\tremaining: 21.9s\n",
      "878:\tlearn: 0.0007225\ttotal: 2m 37s\tremaining: 21.7s\n",
      "879:\tlearn: 0.0007221\ttotal: 2m 37s\tremaining: 21.5s\n",
      "880:\tlearn: 0.0007221\ttotal: 2m 38s\tremaining: 21.3s\n",
      "881:\tlearn: 0.0007220\ttotal: 2m 38s\tremaining: 21.2s\n",
      "882:\tlearn: 0.0007220\ttotal: 2m 38s\tremaining: 21s\n",
      "883:\tlearn: 0.0007220\ttotal: 2m 38s\tremaining: 20.8s\n",
      "884:\tlearn: 0.0007219\ttotal: 2m 38s\tremaining: 20.6s\n",
      "885:\tlearn: 0.0007214\ttotal: 2m 38s\tremaining: 20.4s\n",
      "886:\tlearn: 0.0007214\ttotal: 2m 39s\tremaining: 20.3s\n",
      "887:\tlearn: 0.0007210\ttotal: 2m 39s\tremaining: 20.1s\n",
      "888:\tlearn: 0.0007209\ttotal: 2m 39s\tremaining: 19.9s\n",
      "889:\tlearn: 0.0007208\ttotal: 2m 39s\tremaining: 19.7s\n",
      "890:\tlearn: 0.0007208\ttotal: 2m 39s\tremaining: 19.5s\n",
      "891:\tlearn: 0.0007206\ttotal: 2m 39s\tremaining: 19.4s\n",
      "892:\tlearn: 0.0007204\ttotal: 2m 40s\tremaining: 19.2s\n",
      "893:\tlearn: 0.0007204\ttotal: 2m 40s\tremaining: 19s\n",
      "894:\tlearn: 0.0007204\ttotal: 2m 40s\tremaining: 18.8s\n",
      "895:\tlearn: 0.0007203\ttotal: 2m 40s\tremaining: 18.7s\n",
      "896:\tlearn: 0.0007202\ttotal: 2m 40s\tremaining: 18.5s\n",
      "897:\tlearn: 0.0007198\ttotal: 2m 41s\tremaining: 18.3s\n",
      "898:\tlearn: 0.0007197\ttotal: 2m 41s\tremaining: 18.1s\n",
      "899:\tlearn: 0.0007197\ttotal: 2m 41s\tremaining: 17.9s\n",
      "900:\tlearn: 0.0007179\ttotal: 2m 41s\tremaining: 17.8s\n",
      "901:\tlearn: 0.0007179\ttotal: 2m 41s\tremaining: 17.6s\n",
      "902:\tlearn: 0.0007130\ttotal: 2m 41s\tremaining: 17.4s\n",
      "903:\tlearn: 0.0007130\ttotal: 2m 42s\tremaining: 17.2s\n",
      "904:\tlearn: 0.0007129\ttotal: 2m 42s\tremaining: 17s\n",
      "905:\tlearn: 0.0007129\ttotal: 2m 42s\tremaining: 16.9s\n",
      "906:\tlearn: 0.0007126\ttotal: 2m 42s\tremaining: 16.7s\n",
      "907:\tlearn: 0.0007120\ttotal: 2m 42s\tremaining: 16.5s\n",
      "908:\tlearn: 0.0007120\ttotal: 2m 42s\tremaining: 16.3s\n",
      "909:\tlearn: 0.0007120\ttotal: 2m 43s\tremaining: 16.1s\n",
      "910:\tlearn: 0.0007119\ttotal: 2m 43s\tremaining: 16s\n",
      "911:\tlearn: 0.0007118\ttotal: 2m 43s\tremaining: 15.8s\n",
      "912:\tlearn: 0.0007118\ttotal: 2m 43s\tremaining: 15.6s\n",
      "913:\tlearn: 0.0007118\ttotal: 2m 43s\tremaining: 15.4s\n",
      "914:\tlearn: 0.0007118\ttotal: 2m 44s\tremaining: 15.2s\n",
      "915:\tlearn: 0.0007118\ttotal: 2m 44s\tremaining: 15.1s\n",
      "916:\tlearn: 0.0007118\ttotal: 2m 44s\tremaining: 14.9s\n",
      "917:\tlearn: 0.0007118\ttotal: 2m 44s\tremaining: 14.7s\n",
      "918:\tlearn: 0.0007112\ttotal: 2m 44s\tremaining: 14.5s\n",
      "919:\tlearn: 0.0007110\ttotal: 2m 44s\tremaining: 14.3s\n",
      "920:\tlearn: 0.0007110\ttotal: 2m 45s\tremaining: 14.2s\n",
      "921:\tlearn: 0.0007107\ttotal: 2m 45s\tremaining: 14s\n",
      "922:\tlearn: 0.0007106\ttotal: 2m 45s\tremaining: 13.8s\n",
      "923:\tlearn: 0.0007106\ttotal: 2m 45s\tremaining: 13.6s\n",
      "924:\tlearn: 0.0007106\ttotal: 2m 45s\tremaining: 13.4s\n",
      "925:\tlearn: 0.0007106\ttotal: 2m 45s\tremaining: 13.3s\n",
      "926:\tlearn: 0.0007106\ttotal: 2m 46s\tremaining: 13.1s\n",
      "927:\tlearn: 0.0007100\ttotal: 2m 46s\tremaining: 12.9s\n",
      "928:\tlearn: 0.0007100\ttotal: 2m 46s\tremaining: 12.7s\n",
      "929:\tlearn: 0.0007097\ttotal: 2m 46s\tremaining: 12.5s\n",
      "930:\tlearn: 0.0007097\ttotal: 2m 46s\tremaining: 12.4s\n",
      "931:\tlearn: 0.0007096\ttotal: 2m 47s\tremaining: 12.2s\n",
      "932:\tlearn: 0.0007067\ttotal: 2m 47s\tremaining: 12s\n",
      "933:\tlearn: 0.0007067\ttotal: 2m 47s\tremaining: 11.8s\n",
      "934:\tlearn: 0.0007063\ttotal: 2m 47s\tremaining: 11.6s\n",
      "935:\tlearn: 0.0007063\ttotal: 2m 47s\tremaining: 11.5s\n",
      "936:\tlearn: 0.0007063\ttotal: 2m 47s\tremaining: 11.3s\n",
      "937:\tlearn: 0.0007063\ttotal: 2m 48s\tremaining: 11.1s\n",
      "938:\tlearn: 0.0007060\ttotal: 2m 48s\tremaining: 10.9s\n",
      "939:\tlearn: 0.0007058\ttotal: 2m 48s\tremaining: 10.7s\n",
      "940:\tlearn: 0.0007057\ttotal: 2m 48s\tremaining: 10.6s\n",
      "941:\tlearn: 0.0007057\ttotal: 2m 48s\tremaining: 10.4s\n",
      "942:\tlearn: 0.0007056\ttotal: 2m 48s\tremaining: 10.2s\n",
      "943:\tlearn: 0.0007054\ttotal: 2m 49s\tremaining: 10s\n",
      "944:\tlearn: 0.0007054\ttotal: 2m 49s\tremaining: 9.85s\n",
      "945:\tlearn: 0.0007054\ttotal: 2m 49s\tremaining: 9.68s\n",
      "946:\tlearn: 0.0007054\ttotal: 2m 49s\tremaining: 9.49s\n",
      "947:\tlearn: 0.0007050\ttotal: 2m 49s\tremaining: 9.32s\n",
      "948:\tlearn: 0.0007049\ttotal: 2m 50s\tremaining: 9.14s\n",
      "949:\tlearn: 0.0007048\ttotal: 2m 50s\tremaining: 8.96s\n",
      "950:\tlearn: 0.0007047\ttotal: 2m 50s\tremaining: 8.78s\n",
      "951:\tlearn: 0.0007047\ttotal: 2m 50s\tremaining: 8.6s\n",
      "952:\tlearn: 0.0007047\ttotal: 2m 50s\tremaining: 8.42s\n",
      "953:\tlearn: 0.0007047\ttotal: 2m 50s\tremaining: 8.24s\n",
      "954:\tlearn: 0.0007047\ttotal: 2m 51s\tremaining: 8.06s\n",
      "955:\tlearn: 0.0007047\ttotal: 2m 51s\tremaining: 7.88s\n",
      "956:\tlearn: 0.0007047\ttotal: 2m 51s\tremaining: 7.7s\n",
      "957:\tlearn: 0.0007047\ttotal: 2m 51s\tremaining: 7.53s\n",
      "958:\tlearn: 0.0007041\ttotal: 2m 51s\tremaining: 7.35s\n",
      "959:\tlearn: 0.0007041\ttotal: 2m 52s\tremaining: 7.17s\n",
      "960:\tlearn: 0.0007041\ttotal: 2m 52s\tremaining: 6.99s\n",
      "961:\tlearn: 0.0007040\ttotal: 2m 52s\tremaining: 6.81s\n",
      "962:\tlearn: 0.0007040\ttotal: 2m 52s\tremaining: 6.63s\n",
      "963:\tlearn: 0.0007040\ttotal: 2m 52s\tremaining: 6.45s\n",
      "964:\tlearn: 0.0007040\ttotal: 2m 52s\tremaining: 6.27s\n",
      "965:\tlearn: 0.0007040\ttotal: 2m 53s\tremaining: 6.09s\n",
      "966:\tlearn: 0.0007039\ttotal: 2m 53s\tremaining: 5.91s\n",
      "967:\tlearn: 0.0007039\ttotal: 2m 53s\tremaining: 5.73s\n",
      "968:\tlearn: 0.0007018\ttotal: 2m 53s\tremaining: 5.55s\n",
      "969:\tlearn: 0.0007018\ttotal: 2m 53s\tremaining: 5.37s\n",
      "970:\tlearn: 0.0007016\ttotal: 2m 53s\tremaining: 5.19s\n",
      "971:\tlearn: 0.0007015\ttotal: 2m 54s\tremaining: 5.01s\n",
      "972:\tlearn: 0.0007014\ttotal: 2m 54s\tremaining: 4.84s\n",
      "973:\tlearn: 0.0007014\ttotal: 2m 54s\tremaining: 4.66s\n",
      "974:\tlearn: 0.0007014\ttotal: 2m 54s\tremaining: 4.48s\n",
      "975:\tlearn: 0.0007013\ttotal: 2m 54s\tremaining: 4.3s\n",
      "976:\tlearn: 0.0007012\ttotal: 2m 54s\tremaining: 4.12s\n",
      "977:\tlearn: 0.0006966\ttotal: 2m 55s\tremaining: 3.94s\n",
      "978:\tlearn: 0.0006966\ttotal: 2m 55s\tremaining: 3.76s\n",
      "979:\tlearn: 0.0006964\ttotal: 2m 55s\tremaining: 3.58s\n",
      "980:\tlearn: 0.0006963\ttotal: 2m 55s\tremaining: 3.4s\n",
      "981:\tlearn: 0.0006963\ttotal: 2m 55s\tremaining: 3.22s\n",
      "982:\tlearn: 0.0006962\ttotal: 2m 56s\tremaining: 3.04s\n",
      "983:\tlearn: 0.0006958\ttotal: 2m 56s\tremaining: 2.87s\n",
      "984:\tlearn: 0.0006957\ttotal: 2m 56s\tremaining: 2.69s\n",
      "985:\tlearn: 0.0006956\ttotal: 2m 56s\tremaining: 2.51s\n",
      "986:\tlearn: 0.0006956\ttotal: 2m 56s\tremaining: 2.33s\n",
      "987:\tlearn: 0.0006955\ttotal: 2m 56s\tremaining: 2.15s\n",
      "988:\tlearn: 0.0006955\ttotal: 2m 57s\tremaining: 1.97s\n",
      "989:\tlearn: 0.0006954\ttotal: 2m 57s\tremaining: 1.79s\n",
      "990:\tlearn: 0.0006954\ttotal: 2m 57s\tremaining: 1.61s\n",
      "991:\tlearn: 0.0006954\ttotal: 2m 57s\tremaining: 1.43s\n",
      "992:\tlearn: 0.0006953\ttotal: 2m 57s\tremaining: 1.25s\n",
      "993:\tlearn: 0.0006948\ttotal: 2m 57s\tremaining: 1.07s\n",
      "994:\tlearn: 0.0006948\ttotal: 2m 58s\tremaining: 895ms\n",
      "995:\tlearn: 0.0006947\ttotal: 2m 58s\tremaining: 716ms\n",
      "996:\tlearn: 0.0006947\ttotal: 2m 58s\tremaining: 537ms\n",
      "997:\tlearn: 0.0006947\ttotal: 2m 58s\tremaining: 358ms\n",
      "998:\tlearn: 0.0006947\ttotal: 2m 58s\tremaining: 179ms\n",
      "999:\tlearn: 0.0006946\ttotal: 2m 59s\tremaining: 0us\n",
      "0:\tlearn: 0.5071917\ttotal: 202ms\tremaining: 3m 22s\n",
      "1:\tlearn: 0.4326119\ttotal: 376ms\tremaining: 3m 7s\n",
      "2:\tlearn: 0.3768958\ttotal: 548ms\tremaining: 3m 2s\n",
      "3:\tlearn: 0.2948336\ttotal: 723ms\tremaining: 2m 59s\n",
      "4:\tlearn: 0.2325322\ttotal: 901ms\tremaining: 2m 59s\n",
      "5:\tlearn: 0.1789394\ttotal: 1.08s\tremaining: 2m 58s\n",
      "6:\tlearn: 0.1374551\ttotal: 1.25s\tremaining: 2m 57s\n",
      "7:\tlearn: 0.1183979\ttotal: 1.43s\tremaining: 2m 56s\n",
      "8:\tlearn: 0.0874871\ttotal: 1.6s\tremaining: 2m 56s\n",
      "9:\tlearn: 0.0731294\ttotal: 1.78s\tremaining: 2m 55s\n",
      "10:\tlearn: 0.0596012\ttotal: 1.95s\tremaining: 2m 55s\n",
      "11:\tlearn: 0.0530916\ttotal: 2.13s\tremaining: 2m 55s\n",
      "12:\tlearn: 0.0408922\ttotal: 2.31s\tremaining: 2m 55s\n",
      "13:\tlearn: 0.0345731\ttotal: 2.48s\tremaining: 2m 54s\n",
      "14:\tlearn: 0.0289015\ttotal: 2.65s\tremaining: 2m 54s\n",
      "15:\tlearn: 0.0271654\ttotal: 2.83s\tremaining: 2m 54s\n",
      "16:\tlearn: 0.0221900\ttotal: 3.01s\tremaining: 2m 54s\n",
      "17:\tlearn: 0.0195330\ttotal: 3.18s\tremaining: 2m 53s\n",
      "18:\tlearn: 0.0173346\ttotal: 3.36s\tremaining: 2m 53s\n",
      "19:\tlearn: 0.0157720\ttotal: 3.53s\tremaining: 2m 53s\n",
      "20:\tlearn: 0.0141956\ttotal: 3.71s\tremaining: 2m 52s\n",
      "21:\tlearn: 0.0130011\ttotal: 3.88s\tremaining: 2m 52s\n",
      "22:\tlearn: 0.0118410\ttotal: 4.06s\tremaining: 2m 52s\n",
      "23:\tlearn: 0.0109575\ttotal: 4.23s\tremaining: 2m 52s\n",
      "24:\tlearn: 0.0100277\ttotal: 4.41s\tremaining: 2m 52s\n",
      "25:\tlearn: 0.0091864\ttotal: 4.59s\tremaining: 2m 52s\n",
      "26:\tlearn: 0.0085802\ttotal: 4.77s\tremaining: 2m 52s\n",
      "27:\tlearn: 0.0079649\ttotal: 4.95s\tremaining: 2m 51s\n",
      "28:\tlearn: 0.0074924\ttotal: 5.12s\tremaining: 2m 51s\n",
      "29:\tlearn: 0.0069071\ttotal: 5.3s\tremaining: 2m 51s\n",
      "30:\tlearn: 0.0064533\ttotal: 5.47s\tremaining: 2m 51s\n",
      "31:\tlearn: 0.0059150\ttotal: 5.65s\tremaining: 2m 50s\n",
      "32:\tlearn: 0.0055928\ttotal: 5.82s\tremaining: 2m 50s\n",
      "33:\tlearn: 0.0053247\ttotal: 6s\tremaining: 2m 50s\n",
      "34:\tlearn: 0.0050885\ttotal: 6.17s\tremaining: 2m 50s\n",
      "35:\tlearn: 0.0048431\ttotal: 6.35s\tremaining: 2m 50s\n",
      "36:\tlearn: 0.0047373\ttotal: 6.53s\tremaining: 2m 49s\n",
      "37:\tlearn: 0.0046591\ttotal: 6.7s\tremaining: 2m 49s\n",
      "38:\tlearn: 0.0045602\ttotal: 6.87s\tremaining: 2m 49s\n",
      "39:\tlearn: 0.0043853\ttotal: 7.05s\tremaining: 2m 49s\n",
      "40:\tlearn: 0.0042380\ttotal: 7.22s\tremaining: 2m 48s\n",
      "41:\tlearn: 0.0040426\ttotal: 7.4s\tremaining: 2m 48s\n",
      "42:\tlearn: 0.0039446\ttotal: 7.57s\tremaining: 2m 48s\n",
      "43:\tlearn: 0.0038356\ttotal: 7.75s\tremaining: 2m 48s\n",
      "44:\tlearn: 0.0036924\ttotal: 7.92s\tremaining: 2m 48s\n",
      "45:\tlearn: 0.0036102\ttotal: 8.1s\tremaining: 2m 48s\n",
      "46:\tlearn: 0.0034877\ttotal: 8.28s\tremaining: 2m 47s\n",
      "47:\tlearn: 0.0033773\ttotal: 8.45s\tremaining: 2m 47s\n",
      "48:\tlearn: 0.0032573\ttotal: 8.62s\tremaining: 2m 47s\n",
      "49:\tlearn: 0.0032178\ttotal: 8.8s\tremaining: 2m 47s\n",
      "50:\tlearn: 0.0031478\ttotal: 8.97s\tremaining: 2m 46s\n",
      "51:\tlearn: 0.0030814\ttotal: 9.15s\tremaining: 2m 46s\n",
      "52:\tlearn: 0.0029963\ttotal: 9.32s\tremaining: 2m 46s\n",
      "53:\tlearn: 0.0029174\ttotal: 9.5s\tremaining: 2m 46s\n",
      "54:\tlearn: 0.0028403\ttotal: 9.67s\tremaining: 2m 46s\n",
      "55:\tlearn: 0.0027995\ttotal: 9.85s\tremaining: 2m 46s\n",
      "56:\tlearn: 0.0026858\ttotal: 10s\tremaining: 2m 45s\n",
      "57:\tlearn: 0.0026327\ttotal: 10.2s\tremaining: 2m 45s\n",
      "58:\tlearn: 0.0025838\ttotal: 10.4s\tremaining: 2m 45s\n",
      "59:\tlearn: 0.0025439\ttotal: 10.5s\tremaining: 2m 45s\n",
      "60:\tlearn: 0.0025438\ttotal: 10.7s\tremaining: 2m 44s\n",
      "61:\tlearn: 0.0024608\ttotal: 10.9s\tremaining: 2m 44s\n",
      "62:\tlearn: 0.0024009\ttotal: 11.1s\tremaining: 2m 44s\n",
      "63:\tlearn: 0.0023353\ttotal: 11.2s\tremaining: 2m 44s\n",
      "64:\tlearn: 0.0023159\ttotal: 11.4s\tremaining: 2m 44s\n",
      "65:\tlearn: 0.0022766\ttotal: 11.6s\tremaining: 2m 44s\n",
      "66:\tlearn: 0.0022300\ttotal: 11.8s\tremaining: 2m 43s\n",
      "67:\tlearn: 0.0021928\ttotal: 11.9s\tremaining: 2m 43s\n",
      "68:\tlearn: 0.0021437\ttotal: 12.1s\tremaining: 2m 43s\n",
      "69:\tlearn: 0.0021096\ttotal: 12.3s\tremaining: 2m 43s\n",
      "70:\tlearn: 0.0020868\ttotal: 12.5s\tremaining: 2m 43s\n",
      "71:\tlearn: 0.0020214\ttotal: 12.7s\tremaining: 2m 43s\n",
      "72:\tlearn: 0.0019875\ttotal: 12.8s\tremaining: 2m 42s\n",
      "73:\tlearn: 0.0019801\ttotal: 13s\tremaining: 2m 42s\n",
      "74:\tlearn: 0.0019490\ttotal: 13.2s\tremaining: 2m 42s\n",
      "75:\tlearn: 0.0019218\ttotal: 13.4s\tremaining: 2m 42s\n",
      "76:\tlearn: 0.0018847\ttotal: 13.5s\tremaining: 2m 42s\n",
      "77:\tlearn: 0.0018344\ttotal: 13.7s\tremaining: 2m 42s\n",
      "78:\tlearn: 0.0018058\ttotal: 13.9s\tremaining: 2m 41s\n",
      "79:\tlearn: 0.0017726\ttotal: 14.1s\tremaining: 2m 41s\n",
      "80:\tlearn: 0.0017400\ttotal: 14.2s\tremaining: 2m 41s\n",
      "81:\tlearn: 0.0017156\ttotal: 14.4s\tremaining: 2m 41s\n",
      "82:\tlearn: 0.0016967\ttotal: 14.6s\tremaining: 2m 41s\n",
      "83:\tlearn: 0.0016590\ttotal: 14.8s\tremaining: 2m 40s\n",
      "84:\tlearn: 0.0016217\ttotal: 14.9s\tremaining: 2m 40s\n",
      "85:\tlearn: 0.0015957\ttotal: 15.1s\tremaining: 2m 40s\n",
      "86:\tlearn: 0.0015731\ttotal: 15.3s\tremaining: 2m 40s\n",
      "87:\tlearn: 0.0015566\ttotal: 15.5s\tremaining: 2m 40s\n",
      "88:\tlearn: 0.0015411\ttotal: 15.7s\tremaining: 2m 40s\n",
      "89:\tlearn: 0.0014970\ttotal: 15.8s\tremaining: 2m 40s\n",
      "90:\tlearn: 0.0014826\ttotal: 16s\tremaining: 2m 39s\n",
      "91:\tlearn: 0.0014824\ttotal: 16.2s\tremaining: 2m 39s\n",
      "92:\tlearn: 0.0014707\ttotal: 16.4s\tremaining: 2m 39s\n",
      "93:\tlearn: 0.0014444\ttotal: 16.5s\tremaining: 2m 39s\n",
      "94:\tlearn: 0.0014443\ttotal: 16.7s\tremaining: 2m 39s\n",
      "95:\tlearn: 0.0014443\ttotal: 16.9s\tremaining: 2m 38s\n",
      "96:\tlearn: 0.0014280\ttotal: 17.1s\tremaining: 2m 38s\n",
      "97:\tlearn: 0.0014076\ttotal: 17.2s\tremaining: 2m 38s\n",
      "98:\tlearn: 0.0014074\ttotal: 17.4s\tremaining: 2m 38s\n",
      "99:\tlearn: 0.0013894\ttotal: 17.6s\tremaining: 2m 38s\n",
      "100:\tlearn: 0.0013764\ttotal: 17.8s\tremaining: 2m 38s\n",
      "101:\tlearn: 0.0013763\ttotal: 17.9s\tremaining: 2m 37s\n",
      "102:\tlearn: 0.0013513\ttotal: 18.1s\tremaining: 2m 37s\n",
      "103:\tlearn: 0.0013419\ttotal: 18.3s\tremaining: 2m 37s\n",
      "104:\tlearn: 0.0013258\ttotal: 18.5s\tremaining: 2m 37s\n",
      "105:\tlearn: 0.0013114\ttotal: 18.7s\tremaining: 2m 37s\n",
      "106:\tlearn: 0.0013114\ttotal: 18.8s\tremaining: 2m 37s\n",
      "107:\tlearn: 0.0013114\ttotal: 19s\tremaining: 2m 37s\n",
      "108:\tlearn: 0.0013114\ttotal: 19.2s\tremaining: 2m 36s\n",
      "109:\tlearn: 0.0012965\ttotal: 19.4s\tremaining: 2m 36s\n",
      "110:\tlearn: 0.0012962\ttotal: 19.5s\tremaining: 2m 36s\n",
      "111:\tlearn: 0.0012816\ttotal: 19.7s\tremaining: 2m 36s\n",
      "112:\tlearn: 0.0012635\ttotal: 19.9s\tremaining: 2m 36s\n",
      "113:\tlearn: 0.0012518\ttotal: 20.1s\tremaining: 2m 35s\n",
      "114:\tlearn: 0.0012407\ttotal: 20.2s\tremaining: 2m 35s\n",
      "115:\tlearn: 0.0012395\ttotal: 20.4s\tremaining: 2m 35s\n",
      "116:\tlearn: 0.0012395\ttotal: 20.6s\tremaining: 2m 35s\n",
      "117:\tlearn: 0.0012256\ttotal: 20.8s\tremaining: 2m 35s\n",
      "118:\tlearn: 0.0012232\ttotal: 20.9s\tremaining: 2m 35s\n",
      "119:\tlearn: 0.0012125\ttotal: 21.1s\tremaining: 2m 34s\n",
      "120:\tlearn: 0.0011932\ttotal: 21.3s\tremaining: 2m 34s\n",
      "121:\tlearn: 0.0011769\ttotal: 21.5s\tremaining: 2m 34s\n",
      "122:\tlearn: 0.0011674\ttotal: 21.7s\tremaining: 2m 34s\n",
      "123:\tlearn: 0.0011672\ttotal: 21.8s\tremaining: 2m 34s\n",
      "124:\tlearn: 0.0011667\ttotal: 22s\tremaining: 2m 34s\n",
      "125:\tlearn: 0.0011562\ttotal: 22.2s\tremaining: 2m 33s\n",
      "126:\tlearn: 0.0011480\ttotal: 22.4s\tremaining: 2m 33s\n",
      "127:\tlearn: 0.0011395\ttotal: 22.6s\tremaining: 2m 33s\n",
      "128:\tlearn: 0.0011387\ttotal: 22.7s\tremaining: 2m 33s\n",
      "129:\tlearn: 0.0011378\ttotal: 22.9s\tremaining: 2m 33s\n",
      "130:\tlearn: 0.0011376\ttotal: 23.1s\tremaining: 2m 33s\n",
      "131:\tlearn: 0.0011277\ttotal: 23.3s\tremaining: 2m 33s\n",
      "132:\tlearn: 0.0011132\ttotal: 23.4s\tremaining: 2m 32s\n",
      "133:\tlearn: 0.0011130\ttotal: 23.6s\tremaining: 2m 32s\n",
      "134:\tlearn: 0.0011129\ttotal: 23.8s\tremaining: 2m 32s\n",
      "135:\tlearn: 0.0011129\ttotal: 24s\tremaining: 2m 32s\n",
      "136:\tlearn: 0.0011128\ttotal: 24.2s\tremaining: 2m 32s\n",
      "137:\tlearn: 0.0011045\ttotal: 24.3s\tremaining: 2m 32s\n",
      "138:\tlearn: 0.0011045\ttotal: 24.5s\tremaining: 2m 31s\n",
      "139:\tlearn: 0.0011043\ttotal: 24.7s\tremaining: 2m 31s\n",
      "140:\tlearn: 0.0011043\ttotal: 24.9s\tremaining: 2m 31s\n",
      "141:\tlearn: 0.0011042\ttotal: 25.1s\tremaining: 2m 31s\n",
      "142:\tlearn: 0.0011039\ttotal: 25.2s\tremaining: 2m 31s\n",
      "143:\tlearn: 0.0011034\ttotal: 25.4s\tremaining: 2m 31s\n",
      "144:\tlearn: 0.0011034\ttotal: 25.6s\tremaining: 2m 30s\n",
      "145:\tlearn: 0.0010971\ttotal: 25.8s\tremaining: 2m 30s\n",
      "146:\tlearn: 0.0010951\ttotal: 26s\tremaining: 2m 30s\n",
      "147:\tlearn: 0.0010951\ttotal: 26.1s\tremaining: 2m 30s\n",
      "148:\tlearn: 0.0010951\ttotal: 26.3s\tremaining: 2m 30s\n",
      "149:\tlearn: 0.0010950\ttotal: 26.5s\tremaining: 2m 30s\n",
      "150:\tlearn: 0.0010948\ttotal: 26.7s\tremaining: 2m 29s\n",
      "151:\tlearn: 0.0010821\ttotal: 26.8s\tremaining: 2m 29s\n",
      "152:\tlearn: 0.0010682\ttotal: 27s\tremaining: 2m 29s\n",
      "153:\tlearn: 0.0010625\ttotal: 27.2s\tremaining: 2m 29s\n",
      "154:\tlearn: 0.0010534\ttotal: 27.4s\tremaining: 2m 29s\n",
      "155:\tlearn: 0.0010499\ttotal: 27.6s\tremaining: 2m 29s\n",
      "156:\tlearn: 0.0010499\ttotal: 27.7s\tremaining: 2m 28s\n",
      "157:\tlearn: 0.0010499\ttotal: 27.9s\tremaining: 2m 28s\n",
      "158:\tlearn: 0.0010498\ttotal: 28.1s\tremaining: 2m 28s\n",
      "159:\tlearn: 0.0010498\ttotal: 28.3s\tremaining: 2m 28s\n",
      "160:\tlearn: 0.0010498\ttotal: 28.4s\tremaining: 2m 28s\n",
      "161:\tlearn: 0.0010400\ttotal: 28.6s\tremaining: 2m 27s\n",
      "162:\tlearn: 0.0010399\ttotal: 28.8s\tremaining: 2m 27s\n",
      "163:\tlearn: 0.0010356\ttotal: 29s\tremaining: 2m 27s\n",
      "164:\tlearn: 0.0010355\ttotal: 29.1s\tremaining: 2m 27s\n",
      "165:\tlearn: 0.0010355\ttotal: 29.3s\tremaining: 2m 27s\n",
      "166:\tlearn: 0.0010354\ttotal: 29.5s\tremaining: 2m 27s\n",
      "167:\tlearn: 0.0010352\ttotal: 29.7s\tremaining: 2m 27s\n",
      "168:\tlearn: 0.0010352\ttotal: 29.9s\tremaining: 2m 26s\n",
      "169:\tlearn: 0.0010285\ttotal: 30s\tremaining: 2m 26s\n",
      "170:\tlearn: 0.0010285\ttotal: 30.2s\tremaining: 2m 26s\n",
      "171:\tlearn: 0.0010276\ttotal: 30.4s\tremaining: 2m 26s\n",
      "172:\tlearn: 0.0010276\ttotal: 30.6s\tremaining: 2m 26s\n",
      "173:\tlearn: 0.0010201\ttotal: 30.7s\tremaining: 2m 25s\n",
      "174:\tlearn: 0.0010201\ttotal: 30.9s\tremaining: 2m 25s\n",
      "175:\tlearn: 0.0010198\ttotal: 31.1s\tremaining: 2m 25s\n",
      "176:\tlearn: 0.0010195\ttotal: 31.3s\tremaining: 2m 25s\n",
      "177:\tlearn: 0.0010193\ttotal: 31.4s\tremaining: 2m 25s\n",
      "178:\tlearn: 0.0010193\ttotal: 31.6s\tremaining: 2m 25s\n",
      "179:\tlearn: 0.0010192\ttotal: 31.8s\tremaining: 2m 24s\n",
      "180:\tlearn: 0.0010192\ttotal: 32s\tremaining: 2m 24s\n",
      "181:\tlearn: 0.0010192\ttotal: 32.1s\tremaining: 2m 24s\n",
      "182:\tlearn: 0.0010176\ttotal: 32.3s\tremaining: 2m 24s\n",
      "183:\tlearn: 0.0010176\ttotal: 32.5s\tremaining: 2m 24s\n",
      "184:\tlearn: 0.0010176\ttotal: 32.7s\tremaining: 2m 23s\n",
      "185:\tlearn: 0.0010176\ttotal: 32.8s\tremaining: 2m 23s\n",
      "186:\tlearn: 0.0010176\ttotal: 33s\tremaining: 2m 23s\n",
      "187:\tlearn: 0.0010175\ttotal: 33.2s\tremaining: 2m 23s\n",
      "188:\tlearn: 0.0010175\ttotal: 33.4s\tremaining: 2m 23s\n",
      "189:\tlearn: 0.0010174\ttotal: 33.5s\tremaining: 2m 22s\n",
      "190:\tlearn: 0.0010174\ttotal: 33.7s\tremaining: 2m 22s\n",
      "191:\tlearn: 0.0010099\ttotal: 33.9s\tremaining: 2m 22s\n",
      "192:\tlearn: 0.0010099\ttotal: 34.1s\tremaining: 2m 22s\n",
      "193:\tlearn: 0.0010029\ttotal: 34.2s\tremaining: 2m 22s\n",
      "194:\tlearn: 0.0010029\ttotal: 34.4s\tremaining: 2m 22s\n",
      "195:\tlearn: 0.0010024\ttotal: 34.6s\tremaining: 2m 21s\n",
      "196:\tlearn: 0.0010023\ttotal: 34.8s\tremaining: 2m 21s\n",
      "197:\tlearn: 0.0010023\ttotal: 34.9s\tremaining: 2m 21s\n",
      "198:\tlearn: 0.0010023\ttotal: 35.1s\tremaining: 2m 21s\n",
      "199:\tlearn: 0.0010023\ttotal: 35.3s\tremaining: 2m 21s\n",
      "200:\tlearn: 0.0010023\ttotal: 35.5s\tremaining: 2m 20s\n",
      "201:\tlearn: 0.0010021\ttotal: 35.6s\tremaining: 2m 20s\n",
      "202:\tlearn: 0.0009962\ttotal: 35.8s\tremaining: 2m 20s\n",
      "203:\tlearn: 0.0009960\ttotal: 36s\tremaining: 2m 20s\n",
      "204:\tlearn: 0.0009960\ttotal: 36.2s\tremaining: 2m 20s\n",
      "205:\tlearn: 0.0009958\ttotal: 36.3s\tremaining: 2m 20s\n",
      "206:\tlearn: 0.0009954\ttotal: 36.5s\tremaining: 2m 19s\n",
      "207:\tlearn: 0.0009954\ttotal: 36.7s\tremaining: 2m 19s\n",
      "208:\tlearn: 0.0009847\ttotal: 36.8s\tremaining: 2m 19s\n",
      "209:\tlearn: 0.0009782\ttotal: 37s\tremaining: 2m 19s\n",
      "210:\tlearn: 0.0009715\ttotal: 37.2s\tremaining: 2m 19s\n",
      "211:\tlearn: 0.0009715\ttotal: 37.4s\tremaining: 2m 18s\n",
      "212:\tlearn: 0.0009656\ttotal: 37.6s\tremaining: 2m 18s\n",
      "213:\tlearn: 0.0009654\ttotal: 37.7s\tremaining: 2m 18s\n",
      "214:\tlearn: 0.0009651\ttotal: 37.9s\tremaining: 2m 18s\n",
      "215:\tlearn: 0.0009650\ttotal: 38.1s\tremaining: 2m 18s\n",
      "216:\tlearn: 0.0009649\ttotal: 38.3s\tremaining: 2m 18s\n",
      "217:\tlearn: 0.0009647\ttotal: 38.4s\tremaining: 2m 17s\n",
      "218:\tlearn: 0.0009589\ttotal: 38.6s\tremaining: 2m 17s\n",
      "219:\tlearn: 0.0009587\ttotal: 38.8s\tremaining: 2m 17s\n",
      "220:\tlearn: 0.0009515\ttotal: 39s\tremaining: 2m 17s\n",
      "221:\tlearn: 0.0009497\ttotal: 39.1s\tremaining: 2m 17s\n",
      "222:\tlearn: 0.0009497\ttotal: 39.3s\tremaining: 2m 16s\n",
      "223:\tlearn: 0.0009495\ttotal: 39.5s\tremaining: 2m 16s\n",
      "224:\tlearn: 0.0009495\ttotal: 39.7s\tremaining: 2m 16s\n",
      "225:\tlearn: 0.0009495\ttotal: 39.8s\tremaining: 2m 16s\n",
      "226:\tlearn: 0.0009491\ttotal: 40s\tremaining: 2m 16s\n",
      "227:\tlearn: 0.0009491\ttotal: 40.2s\tremaining: 2m 16s\n",
      "228:\tlearn: 0.0009491\ttotal: 40.4s\tremaining: 2m 15s\n",
      "229:\tlearn: 0.0009490\ttotal: 40.5s\tremaining: 2m 15s\n",
      "230:\tlearn: 0.0009489\ttotal: 40.7s\tremaining: 2m 15s\n",
      "231:\tlearn: 0.0009486\ttotal: 40.9s\tremaining: 2m 15s\n",
      "232:\tlearn: 0.0009483\ttotal: 41.1s\tremaining: 2m 15s\n",
      "233:\tlearn: 0.0009483\ttotal: 41.2s\tremaining: 2m 14s\n",
      "234:\tlearn: 0.0009480\ttotal: 41.4s\tremaining: 2m 14s\n",
      "235:\tlearn: 0.0009403\ttotal: 41.6s\tremaining: 2m 14s\n",
      "236:\tlearn: 0.0009403\ttotal: 41.8s\tremaining: 2m 14s\n",
      "237:\tlearn: 0.0009403\ttotal: 41.9s\tremaining: 2m 14s\n",
      "238:\tlearn: 0.0009402\ttotal: 42.1s\tremaining: 2m 14s\n",
      "239:\tlearn: 0.0009402\ttotal: 42.3s\tremaining: 2m 13s\n",
      "240:\tlearn: 0.0009401\ttotal: 42.5s\tremaining: 2m 13s\n",
      "241:\tlearn: 0.0009401\ttotal: 42.6s\tremaining: 2m 13s\n",
      "242:\tlearn: 0.0009401\ttotal: 42.8s\tremaining: 2m 13s\n",
      "243:\tlearn: 0.0009399\ttotal: 43s\tremaining: 2m 13s\n",
      "244:\tlearn: 0.0009398\ttotal: 43.2s\tremaining: 2m 12s\n",
      "245:\tlearn: 0.0009396\ttotal: 43.3s\tremaining: 2m 12s\n",
      "246:\tlearn: 0.0009395\ttotal: 43.5s\tremaining: 2m 12s\n",
      "247:\tlearn: 0.0009395\ttotal: 43.7s\tremaining: 2m 12s\n",
      "248:\tlearn: 0.0009393\ttotal: 43.9s\tremaining: 2m 12s\n",
      "249:\tlearn: 0.0009393\ttotal: 44s\tremaining: 2m 12s\n",
      "250:\tlearn: 0.0009390\ttotal: 44.2s\tremaining: 2m 11s\n",
      "251:\tlearn: 0.0009315\ttotal: 44.4s\tremaining: 2m 11s\n",
      "252:\tlearn: 0.0009314\ttotal: 44.5s\tremaining: 2m 11s\n",
      "253:\tlearn: 0.0009314\ttotal: 44.7s\tremaining: 2m 11s\n",
      "254:\tlearn: 0.0009290\ttotal: 44.9s\tremaining: 2m 11s\n",
      "255:\tlearn: 0.0009290\ttotal: 45.1s\tremaining: 2m 10s\n",
      "256:\tlearn: 0.0009218\ttotal: 45.2s\tremaining: 2m 10s\n",
      "257:\tlearn: 0.0009218\ttotal: 45.4s\tremaining: 2m 10s\n",
      "258:\tlearn: 0.0009217\ttotal: 45.6s\tremaining: 2m 10s\n",
      "259:\tlearn: 0.0009217\ttotal: 45.8s\tremaining: 2m 10s\n",
      "260:\tlearn: 0.0009216\ttotal: 45.9s\tremaining: 2m 10s\n",
      "261:\tlearn: 0.0009215\ttotal: 46.1s\tremaining: 2m 9s\n",
      "262:\tlearn: 0.0009215\ttotal: 46.3s\tremaining: 2m 9s\n",
      "263:\tlearn: 0.0009156\ttotal: 46.5s\tremaining: 2m 9s\n",
      "264:\tlearn: 0.0009156\ttotal: 46.6s\tremaining: 2m 9s\n",
      "265:\tlearn: 0.0009153\ttotal: 46.8s\tremaining: 2m 9s\n",
      "266:\tlearn: 0.0009151\ttotal: 47s\tremaining: 2m 8s\n",
      "267:\tlearn: 0.0009149\ttotal: 47.2s\tremaining: 2m 8s\n",
      "268:\tlearn: 0.0009147\ttotal: 47.3s\tremaining: 2m 8s\n",
      "269:\tlearn: 0.0009147\ttotal: 47.5s\tremaining: 2m 8s\n",
      "270:\tlearn: 0.0009147\ttotal: 47.7s\tremaining: 2m 8s\n",
      "271:\tlearn: 0.0009147\ttotal: 47.9s\tremaining: 2m 8s\n",
      "272:\tlearn: 0.0009147\ttotal: 48s\tremaining: 2m 7s\n",
      "273:\tlearn: 0.0009099\ttotal: 48.2s\tremaining: 2m 7s\n",
      "274:\tlearn: 0.0009097\ttotal: 48.4s\tremaining: 2m 7s\n",
      "275:\tlearn: 0.0009096\ttotal: 48.6s\tremaining: 2m 7s\n",
      "276:\tlearn: 0.0009095\ttotal: 48.7s\tremaining: 2m 7s\n",
      "277:\tlearn: 0.0009094\ttotal: 48.9s\tremaining: 2m 7s\n",
      "278:\tlearn: 0.0009093\ttotal: 49.1s\tremaining: 2m 6s\n",
      "279:\tlearn: 0.0009078\ttotal: 49.3s\tremaining: 2m 6s\n",
      "280:\tlearn: 0.0009078\ttotal: 49.4s\tremaining: 2m 6s\n",
      "281:\tlearn: 0.0009077\ttotal: 49.6s\tremaining: 2m 6s\n",
      "282:\tlearn: 0.0009076\ttotal: 49.8s\tremaining: 2m 6s\n",
      "283:\tlearn: 0.0009076\ttotal: 50s\tremaining: 2m 5s\n",
      "284:\tlearn: 0.0009076\ttotal: 50.1s\tremaining: 2m 5s\n",
      "285:\tlearn: 0.0009025\ttotal: 50.3s\tremaining: 2m 5s\n",
      "286:\tlearn: 0.0009023\ttotal: 50.5s\tremaining: 2m 5s\n",
      "287:\tlearn: 0.0009016\ttotal: 50.7s\tremaining: 2m 5s\n",
      "288:\tlearn: 0.0009016\ttotal: 50.8s\tremaining: 2m 5s\n",
      "289:\tlearn: 0.0009016\ttotal: 51s\tremaining: 2m 4s\n",
      "290:\tlearn: 0.0009016\ttotal: 51.2s\tremaining: 2m 4s\n",
      "291:\tlearn: 0.0009015\ttotal: 51.4s\tremaining: 2m 4s\n",
      "292:\tlearn: 0.0009013\ttotal: 51.5s\tremaining: 2m 4s\n",
      "293:\tlearn: 0.0009013\ttotal: 51.7s\tremaining: 2m 4s\n",
      "294:\tlearn: 0.0009011\ttotal: 51.9s\tremaining: 2m 4s\n",
      "295:\tlearn: 0.0009009\ttotal: 52.1s\tremaining: 2m 3s\n",
      "296:\tlearn: 0.0009005\ttotal: 52.2s\tremaining: 2m 3s\n",
      "297:\tlearn: 0.0009005\ttotal: 52.4s\tremaining: 2m 3s\n",
      "298:\tlearn: 0.0009004\ttotal: 52.6s\tremaining: 2m 3s\n",
      "299:\tlearn: 0.0009004\ttotal: 52.8s\tremaining: 2m 3s\n",
      "300:\tlearn: 0.0009004\ttotal: 52.9s\tremaining: 2m 2s\n",
      "301:\tlearn: 0.0009004\ttotal: 53.1s\tremaining: 2m 2s\n",
      "302:\tlearn: 0.0009004\ttotal: 53.3s\tremaining: 2m 2s\n",
      "303:\tlearn: 0.0009003\ttotal: 53.5s\tremaining: 2m 2s\n",
      "304:\tlearn: 0.0009001\ttotal: 53.6s\tremaining: 2m 2s\n",
      "305:\tlearn: 0.0008961\ttotal: 53.8s\tremaining: 2m 2s\n",
      "306:\tlearn: 0.0008961\ttotal: 54s\tremaining: 2m 1s\n",
      "307:\tlearn: 0.0008961\ttotal: 54.2s\tremaining: 2m 1s\n",
      "308:\tlearn: 0.0008960\ttotal: 54.3s\tremaining: 2m 1s\n",
      "309:\tlearn: 0.0008959\ttotal: 54.5s\tremaining: 2m 1s\n",
      "310:\tlearn: 0.0008959\ttotal: 54.7s\tremaining: 2m 1s\n",
      "311:\tlearn: 0.0008959\ttotal: 54.9s\tremaining: 2m\n",
      "312:\tlearn: 0.0008958\ttotal: 55s\tremaining: 2m\n",
      "313:\tlearn: 0.0008958\ttotal: 55.2s\tremaining: 2m\n",
      "314:\tlearn: 0.0008938\ttotal: 55.4s\tremaining: 2m\n",
      "315:\tlearn: 0.0008909\ttotal: 55.6s\tremaining: 2m\n",
      "316:\tlearn: 0.0008909\ttotal: 55.7s\tremaining: 2m\n",
      "317:\tlearn: 0.0008887\ttotal: 55.9s\tremaining: 1m 59s\n",
      "318:\tlearn: 0.0008887\ttotal: 56.1s\tremaining: 1m 59s\n",
      "319:\tlearn: 0.0008863\ttotal: 56.3s\tremaining: 1m 59s\n",
      "320:\tlearn: 0.0008862\ttotal: 56.4s\tremaining: 1m 59s\n",
      "321:\tlearn: 0.0008857\ttotal: 56.6s\tremaining: 1m 59s\n",
      "322:\tlearn: 0.0008856\ttotal: 56.8s\tremaining: 1m 59s\n",
      "323:\tlearn: 0.0008856\ttotal: 57s\tremaining: 1m 58s\n",
      "324:\tlearn: 0.0008856\ttotal: 57.1s\tremaining: 1m 58s\n",
      "325:\tlearn: 0.0008855\ttotal: 57.3s\tremaining: 1m 58s\n",
      "326:\tlearn: 0.0008855\ttotal: 57.5s\tremaining: 1m 58s\n",
      "327:\tlearn: 0.0008855\ttotal: 57.7s\tremaining: 1m 58s\n",
      "328:\tlearn: 0.0008854\ttotal: 57.8s\tremaining: 1m 57s\n",
      "329:\tlearn: 0.0008853\ttotal: 58s\tremaining: 1m 57s\n",
      "330:\tlearn: 0.0008853\ttotal: 58.2s\tremaining: 1m 57s\n",
      "331:\tlearn: 0.0008852\ttotal: 58.4s\tremaining: 1m 57s\n",
      "332:\tlearn: 0.0008850\ttotal: 58.5s\tremaining: 1m 57s\n",
      "333:\tlearn: 0.0008850\ttotal: 58.7s\tremaining: 1m 57s\n",
      "334:\tlearn: 0.0008823\ttotal: 58.9s\tremaining: 1m 56s\n",
      "335:\tlearn: 0.0008766\ttotal: 59s\tremaining: 1m 56s\n",
      "336:\tlearn: 0.0008762\ttotal: 59.2s\tremaining: 1m 56s\n",
      "337:\tlearn: 0.0008761\ttotal: 59.4s\tremaining: 1m 56s\n",
      "338:\tlearn: 0.0008755\ttotal: 59.6s\tremaining: 1m 56s\n",
      "339:\tlearn: 0.0008755\ttotal: 59.7s\tremaining: 1m 55s\n",
      "340:\tlearn: 0.0008754\ttotal: 59.9s\tremaining: 1m 55s\n",
      "341:\tlearn: 0.0008754\ttotal: 1m\tremaining: 1m 55s\n",
      "342:\tlearn: 0.0008750\ttotal: 1m\tremaining: 1m 55s\n",
      "343:\tlearn: 0.0008749\ttotal: 1m\tremaining: 1m 55s\n",
      "344:\tlearn: 0.0008719\ttotal: 1m\tremaining: 1m 55s\n",
      "345:\tlearn: 0.0008719\ttotal: 1m\tremaining: 1m 54s\n",
      "346:\tlearn: 0.0008719\ttotal: 1m\tremaining: 1m 54s\n",
      "347:\tlearn: 0.0008719\ttotal: 1m 1s\tremaining: 1m 54s\n",
      "348:\tlearn: 0.0008719\ttotal: 1m 1s\tremaining: 1m 54s\n",
      "349:\tlearn: 0.0008718\ttotal: 1m 1s\tremaining: 1m 54s\n",
      "350:\tlearn: 0.0008716\ttotal: 1m 1s\tremaining: 1m 54s\n",
      "351:\tlearn: 0.0008716\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "352:\tlearn: 0.0008715\ttotal: 1m 2s\tremaining: 1m 53s\n",
      "353:\tlearn: 0.0008715\ttotal: 1m 2s\tremaining: 1m 53s\n",
      "354:\tlearn: 0.0008715\ttotal: 1m 2s\tremaining: 1m 53s\n",
      "355:\tlearn: 0.0008715\ttotal: 1m 2s\tremaining: 1m 53s\n",
      "356:\tlearn: 0.0008711\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "357:\tlearn: 0.0008711\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "358:\tlearn: 0.0008711\ttotal: 1m 3s\tremaining: 1m 52s\n",
      "359:\tlearn: 0.0008709\ttotal: 1m 3s\tremaining: 1m 52s\n",
      "360:\tlearn: 0.0008705\ttotal: 1m 3s\tremaining: 1m 52s\n",
      "361:\tlearn: 0.0008705\ttotal: 1m 3s\tremaining: 1m 52s\n",
      "362:\tlearn: 0.0008701\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "363:\tlearn: 0.0008701\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "364:\tlearn: 0.0008701\ttotal: 1m 4s\tremaining: 1m 51s\n",
      "365:\tlearn: 0.0008701\ttotal: 1m 4s\tremaining: 1m 51s\n",
      "366:\tlearn: 0.0008700\ttotal: 1m 4s\tremaining: 1m 51s\n",
      "367:\tlearn: 0.0008700\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "368:\tlearn: 0.0008700\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "369:\tlearn: 0.0008699\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "370:\tlearn: 0.0008699\ttotal: 1m 5s\tremaining: 1m 50s\n",
      "371:\tlearn: 0.0008698\ttotal: 1m 5s\tremaining: 1m 50s\n",
      "372:\tlearn: 0.0008698\ttotal: 1m 5s\tremaining: 1m 50s\n",
      "373:\tlearn: 0.0008695\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "374:\tlearn: 0.0008694\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "375:\tlearn: 0.0008694\ttotal: 1m 6s\tremaining: 1m 49s\n",
      "376:\tlearn: 0.0008694\ttotal: 1m 6s\tremaining: 1m 49s\n",
      "377:\tlearn: 0.0008691\ttotal: 1m 6s\tremaining: 1m 49s\n",
      "378:\tlearn: 0.0008689\ttotal: 1m 6s\tremaining: 1m 49s\n",
      "379:\tlearn: 0.0008610\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "380:\tlearn: 0.0008610\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "381:\tlearn: 0.0008610\ttotal: 1m 7s\tremaining: 1m 48s\n",
      "382:\tlearn: 0.0008546\ttotal: 1m 7s\tremaining: 1m 48s\n",
      "383:\tlearn: 0.0008545\ttotal: 1m 7s\tremaining: 1m 48s\n",
      "384:\tlearn: 0.0008537\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "385:\tlearn: 0.0008535\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "386:\tlearn: 0.0008535\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "387:\tlearn: 0.0008535\ttotal: 1m 8s\tremaining: 1m 47s\n",
      "388:\tlearn: 0.0008501\ttotal: 1m 8s\tremaining: 1m 47s\n",
      "389:\tlearn: 0.0008501\ttotal: 1m 8s\tremaining: 1m 47s\n",
      "390:\tlearn: 0.0008501\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "391:\tlearn: 0.0008501\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "392:\tlearn: 0.0008500\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "393:\tlearn: 0.0008498\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "394:\tlearn: 0.0008495\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "395:\tlearn: 0.0008436\ttotal: 1m 9s\tremaining: 1m 46s\n",
      "396:\tlearn: 0.0008436\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "397:\tlearn: 0.0008430\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "398:\tlearn: 0.0008429\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "399:\tlearn: 0.0008429\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "400:\tlearn: 0.0008429\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "401:\tlearn: 0.0008428\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "402:\tlearn: 0.0008427\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "403:\tlearn: 0.0008427\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "404:\tlearn: 0.0008422\ttotal: 1m 11s\tremaining: 1m 44s\n",
      "405:\tlearn: 0.0008421\ttotal: 1m 11s\tremaining: 1m 44s\n",
      "406:\tlearn: 0.0008418\ttotal: 1m 11s\tremaining: 1m 44s\n",
      "407:\tlearn: 0.0008418\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "408:\tlearn: 0.0008407\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "409:\tlearn: 0.0008406\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "410:\tlearn: 0.0008406\ttotal: 1m 12s\tremaining: 1m 43s\n",
      "411:\tlearn: 0.0008380\ttotal: 1m 12s\tremaining: 1m 43s\n",
      "412:\tlearn: 0.0008380\ttotal: 1m 12s\tremaining: 1m 43s\n",
      "413:\tlearn: 0.0008379\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "414:\tlearn: 0.0008378\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "415:\tlearn: 0.0008378\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "416:\tlearn: 0.0008378\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "417:\tlearn: 0.0008374\ttotal: 1m 13s\tremaining: 1m 42s\n",
      "418:\tlearn: 0.0008374\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "419:\tlearn: 0.0008374\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "420:\tlearn: 0.0008373\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "421:\tlearn: 0.0008373\ttotal: 1m 14s\tremaining: 1m 41s\n",
      "422:\tlearn: 0.0008370\ttotal: 1m 14s\tremaining: 1m 41s\n",
      "423:\tlearn: 0.0008322\ttotal: 1m 14s\tremaining: 1m 41s\n",
      "424:\tlearn: 0.0008321\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "425:\tlearn: 0.0008320\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "426:\tlearn: 0.0008320\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "427:\tlearn: 0.0008316\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "428:\tlearn: 0.0008275\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "429:\tlearn: 0.0008274\ttotal: 1m 15s\tremaining: 1m 40s\n",
      "430:\tlearn: 0.0008274\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "431:\tlearn: 0.0008274\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "432:\tlearn: 0.0008274\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "433:\tlearn: 0.0008273\ttotal: 1m 16s\tremaining: 1m 39s\n",
      "434:\tlearn: 0.0008273\ttotal: 1m 16s\tremaining: 1m 39s\n",
      "435:\tlearn: 0.0008270\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "436:\tlearn: 0.0008269\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "437:\tlearn: 0.0008269\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "438:\tlearn: 0.0008269\ttotal: 1m 17s\tremaining: 1m 38s\n",
      "439:\tlearn: 0.0008269\ttotal: 1m 17s\tremaining: 1m 38s\n",
      "440:\tlearn: 0.0008266\ttotal: 1m 17s\tremaining: 1m 38s\n",
      "441:\tlearn: 0.0008265\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "442:\tlearn: 0.0008265\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "443:\tlearn: 0.0008265\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "444:\tlearn: 0.0008261\ttotal: 1m 18s\tremaining: 1m 37s\n",
      "445:\tlearn: 0.0008226\ttotal: 1m 18s\tremaining: 1m 37s\n",
      "446:\tlearn: 0.0008226\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "447:\tlearn: 0.0008226\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "448:\tlearn: 0.0008226\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "449:\tlearn: 0.0008225\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "450:\tlearn: 0.0008223\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "451:\tlearn: 0.0008222\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "452:\tlearn: 0.0008221\ttotal: 1m 19s\tremaining: 1m 36s\n",
      "453:\tlearn: 0.0008219\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "454:\tlearn: 0.0008172\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "455:\tlearn: 0.0008164\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "456:\tlearn: 0.0008164\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "457:\tlearn: 0.0008164\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "458:\tlearn: 0.0008163\ttotal: 1m 20s\tremaining: 1m 35s\n",
      "459:\tlearn: 0.0008163\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "460:\tlearn: 0.0008163\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "461:\tlearn: 0.0008160\ttotal: 1m 21s\tremaining: 1m 34s\n",
      "462:\tlearn: 0.0008155\ttotal: 1m 21s\tremaining: 1m 34s\n",
      "463:\tlearn: 0.0008155\ttotal: 1m 21s\tremaining: 1m 34s\n",
      "464:\tlearn: 0.0008154\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "465:\tlearn: 0.0008153\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "466:\tlearn: 0.0008153\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "467:\tlearn: 0.0008152\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "468:\tlearn: 0.0008150\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "469:\tlearn: 0.0008149\ttotal: 1m 22s\tremaining: 1m 33s\n",
      "470:\tlearn: 0.0008149\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "471:\tlearn: 0.0008148\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "472:\tlearn: 0.0008148\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "473:\tlearn: 0.0008147\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "474:\tlearn: 0.0008147\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "475:\tlearn: 0.0008146\ttotal: 1m 23s\tremaining: 1m 32s\n",
      "476:\tlearn: 0.0008145\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "477:\tlearn: 0.0008145\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "478:\tlearn: 0.0008145\ttotal: 1m 24s\tremaining: 1m 31s\n",
      "479:\tlearn: 0.0008145\ttotal: 1m 24s\tremaining: 1m 31s\n",
      "480:\tlearn: 0.0008145\ttotal: 1m 24s\tremaining: 1m 31s\n",
      "481:\tlearn: 0.0008145\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "482:\tlearn: 0.0008144\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "483:\tlearn: 0.0008144\ttotal: 1m 25s\tremaining: 1m 30s\n",
      "484:\tlearn: 0.0008144\ttotal: 1m 25s\tremaining: 1m 30s\n",
      "485:\tlearn: 0.0008144\ttotal: 1m 25s\tremaining: 1m 30s\n",
      "486:\tlearn: 0.0008143\ttotal: 1m 25s\tremaining: 1m 30s\n",
      "487:\tlearn: 0.0008141\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "488:\tlearn: 0.0008140\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "489:\tlearn: 0.0008140\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "490:\tlearn: 0.0008138\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "491:\tlearn: 0.0008136\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "492:\tlearn: 0.0008136\ttotal: 1m 26s\tremaining: 1m 29s\n",
      "493:\tlearn: 0.0008135\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "494:\tlearn: 0.0008134\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "495:\tlearn: 0.0008132\ttotal: 1m 27s\tremaining: 1m 28s\n",
      "496:\tlearn: 0.0008130\ttotal: 1m 27s\tremaining: 1m 28s\n",
      "497:\tlearn: 0.0008129\ttotal: 1m 27s\tremaining: 1m 28s\n",
      "498:\tlearn: 0.0008129\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "499:\tlearn: 0.0008127\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "500:\tlearn: 0.0008124\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "501:\tlearn: 0.0008121\ttotal: 1m 28s\tremaining: 1m 27s\n",
      "502:\tlearn: 0.0008118\ttotal: 1m 28s\tremaining: 1m 27s\n",
      "503:\tlearn: 0.0008118\ttotal: 1m 28s\tremaining: 1m 27s\n",
      "504:\tlearn: 0.0008118\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "505:\tlearn: 0.0008118\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "506:\tlearn: 0.0008118\ttotal: 1m 29s\tremaining: 1m 26s\n",
      "507:\tlearn: 0.0008118\ttotal: 1m 29s\tremaining: 1m 26s\n",
      "508:\tlearn: 0.0008117\ttotal: 1m 29s\tremaining: 1m 26s\n",
      "509:\tlearn: 0.0008117\ttotal: 1m 29s\tremaining: 1m 26s\n",
      "510:\tlearn: 0.0008115\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "511:\tlearn: 0.0008115\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "512:\tlearn: 0.0008115\ttotal: 1m 30s\tremaining: 1m 25s\n",
      "513:\tlearn: 0.0008115\ttotal: 1m 30s\tremaining: 1m 25s\n",
      "514:\tlearn: 0.0008115\ttotal: 1m 30s\tremaining: 1m 25s\n",
      "515:\tlearn: 0.0008115\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "516:\tlearn: 0.0008114\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "517:\tlearn: 0.0008114\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "518:\tlearn: 0.0008114\ttotal: 1m 31s\tremaining: 1m 24s\n",
      "519:\tlearn: 0.0008114\ttotal: 1m 31s\tremaining: 1m 24s\n",
      "520:\tlearn: 0.0008113\ttotal: 1m 31s\tremaining: 1m 24s\n",
      "521:\tlearn: 0.0008113\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "522:\tlearn: 0.0008111\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "523:\tlearn: 0.0008111\ttotal: 1m 32s\tremaining: 1m 23s\n",
      "524:\tlearn: 0.0008110\ttotal: 1m 32s\tremaining: 1m 23s\n",
      "525:\tlearn: 0.0008105\ttotal: 1m 32s\tremaining: 1m 23s\n",
      "526:\tlearn: 0.0008105\ttotal: 1m 32s\tremaining: 1m 23s\n",
      "527:\tlearn: 0.0008103\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "528:\tlearn: 0.0008102\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "529:\tlearn: 0.0008102\ttotal: 1m 33s\tremaining: 1m 22s\n",
      "530:\tlearn: 0.0008102\ttotal: 1m 33s\tremaining: 1m 22s\n",
      "531:\tlearn: 0.0008101\ttotal: 1m 33s\tremaining: 1m 22s\n",
      "532:\tlearn: 0.0008101\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "533:\tlearn: 0.0008099\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "534:\tlearn: 0.0008098\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "535:\tlearn: 0.0008098\ttotal: 1m 34s\tremaining: 1m 21s\n",
      "536:\tlearn: 0.0008097\ttotal: 1m 34s\tremaining: 1m 21s\n",
      "537:\tlearn: 0.0008096\ttotal: 1m 34s\tremaining: 1m 21s\n",
      "538:\tlearn: 0.0008096\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "539:\tlearn: 0.0008096\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "540:\tlearn: 0.0008095\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "541:\tlearn: 0.0008095\ttotal: 1m 35s\tremaining: 1m 20s\n",
      "542:\tlearn: 0.0008053\ttotal: 1m 35s\tremaining: 1m 20s\n",
      "543:\tlearn: 0.0008053\ttotal: 1m 35s\tremaining: 1m 20s\n",
      "544:\tlearn: 0.0008045\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "545:\tlearn: 0.0008044\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "546:\tlearn: 0.0007976\ttotal: 1m 36s\tremaining: 1m 19s\n",
      "547:\tlearn: 0.0007973\ttotal: 1m 36s\tremaining: 1m 19s\n",
      "548:\tlearn: 0.0007973\ttotal: 1m 36s\tremaining: 1m 19s\n",
      "549:\tlearn: 0.0007973\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "550:\tlearn: 0.0007972\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "551:\tlearn: 0.0007972\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "552:\tlearn: 0.0007971\ttotal: 1m 37s\tremaining: 1m 18s\n",
      "553:\tlearn: 0.0007969\ttotal: 1m 37s\tremaining: 1m 18s\n",
      "554:\tlearn: 0.0007968\ttotal: 1m 37s\tremaining: 1m 18s\n",
      "555:\tlearn: 0.0007953\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "556:\tlearn: 0.0007950\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "557:\tlearn: 0.0007950\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "558:\tlearn: 0.0007949\ttotal: 1m 38s\tremaining: 1m 17s\n",
      "559:\tlearn: 0.0007948\ttotal: 1m 38s\tremaining: 1m 17s\n",
      "560:\tlearn: 0.0007948\ttotal: 1m 38s\tremaining: 1m 17s\n",
      "561:\tlearn: 0.0007947\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "562:\tlearn: 0.0007947\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "563:\tlearn: 0.0007946\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "564:\tlearn: 0.0007946\ttotal: 1m 39s\tremaining: 1m 16s\n",
      "565:\tlearn: 0.0007945\ttotal: 1m 39s\tremaining: 1m 16s\n",
      "566:\tlearn: 0.0007943\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "567:\tlearn: 0.0007943\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "568:\tlearn: 0.0007942\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "569:\tlearn: 0.0007942\ttotal: 1m 40s\tremaining: 1m 15s\n",
      "570:\tlearn: 0.0007941\ttotal: 1m 40s\tremaining: 1m 15s\n",
      "571:\tlearn: 0.0007941\ttotal: 1m 40s\tremaining: 1m 15s\n",
      "572:\tlearn: 0.0007939\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "573:\tlearn: 0.0007939\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "574:\tlearn: 0.0007933\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "575:\tlearn: 0.0007914\ttotal: 1m 41s\tremaining: 1m 14s\n",
      "576:\tlearn: 0.0007905\ttotal: 1m 41s\tremaining: 1m 14s\n",
      "577:\tlearn: 0.0007905\ttotal: 1m 41s\tremaining: 1m 14s\n",
      "578:\tlearn: 0.0007904\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "579:\tlearn: 0.0007888\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "580:\tlearn: 0.0007888\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "581:\tlearn: 0.0007887\ttotal: 1m 42s\tremaining: 1m 13s\n",
      "582:\tlearn: 0.0007887\ttotal: 1m 42s\tremaining: 1m 13s\n",
      "583:\tlearn: 0.0007887\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "584:\tlearn: 0.0007886\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "585:\tlearn: 0.0007883\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "586:\tlearn: 0.0007877\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "587:\tlearn: 0.0007871\ttotal: 1m 43s\tremaining: 1m 12s\n",
      "588:\tlearn: 0.0007870\ttotal: 1m 43s\tremaining: 1m 12s\n",
      "589:\tlearn: 0.0007870\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "590:\tlearn: 0.0007869\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "591:\tlearn: 0.0007866\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "592:\tlearn: 0.0007865\ttotal: 1m 44s\tremaining: 1m 11s\n",
      "593:\tlearn: 0.0007865\ttotal: 1m 44s\tremaining: 1m 11s\n",
      "594:\tlearn: 0.0007860\ttotal: 1m 44s\tremaining: 1m 11s\n",
      "595:\tlearn: 0.0007804\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "596:\tlearn: 0.0007803\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "597:\tlearn: 0.0007803\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "598:\tlearn: 0.0007796\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "599:\tlearn: 0.0007796\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "600:\tlearn: 0.0007794\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "601:\tlearn: 0.0007794\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "602:\tlearn: 0.0007793\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "603:\tlearn: 0.0007789\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "604:\tlearn: 0.0007787\ttotal: 1m 46s\tremaining: 1m 9s\n",
      "605:\tlearn: 0.0007786\ttotal: 1m 46s\tremaining: 1m 9s\n",
      "606:\tlearn: 0.0007785\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "607:\tlearn: 0.0007780\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "608:\tlearn: 0.0007780\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "609:\tlearn: 0.0007779\ttotal: 1m 47s\tremaining: 1m 8s\n",
      "610:\tlearn: 0.0007778\ttotal: 1m 47s\tremaining: 1m 8s\n",
      "611:\tlearn: 0.0007778\ttotal: 1m 47s\tremaining: 1m 8s\n",
      "612:\tlearn: 0.0007772\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "613:\tlearn: 0.0007772\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "614:\tlearn: 0.0007771\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "615:\tlearn: 0.0007771\ttotal: 1m 48s\tremaining: 1m 7s\n",
      "616:\tlearn: 0.0007771\ttotal: 1m 48s\tremaining: 1m 7s\n",
      "617:\tlearn: 0.0007771\ttotal: 1m 48s\tremaining: 1m 7s\n",
      "618:\tlearn: 0.0007771\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "619:\tlearn: 0.0007771\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "620:\tlearn: 0.0007770\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "621:\tlearn: 0.0007770\ttotal: 1m 49s\tremaining: 1m 6s\n",
      "622:\tlearn: 0.0007767\ttotal: 1m 49s\tremaining: 1m 6s\n",
      "623:\tlearn: 0.0007767\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "624:\tlearn: 0.0007767\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "625:\tlearn: 0.0007760\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "626:\tlearn: 0.0007759\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "627:\tlearn: 0.0007759\ttotal: 1m 50s\tremaining: 1m 5s\n",
      "628:\tlearn: 0.0007757\ttotal: 1m 50s\tremaining: 1m 5s\n",
      "629:\tlearn: 0.0007757\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "630:\tlearn: 0.0007757\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "631:\tlearn: 0.0007756\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "632:\tlearn: 0.0007755\ttotal: 1m 51s\tremaining: 1m 4s\n",
      "633:\tlearn: 0.0007754\ttotal: 1m 51s\tremaining: 1m 4s\n",
      "634:\tlearn: 0.0007752\ttotal: 1m 51s\tremaining: 1m 4s\n",
      "635:\tlearn: 0.0007752\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "636:\tlearn: 0.0007750\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "637:\tlearn: 0.0007750\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "638:\tlearn: 0.0007750\ttotal: 1m 52s\tremaining: 1m 3s\n",
      "639:\tlearn: 0.0007748\ttotal: 1m 52s\tremaining: 1m 3s\n",
      "640:\tlearn: 0.0007748\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "641:\tlearn: 0.0007742\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "642:\tlearn: 0.0007742\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "643:\tlearn: 0.0007742\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "644:\tlearn: 0.0007740\ttotal: 1m 53s\tremaining: 1m 2s\n",
      "645:\tlearn: 0.0007740\ttotal: 1m 53s\tremaining: 1m 2s\n",
      "646:\tlearn: 0.0007739\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "647:\tlearn: 0.0007738\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "648:\tlearn: 0.0007738\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "649:\tlearn: 0.0007738\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "650:\tlearn: 0.0007738\ttotal: 1m 54s\tremaining: 1m 1s\n",
      "651:\tlearn: 0.0007735\ttotal: 1m 54s\tremaining: 1m 1s\n",
      "652:\tlearn: 0.0007732\ttotal: 1m 54s\tremaining: 1m\n",
      "653:\tlearn: 0.0007731\ttotal: 1m 54s\tremaining: 1m\n",
      "654:\tlearn: 0.0007729\ttotal: 1m 54s\tremaining: 1m\n",
      "655:\tlearn: 0.0007728\ttotal: 1m 55s\tremaining: 1m\n",
      "656:\tlearn: 0.0007728\ttotal: 1m 55s\tremaining: 1m\n",
      "657:\tlearn: 0.0007728\ttotal: 1m 55s\tremaining: 60s\n",
      "658:\tlearn: 0.0007727\ttotal: 1m 55s\tremaining: 59.8s\n",
      "659:\tlearn: 0.0007727\ttotal: 1m 55s\tremaining: 59.6s\n",
      "660:\tlearn: 0.0007727\ttotal: 1m 55s\tremaining: 59.5s\n",
      "661:\tlearn: 0.0007724\ttotal: 1m 56s\tremaining: 59.3s\n",
      "662:\tlearn: 0.0007724\ttotal: 1m 56s\tremaining: 59.1s\n",
      "663:\tlearn: 0.0007724\ttotal: 1m 56s\tremaining: 58.9s\n",
      "664:\tlearn: 0.0007724\ttotal: 1m 56s\tremaining: 58.7s\n",
      "665:\tlearn: 0.0007722\ttotal: 1m 56s\tremaining: 58.6s\n",
      "666:\tlearn: 0.0007721\ttotal: 1m 56s\tremaining: 58.4s\n",
      "667:\tlearn: 0.0007721\ttotal: 1m 57s\tremaining: 58.2s\n",
      "668:\tlearn: 0.0007676\ttotal: 1m 57s\tremaining: 58.1s\n",
      "669:\tlearn: 0.0007676\ttotal: 1m 57s\tremaining: 57.9s\n",
      "670:\tlearn: 0.0007676\ttotal: 1m 57s\tremaining: 57.7s\n",
      "671:\tlearn: 0.0007669\ttotal: 1m 57s\tremaining: 57.5s\n",
      "672:\tlearn: 0.0007669\ttotal: 1m 58s\tremaining: 57.4s\n",
      "673:\tlearn: 0.0007668\ttotal: 1m 58s\tremaining: 57.2s\n",
      "674:\tlearn: 0.0007668\ttotal: 1m 58s\tremaining: 57s\n",
      "675:\tlearn: 0.0007666\ttotal: 1m 58s\tremaining: 56.8s\n",
      "676:\tlearn: 0.0007664\ttotal: 1m 58s\tremaining: 56.6s\n",
      "677:\tlearn: 0.0007663\ttotal: 1m 58s\tremaining: 56.5s\n",
      "678:\tlearn: 0.0007616\ttotal: 1m 59s\tremaining: 56.3s\n",
      "679:\tlearn: 0.0007616\ttotal: 1m 59s\tremaining: 56.1s\n",
      "680:\tlearn: 0.0007616\ttotal: 1m 59s\tremaining: 55.9s\n",
      "681:\tlearn: 0.0007616\ttotal: 1m 59s\tremaining: 55.8s\n",
      "682:\tlearn: 0.0007611\ttotal: 1m 59s\tremaining: 55.6s\n",
      "683:\tlearn: 0.0007609\ttotal: 1m 59s\tremaining: 55.4s\n",
      "684:\tlearn: 0.0007561\ttotal: 2m\tremaining: 55.2s\n",
      "685:\tlearn: 0.0007556\ttotal: 2m\tremaining: 55.1s\n",
      "686:\tlearn: 0.0007555\ttotal: 2m\tremaining: 54.9s\n",
      "687:\tlearn: 0.0007554\ttotal: 2m\tremaining: 54.7s\n",
      "688:\tlearn: 0.0007553\ttotal: 2m\tremaining: 54.5s\n",
      "689:\tlearn: 0.0007550\ttotal: 2m 1s\tremaining: 54.4s\n",
      "690:\tlearn: 0.0007548\ttotal: 2m 1s\tremaining: 54.2s\n",
      "691:\tlearn: 0.0007548\ttotal: 2m 1s\tremaining: 54s\n",
      "692:\tlearn: 0.0007548\ttotal: 2m 1s\tremaining: 53.8s\n",
      "693:\tlearn: 0.0007547\ttotal: 2m 1s\tremaining: 53.7s\n",
      "694:\tlearn: 0.0007547\ttotal: 2m 1s\tremaining: 53.5s\n",
      "695:\tlearn: 0.0007547\ttotal: 2m 2s\tremaining: 53.3s\n",
      "696:\tlearn: 0.0007546\ttotal: 2m 2s\tremaining: 53.1s\n",
      "697:\tlearn: 0.0007546\ttotal: 2m 2s\tremaining: 53s\n",
      "698:\tlearn: 0.0007546\ttotal: 2m 2s\tremaining: 52.8s\n",
      "699:\tlearn: 0.0007544\ttotal: 2m 2s\tremaining: 52.6s\n",
      "700:\tlearn: 0.0007544\ttotal: 2m 2s\tremaining: 52.4s\n",
      "701:\tlearn: 0.0007544\ttotal: 2m 3s\tremaining: 52.3s\n",
      "702:\tlearn: 0.0007544\ttotal: 2m 3s\tremaining: 52.1s\n",
      "703:\tlearn: 0.0007544\ttotal: 2m 3s\tremaining: 51.9s\n",
      "704:\tlearn: 0.0007540\ttotal: 2m 3s\tremaining: 51.7s\n",
      "705:\tlearn: 0.0007539\ttotal: 2m 3s\tremaining: 51.6s\n",
      "706:\tlearn: 0.0007538\ttotal: 2m 3s\tremaining: 51.4s\n",
      "707:\tlearn: 0.0007538\ttotal: 2m 4s\tremaining: 51.2s\n",
      "708:\tlearn: 0.0007537\ttotal: 2m 4s\tremaining: 51s\n",
      "709:\tlearn: 0.0007537\ttotal: 2m 4s\tremaining: 50.9s\n",
      "710:\tlearn: 0.0007483\ttotal: 2m 4s\tremaining: 50.7s\n",
      "711:\tlearn: 0.0007482\ttotal: 2m 4s\tremaining: 50.5s\n",
      "712:\tlearn: 0.0007482\ttotal: 2m 5s\tremaining: 50.3s\n",
      "713:\tlearn: 0.0007482\ttotal: 2m 5s\tremaining: 50.2s\n",
      "714:\tlearn: 0.0007477\ttotal: 2m 5s\tremaining: 50s\n",
      "715:\tlearn: 0.0007476\ttotal: 2m 5s\tremaining: 49.8s\n",
      "716:\tlearn: 0.0007476\ttotal: 2m 5s\tremaining: 49.6s\n",
      "717:\tlearn: 0.0007473\ttotal: 2m 5s\tremaining: 49.5s\n",
      "718:\tlearn: 0.0007472\ttotal: 2m 6s\tremaining: 49.3s\n",
      "719:\tlearn: 0.0007470\ttotal: 2m 6s\tremaining: 49.1s\n",
      "720:\tlearn: 0.0007470\ttotal: 2m 6s\tremaining: 48.9s\n",
      "721:\tlearn: 0.0007470\ttotal: 2m 6s\tremaining: 48.8s\n",
      "722:\tlearn: 0.0007470\ttotal: 2m 6s\tremaining: 48.6s\n",
      "723:\tlearn: 0.0007469\ttotal: 2m 6s\tremaining: 48.4s\n",
      "724:\tlearn: 0.0007468\ttotal: 2m 7s\tremaining: 48.2s\n",
      "725:\tlearn: 0.0007467\ttotal: 2m 7s\tremaining: 48s\n",
      "726:\tlearn: 0.0007465\ttotal: 2m 7s\tremaining: 47.9s\n",
      "727:\tlearn: 0.0007464\ttotal: 2m 7s\tremaining: 47.7s\n",
      "728:\tlearn: 0.0007464\ttotal: 2m 7s\tremaining: 47.5s\n",
      "729:\tlearn: 0.0007464\ttotal: 2m 8s\tremaining: 47.3s\n",
      "730:\tlearn: 0.0007464\ttotal: 2m 8s\tremaining: 47.2s\n",
      "731:\tlearn: 0.0007464\ttotal: 2m 8s\tremaining: 47s\n",
      "732:\tlearn: 0.0007463\ttotal: 2m 8s\tremaining: 46.8s\n",
      "733:\tlearn: 0.0007462\ttotal: 2m 8s\tremaining: 46.6s\n",
      "734:\tlearn: 0.0007461\ttotal: 2m 8s\tremaining: 46.5s\n",
      "735:\tlearn: 0.0007459\ttotal: 2m 9s\tremaining: 46.3s\n",
      "736:\tlearn: 0.0007458\ttotal: 2m 9s\tremaining: 46.1s\n",
      "737:\tlearn: 0.0007457\ttotal: 2m 9s\tremaining: 45.9s\n",
      "738:\tlearn: 0.0007457\ttotal: 2m 9s\tremaining: 45.8s\n",
      "739:\tlearn: 0.0007456\ttotal: 2m 9s\tremaining: 45.6s\n",
      "740:\tlearn: 0.0007453\ttotal: 2m 9s\tremaining: 45.4s\n",
      "741:\tlearn: 0.0007452\ttotal: 2m 10s\tremaining: 45.3s\n",
      "742:\tlearn: 0.0007451\ttotal: 2m 10s\tremaining: 45.1s\n",
      "743:\tlearn: 0.0007448\ttotal: 2m 10s\tremaining: 44.9s\n",
      "744:\tlearn: 0.0007447\ttotal: 2m 10s\tremaining: 44.7s\n",
      "745:\tlearn: 0.0007443\ttotal: 2m 10s\tremaining: 44.6s\n",
      "746:\tlearn: 0.0007442\ttotal: 2m 11s\tremaining: 44.4s\n",
      "747:\tlearn: 0.0007442\ttotal: 2m 11s\tremaining: 44.2s\n",
      "748:\tlearn: 0.0007442\ttotal: 2m 11s\tremaining: 44.1s\n",
      "749:\tlearn: 0.0007442\ttotal: 2m 11s\tremaining: 43.9s\n",
      "750:\tlearn: 0.0007442\ttotal: 2m 11s\tremaining: 43.7s\n",
      "751:\tlearn: 0.0007442\ttotal: 2m 12s\tremaining: 43.6s\n",
      "752:\tlearn: 0.0007441\ttotal: 2m 12s\tremaining: 43.4s\n",
      "753:\tlearn: 0.0007441\ttotal: 2m 12s\tremaining: 43.2s\n",
      "754:\tlearn: 0.0007441\ttotal: 2m 12s\tremaining: 43.1s\n",
      "755:\tlearn: 0.0007441\ttotal: 2m 12s\tremaining: 42.9s\n",
      "756:\tlearn: 0.0007439\ttotal: 2m 13s\tremaining: 42.7s\n",
      "757:\tlearn: 0.0007439\ttotal: 2m 13s\tremaining: 42.6s\n",
      "758:\tlearn: 0.0007438\ttotal: 2m 13s\tremaining: 42.4s\n",
      "759:\tlearn: 0.0007435\ttotal: 2m 13s\tremaining: 42.2s\n",
      "760:\tlearn: 0.0007435\ttotal: 2m 13s\tremaining: 42.1s\n",
      "761:\tlearn: 0.0007435\ttotal: 2m 14s\tremaining: 41.9s\n",
      "762:\tlearn: 0.0007433\ttotal: 2m 14s\tremaining: 41.7s\n",
      "763:\tlearn: 0.0007431\ttotal: 2m 14s\tremaining: 41.6s\n",
      "764:\tlearn: 0.0007430\ttotal: 2m 14s\tremaining: 41.4s\n",
      "765:\tlearn: 0.0007428\ttotal: 2m 14s\tremaining: 41.2s\n",
      "766:\tlearn: 0.0007427\ttotal: 2m 15s\tremaining: 41s\n",
      "767:\tlearn: 0.0007427\ttotal: 2m 15s\tremaining: 40.9s\n",
      "768:\tlearn: 0.0007427\ttotal: 2m 15s\tremaining: 40.7s\n",
      "769:\tlearn: 0.0007427\ttotal: 2m 15s\tremaining: 40.5s\n",
      "770:\tlearn: 0.0007427\ttotal: 2m 15s\tremaining: 40.4s\n",
      "771:\tlearn: 0.0007426\ttotal: 2m 16s\tremaining: 40.2s\n",
      "772:\tlearn: 0.0007426\ttotal: 2m 16s\tremaining: 40s\n",
      "773:\tlearn: 0.0007425\ttotal: 2m 16s\tremaining: 39.9s\n",
      "774:\tlearn: 0.0007425\ttotal: 2m 16s\tremaining: 39.7s\n",
      "775:\tlearn: 0.0007424\ttotal: 2m 16s\tremaining: 39.5s\n",
      "776:\tlearn: 0.0007424\ttotal: 2m 17s\tremaining: 39.4s\n",
      "777:\tlearn: 0.0007373\ttotal: 2m 17s\tremaining: 39.2s\n",
      "778:\tlearn: 0.0007372\ttotal: 2m 17s\tremaining: 39s\n",
      "779:\tlearn: 0.0007372\ttotal: 2m 17s\tremaining: 38.8s\n",
      "780:\tlearn: 0.0007372\ttotal: 2m 17s\tremaining: 38.7s\n",
      "781:\tlearn: 0.0007372\ttotal: 2m 18s\tremaining: 38.5s\n",
      "782:\tlearn: 0.0007372\ttotal: 2m 18s\tremaining: 38.3s\n",
      "783:\tlearn: 0.0007371\ttotal: 2m 18s\tremaining: 38.1s\n",
      "784:\tlearn: 0.0007371\ttotal: 2m 18s\tremaining: 38s\n",
      "785:\tlearn: 0.0007370\ttotal: 2m 18s\tremaining: 37.8s\n",
      "786:\tlearn: 0.0007367\ttotal: 2m 18s\tremaining: 37.6s\n",
      "787:\tlearn: 0.0007367\ttotal: 2m 19s\tremaining: 37.4s\n",
      "788:\tlearn: 0.0007367\ttotal: 2m 19s\tremaining: 37.2s\n",
      "789:\tlearn: 0.0007367\ttotal: 2m 19s\tremaining: 37.1s\n",
      "790:\tlearn: 0.0007366\ttotal: 2m 19s\tremaining: 36.9s\n",
      "791:\tlearn: 0.0007366\ttotal: 2m 19s\tremaining: 36.7s\n",
      "792:\tlearn: 0.0007365\ttotal: 2m 19s\tremaining: 36.5s\n",
      "793:\tlearn: 0.0007364\ttotal: 2m 20s\tremaining: 36.4s\n",
      "794:\tlearn: 0.0007364\ttotal: 2m 20s\tremaining: 36.2s\n",
      "795:\tlearn: 0.0007363\ttotal: 2m 20s\tremaining: 36s\n",
      "796:\tlearn: 0.0007357\ttotal: 2m 20s\tremaining: 35.8s\n",
      "797:\tlearn: 0.0007352\ttotal: 2m 20s\tremaining: 35.7s\n",
      "798:\tlearn: 0.0007349\ttotal: 2m 21s\tremaining: 35.5s\n",
      "799:\tlearn: 0.0007349\ttotal: 2m 21s\tremaining: 35.3s\n",
      "800:\tlearn: 0.0007322\ttotal: 2m 21s\tremaining: 35.1s\n",
      "801:\tlearn: 0.0007321\ttotal: 2m 21s\tremaining: 34.9s\n",
      "802:\tlearn: 0.0007321\ttotal: 2m 21s\tremaining: 34.8s\n",
      "803:\tlearn: 0.0007320\ttotal: 2m 21s\tremaining: 34.6s\n",
      "804:\tlearn: 0.0007258\ttotal: 2m 22s\tremaining: 34.4s\n",
      "805:\tlearn: 0.0007258\ttotal: 2m 22s\tremaining: 34.2s\n",
      "806:\tlearn: 0.0007256\ttotal: 2m 22s\tremaining: 34.1s\n",
      "807:\tlearn: 0.0007255\ttotal: 2m 22s\tremaining: 33.9s\n",
      "808:\tlearn: 0.0007255\ttotal: 2m 22s\tremaining: 33.7s\n",
      "809:\tlearn: 0.0007255\ttotal: 2m 22s\tremaining: 33.5s\n",
      "810:\tlearn: 0.0007252\ttotal: 2m 23s\tremaining: 33.4s\n",
      "811:\tlearn: 0.0007251\ttotal: 2m 23s\tremaining: 33.2s\n",
      "812:\tlearn: 0.0007251\ttotal: 2m 23s\tremaining: 33s\n",
      "813:\tlearn: 0.0007251\ttotal: 2m 23s\tremaining: 32.8s\n",
      "814:\tlearn: 0.0007249\ttotal: 2m 23s\tremaining: 32.6s\n",
      "815:\tlearn: 0.0007248\ttotal: 2m 23s\tremaining: 32.5s\n",
      "816:\tlearn: 0.0007248\ttotal: 2m 24s\tremaining: 32.3s\n",
      "817:\tlearn: 0.0007248\ttotal: 2m 24s\tremaining: 32.1s\n",
      "818:\tlearn: 0.0007245\ttotal: 2m 24s\tremaining: 31.9s\n",
      "819:\tlearn: 0.0007245\ttotal: 2m 24s\tremaining: 31.8s\n",
      "820:\tlearn: 0.0007244\ttotal: 2m 24s\tremaining: 31.6s\n",
      "821:\tlearn: 0.0007243\ttotal: 2m 25s\tremaining: 31.4s\n",
      "822:\tlearn: 0.0007242\ttotal: 2m 25s\tremaining: 31.2s\n",
      "823:\tlearn: 0.0007241\ttotal: 2m 25s\tremaining: 31.1s\n",
      "824:\tlearn: 0.0007241\ttotal: 2m 25s\tremaining: 30.9s\n",
      "825:\tlearn: 0.0007241\ttotal: 2m 25s\tremaining: 30.7s\n",
      "826:\tlearn: 0.0007238\ttotal: 2m 25s\tremaining: 30.5s\n",
      "827:\tlearn: 0.0007238\ttotal: 2m 26s\tremaining: 30.3s\n",
      "828:\tlearn: 0.0007238\ttotal: 2m 26s\tremaining: 30.2s\n",
      "829:\tlearn: 0.0007236\ttotal: 2m 26s\tremaining: 30s\n",
      "830:\tlearn: 0.0007234\ttotal: 2m 26s\tremaining: 29.8s\n",
      "831:\tlearn: 0.0007234\ttotal: 2m 26s\tremaining: 29.6s\n",
      "832:\tlearn: 0.0007229\ttotal: 2m 26s\tremaining: 29.5s\n",
      "833:\tlearn: 0.0007229\ttotal: 2m 27s\tremaining: 29.3s\n",
      "834:\tlearn: 0.0007229\ttotal: 2m 27s\tremaining: 29.1s\n",
      "835:\tlearn: 0.0007229\ttotal: 2m 27s\tremaining: 28.9s\n",
      "836:\tlearn: 0.0007228\ttotal: 2m 27s\tremaining: 28.8s\n",
      "837:\tlearn: 0.0007228\ttotal: 2m 27s\tremaining: 28.6s\n",
      "838:\tlearn: 0.0007227\ttotal: 2m 28s\tremaining: 28.4s\n",
      "839:\tlearn: 0.0007227\ttotal: 2m 28s\tremaining: 28.2s\n",
      "840:\tlearn: 0.0007227\ttotal: 2m 28s\tremaining: 28s\n",
      "841:\tlearn: 0.0007224\ttotal: 2m 28s\tremaining: 27.9s\n",
      "842:\tlearn: 0.0007224\ttotal: 2m 28s\tremaining: 27.7s\n",
      "843:\tlearn: 0.0007223\ttotal: 2m 28s\tremaining: 27.5s\n",
      "844:\tlearn: 0.0007223\ttotal: 2m 29s\tremaining: 27.3s\n",
      "845:\tlearn: 0.0007223\ttotal: 2m 29s\tremaining: 27.2s\n",
      "846:\tlearn: 0.0007223\ttotal: 2m 29s\tremaining: 27s\n",
      "847:\tlearn: 0.0007223\ttotal: 2m 29s\tremaining: 26.8s\n",
      "848:\tlearn: 0.0007222\ttotal: 2m 29s\tremaining: 26.6s\n",
      "849:\tlearn: 0.0007222\ttotal: 2m 29s\tremaining: 26.5s\n",
      "850:\tlearn: 0.0007222\ttotal: 2m 30s\tremaining: 26.3s\n",
      "851:\tlearn: 0.0007221\ttotal: 2m 30s\tremaining: 26.1s\n",
      "852:\tlearn: 0.0007221\ttotal: 2m 30s\tremaining: 25.9s\n",
      "853:\tlearn: 0.0007221\ttotal: 2m 30s\tremaining: 25.8s\n",
      "854:\tlearn: 0.0007221\ttotal: 2m 30s\tremaining: 25.6s\n",
      "855:\tlearn: 0.0007221\ttotal: 2m 30s\tremaining: 25.4s\n",
      "856:\tlearn: 0.0007220\ttotal: 2m 31s\tremaining: 25.2s\n",
      "857:\tlearn: 0.0007218\ttotal: 2m 31s\tremaining: 25s\n",
      "858:\tlearn: 0.0007215\ttotal: 2m 31s\tremaining: 24.9s\n",
      "859:\tlearn: 0.0007213\ttotal: 2m 31s\tremaining: 24.7s\n",
      "860:\tlearn: 0.0007213\ttotal: 2m 31s\tremaining: 24.5s\n",
      "861:\tlearn: 0.0007212\ttotal: 2m 32s\tremaining: 24.3s\n",
      "862:\tlearn: 0.0007212\ttotal: 2m 32s\tremaining: 24.2s\n",
      "863:\tlearn: 0.0007212\ttotal: 2m 32s\tremaining: 24s\n",
      "864:\tlearn: 0.0007212\ttotal: 2m 32s\tremaining: 23.8s\n",
      "865:\tlearn: 0.0007212\ttotal: 2m 32s\tremaining: 23.6s\n",
      "866:\tlearn: 0.0007210\ttotal: 2m 32s\tremaining: 23.5s\n",
      "867:\tlearn: 0.0007210\ttotal: 2m 33s\tremaining: 23.3s\n",
      "868:\tlearn: 0.0007202\ttotal: 2m 33s\tremaining: 23.1s\n",
      "869:\tlearn: 0.0007197\ttotal: 2m 33s\tremaining: 22.9s\n",
      "870:\tlearn: 0.0007197\ttotal: 2m 33s\tremaining: 22.8s\n",
      "871:\tlearn: 0.0007197\ttotal: 2m 33s\tremaining: 22.6s\n",
      "872:\tlearn: 0.0007197\ttotal: 2m 33s\tremaining: 22.4s\n",
      "873:\tlearn: 0.0007197\ttotal: 2m 34s\tremaining: 22.2s\n",
      "874:\tlearn: 0.0007196\ttotal: 2m 34s\tremaining: 22s\n",
      "875:\tlearn: 0.0007194\ttotal: 2m 34s\tremaining: 21.9s\n",
      "876:\tlearn: 0.0007192\ttotal: 2m 34s\tremaining: 21.7s\n",
      "877:\tlearn: 0.0007191\ttotal: 2m 34s\tremaining: 21.5s\n",
      "878:\tlearn: 0.0007189\ttotal: 2m 35s\tremaining: 21.3s\n",
      "879:\tlearn: 0.0007187\ttotal: 2m 35s\tremaining: 21.2s\n",
      "880:\tlearn: 0.0007183\ttotal: 2m 35s\tremaining: 21s\n",
      "881:\tlearn: 0.0007181\ttotal: 2m 35s\tremaining: 20.8s\n",
      "882:\tlearn: 0.0007180\ttotal: 2m 35s\tremaining: 20.6s\n",
      "883:\tlearn: 0.0007180\ttotal: 2m 35s\tremaining: 20.5s\n",
      "884:\tlearn: 0.0007179\ttotal: 2m 36s\tremaining: 20.3s\n",
      "885:\tlearn: 0.0007178\ttotal: 2m 36s\tremaining: 20.1s\n",
      "886:\tlearn: 0.0007175\ttotal: 2m 36s\tremaining: 19.9s\n",
      "887:\tlearn: 0.0007175\ttotal: 2m 36s\tremaining: 19.8s\n",
      "888:\tlearn: 0.0007175\ttotal: 2m 36s\tremaining: 19.6s\n",
      "889:\tlearn: 0.0007175\ttotal: 2m 36s\tremaining: 19.4s\n",
      "890:\tlearn: 0.0007175\ttotal: 2m 37s\tremaining: 19.2s\n",
      "891:\tlearn: 0.0007174\ttotal: 2m 37s\tremaining: 19s\n",
      "892:\tlearn: 0.0007174\ttotal: 2m 37s\tremaining: 18.9s\n",
      "893:\tlearn: 0.0007174\ttotal: 2m 37s\tremaining: 18.7s\n",
      "894:\tlearn: 0.0007174\ttotal: 2m 37s\tremaining: 18.5s\n",
      "895:\tlearn: 0.0007174\ttotal: 2m 38s\tremaining: 18.3s\n",
      "896:\tlearn: 0.0007173\ttotal: 2m 38s\tremaining: 18.2s\n",
      "897:\tlearn: 0.0007172\ttotal: 2m 38s\tremaining: 18s\n",
      "898:\tlearn: 0.0007172\ttotal: 2m 38s\tremaining: 17.8s\n",
      "899:\tlearn: 0.0007170\ttotal: 2m 38s\tremaining: 17.6s\n",
      "900:\tlearn: 0.0007167\ttotal: 2m 38s\tremaining: 17.5s\n",
      "901:\tlearn: 0.0007117\ttotal: 2m 39s\tremaining: 17.3s\n",
      "902:\tlearn: 0.0007117\ttotal: 2m 39s\tremaining: 17.1s\n",
      "903:\tlearn: 0.0007117\ttotal: 2m 39s\tremaining: 16.9s\n",
      "904:\tlearn: 0.0007117\ttotal: 2m 39s\tremaining: 16.8s\n",
      "905:\tlearn: 0.0007117\ttotal: 2m 39s\tremaining: 16.6s\n",
      "906:\tlearn: 0.0007117\ttotal: 2m 39s\tremaining: 16.4s\n",
      "907:\tlearn: 0.0007108\ttotal: 2m 40s\tremaining: 16.2s\n",
      "908:\tlearn: 0.0007108\ttotal: 2m 40s\tremaining: 16s\n",
      "909:\tlearn: 0.0007108\ttotal: 2m 40s\tremaining: 15.9s\n",
      "910:\tlearn: 0.0007108\ttotal: 2m 40s\tremaining: 15.7s\n",
      "911:\tlearn: 0.0007106\ttotal: 2m 40s\tremaining: 15.5s\n",
      "912:\tlearn: 0.0007104\ttotal: 2m 41s\tremaining: 15.3s\n",
      "913:\tlearn: 0.0007060\ttotal: 2m 41s\tremaining: 15.2s\n",
      "914:\tlearn: 0.0007059\ttotal: 2m 41s\tremaining: 15s\n",
      "915:\tlearn: 0.0007059\ttotal: 2m 41s\tremaining: 14.8s\n",
      "916:\tlearn: 0.0007059\ttotal: 2m 41s\tremaining: 14.6s\n",
      "917:\tlearn: 0.0007058\ttotal: 2m 41s\tremaining: 14.5s\n",
      "918:\tlearn: 0.0007058\ttotal: 2m 42s\tremaining: 14.3s\n",
      "919:\tlearn: 0.0007056\ttotal: 2m 42s\tremaining: 14.1s\n",
      "920:\tlearn: 0.0007055\ttotal: 2m 42s\tremaining: 13.9s\n",
      "921:\tlearn: 0.0007055\ttotal: 2m 42s\tremaining: 13.8s\n",
      "922:\tlearn: 0.0007052\ttotal: 2m 42s\tremaining: 13.6s\n",
      "923:\tlearn: 0.0007052\ttotal: 2m 42s\tremaining: 13.4s\n",
      "924:\tlearn: 0.0007051\ttotal: 2m 43s\tremaining: 13.2s\n",
      "925:\tlearn: 0.0007049\ttotal: 2m 43s\tremaining: 13s\n",
      "926:\tlearn: 0.0007048\ttotal: 2m 43s\tremaining: 12.9s\n",
      "927:\tlearn: 0.0007044\ttotal: 2m 43s\tremaining: 12.7s\n",
      "928:\tlearn: 0.0007043\ttotal: 2m 43s\tremaining: 12.5s\n",
      "929:\tlearn: 0.0007042\ttotal: 2m 43s\tremaining: 12.3s\n",
      "930:\tlearn: 0.0007042\ttotal: 2m 44s\tremaining: 12.2s\n",
      "931:\tlearn: 0.0007042\ttotal: 2m 44s\tremaining: 12s\n",
      "932:\tlearn: 0.0007042\ttotal: 2m 44s\tremaining: 11.8s\n",
      "933:\tlearn: 0.0007042\ttotal: 2m 44s\tremaining: 11.6s\n",
      "934:\tlearn: 0.0007041\ttotal: 2m 44s\tremaining: 11.5s\n",
      "935:\tlearn: 0.0007040\ttotal: 2m 45s\tremaining: 11.3s\n",
      "936:\tlearn: 0.0007038\ttotal: 2m 45s\tremaining: 11.1s\n",
      "937:\tlearn: 0.0007037\ttotal: 2m 45s\tremaining: 10.9s\n",
      "938:\tlearn: 0.0007037\ttotal: 2m 45s\tremaining: 10.8s\n",
      "939:\tlearn: 0.0007037\ttotal: 2m 45s\tremaining: 10.6s\n",
      "940:\tlearn: 0.0007036\ttotal: 2m 45s\tremaining: 10.4s\n",
      "941:\tlearn: 0.0007036\ttotal: 2m 46s\tremaining: 10.2s\n",
      "942:\tlearn: 0.0007034\ttotal: 2m 46s\tremaining: 10s\n",
      "943:\tlearn: 0.0007034\ttotal: 2m 46s\tremaining: 9.87s\n",
      "944:\tlearn: 0.0007018\ttotal: 2m 46s\tremaining: 9.7s\n",
      "945:\tlearn: 0.0007017\ttotal: 2m 46s\tremaining: 9.52s\n",
      "946:\tlearn: 0.0007017\ttotal: 2m 46s\tremaining: 9.34s\n",
      "947:\tlearn: 0.0007017\ttotal: 2m 47s\tremaining: 9.17s\n",
      "948:\tlearn: 0.0007017\ttotal: 2m 47s\tremaining: 8.99s\n",
      "949:\tlearn: 0.0007017\ttotal: 2m 47s\tremaining: 8.81s\n",
      "950:\tlearn: 0.0007016\ttotal: 2m 47s\tremaining: 8.64s\n",
      "951:\tlearn: 0.0007015\ttotal: 2m 47s\tremaining: 8.46s\n",
      "952:\tlearn: 0.0007015\ttotal: 2m 47s\tremaining: 8.29s\n",
      "953:\tlearn: 0.0007014\ttotal: 2m 48s\tremaining: 8.11s\n",
      "954:\tlearn: 0.0007014\ttotal: 2m 48s\tremaining: 7.93s\n",
      "955:\tlearn: 0.0007011\ttotal: 2m 48s\tremaining: 7.76s\n",
      "956:\tlearn: 0.0007005\ttotal: 2m 48s\tremaining: 7.58s\n",
      "957:\tlearn: 0.0007005\ttotal: 2m 48s\tremaining: 7.4s\n",
      "958:\tlearn: 0.0007002\ttotal: 2m 49s\tremaining: 7.23s\n",
      "959:\tlearn: 0.0007002\ttotal: 2m 49s\tremaining: 7.05s\n",
      "960:\tlearn: 0.0007001\ttotal: 2m 49s\tremaining: 6.87s\n",
      "961:\tlearn: 0.0007000\ttotal: 2m 49s\tremaining: 6.7s\n",
      "962:\tlearn: 0.0007000\ttotal: 2m 49s\tremaining: 6.52s\n",
      "963:\tlearn: 0.0006999\ttotal: 2m 49s\tremaining: 6.34s\n",
      "964:\tlearn: 0.0006999\ttotal: 2m 50s\tremaining: 6.17s\n",
      "965:\tlearn: 0.0006999\ttotal: 2m 50s\tremaining: 5.99s\n",
      "966:\tlearn: 0.0006999\ttotal: 2m 50s\tremaining: 5.82s\n",
      "967:\tlearn: 0.0006993\ttotal: 2m 50s\tremaining: 5.64s\n",
      "968:\tlearn: 0.0006993\ttotal: 2m 50s\tremaining: 5.46s\n",
      "969:\tlearn: 0.0006993\ttotal: 2m 50s\tremaining: 5.29s\n",
      "970:\tlearn: 0.0006993\ttotal: 2m 51s\tremaining: 5.11s\n",
      "971:\tlearn: 0.0006992\ttotal: 2m 51s\tremaining: 4.93s\n",
      "972:\tlearn: 0.0006992\ttotal: 2m 51s\tremaining: 4.76s\n",
      "973:\tlearn: 0.0006991\ttotal: 2m 51s\tremaining: 4.58s\n",
      "974:\tlearn: 0.0006990\ttotal: 2m 51s\tremaining: 4.41s\n",
      "975:\tlearn: 0.0006989\ttotal: 2m 52s\tremaining: 4.23s\n",
      "976:\tlearn: 0.0006989\ttotal: 2m 52s\tremaining: 4.05s\n",
      "977:\tlearn: 0.0006989\ttotal: 2m 52s\tremaining: 3.88s\n",
      "978:\tlearn: 0.0006979\ttotal: 2m 52s\tremaining: 3.7s\n",
      "979:\tlearn: 0.0006979\ttotal: 2m 52s\tremaining: 3.52s\n",
      "980:\tlearn: 0.0006979\ttotal: 2m 52s\tremaining: 3.35s\n",
      "981:\tlearn: 0.0006978\ttotal: 2m 53s\tremaining: 3.17s\n",
      "982:\tlearn: 0.0006975\ttotal: 2m 53s\tremaining: 3s\n",
      "983:\tlearn: 0.0006975\ttotal: 2m 53s\tremaining: 2.82s\n",
      "984:\tlearn: 0.0006974\ttotal: 2m 53s\tremaining: 2.64s\n",
      "985:\tlearn: 0.0006973\ttotal: 2m 53s\tremaining: 2.47s\n",
      "986:\tlearn: 0.0006971\ttotal: 2m 53s\tremaining: 2.29s\n",
      "987:\tlearn: 0.0006971\ttotal: 2m 54s\tremaining: 2.11s\n",
      "988:\tlearn: 0.0006970\ttotal: 2m 54s\tremaining: 1.94s\n",
      "989:\tlearn: 0.0006969\ttotal: 2m 54s\tremaining: 1.76s\n",
      "990:\tlearn: 0.0006968\ttotal: 2m 54s\tremaining: 1.59s\n",
      "991:\tlearn: 0.0006949\ttotal: 2m 54s\tremaining: 1.41s\n",
      "992:\tlearn: 0.0006948\ttotal: 2m 54s\tremaining: 1.23s\n",
      "993:\tlearn: 0.0006948\ttotal: 2m 55s\tremaining: 1.06s\n",
      "994:\tlearn: 0.0006947\ttotal: 2m 55s\tremaining: 881ms\n",
      "995:\tlearn: 0.0006946\ttotal: 2m 55s\tremaining: 705ms\n",
      "996:\tlearn: 0.0006945\ttotal: 2m 55s\tremaining: 529ms\n",
      "997:\tlearn: 0.0006945\ttotal: 2m 55s\tremaining: 352ms\n",
      "998:\tlearn: 0.0006945\ttotal: 2m 56s\tremaining: 176ms\n",
      "999:\tlearn: 0.0006945\ttotal: 2m 56s\tremaining: 0us\n",
      "0:\tlearn: 0.5454527\ttotal: 203ms\tremaining: 3m 22s\n",
      "1:\tlearn: 0.4116672\ttotal: 374ms\tremaining: 3m 6s\n",
      "2:\tlearn: 0.3346231\ttotal: 545ms\tremaining: 3m 1s\n",
      "3:\tlearn: 0.2730053\ttotal: 720ms\tremaining: 2m 59s\n",
      "4:\tlearn: 0.2096879\ttotal: 901ms\tremaining: 2m 59s\n",
      "5:\tlearn: 0.1591358\ttotal: 1.07s\tremaining: 2m 58s\n",
      "6:\tlearn: 0.1216646\ttotal: 1.25s\tremaining: 2m 56s\n",
      "7:\tlearn: 0.1051906\ttotal: 1.42s\tremaining: 2m 56s\n",
      "8:\tlearn: 0.0906731\ttotal: 1.59s\tremaining: 2m 55s\n",
      "9:\tlearn: 0.0853736\ttotal: 1.77s\tremaining: 2m 54s\n",
      "10:\tlearn: 0.0622027\ttotal: 1.94s\tremaining: 2m 54s\n",
      "11:\tlearn: 0.0597139\ttotal: 2.11s\tremaining: 2m 53s\n",
      "12:\tlearn: 0.0584399\ttotal: 2.28s\tremaining: 2m 53s\n",
      "13:\tlearn: 0.0508606\ttotal: 2.45s\tremaining: 2m 52s\n",
      "14:\tlearn: 0.0454823\ttotal: 2.63s\tremaining: 2m 52s\n",
      "15:\tlearn: 0.0360925\ttotal: 2.8s\tremaining: 2m 52s\n",
      "16:\tlearn: 0.0314049\ttotal: 2.98s\tremaining: 2m 52s\n",
      "17:\tlearn: 0.0260630\ttotal: 3.15s\tremaining: 2m 51s\n",
      "18:\tlearn: 0.0249213\ttotal: 3.33s\tremaining: 2m 51s\n",
      "19:\tlearn: 0.0209578\ttotal: 3.5s\tremaining: 2m 51s\n",
      "20:\tlearn: 0.0189492\ttotal: 3.67s\tremaining: 2m 51s\n",
      "21:\tlearn: 0.0173396\ttotal: 3.84s\tremaining: 2m 50s\n",
      "22:\tlearn: 0.0160936\ttotal: 4.02s\tremaining: 2m 50s\n",
      "23:\tlearn: 0.0150063\ttotal: 4.2s\tremaining: 2m 50s\n",
      "24:\tlearn: 0.0135650\ttotal: 4.37s\tremaining: 2m 50s\n",
      "25:\tlearn: 0.0126426\ttotal: 4.54s\tremaining: 2m 50s\n",
      "26:\tlearn: 0.0114797\ttotal: 4.72s\tremaining: 2m 49s\n",
      "27:\tlearn: 0.0104912\ttotal: 4.89s\tremaining: 2m 49s\n",
      "28:\tlearn: 0.0099849\ttotal: 5.07s\tremaining: 2m 49s\n",
      "29:\tlearn: 0.0095452\ttotal: 5.25s\tremaining: 2m 49s\n",
      "30:\tlearn: 0.0089062\ttotal: 5.42s\tremaining: 2m 49s\n",
      "31:\tlearn: 0.0084004\ttotal: 5.6s\tremaining: 2m 49s\n",
      "32:\tlearn: 0.0078152\ttotal: 5.77s\tremaining: 2m 49s\n",
      "33:\tlearn: 0.0073210\ttotal: 5.95s\tremaining: 2m 49s\n",
      "34:\tlearn: 0.0069225\ttotal: 6.12s\tremaining: 2m 48s\n",
      "35:\tlearn: 0.0064816\ttotal: 6.3s\tremaining: 2m 48s\n",
      "36:\tlearn: 0.0061771\ttotal: 6.47s\tremaining: 2m 48s\n",
      "37:\tlearn: 0.0057733\ttotal: 6.66s\tremaining: 2m 48s\n",
      "38:\tlearn: 0.0054322\ttotal: 6.84s\tremaining: 2m 48s\n",
      "39:\tlearn: 0.0052881\ttotal: 7.02s\tremaining: 2m 48s\n",
      "40:\tlearn: 0.0049780\ttotal: 7.2s\tremaining: 2m 48s\n",
      "41:\tlearn: 0.0047384\ttotal: 7.37s\tremaining: 2m 48s\n",
      "42:\tlearn: 0.0045680\ttotal: 7.54s\tremaining: 2m 47s\n",
      "43:\tlearn: 0.0044200\ttotal: 7.73s\tremaining: 2m 48s\n",
      "44:\tlearn: 0.0043180\ttotal: 7.91s\tremaining: 2m 47s\n",
      "45:\tlearn: 0.0041660\ttotal: 8.08s\tremaining: 2m 47s\n",
      "46:\tlearn: 0.0040457\ttotal: 8.26s\tremaining: 2m 47s\n",
      "47:\tlearn: 0.0040114\ttotal: 8.43s\tremaining: 2m 47s\n",
      "48:\tlearn: 0.0038465\ttotal: 8.6s\tremaining: 2m 46s\n",
      "49:\tlearn: 0.0036358\ttotal: 8.78s\tremaining: 2m 46s\n",
      "50:\tlearn: 0.0035303\ttotal: 8.95s\tremaining: 2m 46s\n",
      "51:\tlearn: 0.0034299\ttotal: 9.13s\tremaining: 2m 46s\n",
      "52:\tlearn: 0.0034092\ttotal: 9.3s\tremaining: 2m 46s\n",
      "53:\tlearn: 0.0033435\ttotal: 9.47s\tremaining: 2m 45s\n",
      "54:\tlearn: 0.0032593\ttotal: 9.65s\tremaining: 2m 45s\n",
      "55:\tlearn: 0.0032004\ttotal: 9.82s\tremaining: 2m 45s\n",
      "56:\tlearn: 0.0031287\ttotal: 10s\tremaining: 2m 45s\n",
      "57:\tlearn: 0.0030959\ttotal: 10.2s\tremaining: 2m 45s\n",
      "58:\tlearn: 0.0029810\ttotal: 10.3s\tremaining: 2m 45s\n",
      "59:\tlearn: 0.0028970\ttotal: 10.5s\tremaining: 2m 44s\n",
      "60:\tlearn: 0.0028383\ttotal: 10.7s\tremaining: 2m 44s\n",
      "61:\tlearn: 0.0027856\ttotal: 10.9s\tremaining: 2m 44s\n",
      "62:\tlearn: 0.0027399\ttotal: 11.1s\tremaining: 2m 44s\n",
      "63:\tlearn: 0.0026274\ttotal: 11.2s\tremaining: 2m 44s\n",
      "64:\tlearn: 0.0026270\ttotal: 11.4s\tremaining: 2m 44s\n",
      "65:\tlearn: 0.0025620\ttotal: 11.6s\tremaining: 2m 43s\n",
      "66:\tlearn: 0.0025115\ttotal: 11.8s\tremaining: 2m 43s\n",
      "67:\tlearn: 0.0024744\ttotal: 11.9s\tremaining: 2m 43s\n",
      "68:\tlearn: 0.0024237\ttotal: 12.1s\tremaining: 2m 43s\n",
      "69:\tlearn: 0.0023793\ttotal: 12.3s\tremaining: 2m 43s\n",
      "70:\tlearn: 0.0023246\ttotal: 12.4s\tremaining: 2m 42s\n",
      "71:\tlearn: 0.0022594\ttotal: 12.6s\tremaining: 2m 42s\n",
      "72:\tlearn: 0.0022345\ttotal: 12.8s\tremaining: 2m 42s\n",
      "73:\tlearn: 0.0021931\ttotal: 13s\tremaining: 2m 42s\n",
      "74:\tlearn: 0.0021652\ttotal: 13.1s\tremaining: 2m 42s\n",
      "75:\tlearn: 0.0020987\ttotal: 13.3s\tremaining: 2m 41s\n",
      "76:\tlearn: 0.0020273\ttotal: 13.5s\tremaining: 2m 41s\n",
      "77:\tlearn: 0.0019872\ttotal: 13.7s\tremaining: 2m 41s\n",
      "78:\tlearn: 0.0019643\ttotal: 13.8s\tremaining: 2m 41s\n",
      "79:\tlearn: 0.0019416\ttotal: 14s\tremaining: 2m 41s\n",
      "80:\tlearn: 0.0019262\ttotal: 14.2s\tremaining: 2m 41s\n",
      "81:\tlearn: 0.0018959\ttotal: 14.4s\tremaining: 2m 40s\n",
      "82:\tlearn: 0.0018770\ttotal: 14.5s\tremaining: 2m 40s\n",
      "83:\tlearn: 0.0018770\ttotal: 14.7s\tremaining: 2m 40s\n",
      "84:\tlearn: 0.0018768\ttotal: 14.9s\tremaining: 2m 40s\n",
      "85:\tlearn: 0.0018529\ttotal: 15.1s\tremaining: 2m 40s\n",
      "86:\tlearn: 0.0018283\ttotal: 15.2s\tremaining: 2m 40s\n",
      "87:\tlearn: 0.0018013\ttotal: 15.4s\tremaining: 2m 39s\n",
      "88:\tlearn: 0.0017773\ttotal: 15.6s\tremaining: 2m 39s\n",
      "89:\tlearn: 0.0017469\ttotal: 15.8s\tremaining: 2m 39s\n",
      "90:\tlearn: 0.0017324\ttotal: 15.9s\tremaining: 2m 39s\n",
      "91:\tlearn: 0.0017095\ttotal: 16.1s\tremaining: 2m 39s\n",
      "92:\tlearn: 0.0016956\ttotal: 16.3s\tremaining: 2m 38s\n",
      "93:\tlearn: 0.0016721\ttotal: 16.5s\tremaining: 2m 38s\n",
      "94:\tlearn: 0.0016384\ttotal: 16.6s\tremaining: 2m 38s\n",
      "95:\tlearn: 0.0016382\ttotal: 16.8s\tremaining: 2m 38s\n",
      "96:\tlearn: 0.0016201\ttotal: 17s\tremaining: 2m 38s\n",
      "97:\tlearn: 0.0015964\ttotal: 17.2s\tremaining: 2m 37s\n",
      "98:\tlearn: 0.0015776\ttotal: 17.3s\tremaining: 2m 37s\n",
      "99:\tlearn: 0.0015594\ttotal: 17.5s\tremaining: 2m 37s\n",
      "100:\tlearn: 0.0015310\ttotal: 17.7s\tremaining: 2m 37s\n",
      "101:\tlearn: 0.0015195\ttotal: 17.9s\tremaining: 2m 37s\n",
      "102:\tlearn: 0.0015190\ttotal: 18s\tremaining: 2m 37s\n",
      "103:\tlearn: 0.0015189\ttotal: 18.2s\tremaining: 2m 36s\n",
      "104:\tlearn: 0.0015183\ttotal: 18.4s\tremaining: 2m 36s\n",
      "105:\tlearn: 0.0015183\ttotal: 18.6s\tremaining: 2m 36s\n",
      "106:\tlearn: 0.0015182\ttotal: 18.7s\tremaining: 2m 36s\n",
      "107:\tlearn: 0.0014961\ttotal: 18.9s\tremaining: 2m 36s\n",
      "108:\tlearn: 0.0014775\ttotal: 19.1s\tremaining: 2m 35s\n",
      "109:\tlearn: 0.0014538\ttotal: 19.3s\tremaining: 2m 35s\n",
      "110:\tlearn: 0.0014532\ttotal: 19.4s\tremaining: 2m 35s\n",
      "111:\tlearn: 0.0014339\ttotal: 19.6s\tremaining: 2m 35s\n",
      "112:\tlearn: 0.0014156\ttotal: 19.8s\tremaining: 2m 35s\n",
      "113:\tlearn: 0.0013954\ttotal: 19.9s\tremaining: 2m 34s\n",
      "114:\tlearn: 0.0013811\ttotal: 20.1s\tremaining: 2m 34s\n",
      "115:\tlearn: 0.0013605\ttotal: 20.3s\tremaining: 2m 34s\n",
      "116:\tlearn: 0.0013476\ttotal: 20.5s\tremaining: 2m 34s\n",
      "117:\tlearn: 0.0013247\ttotal: 20.6s\tremaining: 2m 34s\n",
      "118:\tlearn: 0.0013118\ttotal: 20.8s\tremaining: 2m 34s\n",
      "119:\tlearn: 0.0012984\ttotal: 21s\tremaining: 2m 33s\n",
      "120:\tlearn: 0.0012916\ttotal: 21.2s\tremaining: 2m 33s\n",
      "121:\tlearn: 0.0012756\ttotal: 21.3s\tremaining: 2m 33s\n",
      "122:\tlearn: 0.0012603\ttotal: 21.5s\tremaining: 2m 33s\n",
      "123:\tlearn: 0.0012518\ttotal: 21.7s\tremaining: 2m 33s\n",
      "124:\tlearn: 0.0012518\ttotal: 21.9s\tremaining: 2m 33s\n",
      "125:\tlearn: 0.0012518\ttotal: 22s\tremaining: 2m 32s\n",
      "126:\tlearn: 0.0012516\ttotal: 22.2s\tremaining: 2m 32s\n",
      "127:\tlearn: 0.0012512\ttotal: 22.4s\tremaining: 2m 32s\n",
      "128:\tlearn: 0.0012407\ttotal: 22.6s\tremaining: 2m 32s\n",
      "129:\tlearn: 0.0012406\ttotal: 22.7s\tremaining: 2m 32s\n",
      "130:\tlearn: 0.0012405\ttotal: 22.9s\tremaining: 2m 31s\n",
      "131:\tlearn: 0.0012291\ttotal: 23.1s\tremaining: 2m 31s\n",
      "132:\tlearn: 0.0012288\ttotal: 23.3s\tremaining: 2m 31s\n",
      "133:\tlearn: 0.0012288\ttotal: 23.4s\tremaining: 2m 31s\n",
      "134:\tlearn: 0.0012136\ttotal: 23.6s\tremaining: 2m 31s\n",
      "135:\tlearn: 0.0012135\ttotal: 23.8s\tremaining: 2m 31s\n",
      "136:\tlearn: 0.0012021\ttotal: 24s\tremaining: 2m 30s\n",
      "137:\tlearn: 0.0012016\ttotal: 24.1s\tremaining: 2m 30s\n",
      "138:\tlearn: 0.0012010\ttotal: 24.3s\tremaining: 2m 30s\n",
      "139:\tlearn: 0.0011842\ttotal: 24.5s\tremaining: 2m 30s\n",
      "140:\tlearn: 0.0011733\ttotal: 24.6s\tremaining: 2m 30s\n",
      "141:\tlearn: 0.0011646\ttotal: 24.8s\tremaining: 2m 29s\n",
      "142:\tlearn: 0.0011510\ttotal: 25s\tremaining: 2m 29s\n",
      "143:\tlearn: 0.0011509\ttotal: 25.2s\tremaining: 2m 29s\n",
      "144:\tlearn: 0.0011407\ttotal: 25.3s\tremaining: 2m 29s\n",
      "145:\tlearn: 0.0011347\ttotal: 25.5s\tremaining: 2m 29s\n",
      "146:\tlearn: 0.0011345\ttotal: 25.7s\tremaining: 2m 29s\n",
      "147:\tlearn: 0.0011228\ttotal: 25.9s\tremaining: 2m 28s\n",
      "148:\tlearn: 0.0011166\ttotal: 26s\tremaining: 2m 28s\n",
      "149:\tlearn: 0.0011061\ttotal: 26.2s\tremaining: 2m 28s\n",
      "150:\tlearn: 0.0011002\ttotal: 26.4s\tremaining: 2m 28s\n",
      "151:\tlearn: 0.0010890\ttotal: 26.6s\tremaining: 2m 28s\n",
      "152:\tlearn: 0.0010889\ttotal: 26.7s\tremaining: 2m 28s\n",
      "153:\tlearn: 0.0010798\ttotal: 26.9s\tremaining: 2m 27s\n",
      "154:\tlearn: 0.0010725\ttotal: 27.1s\tremaining: 2m 27s\n",
      "155:\tlearn: 0.0010638\ttotal: 27.3s\tremaining: 2m 27s\n",
      "156:\tlearn: 0.0010634\ttotal: 27.4s\tremaining: 2m 27s\n",
      "157:\tlearn: 0.0010630\ttotal: 27.6s\tremaining: 2m 27s\n",
      "158:\tlearn: 0.0010630\ttotal: 27.8s\tremaining: 2m 26s\n",
      "159:\tlearn: 0.0010630\ttotal: 28s\tremaining: 2m 26s\n",
      "160:\tlearn: 0.0010628\ttotal: 28.1s\tremaining: 2m 26s\n",
      "161:\tlearn: 0.0010579\ttotal: 28.3s\tremaining: 2m 26s\n",
      "162:\tlearn: 0.0010497\ttotal: 28.5s\tremaining: 2m 26s\n",
      "163:\tlearn: 0.0010496\ttotal: 28.7s\tremaining: 2m 26s\n",
      "164:\tlearn: 0.0010496\ttotal: 28.8s\tremaining: 2m 25s\n",
      "165:\tlearn: 0.0010496\ttotal: 29s\tremaining: 2m 25s\n",
      "166:\tlearn: 0.0010494\ttotal: 29.2s\tremaining: 2m 25s\n",
      "167:\tlearn: 0.0010381\ttotal: 29.4s\tremaining: 2m 25s\n",
      "168:\tlearn: 0.0010381\ttotal: 29.5s\tremaining: 2m 25s\n",
      "169:\tlearn: 0.0010285\ttotal: 29.7s\tremaining: 2m 25s\n",
      "170:\tlearn: 0.0010230\ttotal: 29.9s\tremaining: 2m 24s\n",
      "171:\tlearn: 0.0010229\ttotal: 30.1s\tremaining: 2m 24s\n",
      "172:\tlearn: 0.0010228\ttotal: 30.2s\tremaining: 2m 24s\n",
      "173:\tlearn: 0.0010228\ttotal: 30.4s\tremaining: 2m 24s\n",
      "174:\tlearn: 0.0010228\ttotal: 30.6s\tremaining: 2m 24s\n",
      "175:\tlearn: 0.0010228\ttotal: 30.8s\tremaining: 2m 24s\n",
      "176:\tlearn: 0.0010226\ttotal: 30.9s\tremaining: 2m 23s\n",
      "177:\tlearn: 0.0010176\ttotal: 31.1s\tremaining: 2m 23s\n",
      "178:\tlearn: 0.0010176\ttotal: 31.3s\tremaining: 2m 23s\n",
      "179:\tlearn: 0.0010105\ttotal: 31.5s\tremaining: 2m 23s\n",
      "180:\tlearn: 0.0009990\ttotal: 31.6s\tremaining: 2m 23s\n",
      "181:\tlearn: 0.0009990\ttotal: 31.8s\tremaining: 2m 22s\n",
      "182:\tlearn: 0.0009989\ttotal: 32s\tremaining: 2m 22s\n",
      "183:\tlearn: 0.0009988\ttotal: 32.2s\tremaining: 2m 22s\n",
      "184:\tlearn: 0.0009987\ttotal: 32.3s\tremaining: 2m 22s\n",
      "185:\tlearn: 0.0009986\ttotal: 32.5s\tremaining: 2m 22s\n",
      "186:\tlearn: 0.0009979\ttotal: 32.7s\tremaining: 2m 22s\n",
      "187:\tlearn: 0.0009976\ttotal: 32.8s\tremaining: 2m 21s\n",
      "188:\tlearn: 0.0009974\ttotal: 33s\tremaining: 2m 21s\n",
      "189:\tlearn: 0.0009974\ttotal: 33.2s\tremaining: 2m 21s\n",
      "190:\tlearn: 0.0009974\ttotal: 33.4s\tremaining: 2m 21s\n",
      "191:\tlearn: 0.0009973\ttotal: 33.5s\tremaining: 2m 21s\n",
      "192:\tlearn: 0.0009973\ttotal: 33.7s\tremaining: 2m 20s\n",
      "193:\tlearn: 0.0009970\ttotal: 33.9s\tremaining: 2m 20s\n",
      "194:\tlearn: 0.0009890\ttotal: 34.1s\tremaining: 2m 20s\n",
      "195:\tlearn: 0.0009882\ttotal: 34.2s\tremaining: 2m 20s\n",
      "196:\tlearn: 0.0009857\ttotal: 34.4s\tremaining: 2m 20s\n",
      "197:\tlearn: 0.0009800\ttotal: 34.6s\tremaining: 2m 20s\n",
      "198:\tlearn: 0.0009799\ttotal: 34.8s\tremaining: 2m 19s\n",
      "199:\tlearn: 0.0009799\ttotal: 34.9s\tremaining: 2m 19s\n",
      "200:\tlearn: 0.0009799\ttotal: 35.1s\tremaining: 2m 19s\n",
      "201:\tlearn: 0.0009799\ttotal: 35.3s\tremaining: 2m 19s\n",
      "202:\tlearn: 0.0009799\ttotal: 35.5s\tremaining: 2m 19s\n",
      "203:\tlearn: 0.0009741\ttotal: 35.6s\tremaining: 2m 19s\n",
      "204:\tlearn: 0.0009736\ttotal: 35.8s\tremaining: 2m 18s\n",
      "205:\tlearn: 0.0009736\ttotal: 36s\tremaining: 2m 18s\n",
      "206:\tlearn: 0.0009681\ttotal: 36.1s\tremaining: 2m 18s\n",
      "207:\tlearn: 0.0009681\ttotal: 36.3s\tremaining: 2m 18s\n",
      "208:\tlearn: 0.0009680\ttotal: 36.5s\tremaining: 2m 18s\n",
      "209:\tlearn: 0.0009668\ttotal: 36.7s\tremaining: 2m 17s\n",
      "210:\tlearn: 0.0009668\ttotal: 36.8s\tremaining: 2m 17s\n",
      "211:\tlearn: 0.0009667\ttotal: 37s\tremaining: 2m 17s\n",
      "212:\tlearn: 0.0009661\ttotal: 37.2s\tremaining: 2m 17s\n",
      "213:\tlearn: 0.0009660\ttotal: 37.4s\tremaining: 2m 17s\n",
      "214:\tlearn: 0.0009656\ttotal: 37.5s\tremaining: 2m 17s\n",
      "215:\tlearn: 0.0009656\ttotal: 37.7s\tremaining: 2m 16s\n",
      "216:\tlearn: 0.0009656\ttotal: 37.9s\tremaining: 2m 16s\n",
      "217:\tlearn: 0.0009651\ttotal: 38.1s\tremaining: 2m 16s\n",
      "218:\tlearn: 0.0009642\ttotal: 38.2s\tremaining: 2m 16s\n",
      "219:\tlearn: 0.0009641\ttotal: 38.4s\tremaining: 2m 16s\n",
      "220:\tlearn: 0.0009588\ttotal: 38.6s\tremaining: 2m 16s\n",
      "221:\tlearn: 0.0009566\ttotal: 38.8s\tremaining: 2m 15s\n",
      "222:\tlearn: 0.0009467\ttotal: 38.9s\tremaining: 2m 15s\n",
      "223:\tlearn: 0.0009450\ttotal: 39.1s\tremaining: 2m 15s\n",
      "224:\tlearn: 0.0009446\ttotal: 39.3s\tremaining: 2m 15s\n",
      "225:\tlearn: 0.0009446\ttotal: 39.5s\tremaining: 2m 15s\n",
      "226:\tlearn: 0.0009405\ttotal: 39.6s\tremaining: 2m 14s\n",
      "227:\tlearn: 0.0009404\ttotal: 39.8s\tremaining: 2m 14s\n",
      "228:\tlearn: 0.0009403\ttotal: 40s\tremaining: 2m 14s\n",
      "229:\tlearn: 0.0009348\ttotal: 40.2s\tremaining: 2m 14s\n",
      "230:\tlearn: 0.0009348\ttotal: 40.3s\tremaining: 2m 14s\n",
      "231:\tlearn: 0.0009343\ttotal: 40.5s\tremaining: 2m 14s\n",
      "232:\tlearn: 0.0009342\ttotal: 40.7s\tremaining: 2m 13s\n",
      "233:\tlearn: 0.0009338\ttotal: 40.9s\tremaining: 2m 13s\n",
      "234:\tlearn: 0.0009336\ttotal: 41s\tremaining: 2m 13s\n",
      "235:\tlearn: 0.0009334\ttotal: 41.2s\tremaining: 2m 13s\n",
      "236:\tlearn: 0.0009334\ttotal: 41.4s\tremaining: 2m 13s\n",
      "237:\tlearn: 0.0009333\ttotal: 41.6s\tremaining: 2m 13s\n",
      "238:\tlearn: 0.0009333\ttotal: 41.7s\tremaining: 2m 12s\n",
      "239:\tlearn: 0.0009329\ttotal: 41.9s\tremaining: 2m 12s\n",
      "240:\tlearn: 0.0009329\ttotal: 42.1s\tremaining: 2m 12s\n",
      "241:\tlearn: 0.0009329\ttotal: 42.2s\tremaining: 2m 12s\n",
      "242:\tlearn: 0.0009328\ttotal: 42.4s\tremaining: 2m 12s\n",
      "243:\tlearn: 0.0009328\ttotal: 42.6s\tremaining: 2m 11s\n",
      "244:\tlearn: 0.0009327\ttotal: 42.8s\tremaining: 2m 11s\n",
      "245:\tlearn: 0.0009326\ttotal: 42.9s\tremaining: 2m 11s\n",
      "246:\tlearn: 0.0009292\ttotal: 43.1s\tremaining: 2m 11s\n",
      "247:\tlearn: 0.0009292\ttotal: 43.3s\tremaining: 2m 11s\n",
      "248:\tlearn: 0.0009292\ttotal: 43.5s\tremaining: 2m 11s\n",
      "249:\tlearn: 0.0009292\ttotal: 43.6s\tremaining: 2m 10s\n",
      "250:\tlearn: 0.0009292\ttotal: 43.8s\tremaining: 2m 10s\n",
      "251:\tlearn: 0.0009292\ttotal: 44s\tremaining: 2m 10s\n",
      "252:\tlearn: 0.0009244\ttotal: 44.2s\tremaining: 2m 10s\n",
      "253:\tlearn: 0.0009244\ttotal: 44.3s\tremaining: 2m 10s\n",
      "254:\tlearn: 0.0009239\ttotal: 44.5s\tremaining: 2m 10s\n",
      "255:\tlearn: 0.0009232\ttotal: 44.7s\tremaining: 2m 9s\n",
      "256:\tlearn: 0.0009232\ttotal: 44.9s\tremaining: 2m 9s\n",
      "257:\tlearn: 0.0009232\ttotal: 45s\tremaining: 2m 9s\n",
      "258:\tlearn: 0.0009232\ttotal: 45.2s\tremaining: 2m 9s\n",
      "259:\tlearn: 0.0009225\ttotal: 45.4s\tremaining: 2m 9s\n",
      "260:\tlearn: 0.0009225\ttotal: 45.6s\tremaining: 2m 9s\n",
      "261:\tlearn: 0.0009225\ttotal: 45.8s\tremaining: 2m 8s\n",
      "262:\tlearn: 0.0009141\ttotal: 46s\tremaining: 2m 8s\n",
      "263:\tlearn: 0.0009139\ttotal: 46.2s\tremaining: 2m 8s\n",
      "264:\tlearn: 0.0009137\ttotal: 46.3s\tremaining: 2m 8s\n",
      "265:\tlearn: 0.0009137\ttotal: 46.5s\tremaining: 2m 8s\n",
      "266:\tlearn: 0.0009135\ttotal: 46.7s\tremaining: 2m 8s\n",
      "267:\tlearn: 0.0009080\ttotal: 46.9s\tremaining: 2m 7s\n",
      "268:\tlearn: 0.0009080\ttotal: 47s\tremaining: 2m 7s\n",
      "269:\tlearn: 0.0009011\ttotal: 47.2s\tremaining: 2m 7s\n",
      "270:\tlearn: 0.0009010\ttotal: 47.4s\tremaining: 2m 7s\n",
      "271:\tlearn: 0.0009009\ttotal: 47.6s\tremaining: 2m 7s\n",
      "272:\tlearn: 0.0009009\ttotal: 47.7s\tremaining: 2m 7s\n",
      "273:\tlearn: 0.0009007\ttotal: 47.9s\tremaining: 2m 6s\n",
      "274:\tlearn: 0.0009007\ttotal: 48.1s\tremaining: 2m 6s\n",
      "275:\tlearn: 0.0009007\ttotal: 48.2s\tremaining: 2m 6s\n",
      "276:\tlearn: 0.0009007\ttotal: 48.4s\tremaining: 2m 6s\n",
      "277:\tlearn: 0.0009007\ttotal: 48.6s\tremaining: 2m 6s\n",
      "278:\tlearn: 0.0009005\ttotal: 48.8s\tremaining: 2m 6s\n",
      "279:\tlearn: 0.0009005\ttotal: 49s\tremaining: 2m 5s\n",
      "280:\tlearn: 0.0009002\ttotal: 49.1s\tremaining: 2m 5s\n",
      "281:\tlearn: 0.0009002\ttotal: 49.3s\tremaining: 2m 5s\n",
      "282:\tlearn: 0.0009002\ttotal: 49.5s\tremaining: 2m 5s\n",
      "283:\tlearn: 0.0008942\ttotal: 49.6s\tremaining: 2m 5s\n",
      "284:\tlearn: 0.0008842\ttotal: 49.8s\tremaining: 2m 4s\n",
      "285:\tlearn: 0.0008842\ttotal: 50s\tremaining: 2m 4s\n",
      "286:\tlearn: 0.0008815\ttotal: 50.2s\tremaining: 2m 4s\n",
      "287:\tlearn: 0.0008815\ttotal: 50.3s\tremaining: 2m 4s\n",
      "288:\tlearn: 0.0008808\ttotal: 50.5s\tremaining: 2m 4s\n",
      "289:\tlearn: 0.0008806\ttotal: 50.7s\tremaining: 2m 4s\n",
      "290:\tlearn: 0.0008806\ttotal: 50.9s\tremaining: 2m 3s\n",
      "291:\tlearn: 0.0008806\ttotal: 51s\tremaining: 2m 3s\n",
      "292:\tlearn: 0.0008801\ttotal: 51.2s\tremaining: 2m 3s\n",
      "293:\tlearn: 0.0008800\ttotal: 51.4s\tremaining: 2m 3s\n",
      "294:\tlearn: 0.0008725\ttotal: 51.6s\tremaining: 2m 3s\n",
      "295:\tlearn: 0.0008723\ttotal: 51.7s\tremaining: 2m 3s\n",
      "296:\tlearn: 0.0008709\ttotal: 51.9s\tremaining: 2m 2s\n",
      "297:\tlearn: 0.0008709\ttotal: 52.1s\tremaining: 2m 2s\n",
      "298:\tlearn: 0.0008683\ttotal: 52.2s\tremaining: 2m 2s\n",
      "299:\tlearn: 0.0008678\ttotal: 52.4s\tremaining: 2m 2s\n",
      "300:\tlearn: 0.0008678\ttotal: 52.6s\tremaining: 2m 2s\n",
      "301:\tlearn: 0.0008631\ttotal: 52.8s\tremaining: 2m 1s\n",
      "302:\tlearn: 0.0008628\ttotal: 52.9s\tremaining: 2m 1s\n",
      "303:\tlearn: 0.0008587\ttotal: 53.1s\tremaining: 2m 1s\n",
      "304:\tlearn: 0.0008587\ttotal: 53.3s\tremaining: 2m 1s\n",
      "305:\tlearn: 0.0008548\ttotal: 53.5s\tremaining: 2m 1s\n",
      "306:\tlearn: 0.0008496\ttotal: 53.6s\tremaining: 2m 1s\n",
      "307:\tlearn: 0.0008491\ttotal: 53.8s\tremaining: 2m\n",
      "308:\tlearn: 0.0008452\ttotal: 54s\tremaining: 2m\n",
      "309:\tlearn: 0.0008451\ttotal: 54.2s\tremaining: 2m\n",
      "310:\tlearn: 0.0008406\ttotal: 54.3s\tremaining: 2m\n",
      "311:\tlearn: 0.0008404\ttotal: 54.5s\tremaining: 2m\n",
      "312:\tlearn: 0.0008403\ttotal: 54.7s\tremaining: 2m\n",
      "313:\tlearn: 0.0008403\ttotal: 54.9s\tremaining: 1m 59s\n",
      "314:\tlearn: 0.0008403\ttotal: 55s\tremaining: 1m 59s\n",
      "315:\tlearn: 0.0008403\ttotal: 55.2s\tremaining: 1m 59s\n",
      "316:\tlearn: 0.0008397\ttotal: 55.4s\tremaining: 1m 59s\n",
      "317:\tlearn: 0.0008396\ttotal: 55.5s\tremaining: 1m 59s\n",
      "318:\tlearn: 0.0008396\ttotal: 55.7s\tremaining: 1m 58s\n",
      "319:\tlearn: 0.0008396\ttotal: 55.9s\tremaining: 1m 58s\n",
      "320:\tlearn: 0.0008383\ttotal: 56.1s\tremaining: 1m 58s\n",
      "321:\tlearn: 0.0008383\ttotal: 56.2s\tremaining: 1m 58s\n",
      "322:\tlearn: 0.0008383\ttotal: 56.4s\tremaining: 1m 58s\n",
      "323:\tlearn: 0.0008383\ttotal: 56.6s\tremaining: 1m 58s\n",
      "324:\tlearn: 0.0008383\ttotal: 56.8s\tremaining: 1m 57s\n",
      "325:\tlearn: 0.0008383\ttotal: 56.9s\tremaining: 1m 57s\n",
      "326:\tlearn: 0.0008383\ttotal: 57.1s\tremaining: 1m 57s\n",
      "327:\tlearn: 0.0008383\ttotal: 57.3s\tremaining: 1m 57s\n",
      "328:\tlearn: 0.0008381\ttotal: 57.5s\tremaining: 1m 57s\n",
      "329:\tlearn: 0.0008380\ttotal: 57.6s\tremaining: 1m 57s\n",
      "330:\tlearn: 0.0008332\ttotal: 57.8s\tremaining: 1m 56s\n",
      "331:\tlearn: 0.0008279\ttotal: 58s\tremaining: 1m 56s\n",
      "332:\tlearn: 0.0008278\ttotal: 58.2s\tremaining: 1m 56s\n",
      "333:\tlearn: 0.0008278\ttotal: 58.3s\tremaining: 1m 56s\n",
      "334:\tlearn: 0.0008277\ttotal: 58.5s\tremaining: 1m 56s\n",
      "335:\tlearn: 0.0008276\ttotal: 58.7s\tremaining: 1m 55s\n",
      "336:\tlearn: 0.0008276\ttotal: 58.9s\tremaining: 1m 55s\n",
      "337:\tlearn: 0.0008276\ttotal: 59s\tremaining: 1m 55s\n",
      "338:\tlearn: 0.0008272\ttotal: 59.2s\tremaining: 1m 55s\n",
      "339:\tlearn: 0.0008272\ttotal: 59.4s\tremaining: 1m 55s\n",
      "340:\tlearn: 0.0008269\ttotal: 59.6s\tremaining: 1m 55s\n",
      "341:\tlearn: 0.0008266\ttotal: 59.7s\tremaining: 1m 54s\n",
      "342:\tlearn: 0.0008266\ttotal: 59.9s\tremaining: 1m 54s\n",
      "343:\tlearn: 0.0008265\ttotal: 1m\tremaining: 1m 54s\n",
      "344:\tlearn: 0.0008265\ttotal: 1m\tremaining: 1m 54s\n",
      "345:\tlearn: 0.0008265\ttotal: 1m\tremaining: 1m 54s\n",
      "346:\tlearn: 0.0008245\ttotal: 1m\tremaining: 1m 54s\n",
      "347:\tlearn: 0.0008245\ttotal: 1m\tremaining: 1m 53s\n",
      "348:\tlearn: 0.0008235\ttotal: 1m\tremaining: 1m 53s\n",
      "349:\tlearn: 0.0008234\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "350:\tlearn: 0.0008234\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "351:\tlearn: 0.0008234\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "352:\tlearn: 0.0008231\ttotal: 1m 1s\tremaining: 1m 53s\n",
      "353:\tlearn: 0.0008226\ttotal: 1m 1s\tremaining: 1m 52s\n",
      "354:\tlearn: 0.0008226\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "355:\tlearn: 0.0008211\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "356:\tlearn: 0.0008210\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "357:\tlearn: 0.0008209\ttotal: 1m 2s\tremaining: 1m 52s\n",
      "358:\tlearn: 0.0008209\ttotal: 1m 2s\tremaining: 1m 51s\n",
      "359:\tlearn: 0.0008209\ttotal: 1m 2s\tremaining: 1m 51s\n",
      "360:\tlearn: 0.0008209\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "361:\tlearn: 0.0008209\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "362:\tlearn: 0.0008208\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "363:\tlearn: 0.0008206\ttotal: 1m 3s\tremaining: 1m 51s\n",
      "364:\tlearn: 0.0008206\ttotal: 1m 3s\tremaining: 1m 50s\n",
      "365:\tlearn: 0.0008201\ttotal: 1m 3s\tremaining: 1m 50s\n",
      "366:\tlearn: 0.0008145\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "367:\tlearn: 0.0008144\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "368:\tlearn: 0.0008142\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "369:\tlearn: 0.0008120\ttotal: 1m 4s\tremaining: 1m 50s\n",
      "370:\tlearn: 0.0008119\ttotal: 1m 4s\tremaining: 1m 49s\n",
      "371:\tlearn: 0.0008119\ttotal: 1m 4s\tremaining: 1m 49s\n",
      "372:\tlearn: 0.0008118\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "373:\tlearn: 0.0008118\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "374:\tlearn: 0.0008118\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "375:\tlearn: 0.0008114\ttotal: 1m 5s\tremaining: 1m 49s\n",
      "376:\tlearn: 0.0008114\ttotal: 1m 5s\tremaining: 1m 48s\n",
      "377:\tlearn: 0.0008114\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "378:\tlearn: 0.0008114\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "379:\tlearn: 0.0008112\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "380:\tlearn: 0.0008111\ttotal: 1m 6s\tremaining: 1m 48s\n",
      "381:\tlearn: 0.0008108\ttotal: 1m 6s\tremaining: 1m 47s\n",
      "382:\tlearn: 0.0008106\ttotal: 1m 6s\tremaining: 1m 47s\n",
      "383:\tlearn: 0.0008105\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "384:\tlearn: 0.0008105\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "385:\tlearn: 0.0008105\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "386:\tlearn: 0.0008105\ttotal: 1m 7s\tremaining: 1m 47s\n",
      "387:\tlearn: 0.0008105\ttotal: 1m 7s\tremaining: 1m 46s\n",
      "388:\tlearn: 0.0008105\ttotal: 1m 7s\tremaining: 1m 46s\n",
      "389:\tlearn: 0.0008105\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "390:\tlearn: 0.0008068\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "391:\tlearn: 0.0008068\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "392:\tlearn: 0.0008068\ttotal: 1m 8s\tremaining: 1m 46s\n",
      "393:\tlearn: 0.0008067\ttotal: 1m 8s\tremaining: 1m 45s\n",
      "394:\tlearn: 0.0008065\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "395:\tlearn: 0.0008064\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "396:\tlearn: 0.0008060\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "397:\tlearn: 0.0008060\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "398:\tlearn: 0.0008060\ttotal: 1m 9s\tremaining: 1m 45s\n",
      "399:\tlearn: 0.0008059\ttotal: 1m 9s\tremaining: 1m 44s\n",
      "400:\tlearn: 0.0008008\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "401:\tlearn: 0.0007942\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "402:\tlearn: 0.0007940\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "403:\tlearn: 0.0007938\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "404:\tlearn: 0.0007891\ttotal: 1m 10s\tremaining: 1m 43s\n",
      "405:\tlearn: 0.0007889\ttotal: 1m 10s\tremaining: 1m 43s\n",
      "406:\tlearn: 0.0007872\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "407:\tlearn: 0.0007870\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "408:\tlearn: 0.0007863\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "409:\tlearn: 0.0007784\ttotal: 1m 11s\tremaining: 1m 43s\n",
      "410:\tlearn: 0.0007784\ttotal: 1m 11s\tremaining: 1m 42s\n",
      "411:\tlearn: 0.0007784\ttotal: 1m 11s\tremaining: 1m 42s\n",
      "412:\tlearn: 0.0007784\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "413:\tlearn: 0.0007784\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "414:\tlearn: 0.0007781\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "415:\tlearn: 0.0007775\ttotal: 1m 12s\tremaining: 1m 42s\n",
      "416:\tlearn: 0.0007775\ttotal: 1m 12s\tremaining: 1m 41s\n",
      "417:\tlearn: 0.0007773\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "418:\tlearn: 0.0007772\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "419:\tlearn: 0.0007767\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "420:\tlearn: 0.0007767\ttotal: 1m 13s\tremaining: 1m 41s\n",
      "421:\tlearn: 0.0007767\ttotal: 1m 13s\tremaining: 1m 40s\n",
      "422:\tlearn: 0.0007767\ttotal: 1m 13s\tremaining: 1m 40s\n",
      "423:\tlearn: 0.0007728\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "424:\tlearn: 0.0007657\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "425:\tlearn: 0.0007657\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "426:\tlearn: 0.0007657\ttotal: 1m 14s\tremaining: 1m 40s\n",
      "427:\tlearn: 0.0007657\ttotal: 1m 14s\tremaining: 1m 39s\n",
      "428:\tlearn: 0.0007656\ttotal: 1m 14s\tremaining: 1m 39s\n",
      "429:\tlearn: 0.0007655\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "430:\tlearn: 0.0007655\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "431:\tlearn: 0.0007655\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "432:\tlearn: 0.0007651\ttotal: 1m 15s\tremaining: 1m 39s\n",
      "433:\tlearn: 0.0007646\ttotal: 1m 15s\tremaining: 1m 38s\n",
      "434:\tlearn: 0.0007645\ttotal: 1m 15s\tremaining: 1m 38s\n",
      "435:\tlearn: 0.0007645\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "436:\tlearn: 0.0007644\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "437:\tlearn: 0.0007640\ttotal: 1m 16s\tremaining: 1m 38s\n",
      "438:\tlearn: 0.0007639\ttotal: 1m 16s\tremaining: 1m 37s\n",
      "439:\tlearn: 0.0007639\ttotal: 1m 16s\tremaining: 1m 37s\n",
      "440:\tlearn: 0.0007638\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "441:\tlearn: 0.0007638\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "442:\tlearn: 0.0007638\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "443:\tlearn: 0.0007633\ttotal: 1m 17s\tremaining: 1m 37s\n",
      "444:\tlearn: 0.0007633\ttotal: 1m 17s\tremaining: 1m 36s\n",
      "445:\tlearn: 0.0007631\ttotal: 1m 17s\tremaining: 1m 36s\n",
      "446:\tlearn: 0.0007631\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "447:\tlearn: 0.0007631\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "448:\tlearn: 0.0007631\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "449:\tlearn: 0.0007631\ttotal: 1m 18s\tremaining: 1m 36s\n",
      "450:\tlearn: 0.0007629\ttotal: 1m 18s\tremaining: 1m 35s\n",
      "451:\tlearn: 0.0007628\ttotal: 1m 18s\tremaining: 1m 35s\n",
      "452:\tlearn: 0.0007623\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "453:\tlearn: 0.0007622\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "454:\tlearn: 0.0007622\ttotal: 1m 19s\tremaining: 1m 35s\n",
      "455:\tlearn: 0.0007621\ttotal: 1m 19s\tremaining: 1m 34s\n",
      "456:\tlearn: 0.0007620\ttotal: 1m 19s\tremaining: 1m 34s\n",
      "457:\tlearn: 0.0007620\ttotal: 1m 19s\tremaining: 1m 34s\n",
      "458:\tlearn: 0.0007618\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "459:\tlearn: 0.0007618\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "460:\tlearn: 0.0007618\ttotal: 1m 20s\tremaining: 1m 34s\n",
      "461:\tlearn: 0.0007616\ttotal: 1m 20s\tremaining: 1m 33s\n",
      "462:\tlearn: 0.0007615\ttotal: 1m 20s\tremaining: 1m 33s\n",
      "463:\tlearn: 0.0007614\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "464:\tlearn: 0.0007614\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "465:\tlearn: 0.0007613\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "466:\tlearn: 0.0007577\ttotal: 1m 21s\tremaining: 1m 33s\n",
      "467:\tlearn: 0.0007571\ttotal: 1m 21s\tremaining: 1m 32s\n",
      "468:\tlearn: 0.0007571\ttotal: 1m 21s\tremaining: 1m 32s\n",
      "469:\tlearn: 0.0007570\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "470:\tlearn: 0.0007568\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "471:\tlearn: 0.0007568\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "472:\tlearn: 0.0007567\ttotal: 1m 22s\tremaining: 1m 32s\n",
      "473:\tlearn: 0.0007567\ttotal: 1m 22s\tremaining: 1m 31s\n",
      "474:\tlearn: 0.0007567\ttotal: 1m 22s\tremaining: 1m 31s\n",
      "475:\tlearn: 0.0007566\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "476:\tlearn: 0.0007565\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "477:\tlearn: 0.0007563\ttotal: 1m 23s\tremaining: 1m 31s\n",
      "478:\tlearn: 0.0007563\ttotal: 1m 23s\tremaining: 1m 30s\n",
      "479:\tlearn: 0.0007563\ttotal: 1m 23s\tremaining: 1m 30s\n",
      "480:\tlearn: 0.0007558\ttotal: 1m 23s\tremaining: 1m 30s\n",
      "481:\tlearn: 0.0007517\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "482:\tlearn: 0.0007516\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "483:\tlearn: 0.0007482\ttotal: 1m 24s\tremaining: 1m 30s\n",
      "484:\tlearn: 0.0007482\ttotal: 1m 24s\tremaining: 1m 29s\n",
      "485:\tlearn: 0.0007482\ttotal: 1m 24s\tremaining: 1m 29s\n",
      "486:\tlearn: 0.0007482\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "487:\tlearn: 0.0007480\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "488:\tlearn: 0.0007480\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "489:\tlearn: 0.0007480\ttotal: 1m 25s\tremaining: 1m 29s\n",
      "490:\tlearn: 0.0007480\ttotal: 1m 25s\tremaining: 1m 28s\n",
      "491:\tlearn: 0.0007479\ttotal: 1m 25s\tremaining: 1m 28s\n",
      "492:\tlearn: 0.0007478\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "493:\tlearn: 0.0007477\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "494:\tlearn: 0.0007477\ttotal: 1m 26s\tremaining: 1m 28s\n",
      "495:\tlearn: 0.0007475\ttotal: 1m 26s\tremaining: 1m 27s\n",
      "496:\tlearn: 0.0007460\ttotal: 1m 26s\tremaining: 1m 27s\n",
      "497:\tlearn: 0.0007459\ttotal: 1m 26s\tremaining: 1m 27s\n",
      "498:\tlearn: 0.0007457\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "499:\tlearn: 0.0007450\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "500:\tlearn: 0.0007445\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "501:\tlearn: 0.0007443\ttotal: 1m 27s\tremaining: 1m 26s\n",
      "502:\tlearn: 0.0007443\ttotal: 1m 27s\tremaining: 1m 26s\n",
      "503:\tlearn: 0.0007443\ttotal: 1m 27s\tremaining: 1m 26s\n",
      "504:\tlearn: 0.0007443\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "505:\tlearn: 0.0007443\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "506:\tlearn: 0.0007443\ttotal: 1m 28s\tremaining: 1m 26s\n",
      "507:\tlearn: 0.0007443\ttotal: 1m 28s\tremaining: 1m 25s\n",
      "508:\tlearn: 0.0007443\ttotal: 1m 28s\tremaining: 1m 25s\n",
      "509:\tlearn: 0.0007443\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "510:\tlearn: 0.0007443\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "511:\tlearn: 0.0007443\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "512:\tlearn: 0.0007442\ttotal: 1m 29s\tremaining: 1m 25s\n",
      "513:\tlearn: 0.0007441\ttotal: 1m 29s\tremaining: 1m 24s\n",
      "514:\tlearn: 0.0007439\ttotal: 1m 29s\tremaining: 1m 24s\n",
      "515:\tlearn: 0.0007439\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "516:\tlearn: 0.0007429\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "517:\tlearn: 0.0007425\ttotal: 1m 30s\tremaining: 1m 24s\n",
      "518:\tlearn: 0.0007424\ttotal: 1m 30s\tremaining: 1m 23s\n",
      "519:\tlearn: 0.0007420\ttotal: 1m 30s\tremaining: 1m 23s\n",
      "520:\tlearn: 0.0007417\ttotal: 1m 30s\tremaining: 1m 23s\n",
      "521:\tlearn: 0.0007417\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "522:\tlearn: 0.0007417\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "523:\tlearn: 0.0007401\ttotal: 1m 31s\tremaining: 1m 23s\n",
      "524:\tlearn: 0.0007401\ttotal: 1m 31s\tremaining: 1m 22s\n",
      "525:\tlearn: 0.0007349\ttotal: 1m 31s\tremaining: 1m 22s\n",
      "526:\tlearn: 0.0007349\ttotal: 1m 31s\tremaining: 1m 22s\n",
      "527:\tlearn: 0.0007347\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "528:\tlearn: 0.0007328\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "529:\tlearn: 0.0007326\ttotal: 1m 32s\tremaining: 1m 22s\n",
      "530:\tlearn: 0.0007324\ttotal: 1m 32s\tremaining: 1m 21s\n",
      "531:\tlearn: 0.0007324\ttotal: 1m 32s\tremaining: 1m 21s\n",
      "532:\tlearn: 0.0007322\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "533:\tlearn: 0.0007322\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "534:\tlearn: 0.0007322\ttotal: 1m 33s\tremaining: 1m 21s\n",
      "535:\tlearn: 0.0007319\ttotal: 1m 33s\tremaining: 1m 20s\n",
      "536:\tlearn: 0.0007319\ttotal: 1m 33s\tremaining: 1m 20s\n",
      "537:\tlearn: 0.0007317\ttotal: 1m 33s\tremaining: 1m 20s\n",
      "538:\tlearn: 0.0007314\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "539:\tlearn: 0.0007312\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "540:\tlearn: 0.0007312\ttotal: 1m 34s\tremaining: 1m 20s\n",
      "541:\tlearn: 0.0007312\ttotal: 1m 34s\tremaining: 1m 19s\n",
      "542:\tlearn: 0.0007311\ttotal: 1m 34s\tremaining: 1m 19s\n",
      "543:\tlearn: 0.0007309\ttotal: 1m 34s\tremaining: 1m 19s\n",
      "544:\tlearn: 0.0007305\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "545:\tlearn: 0.0007284\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "546:\tlearn: 0.0007278\ttotal: 1m 35s\tremaining: 1m 19s\n",
      "547:\tlearn: 0.0007278\ttotal: 1m 35s\tremaining: 1m 18s\n",
      "548:\tlearn: 0.0007272\ttotal: 1m 35s\tremaining: 1m 18s\n",
      "549:\tlearn: 0.0007272\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "550:\tlearn: 0.0007270\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "551:\tlearn: 0.0007269\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "552:\tlearn: 0.0007269\ttotal: 1m 36s\tremaining: 1m 18s\n",
      "553:\tlearn: 0.0007269\ttotal: 1m 36s\tremaining: 1m 17s\n",
      "554:\tlearn: 0.0007269\ttotal: 1m 36s\tremaining: 1m 17s\n",
      "555:\tlearn: 0.0007269\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "556:\tlearn: 0.0007268\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "557:\tlearn: 0.0007268\ttotal: 1m 37s\tremaining: 1m 17s\n",
      "558:\tlearn: 0.0007266\ttotal: 1m 37s\tremaining: 1m 16s\n",
      "559:\tlearn: 0.0007266\ttotal: 1m 37s\tremaining: 1m 16s\n",
      "560:\tlearn: 0.0007265\ttotal: 1m 37s\tremaining: 1m 16s\n",
      "561:\tlearn: 0.0007265\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "562:\tlearn: 0.0007264\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "563:\tlearn: 0.0007264\ttotal: 1m 38s\tremaining: 1m 16s\n",
      "564:\tlearn: 0.0007235\ttotal: 1m 38s\tremaining: 1m 15s\n",
      "565:\tlearn: 0.0007232\ttotal: 1m 38s\tremaining: 1m 15s\n",
      "566:\tlearn: 0.0007232\ttotal: 1m 38s\tremaining: 1m 15s\n",
      "567:\tlearn: 0.0007231\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "568:\tlearn: 0.0007231\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "569:\tlearn: 0.0007231\ttotal: 1m 39s\tremaining: 1m 15s\n",
      "570:\tlearn: 0.0007230\ttotal: 1m 39s\tremaining: 1m 14s\n",
      "571:\tlearn: 0.0007230\ttotal: 1m 39s\tremaining: 1m 14s\n",
      "572:\tlearn: 0.0007220\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "573:\tlearn: 0.0007219\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "574:\tlearn: 0.0007219\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "575:\tlearn: 0.0007219\ttotal: 1m 40s\tremaining: 1m 14s\n",
      "576:\tlearn: 0.0007219\ttotal: 1m 40s\tremaining: 1m 13s\n",
      "577:\tlearn: 0.0007219\ttotal: 1m 40s\tremaining: 1m 13s\n",
      "578:\tlearn: 0.0007219\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "579:\tlearn: 0.0007219\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "580:\tlearn: 0.0007219\ttotal: 1m 41s\tremaining: 1m 13s\n",
      "581:\tlearn: 0.0007219\ttotal: 1m 41s\tremaining: 1m 12s\n",
      "582:\tlearn: 0.0007218\ttotal: 1m 41s\tremaining: 1m 12s\n",
      "583:\tlearn: 0.0007218\ttotal: 1m 41s\tremaining: 1m 12s\n",
      "584:\tlearn: 0.0007217\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "585:\tlearn: 0.0007191\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "586:\tlearn: 0.0007189\ttotal: 1m 42s\tremaining: 1m 12s\n",
      "587:\tlearn: 0.0007189\ttotal: 1m 42s\tremaining: 1m 11s\n",
      "588:\tlearn: 0.0007189\ttotal: 1m 42s\tremaining: 1m 11s\n",
      "589:\tlearn: 0.0007189\ttotal: 1m 42s\tremaining: 1m 11s\n",
      "590:\tlearn: 0.0007189\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "591:\tlearn: 0.0007188\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "592:\tlearn: 0.0007188\ttotal: 1m 43s\tremaining: 1m 11s\n",
      "593:\tlearn: 0.0007187\ttotal: 1m 43s\tremaining: 1m 10s\n",
      "594:\tlearn: 0.0007186\ttotal: 1m 43s\tremaining: 1m 10s\n",
      "595:\tlearn: 0.0007185\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "596:\tlearn: 0.0007185\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "597:\tlearn: 0.0007183\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "598:\tlearn: 0.0007183\ttotal: 1m 44s\tremaining: 1m 10s\n",
      "599:\tlearn: 0.0007183\ttotal: 1m 44s\tremaining: 1m 9s\n",
      "600:\tlearn: 0.0007183\ttotal: 1m 44s\tremaining: 1m 9s\n",
      "601:\tlearn: 0.0007181\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "602:\tlearn: 0.0007180\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "603:\tlearn: 0.0007178\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "604:\tlearn: 0.0007161\ttotal: 1m 45s\tremaining: 1m 8s\n",
      "605:\tlearn: 0.0007160\ttotal: 1m 45s\tremaining: 1m 8s\n",
      "606:\tlearn: 0.0007159\ttotal: 1m 45s\tremaining: 1m 8s\n",
      "607:\tlearn: 0.0007153\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "608:\tlearn: 0.0007152\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "609:\tlearn: 0.0007152\ttotal: 1m 46s\tremaining: 1m 8s\n",
      "610:\tlearn: 0.0007151\ttotal: 1m 46s\tremaining: 1m 7s\n",
      "611:\tlearn: 0.0007148\ttotal: 1m 46s\tremaining: 1m 7s\n",
      "612:\tlearn: 0.0007147\ttotal: 1m 46s\tremaining: 1m 7s\n",
      "613:\tlearn: 0.0007147\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "614:\tlearn: 0.0007147\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "615:\tlearn: 0.0007147\ttotal: 1m 47s\tremaining: 1m 7s\n",
      "616:\tlearn: 0.0007147\ttotal: 1m 47s\tremaining: 1m 6s\n",
      "617:\tlearn: 0.0007147\ttotal: 1m 47s\tremaining: 1m 6s\n",
      "618:\tlearn: 0.0007147\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "619:\tlearn: 0.0007147\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "620:\tlearn: 0.0007147\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "621:\tlearn: 0.0007147\ttotal: 1m 48s\tremaining: 1m 5s\n",
      "622:\tlearn: 0.0007147\ttotal: 1m 48s\tremaining: 1m 5s\n",
      "623:\tlearn: 0.0007144\ttotal: 1m 48s\tremaining: 1m 5s\n",
      "624:\tlearn: 0.0007143\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "625:\tlearn: 0.0007142\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "626:\tlearn: 0.0007139\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "627:\tlearn: 0.0007138\ttotal: 1m 49s\tremaining: 1m 4s\n",
      "628:\tlearn: 0.0007138\ttotal: 1m 49s\tremaining: 1m 4s\n",
      "629:\tlearn: 0.0007138\ttotal: 1m 49s\tremaining: 1m 4s\n",
      "630:\tlearn: 0.0007138\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "631:\tlearn: 0.0007138\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "632:\tlearn: 0.0007138\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "633:\tlearn: 0.0007138\ttotal: 1m 50s\tremaining: 1m 3s\n",
      "634:\tlearn: 0.0007137\ttotal: 1m 50s\tremaining: 1m 3s\n",
      "635:\tlearn: 0.0007136\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "636:\tlearn: 0.0007135\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "637:\tlearn: 0.0007135\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "638:\tlearn: 0.0007134\ttotal: 1m 51s\tremaining: 1m 3s\n",
      "639:\tlearn: 0.0007133\ttotal: 1m 51s\tremaining: 1m 2s\n",
      "640:\tlearn: 0.0007111\ttotal: 1m 51s\tremaining: 1m 2s\n",
      "641:\tlearn: 0.0007109\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "642:\tlearn: 0.0007108\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "643:\tlearn: 0.0007108\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "644:\tlearn: 0.0007106\ttotal: 1m 52s\tremaining: 1m 1s\n",
      "645:\tlearn: 0.0007106\ttotal: 1m 52s\tremaining: 1m 1s\n",
      "646:\tlearn: 0.0007105\ttotal: 1m 52s\tremaining: 1m 1s\n",
      "647:\tlearn: 0.0007100\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "648:\tlearn: 0.0007100\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "649:\tlearn: 0.0007100\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "650:\tlearn: 0.0007089\ttotal: 1m 53s\tremaining: 1m\n",
      "651:\tlearn: 0.0007085\ttotal: 1m 53s\tremaining: 1m\n",
      "652:\tlearn: 0.0007084\ttotal: 1m 53s\tremaining: 1m\n",
      "653:\tlearn: 0.0007084\ttotal: 1m 54s\tremaining: 1m\n",
      "654:\tlearn: 0.0007084\ttotal: 1m 54s\tremaining: 1m\n",
      "655:\tlearn: 0.0007084\ttotal: 1m 54s\tremaining: 1m\n",
      "656:\tlearn: 0.0007084\ttotal: 1m 54s\tremaining: 59.9s\n",
      "657:\tlearn: 0.0007084\ttotal: 1m 54s\tremaining: 59.7s\n",
      "658:\tlearn: 0.0007080\ttotal: 1m 54s\tremaining: 59.5s\n",
      "659:\tlearn: 0.0007078\ttotal: 1m 55s\tremaining: 59.3s\n",
      "660:\tlearn: 0.0007077\ttotal: 1m 55s\tremaining: 59.2s\n",
      "661:\tlearn: 0.0007077\ttotal: 1m 55s\tremaining: 59s\n",
      "662:\tlearn: 0.0007077\ttotal: 1m 55s\tremaining: 58.8s\n",
      "663:\tlearn: 0.0007074\ttotal: 1m 55s\tremaining: 58.6s\n",
      "664:\tlearn: 0.0007074\ttotal: 1m 56s\tremaining: 58.5s\n",
      "665:\tlearn: 0.0007074\ttotal: 1m 56s\tremaining: 58.3s\n",
      "666:\tlearn: 0.0007073\ttotal: 1m 56s\tremaining: 58.1s\n",
      "667:\tlearn: 0.0007073\ttotal: 1m 56s\tremaining: 57.9s\n",
      "668:\tlearn: 0.0007071\ttotal: 1m 56s\tremaining: 57.8s\n",
      "669:\tlearn: 0.0007070\ttotal: 1m 56s\tremaining: 57.6s\n",
      "670:\tlearn: 0.0007069\ttotal: 1m 57s\tremaining: 57.4s\n",
      "671:\tlearn: 0.0007069\ttotal: 1m 57s\tremaining: 57.2s\n",
      "672:\tlearn: 0.0007069\ttotal: 1m 57s\tremaining: 57.1s\n",
      "673:\tlearn: 0.0007069\ttotal: 1m 57s\tremaining: 56.9s\n",
      "674:\tlearn: 0.0007067\ttotal: 1m 57s\tremaining: 56.7s\n",
      "675:\tlearn: 0.0007066\ttotal: 1m 57s\tremaining: 56.5s\n",
      "676:\tlearn: 0.0007066\ttotal: 1m 58s\tremaining: 56.4s\n",
      "677:\tlearn: 0.0007066\ttotal: 1m 58s\tremaining: 56.2s\n",
      "678:\tlearn: 0.0007065\ttotal: 1m 58s\tremaining: 56s\n",
      "679:\tlearn: 0.0007064\ttotal: 1m 58s\tremaining: 55.8s\n",
      "680:\tlearn: 0.0007064\ttotal: 1m 58s\tremaining: 55.7s\n",
      "681:\tlearn: 0.0007061\ttotal: 1m 58s\tremaining: 55.5s\n",
      "682:\tlearn: 0.0007057\ttotal: 1m 59s\tremaining: 55.3s\n",
      "683:\tlearn: 0.0007057\ttotal: 1m 59s\tremaining: 55.1s\n",
      "684:\tlearn: 0.0007056\ttotal: 1m 59s\tremaining: 55s\n",
      "685:\tlearn: 0.0007056\ttotal: 1m 59s\tremaining: 54.8s\n",
      "686:\tlearn: 0.0007056\ttotal: 1m 59s\tremaining: 54.6s\n",
      "687:\tlearn: 0.0007056\ttotal: 2m\tremaining: 54.4s\n",
      "688:\tlearn: 0.0007055\ttotal: 2m\tremaining: 54.3s\n",
      "689:\tlearn: 0.0007048\ttotal: 2m\tremaining: 54.1s\n",
      "690:\tlearn: 0.0007047\ttotal: 2m\tremaining: 53.9s\n",
      "691:\tlearn: 0.0007042\ttotal: 2m\tremaining: 53.7s\n",
      "692:\tlearn: 0.0007042\ttotal: 2m\tremaining: 53.6s\n",
      "693:\tlearn: 0.0007042\ttotal: 2m 1s\tremaining: 53.4s\n",
      "694:\tlearn: 0.0007042\ttotal: 2m 1s\tremaining: 53.2s\n",
      "695:\tlearn: 0.0007040\ttotal: 2m 1s\tremaining: 53s\n",
      "696:\tlearn: 0.0007014\ttotal: 2m 1s\tremaining: 52.9s\n",
      "697:\tlearn: 0.0007012\ttotal: 2m 1s\tremaining: 52.7s\n",
      "698:\tlearn: 0.0007011\ttotal: 2m 1s\tremaining: 52.5s\n",
      "699:\tlearn: 0.0006980\ttotal: 2m 2s\tremaining: 52.3s\n",
      "700:\tlearn: 0.0006979\ttotal: 2m 2s\tremaining: 52.2s\n",
      "701:\tlearn: 0.0006979\ttotal: 2m 2s\tremaining: 52s\n",
      "702:\tlearn: 0.0006979\ttotal: 2m 2s\tremaining: 51.8s\n",
      "703:\tlearn: 0.0006978\ttotal: 2m 2s\tremaining: 51.6s\n",
      "704:\tlearn: 0.0006974\ttotal: 2m 2s\tremaining: 51.5s\n",
      "705:\tlearn: 0.0006973\ttotal: 2m 3s\tremaining: 51.3s\n",
      "706:\tlearn: 0.0006973\ttotal: 2m 3s\tremaining: 51.1s\n",
      "707:\tlearn: 0.0006973\ttotal: 2m 3s\tremaining: 50.9s\n",
      "708:\tlearn: 0.0006973\ttotal: 2m 3s\tremaining: 50.8s\n",
      "709:\tlearn: 0.0006971\ttotal: 2m 3s\tremaining: 50.6s\n",
      "710:\tlearn: 0.0006970\ttotal: 2m 4s\tremaining: 50.4s\n",
      "711:\tlearn: 0.0006969\ttotal: 2m 4s\tremaining: 50.2s\n",
      "712:\tlearn: 0.0006968\ttotal: 2m 4s\tremaining: 50.1s\n",
      "713:\tlearn: 0.0006968\ttotal: 2m 4s\tremaining: 49.9s\n",
      "714:\tlearn: 0.0006967\ttotal: 2m 4s\tremaining: 49.7s\n",
      "715:\tlearn: 0.0006967\ttotal: 2m 4s\tremaining: 49.5s\n",
      "716:\tlearn: 0.0006967\ttotal: 2m 5s\tremaining: 49.4s\n",
      "717:\tlearn: 0.0006967\ttotal: 2m 5s\tremaining: 49.2s\n",
      "718:\tlearn: 0.0006966\ttotal: 2m 5s\tremaining: 49s\n",
      "719:\tlearn: 0.0006965\ttotal: 2m 5s\tremaining: 48.8s\n",
      "720:\tlearn: 0.0006964\ttotal: 2m 5s\tremaining: 48.7s\n",
      "721:\tlearn: 0.0006963\ttotal: 2m 5s\tremaining: 48.5s\n",
      "722:\tlearn: 0.0006963\ttotal: 2m 6s\tremaining: 48.3s\n",
      "723:\tlearn: 0.0006963\ttotal: 2m 6s\tremaining: 48.1s\n",
      "724:\tlearn: 0.0006935\ttotal: 2m 6s\tremaining: 48s\n",
      "725:\tlearn: 0.0006935\ttotal: 2m 6s\tremaining: 47.8s\n",
      "726:\tlearn: 0.0006935\ttotal: 2m 6s\tremaining: 47.6s\n",
      "727:\tlearn: 0.0006935\ttotal: 2m 6s\tremaining: 47.4s\n",
      "728:\tlearn: 0.0006930\ttotal: 2m 7s\tremaining: 47.3s\n",
      "729:\tlearn: 0.0006930\ttotal: 2m 7s\tremaining: 47.1s\n",
      "730:\tlearn: 0.0006930\ttotal: 2m 7s\tremaining: 46.9s\n",
      "731:\tlearn: 0.0006930\ttotal: 2m 7s\tremaining: 46.8s\n",
      "732:\tlearn: 0.0006930\ttotal: 2m 7s\tremaining: 46.6s\n",
      "733:\tlearn: 0.0006929\ttotal: 2m 8s\tremaining: 46.4s\n",
      "734:\tlearn: 0.0006929\ttotal: 2m 8s\tremaining: 46.2s\n",
      "735:\tlearn: 0.0006929\ttotal: 2m 8s\tremaining: 46.1s\n",
      "736:\tlearn: 0.0006929\ttotal: 2m 8s\tremaining: 45.9s\n",
      "737:\tlearn: 0.0006926\ttotal: 2m 8s\tremaining: 45.7s\n",
      "738:\tlearn: 0.0006924\ttotal: 2m 8s\tremaining: 45.5s\n",
      "739:\tlearn: 0.0006924\ttotal: 2m 9s\tremaining: 45.4s\n",
      "740:\tlearn: 0.0006923\ttotal: 2m 9s\tremaining: 45.2s\n",
      "741:\tlearn: 0.0006921\ttotal: 2m 9s\tremaining: 45s\n",
      "742:\tlearn: 0.0006920\ttotal: 2m 9s\tremaining: 44.9s\n",
      "743:\tlearn: 0.0006919\ttotal: 2m 9s\tremaining: 44.7s\n",
      "744:\tlearn: 0.0006904\ttotal: 2m 10s\tremaining: 44.5s\n",
      "745:\tlearn: 0.0006902\ttotal: 2m 10s\tremaining: 44.4s\n",
      "746:\tlearn: 0.0006901\ttotal: 2m 10s\tremaining: 44.2s\n",
      "747:\tlearn: 0.0006900\ttotal: 2m 10s\tremaining: 44s\n",
      "748:\tlearn: 0.0006900\ttotal: 2m 10s\tremaining: 43.9s\n",
      "749:\tlearn: 0.0006899\ttotal: 2m 11s\tremaining: 43.7s\n",
      "750:\tlearn: 0.0006891\ttotal: 2m 11s\tremaining: 43.5s\n",
      "751:\tlearn: 0.0006891\ttotal: 2m 11s\tremaining: 43.4s\n",
      "752:\tlearn: 0.0006889\ttotal: 2m 11s\tremaining: 43.2s\n",
      "753:\tlearn: 0.0006889\ttotal: 2m 11s\tremaining: 43s\n",
      "754:\tlearn: 0.0006887\ttotal: 2m 12s\tremaining: 42.9s\n",
      "755:\tlearn: 0.0006887\ttotal: 2m 12s\tremaining: 42.7s\n",
      "756:\tlearn: 0.0006887\ttotal: 2m 12s\tremaining: 42.5s\n",
      "757:\tlearn: 0.0006885\ttotal: 2m 12s\tremaining: 42.4s\n",
      "758:\tlearn: 0.0006884\ttotal: 2m 12s\tremaining: 42.2s\n",
      "759:\tlearn: 0.0006884\ttotal: 2m 13s\tremaining: 42s\n",
      "760:\tlearn: 0.0006881\ttotal: 2m 13s\tremaining: 41.9s\n",
      "761:\tlearn: 0.0006881\ttotal: 2m 13s\tremaining: 41.7s\n",
      "762:\tlearn: 0.0006881\ttotal: 2m 13s\tremaining: 41.5s\n",
      "763:\tlearn: 0.0006880\ttotal: 2m 13s\tremaining: 41.4s\n",
      "764:\tlearn: 0.0006880\ttotal: 2m 14s\tremaining: 41.2s\n",
      "765:\tlearn: 0.0006876\ttotal: 2m 14s\tremaining: 41s\n",
      "766:\tlearn: 0.0006875\ttotal: 2m 14s\tremaining: 40.8s\n",
      "767:\tlearn: 0.0006874\ttotal: 2m 14s\tremaining: 40.7s\n",
      "768:\tlearn: 0.0006873\ttotal: 2m 14s\tremaining: 40.5s\n",
      "769:\tlearn: 0.0006872\ttotal: 2m 15s\tremaining: 40.3s\n",
      "770:\tlearn: 0.0006872\ttotal: 2m 15s\tremaining: 40.2s\n",
      "771:\tlearn: 0.0006872\ttotal: 2m 15s\tremaining: 40s\n",
      "772:\tlearn: 0.0006867\ttotal: 2m 15s\tremaining: 39.8s\n",
      "773:\tlearn: 0.0006867\ttotal: 2m 15s\tremaining: 39.7s\n",
      "774:\tlearn: 0.0006867\ttotal: 2m 16s\tremaining: 39.5s\n",
      "775:\tlearn: 0.0006866\ttotal: 2m 16s\tremaining: 39.3s\n",
      "776:\tlearn: 0.0006865\ttotal: 2m 16s\tremaining: 39.2s\n",
      "777:\tlearn: 0.0006865\ttotal: 2m 16s\tremaining: 39s\n",
      "778:\tlearn: 0.0006863\ttotal: 2m 16s\tremaining: 38.8s\n",
      "779:\tlearn: 0.0006862\ttotal: 2m 17s\tremaining: 38.6s\n",
      "780:\tlearn: 0.0006862\ttotal: 2m 17s\tremaining: 38.5s\n",
      "781:\tlearn: 0.0006862\ttotal: 2m 17s\tremaining: 38.3s\n",
      "782:\tlearn: 0.0006861\ttotal: 2m 17s\tremaining: 38.1s\n",
      "783:\tlearn: 0.0006860\ttotal: 2m 17s\tremaining: 37.9s\n",
      "784:\tlearn: 0.0006859\ttotal: 2m 17s\tremaining: 37.8s\n",
      "785:\tlearn: 0.0006858\ttotal: 2m 18s\tremaining: 37.6s\n",
      "786:\tlearn: 0.0006858\ttotal: 2m 18s\tremaining: 37.4s\n",
      "787:\tlearn: 0.0006854\ttotal: 2m 18s\tremaining: 37.2s\n",
      "788:\tlearn: 0.0006854\ttotal: 2m 18s\tremaining: 37.1s\n",
      "789:\tlearn: 0.0006852\ttotal: 2m 18s\tremaining: 36.9s\n",
      "790:\tlearn: 0.0006852\ttotal: 2m 18s\tremaining: 36.7s\n",
      "791:\tlearn: 0.0006849\ttotal: 2m 19s\tremaining: 36.5s\n",
      "792:\tlearn: 0.0006849\ttotal: 2m 19s\tremaining: 36.4s\n",
      "793:\tlearn: 0.0006848\ttotal: 2m 19s\tremaining: 36.2s\n",
      "794:\tlearn: 0.0006846\ttotal: 2m 19s\tremaining: 36s\n",
      "795:\tlearn: 0.0006845\ttotal: 2m 19s\tremaining: 35.8s\n",
      "796:\tlearn: 0.0006845\ttotal: 2m 20s\tremaining: 35.7s\n",
      "797:\tlearn: 0.0006844\ttotal: 2m 20s\tremaining: 35.5s\n",
      "798:\tlearn: 0.0006844\ttotal: 2m 20s\tremaining: 35.3s\n",
      "799:\tlearn: 0.0006844\ttotal: 2m 20s\tremaining: 35.1s\n",
      "800:\tlearn: 0.0006825\ttotal: 2m 20s\tremaining: 35s\n",
      "801:\tlearn: 0.0006822\ttotal: 2m 20s\tremaining: 34.8s\n",
      "802:\tlearn: 0.0006822\ttotal: 2m 21s\tremaining: 34.6s\n",
      "803:\tlearn: 0.0006822\ttotal: 2m 21s\tremaining: 34.4s\n",
      "804:\tlearn: 0.0006822\ttotal: 2m 21s\tremaining: 34.3s\n",
      "805:\tlearn: 0.0006805\ttotal: 2m 21s\tremaining: 34.1s\n",
      "806:\tlearn: 0.0006804\ttotal: 2m 21s\tremaining: 33.9s\n",
      "807:\tlearn: 0.0006803\ttotal: 2m 21s\tremaining: 33.7s\n",
      "808:\tlearn: 0.0006801\ttotal: 2m 22s\tremaining: 33.6s\n",
      "809:\tlearn: 0.0006797\ttotal: 2m 22s\tremaining: 33.4s\n",
      "810:\tlearn: 0.0006796\ttotal: 2m 22s\tremaining: 33.2s\n",
      "811:\tlearn: 0.0006796\ttotal: 2m 22s\tremaining: 33s\n",
      "812:\tlearn: 0.0006785\ttotal: 2m 22s\tremaining: 32.9s\n",
      "813:\tlearn: 0.0006784\ttotal: 2m 23s\tremaining: 32.7s\n",
      "814:\tlearn: 0.0006784\ttotal: 2m 23s\tremaining: 32.5s\n",
      "815:\tlearn: 0.0006784\ttotal: 2m 23s\tremaining: 32.3s\n",
      "816:\tlearn: 0.0006783\ttotal: 2m 23s\tremaining: 32.1s\n",
      "817:\tlearn: 0.0006783\ttotal: 2m 23s\tremaining: 32s\n",
      "818:\tlearn: 0.0006782\ttotal: 2m 23s\tremaining: 31.8s\n",
      "819:\tlearn: 0.0006779\ttotal: 2m 24s\tremaining: 31.6s\n",
      "820:\tlearn: 0.0006778\ttotal: 2m 24s\tremaining: 31.4s\n",
      "821:\tlearn: 0.0006778\ttotal: 2m 24s\tremaining: 31.3s\n",
      "822:\tlearn: 0.0006775\ttotal: 2m 24s\tremaining: 31.1s\n",
      "823:\tlearn: 0.0006774\ttotal: 2m 24s\tremaining: 30.9s\n",
      "824:\tlearn: 0.0006774\ttotal: 2m 24s\tremaining: 30.7s\n",
      "825:\tlearn: 0.0006772\ttotal: 2m 25s\tremaining: 30.6s\n",
      "826:\tlearn: 0.0006772\ttotal: 2m 25s\tremaining: 30.4s\n",
      "827:\tlearn: 0.0006771\ttotal: 2m 25s\tremaining: 30.2s\n",
      "828:\tlearn: 0.0006769\ttotal: 2m 25s\tremaining: 30s\n",
      "829:\tlearn: 0.0006767\ttotal: 2m 25s\tremaining: 29.9s\n",
      "830:\tlearn: 0.0006767\ttotal: 2m 25s\tremaining: 29.7s\n",
      "831:\tlearn: 0.0006765\ttotal: 2m 26s\tremaining: 29.5s\n",
      "832:\tlearn: 0.0006762\ttotal: 2m 26s\tremaining: 29.3s\n",
      "833:\tlearn: 0.0006762\ttotal: 2m 26s\tremaining: 29.2s\n",
      "834:\tlearn: 0.0006761\ttotal: 2m 26s\tremaining: 29s\n",
      "835:\tlearn: 0.0006761\ttotal: 2m 26s\tremaining: 28.8s\n",
      "836:\tlearn: 0.0006761\ttotal: 2m 27s\tremaining: 28.6s\n",
      "837:\tlearn: 0.0006761\ttotal: 2m 27s\tremaining: 28.5s\n",
      "838:\tlearn: 0.0006759\ttotal: 2m 27s\tremaining: 28.3s\n",
      "839:\tlearn: 0.0006759\ttotal: 2m 27s\tremaining: 28.1s\n",
      "840:\tlearn: 0.0006759\ttotal: 2m 27s\tremaining: 27.9s\n",
      "841:\tlearn: 0.0006759\ttotal: 2m 27s\tremaining: 27.7s\n",
      "842:\tlearn: 0.0006758\ttotal: 2m 28s\tremaining: 27.6s\n",
      "843:\tlearn: 0.0006758\ttotal: 2m 28s\tremaining: 27.4s\n",
      "844:\tlearn: 0.0006756\ttotal: 2m 28s\tremaining: 27.2s\n",
      "845:\tlearn: 0.0006756\ttotal: 2m 28s\tremaining: 27s\n",
      "846:\tlearn: 0.0006756\ttotal: 2m 28s\tremaining: 26.9s\n",
      "847:\tlearn: 0.0006756\ttotal: 2m 28s\tremaining: 26.7s\n",
      "848:\tlearn: 0.0006745\ttotal: 2m 29s\tremaining: 26.5s\n",
      "849:\tlearn: 0.0006745\ttotal: 2m 29s\tremaining: 26.3s\n",
      "850:\tlearn: 0.0006745\ttotal: 2m 29s\tremaining: 26.2s\n",
      "851:\tlearn: 0.0006710\ttotal: 2m 29s\tremaining: 26s\n",
      "852:\tlearn: 0.0006710\ttotal: 2m 29s\tremaining: 25.8s\n",
      "853:\tlearn: 0.0006709\ttotal: 2m 29s\tremaining: 25.6s\n",
      "854:\tlearn: 0.0006707\ttotal: 2m 30s\tremaining: 25.5s\n",
      "855:\tlearn: 0.0006698\ttotal: 2m 30s\tremaining: 25.3s\n",
      "856:\tlearn: 0.0006698\ttotal: 2m 30s\tremaining: 25.1s\n",
      "857:\tlearn: 0.0006698\ttotal: 2m 30s\tremaining: 24.9s\n",
      "858:\tlearn: 0.0006697\ttotal: 2m 30s\tremaining: 24.8s\n",
      "859:\tlearn: 0.0006696\ttotal: 2m 31s\tremaining: 24.6s\n",
      "860:\tlearn: 0.0006696\ttotal: 2m 31s\tremaining: 24.4s\n",
      "861:\tlearn: 0.0006695\ttotal: 2m 31s\tremaining: 24.2s\n",
      "862:\tlearn: 0.0006694\ttotal: 2m 31s\tremaining: 24.1s\n",
      "863:\tlearn: 0.0006693\ttotal: 2m 31s\tremaining: 23.9s\n",
      "864:\tlearn: 0.0006693\ttotal: 2m 31s\tremaining: 23.7s\n",
      "865:\tlearn: 0.0006652\ttotal: 2m 32s\tremaining: 23.5s\n",
      "866:\tlearn: 0.0006652\ttotal: 2m 32s\tremaining: 23.4s\n",
      "867:\tlearn: 0.0006652\ttotal: 2m 32s\tremaining: 23.2s\n",
      "868:\tlearn: 0.0006652\ttotal: 2m 32s\tremaining: 23s\n",
      "869:\tlearn: 0.0006650\ttotal: 2m 32s\tremaining: 22.8s\n",
      "870:\tlearn: 0.0006650\ttotal: 2m 32s\tremaining: 22.6s\n",
      "871:\tlearn: 0.0006650\ttotal: 2m 33s\tremaining: 22.5s\n",
      "872:\tlearn: 0.0006647\ttotal: 2m 33s\tremaining: 22.3s\n",
      "873:\tlearn: 0.0006646\ttotal: 2m 33s\tremaining: 22.1s\n",
      "874:\tlearn: 0.0006645\ttotal: 2m 33s\tremaining: 21.9s\n",
      "875:\tlearn: 0.0006645\ttotal: 2m 33s\tremaining: 21.8s\n",
      "876:\tlearn: 0.0006644\ttotal: 2m 33s\tremaining: 21.6s\n",
      "877:\tlearn: 0.0006644\ttotal: 2m 34s\tremaining: 21.4s\n",
      "878:\tlearn: 0.0006644\ttotal: 2m 34s\tremaining: 21.2s\n",
      "879:\tlearn: 0.0006642\ttotal: 2m 34s\tremaining: 21.1s\n",
      "880:\tlearn: 0.0006642\ttotal: 2m 34s\tremaining: 20.9s\n",
      "881:\tlearn: 0.0006642\ttotal: 2m 34s\tremaining: 20.7s\n",
      "882:\tlearn: 0.0006642\ttotal: 2m 35s\tremaining: 20.5s\n",
      "883:\tlearn: 0.0006641\ttotal: 2m 35s\tremaining: 20.4s\n",
      "884:\tlearn: 0.0006640\ttotal: 2m 35s\tremaining: 20.2s\n",
      "885:\tlearn: 0.0006638\ttotal: 2m 35s\tremaining: 20s\n",
      "886:\tlearn: 0.0006638\ttotal: 2m 35s\tremaining: 19.8s\n",
      "887:\tlearn: 0.0006638\ttotal: 2m 35s\tremaining: 19.7s\n",
      "888:\tlearn: 0.0006636\ttotal: 2m 36s\tremaining: 19.5s\n",
      "889:\tlearn: 0.0006632\ttotal: 2m 36s\tremaining: 19.3s\n",
      "890:\tlearn: 0.0006632\ttotal: 2m 36s\tremaining: 19.1s\n",
      "891:\tlearn: 0.0006632\ttotal: 2m 36s\tremaining: 19s\n",
      "892:\tlearn: 0.0006630\ttotal: 2m 36s\tremaining: 18.8s\n",
      "893:\tlearn: 0.0006630\ttotal: 2m 36s\tremaining: 18.6s\n",
      "894:\tlearn: 0.0006601\ttotal: 2m 37s\tremaining: 18.4s\n",
      "895:\tlearn: 0.0006601\ttotal: 2m 37s\tremaining: 18.3s\n",
      "896:\tlearn: 0.0006601\ttotal: 2m 37s\tremaining: 18.1s\n",
      "897:\tlearn: 0.0006601\ttotal: 2m 37s\tremaining: 17.9s\n",
      "898:\tlearn: 0.0006594\ttotal: 2m 37s\tremaining: 17.7s\n",
      "899:\tlearn: 0.0006593\ttotal: 2m 38s\tremaining: 17.6s\n",
      "900:\tlearn: 0.0006593\ttotal: 2m 38s\tremaining: 17.4s\n",
      "901:\tlearn: 0.0006593\ttotal: 2m 38s\tremaining: 17.2s\n",
      "902:\tlearn: 0.0006592\ttotal: 2m 38s\tremaining: 17s\n",
      "903:\tlearn: 0.0006592\ttotal: 2m 38s\tremaining: 16.9s\n",
      "904:\tlearn: 0.0006591\ttotal: 2m 38s\tremaining: 16.7s\n",
      "905:\tlearn: 0.0006590\ttotal: 2m 39s\tremaining: 16.5s\n",
      "906:\tlearn: 0.0006590\ttotal: 2m 39s\tremaining: 16.3s\n",
      "907:\tlearn: 0.0006589\ttotal: 2m 39s\tremaining: 16.2s\n",
      "908:\tlearn: 0.0006589\ttotal: 2m 39s\tremaining: 16s\n",
      "909:\tlearn: 0.0006588\ttotal: 2m 39s\tremaining: 15.8s\n",
      "910:\tlearn: 0.0006587\ttotal: 2m 40s\tremaining: 15.6s\n",
      "911:\tlearn: 0.0006587\ttotal: 2m 40s\tremaining: 15.5s\n",
      "912:\tlearn: 0.0006587\ttotal: 2m 40s\tremaining: 15.3s\n",
      "913:\tlearn: 0.0006587\ttotal: 2m 40s\tremaining: 15.1s\n",
      "914:\tlearn: 0.0006587\ttotal: 2m 40s\tremaining: 14.9s\n",
      "915:\tlearn: 0.0006586\ttotal: 2m 40s\tremaining: 14.8s\n",
      "916:\tlearn: 0.0006570\ttotal: 2m 41s\tremaining: 14.6s\n",
      "917:\tlearn: 0.0006567\ttotal: 2m 41s\tremaining: 14.4s\n",
      "918:\tlearn: 0.0006567\ttotal: 2m 41s\tremaining: 14.2s\n",
      "919:\tlearn: 0.0006559\ttotal: 2m 41s\tremaining: 14.1s\n",
      "920:\tlearn: 0.0006559\ttotal: 2m 41s\tremaining: 13.9s\n",
      "921:\tlearn: 0.0006559\ttotal: 2m 41s\tremaining: 13.7s\n",
      "922:\tlearn: 0.0006555\ttotal: 2m 42s\tremaining: 13.5s\n",
      "923:\tlearn: 0.0006554\ttotal: 2m 42s\tremaining: 13.3s\n",
      "924:\tlearn: 0.0006553\ttotal: 2m 42s\tremaining: 13.2s\n",
      "925:\tlearn: 0.0006553\ttotal: 2m 42s\tremaining: 13s\n",
      "926:\tlearn: 0.0006552\ttotal: 2m 42s\tremaining: 12.8s\n",
      "927:\tlearn: 0.0006550\ttotal: 2m 43s\tremaining: 12.6s\n",
      "928:\tlearn: 0.0006550\ttotal: 2m 43s\tremaining: 12.5s\n",
      "929:\tlearn: 0.0006550\ttotal: 2m 43s\tremaining: 12.3s\n",
      "930:\tlearn: 0.0006550\ttotal: 2m 43s\tremaining: 12.1s\n",
      "931:\tlearn: 0.0006550\ttotal: 2m 43s\tremaining: 11.9s\n",
      "932:\tlearn: 0.0006549\ttotal: 2m 43s\tremaining: 11.8s\n",
      "933:\tlearn: 0.0006549\ttotal: 2m 44s\tremaining: 11.6s\n",
      "934:\tlearn: 0.0006549\ttotal: 2m 44s\tremaining: 11.4s\n",
      "935:\tlearn: 0.0006548\ttotal: 2m 44s\tremaining: 11.2s\n",
      "936:\tlearn: 0.0006547\ttotal: 2m 44s\tremaining: 11.1s\n",
      "937:\tlearn: 0.0006547\ttotal: 2m 44s\tremaining: 10.9s\n",
      "938:\tlearn: 0.0006547\ttotal: 2m 44s\tremaining: 10.7s\n",
      "939:\tlearn: 0.0006545\ttotal: 2m 45s\tremaining: 10.5s\n",
      "940:\tlearn: 0.0006545\ttotal: 2m 45s\tremaining: 10.4s\n",
      "941:\tlearn: 0.0006542\ttotal: 2m 45s\tremaining: 10.2s\n",
      "942:\tlearn: 0.0006541\ttotal: 2m 45s\tremaining: 10s\n",
      "943:\tlearn: 0.0006540\ttotal: 2m 45s\tremaining: 9.84s\n",
      "944:\tlearn: 0.0006539\ttotal: 2m 46s\tremaining: 9.66s\n",
      "945:\tlearn: 0.0006539\ttotal: 2m 46s\tremaining: 9.49s\n",
      "946:\tlearn: 0.0006539\ttotal: 2m 46s\tremaining: 9.31s\n",
      "947:\tlearn: 0.0006539\ttotal: 2m 46s\tremaining: 9.13s\n",
      "948:\tlearn: 0.0006538\ttotal: 2m 46s\tremaining: 8.96s\n",
      "949:\tlearn: 0.0006506\ttotal: 2m 46s\tremaining: 8.78s\n",
      "950:\tlearn: 0.0006472\ttotal: 2m 47s\tremaining: 8.61s\n",
      "951:\tlearn: 0.0006470\ttotal: 2m 47s\tremaining: 8.43s\n",
      "952:\tlearn: 0.0006469\ttotal: 2m 47s\tremaining: 8.26s\n",
      "953:\tlearn: 0.0006468\ttotal: 2m 47s\tremaining: 8.08s\n",
      "954:\tlearn: 0.0006468\ttotal: 2m 47s\tremaining: 7.9s\n",
      "955:\tlearn: 0.0006468\ttotal: 2m 47s\tremaining: 7.73s\n",
      "956:\tlearn: 0.0006468\ttotal: 2m 48s\tremaining: 7.55s\n",
      "957:\tlearn: 0.0006468\ttotal: 2m 48s\tremaining: 7.38s\n",
      "958:\tlearn: 0.0006468\ttotal: 2m 48s\tremaining: 7.2s\n",
      "959:\tlearn: 0.0006468\ttotal: 2m 48s\tremaining: 7.03s\n",
      "960:\tlearn: 0.0006468\ttotal: 2m 48s\tremaining: 6.85s\n",
      "961:\tlearn: 0.0006468\ttotal: 2m 48s\tremaining: 6.67s\n",
      "962:\tlearn: 0.0006468\ttotal: 2m 49s\tremaining: 6.5s\n",
      "963:\tlearn: 0.0006467\ttotal: 2m 49s\tremaining: 6.32s\n",
      "964:\tlearn: 0.0006467\ttotal: 2m 49s\tremaining: 6.15s\n",
      "965:\tlearn: 0.0006467\ttotal: 2m 49s\tremaining: 5.97s\n",
      "966:\tlearn: 0.0006467\ttotal: 2m 49s\tremaining: 5.79s\n",
      "967:\tlearn: 0.0006467\ttotal: 2m 50s\tremaining: 5.62s\n",
      "968:\tlearn: 0.0006467\ttotal: 2m 50s\tremaining: 5.44s\n",
      "969:\tlearn: 0.0006467\ttotal: 2m 50s\tremaining: 5.27s\n",
      "970:\tlearn: 0.0006467\ttotal: 2m 50s\tremaining: 5.09s\n",
      "971:\tlearn: 0.0006464\ttotal: 2m 50s\tremaining: 4.92s\n",
      "972:\tlearn: 0.0006464\ttotal: 2m 50s\tremaining: 4.74s\n",
      "973:\tlearn: 0.0006461\ttotal: 2m 51s\tremaining: 4.57s\n",
      "974:\tlearn: 0.0006461\ttotal: 2m 51s\tremaining: 4.39s\n",
      "975:\tlearn: 0.0006460\ttotal: 2m 51s\tremaining: 4.21s\n",
      "976:\tlearn: 0.0006458\ttotal: 2m 51s\tremaining: 4.04s\n",
      "977:\tlearn: 0.0006457\ttotal: 2m 51s\tremaining: 3.86s\n",
      "978:\tlearn: 0.0006457\ttotal: 2m 51s\tremaining: 3.69s\n",
      "979:\tlearn: 0.0006457\ttotal: 2m 52s\tremaining: 3.51s\n",
      "980:\tlearn: 0.0006456\ttotal: 2m 52s\tremaining: 3.34s\n",
      "981:\tlearn: 0.0006446\ttotal: 2m 52s\tremaining: 3.16s\n",
      "982:\tlearn: 0.0006446\ttotal: 2m 52s\tremaining: 2.98s\n",
      "983:\tlearn: 0.0006441\ttotal: 2m 52s\tremaining: 2.81s\n",
      "984:\tlearn: 0.0006441\ttotal: 2m 52s\tremaining: 2.63s\n",
      "985:\tlearn: 0.0006439\ttotal: 2m 53s\tremaining: 2.46s\n",
      "986:\tlearn: 0.0006438\ttotal: 2m 53s\tremaining: 2.28s\n",
      "987:\tlearn: 0.0006438\ttotal: 2m 53s\tremaining: 2.11s\n",
      "988:\tlearn: 0.0006438\ttotal: 2m 53s\tremaining: 1.93s\n",
      "989:\tlearn: 0.0006438\ttotal: 2m 53s\tremaining: 1.76s\n",
      "990:\tlearn: 0.0006437\ttotal: 2m 54s\tremaining: 1.58s\n",
      "991:\tlearn: 0.0006419\ttotal: 2m 54s\tremaining: 1.41s\n",
      "992:\tlearn: 0.0006417\ttotal: 2m 54s\tremaining: 1.23s\n",
      "993:\tlearn: 0.0006417\ttotal: 2m 54s\tremaining: 1.05s\n",
      "994:\tlearn: 0.0006417\ttotal: 2m 54s\tremaining: 878ms\n",
      "995:\tlearn: 0.0006417\ttotal: 2m 54s\tremaining: 702ms\n",
      "996:\tlearn: 0.0006416\ttotal: 2m 55s\tremaining: 527ms\n",
      "997:\tlearn: 0.0006416\ttotal: 2m 55s\tremaining: 351ms\n",
      "998:\tlearn: 0.0006415\ttotal: 2m 55s\tremaining: 176ms\n",
      "999:\tlearn: 0.0006415\ttotal: 2m 55s\tremaining: 0us\n",
      "UPDRS: updrs_3\n",
      "Hyperparameters: {'learning_rate': 0.9738759584780804, 'depth': 4, 'l2_leaf_reg': 7.35671032601355, 'bagging_temperature': 1.1623592388074029, 'min_data_in_leaf': 4}\n",
      "\n",
      "\n",
      "0:\tlearn: 0.6158547\ttotal: 71.4ms\tremaining: 1m 11s\n",
      "1:\tlearn: 0.5478065\ttotal: 127ms\tremaining: 1m 3s\n",
      "2:\tlearn: 0.4827056\ttotal: 184ms\tremaining: 1m 1s\n",
      "3:\tlearn: 0.4428502\ttotal: 239ms\tremaining: 59.6s\n",
      "4:\tlearn: 0.4003580\ttotal: 295ms\tremaining: 58.7s\n",
      "5:\tlearn: 0.3448870\ttotal: 351ms\tremaining: 58.2s\n",
      "6:\tlearn: 0.3018729\ttotal: 407ms\tremaining: 57.7s\n",
      "7:\tlearn: 0.2413127\ttotal: 462ms\tremaining: 57.3s\n",
      "8:\tlearn: 0.1859560\ttotal: 518ms\tremaining: 57.1s\n",
      "9:\tlearn: 0.1570885\ttotal: 577ms\tremaining: 57.1s\n",
      "10:\tlearn: 0.1299770\ttotal: 633ms\tremaining: 56.9s\n",
      "11:\tlearn: 0.1158080\ttotal: 689ms\tremaining: 56.7s\n",
      "12:\tlearn: 0.0894615\ttotal: 746ms\tremaining: 56.7s\n",
      "13:\tlearn: 0.0843509\ttotal: 802ms\tremaining: 56.5s\n",
      "14:\tlearn: 0.0711015\ttotal: 859ms\tremaining: 56.4s\n",
      "15:\tlearn: 0.0606355\ttotal: 916ms\tremaining: 56.3s\n",
      "16:\tlearn: 0.0524577\ttotal: 972ms\tremaining: 56.2s\n",
      "17:\tlearn: 0.0445957\ttotal: 1.03s\tremaining: 56.2s\n",
      "18:\tlearn: 0.0434090\ttotal: 1.08s\tremaining: 56.1s\n",
      "19:\tlearn: 0.0402865\ttotal: 1.14s\tremaining: 56s\n",
      "20:\tlearn: 0.0352074\ttotal: 1.2s\tremaining: 55.9s\n",
      "21:\tlearn: 0.0300567\ttotal: 1.25s\tremaining: 55.8s\n",
      "22:\tlearn: 0.0278218\ttotal: 1.31s\tremaining: 55.7s\n",
      "23:\tlearn: 0.0236181\ttotal: 1.37s\tremaining: 55.7s\n",
      "24:\tlearn: 0.0212953\ttotal: 1.43s\tremaining: 55.6s\n",
      "25:\tlearn: 0.0191352\ttotal: 1.48s\tremaining: 55.6s\n",
      "26:\tlearn: 0.0175000\ttotal: 1.54s\tremaining: 55.6s\n",
      "27:\tlearn: 0.0157376\ttotal: 1.6s\tremaining: 55.5s\n",
      "28:\tlearn: 0.0146380\ttotal: 1.66s\tremaining: 55.5s\n",
      "29:\tlearn: 0.0142277\ttotal: 1.71s\tremaining: 55.4s\n",
      "30:\tlearn: 0.0133233\ttotal: 1.77s\tremaining: 55.3s\n",
      "31:\tlearn: 0.0120135\ttotal: 1.83s\tremaining: 55.3s\n",
      "32:\tlearn: 0.0108360\ttotal: 1.88s\tremaining: 55.2s\n",
      "33:\tlearn: 0.0101830\ttotal: 1.94s\tremaining: 55.2s\n",
      "34:\tlearn: 0.0095924\ttotal: 2s\tremaining: 55.1s\n",
      "35:\tlearn: 0.0089019\ttotal: 2.05s\tremaining: 55s\n",
      "36:\tlearn: 0.0084855\ttotal: 2.11s\tremaining: 54.9s\n",
      "37:\tlearn: 0.0081623\ttotal: 2.17s\tremaining: 54.9s\n",
      "38:\tlearn: 0.0078980\ttotal: 2.22s\tremaining: 54.8s\n",
      "39:\tlearn: 0.0077761\ttotal: 2.28s\tremaining: 54.7s\n",
      "40:\tlearn: 0.0074747\ttotal: 2.34s\tremaining: 54.7s\n",
      "41:\tlearn: 0.0072637\ttotal: 2.39s\tremaining: 54.6s\n",
      "42:\tlearn: 0.0068379\ttotal: 2.45s\tremaining: 54.5s\n",
      "43:\tlearn: 0.0065311\ttotal: 2.5s\tremaining: 54.4s\n",
      "44:\tlearn: 0.0062614\ttotal: 2.56s\tremaining: 54.4s\n",
      "45:\tlearn: 0.0058135\ttotal: 2.62s\tremaining: 54.3s\n",
      "46:\tlearn: 0.0055344\ttotal: 2.67s\tremaining: 54.2s\n",
      "47:\tlearn: 0.0053985\ttotal: 2.73s\tremaining: 54.2s\n",
      "48:\tlearn: 0.0051875\ttotal: 2.79s\tremaining: 54.1s\n",
      "49:\tlearn: 0.0049903\ttotal: 2.85s\tremaining: 54.1s\n",
      "50:\tlearn: 0.0048161\ttotal: 2.9s\tremaining: 54s\n",
      "51:\tlearn: 0.0045395\ttotal: 2.96s\tremaining: 54s\n",
      "52:\tlearn: 0.0044281\ttotal: 3.02s\tremaining: 53.9s\n",
      "53:\tlearn: 0.0043027\ttotal: 3.07s\tremaining: 53.9s\n",
      "54:\tlearn: 0.0042132\ttotal: 3.13s\tremaining: 53.8s\n",
      "55:\tlearn: 0.0041453\ttotal: 3.19s\tremaining: 53.7s\n",
      "56:\tlearn: 0.0040974\ttotal: 3.24s\tremaining: 53.6s\n",
      "57:\tlearn: 0.0039877\ttotal: 3.3s\tremaining: 53.6s\n",
      "58:\tlearn: 0.0038427\ttotal: 3.36s\tremaining: 53.5s\n",
      "59:\tlearn: 0.0037196\ttotal: 3.41s\tremaining: 53.5s\n",
      "60:\tlearn: 0.0036505\ttotal: 3.47s\tremaining: 53.4s\n",
      "61:\tlearn: 0.0035363\ttotal: 3.52s\tremaining: 53.3s\n",
      "62:\tlearn: 0.0034653\ttotal: 3.58s\tremaining: 53.3s\n",
      "63:\tlearn: 0.0034048\ttotal: 3.64s\tremaining: 53.3s\n",
      "64:\tlearn: 0.0033101\ttotal: 3.7s\tremaining: 53.2s\n",
      "65:\tlearn: 0.0032134\ttotal: 3.76s\tremaining: 53.2s\n",
      "66:\tlearn: 0.0031467\ttotal: 3.81s\tremaining: 53.1s\n",
      "67:\tlearn: 0.0030709\ttotal: 3.87s\tremaining: 53s\n",
      "68:\tlearn: 0.0030401\ttotal: 3.92s\tremaining: 53s\n",
      "69:\tlearn: 0.0029465\ttotal: 3.98s\tremaining: 52.9s\n",
      "70:\tlearn: 0.0028368\ttotal: 4.04s\tremaining: 52.9s\n",
      "71:\tlearn: 0.0027717\ttotal: 4.1s\tremaining: 52.8s\n",
      "72:\tlearn: 0.0026789\ttotal: 4.15s\tremaining: 52.7s\n",
      "73:\tlearn: 0.0026298\ttotal: 4.21s\tremaining: 52.7s\n",
      "74:\tlearn: 0.0025772\ttotal: 4.26s\tremaining: 52.6s\n",
      "75:\tlearn: 0.0025308\ttotal: 4.32s\tremaining: 52.5s\n",
      "76:\tlearn: 0.0025299\ttotal: 4.38s\tremaining: 52.5s\n",
      "77:\tlearn: 0.0025293\ttotal: 4.43s\tremaining: 52.4s\n",
      "78:\tlearn: 0.0025272\ttotal: 4.49s\tremaining: 52.4s\n",
      "79:\tlearn: 0.0024591\ttotal: 4.55s\tremaining: 52.3s\n",
      "80:\tlearn: 0.0024345\ttotal: 4.61s\tremaining: 52.3s\n",
      "81:\tlearn: 0.0023921\ttotal: 4.66s\tremaining: 52.2s\n",
      "82:\tlearn: 0.0023466\ttotal: 4.72s\tremaining: 52.1s\n",
      "83:\tlearn: 0.0023156\ttotal: 4.77s\tremaining: 52.1s\n",
      "84:\tlearn: 0.0022600\ttotal: 4.83s\tremaining: 52s\n",
      "85:\tlearn: 0.0022597\ttotal: 4.89s\tremaining: 51.9s\n",
      "86:\tlearn: 0.0022596\ttotal: 4.94s\tremaining: 51.9s\n",
      "87:\tlearn: 0.0022587\ttotal: 5s\tremaining: 51.8s\n",
      "88:\tlearn: 0.0021878\ttotal: 5.06s\tremaining: 51.8s\n",
      "89:\tlearn: 0.0021876\ttotal: 5.12s\tremaining: 51.7s\n",
      "90:\tlearn: 0.0021638\ttotal: 5.17s\tremaining: 51.7s\n",
      "91:\tlearn: 0.0021447\ttotal: 5.23s\tremaining: 51.6s\n",
      "92:\tlearn: 0.0021072\ttotal: 5.28s\tremaining: 51.5s\n",
      "93:\tlearn: 0.0021067\ttotal: 5.34s\tremaining: 51.5s\n",
      "94:\tlearn: 0.0021065\ttotal: 5.4s\tremaining: 51.4s\n",
      "95:\tlearn: 0.0021065\ttotal: 5.45s\tremaining: 51.3s\n",
      "96:\tlearn: 0.0021064\ttotal: 5.52s\tremaining: 51.4s\n",
      "97:\tlearn: 0.0020815\ttotal: 5.57s\tremaining: 51.3s\n",
      "98:\tlearn: 0.0020806\ttotal: 5.63s\tremaining: 51.2s\n",
      "99:\tlearn: 0.0020806\ttotal: 5.69s\tremaining: 51.2s\n",
      "100:\tlearn: 0.0020534\ttotal: 5.74s\tremaining: 51.1s\n",
      "101:\tlearn: 0.0020531\ttotal: 5.8s\tremaining: 51.1s\n",
      "102:\tlearn: 0.0020530\ttotal: 5.86s\tremaining: 51s\n",
      "103:\tlearn: 0.0020254\ttotal: 5.91s\tremaining: 50.9s\n",
      "104:\tlearn: 0.0020178\ttotal: 5.97s\tremaining: 50.9s\n",
      "105:\tlearn: 0.0020177\ttotal: 6.02s\tremaining: 50.8s\n",
      "106:\tlearn: 0.0020165\ttotal: 6.08s\tremaining: 50.7s\n",
      "107:\tlearn: 0.0020165\ttotal: 6.13s\tremaining: 50.7s\n",
      "108:\tlearn: 0.0020029\ttotal: 6.19s\tremaining: 50.6s\n",
      "109:\tlearn: 0.0020027\ttotal: 6.25s\tremaining: 50.6s\n",
      "110:\tlearn: 0.0019772\ttotal: 6.3s\tremaining: 50.5s\n",
      "111:\tlearn: 0.0019771\ttotal: 6.36s\tremaining: 50.4s\n",
      "112:\tlearn: 0.0019627\ttotal: 6.42s\tremaining: 50.4s\n",
      "113:\tlearn: 0.0019141\ttotal: 6.47s\tremaining: 50.3s\n",
      "114:\tlearn: 0.0019138\ttotal: 6.53s\tremaining: 50.2s\n",
      "115:\tlearn: 0.0019003\ttotal: 6.58s\tremaining: 50.2s\n",
      "116:\tlearn: 0.0018999\ttotal: 6.64s\tremaining: 50.1s\n",
      "117:\tlearn: 0.0018997\ttotal: 6.7s\tremaining: 50.1s\n",
      "118:\tlearn: 0.0018537\ttotal: 6.75s\tremaining: 50s\n",
      "119:\tlearn: 0.0018536\ttotal: 6.81s\tremaining: 49.9s\n",
      "120:\tlearn: 0.0018126\ttotal: 6.87s\tremaining: 49.9s\n",
      "121:\tlearn: 0.0018126\ttotal: 6.92s\tremaining: 49.8s\n",
      "122:\tlearn: 0.0018125\ttotal: 6.98s\tremaining: 49.8s\n",
      "123:\tlearn: 0.0018123\ttotal: 7.03s\tremaining: 49.7s\n",
      "124:\tlearn: 0.0018009\ttotal: 7.09s\tremaining: 49.6s\n",
      "125:\tlearn: 0.0018009\ttotal: 7.15s\tremaining: 49.6s\n",
      "126:\tlearn: 0.0017939\ttotal: 7.2s\tremaining: 49.5s\n",
      "127:\tlearn: 0.0017939\ttotal: 7.26s\tremaining: 49.4s\n",
      "128:\tlearn: 0.0017937\ttotal: 7.31s\tremaining: 49.4s\n",
      "129:\tlearn: 0.0017931\ttotal: 7.37s\tremaining: 49.3s\n",
      "130:\tlearn: 0.0017931\ttotal: 7.42s\tremaining: 49.3s\n",
      "131:\tlearn: 0.0017927\ttotal: 7.48s\tremaining: 49.2s\n",
      "132:\tlearn: 0.0017925\ttotal: 7.54s\tremaining: 49.1s\n",
      "133:\tlearn: 0.0017834\ttotal: 7.59s\tremaining: 49.1s\n",
      "134:\tlearn: 0.0017741\ttotal: 7.65s\tremaining: 49s\n",
      "135:\tlearn: 0.0017740\ttotal: 7.7s\tremaining: 48.9s\n",
      "136:\tlearn: 0.0017740\ttotal: 7.76s\tremaining: 48.9s\n",
      "137:\tlearn: 0.0017621\ttotal: 7.82s\tremaining: 48.8s\n",
      "138:\tlearn: 0.0017620\ttotal: 7.87s\tremaining: 48.8s\n",
      "139:\tlearn: 0.0017614\ttotal: 7.93s\tremaining: 48.7s\n",
      "140:\tlearn: 0.0017610\ttotal: 7.98s\tremaining: 48.6s\n",
      "141:\tlearn: 0.0017466\ttotal: 8.04s\tremaining: 48.6s\n",
      "142:\tlearn: 0.0017464\ttotal: 8.1s\tremaining: 48.5s\n",
      "143:\tlearn: 0.0017355\ttotal: 8.15s\tremaining: 48.5s\n",
      "144:\tlearn: 0.0017355\ttotal: 8.21s\tremaining: 48.4s\n",
      "145:\tlearn: 0.0017353\ttotal: 8.27s\tremaining: 48.4s\n",
      "146:\tlearn: 0.0017347\ttotal: 8.32s\tremaining: 48.3s\n",
      "147:\tlearn: 0.0017040\ttotal: 8.38s\tremaining: 48.2s\n",
      "148:\tlearn: 0.0016703\ttotal: 8.44s\tremaining: 48.2s\n",
      "149:\tlearn: 0.0016511\ttotal: 8.49s\tremaining: 48.1s\n",
      "150:\tlearn: 0.0016369\ttotal: 8.55s\tremaining: 48.1s\n",
      "151:\tlearn: 0.0016264\ttotal: 8.61s\tremaining: 48s\n",
      "152:\tlearn: 0.0016263\ttotal: 8.66s\tremaining: 48s\n",
      "153:\tlearn: 0.0016169\ttotal: 8.72s\tremaining: 47.9s\n",
      "154:\tlearn: 0.0015930\ttotal: 8.77s\tremaining: 47.8s\n",
      "155:\tlearn: 0.0015929\ttotal: 8.83s\tremaining: 47.8s\n",
      "156:\tlearn: 0.0015877\ttotal: 8.89s\tremaining: 47.7s\n",
      "157:\tlearn: 0.0015873\ttotal: 8.94s\tremaining: 47.7s\n",
      "158:\tlearn: 0.0015873\ttotal: 9s\tremaining: 47.6s\n",
      "159:\tlearn: 0.0015873\ttotal: 9.06s\tremaining: 47.5s\n",
      "160:\tlearn: 0.0015872\ttotal: 9.11s\tremaining: 47.5s\n",
      "161:\tlearn: 0.0015869\ttotal: 9.17s\tremaining: 47.4s\n",
      "162:\tlearn: 0.0015869\ttotal: 9.23s\tremaining: 47.4s\n",
      "163:\tlearn: 0.0015867\ttotal: 9.28s\tremaining: 47.3s\n",
      "164:\tlearn: 0.0015867\ttotal: 9.34s\tremaining: 47.3s\n",
      "165:\tlearn: 0.0015865\ttotal: 9.4s\tremaining: 47.2s\n",
      "166:\tlearn: 0.0015610\ttotal: 9.45s\tremaining: 47.2s\n",
      "167:\tlearn: 0.0015609\ttotal: 9.51s\tremaining: 47.1s\n",
      "168:\tlearn: 0.0015609\ttotal: 9.57s\tremaining: 47s\n",
      "169:\tlearn: 0.0015606\ttotal: 9.63s\tremaining: 47s\n",
      "170:\tlearn: 0.0015417\ttotal: 9.69s\tremaining: 47s\n",
      "171:\tlearn: 0.0015414\ttotal: 9.75s\tremaining: 46.9s\n",
      "172:\tlearn: 0.0015188\ttotal: 9.81s\tremaining: 46.9s\n",
      "173:\tlearn: 0.0015187\ttotal: 9.87s\tremaining: 46.8s\n",
      "174:\tlearn: 0.0015184\ttotal: 9.92s\tremaining: 46.8s\n",
      "175:\tlearn: 0.0015125\ttotal: 9.98s\tremaining: 46.7s\n",
      "176:\tlearn: 0.0015125\ttotal: 10s\tremaining: 46.7s\n",
      "177:\tlearn: 0.0015123\ttotal: 10.1s\tremaining: 46.6s\n",
      "178:\tlearn: 0.0015117\ttotal: 10.1s\tremaining: 46.5s\n",
      "179:\tlearn: 0.0015116\ttotal: 10.2s\tremaining: 46.5s\n",
      "180:\tlearn: 0.0015060\ttotal: 10.3s\tremaining: 46.4s\n",
      "181:\tlearn: 0.0015048\ttotal: 10.3s\tremaining: 46.4s\n",
      "182:\tlearn: 0.0015047\ttotal: 10.4s\tremaining: 46.3s\n",
      "183:\tlearn: 0.0015047\ttotal: 10.4s\tremaining: 46.3s\n",
      "184:\tlearn: 0.0014868\ttotal: 10.5s\tremaining: 46.2s\n",
      "185:\tlearn: 0.0014751\ttotal: 10.5s\tremaining: 46.1s\n",
      "186:\tlearn: 0.0014685\ttotal: 10.6s\tremaining: 46.1s\n",
      "187:\tlearn: 0.0014685\ttotal: 10.7s\tremaining: 46s\n",
      "188:\tlearn: 0.0014683\ttotal: 10.7s\tremaining: 46s\n",
      "189:\tlearn: 0.0014681\ttotal: 10.8s\tremaining: 45.9s\n",
      "190:\tlearn: 0.0014654\ttotal: 10.8s\tremaining: 45.8s\n",
      "191:\tlearn: 0.0014650\ttotal: 10.9s\tremaining: 45.8s\n",
      "192:\tlearn: 0.0014650\ttotal: 10.9s\tremaining: 45.7s\n",
      "193:\tlearn: 0.0014649\ttotal: 11s\tremaining: 45.7s\n",
      "194:\tlearn: 0.0014647\ttotal: 11s\tremaining: 45.6s\n",
      "195:\tlearn: 0.0014645\ttotal: 11.1s\tremaining: 45.5s\n",
      "196:\tlearn: 0.0014644\ttotal: 11.2s\tremaining: 45.5s\n",
      "197:\tlearn: 0.0014506\ttotal: 11.2s\tremaining: 45.4s\n",
      "198:\tlearn: 0.0014505\ttotal: 11.3s\tremaining: 45.4s\n",
      "199:\tlearn: 0.0014502\ttotal: 11.3s\tremaining: 45.3s\n",
      "200:\tlearn: 0.0014502\ttotal: 11.4s\tremaining: 45.2s\n",
      "201:\tlearn: 0.0014500\ttotal: 11.4s\tremaining: 45.2s\n",
      "202:\tlearn: 0.0014500\ttotal: 11.5s\tremaining: 45.2s\n",
      "203:\tlearn: 0.0014499\ttotal: 11.6s\tremaining: 45.1s\n",
      "204:\tlearn: 0.0014498\ttotal: 11.6s\tremaining: 45.1s\n",
      "205:\tlearn: 0.0014497\ttotal: 11.7s\tremaining: 45s\n",
      "206:\tlearn: 0.0014497\ttotal: 11.7s\tremaining: 45s\n",
      "207:\tlearn: 0.0014481\ttotal: 11.8s\tremaining: 44.9s\n",
      "208:\tlearn: 0.0014477\ttotal: 11.8s\tremaining: 44.8s\n",
      "209:\tlearn: 0.0014465\ttotal: 11.9s\tremaining: 44.8s\n",
      "210:\tlearn: 0.0014322\ttotal: 12s\tremaining: 44.7s\n",
      "211:\tlearn: 0.0014322\ttotal: 12s\tremaining: 44.7s\n",
      "212:\tlearn: 0.0014322\ttotal: 12.1s\tremaining: 44.6s\n",
      "213:\tlearn: 0.0014321\ttotal: 12.1s\tremaining: 44.6s\n",
      "214:\tlearn: 0.0014320\ttotal: 12.2s\tremaining: 44.5s\n",
      "215:\tlearn: 0.0014319\ttotal: 12.2s\tremaining: 44.4s\n",
      "216:\tlearn: 0.0014304\ttotal: 12.3s\tremaining: 44.4s\n",
      "217:\tlearn: 0.0014304\ttotal: 12.4s\tremaining: 44.3s\n",
      "218:\tlearn: 0.0014290\ttotal: 12.4s\tremaining: 44.3s\n",
      "219:\tlearn: 0.0014186\ttotal: 12.5s\tremaining: 44.2s\n",
      "220:\tlearn: 0.0014186\ttotal: 12.5s\tremaining: 44.1s\n",
      "221:\tlearn: 0.0014095\ttotal: 12.6s\tremaining: 44.1s\n",
      "222:\tlearn: 0.0014091\ttotal: 12.6s\tremaining: 44s\n",
      "223:\tlearn: 0.0014090\ttotal: 12.7s\tremaining: 44s\n",
      "224:\tlearn: 0.0014089\ttotal: 12.7s\tremaining: 43.9s\n",
      "225:\tlearn: 0.0014089\ttotal: 12.8s\tremaining: 43.9s\n",
      "226:\tlearn: 0.0014087\ttotal: 12.9s\tremaining: 43.8s\n",
      "227:\tlearn: 0.0014052\ttotal: 12.9s\tremaining: 43.7s\n",
      "228:\tlearn: 0.0014045\ttotal: 13s\tremaining: 43.7s\n",
      "229:\tlearn: 0.0014043\ttotal: 13s\tremaining: 43.6s\n",
      "230:\tlearn: 0.0014043\ttotal: 13.1s\tremaining: 43.6s\n",
      "231:\tlearn: 0.0014018\ttotal: 13.1s\tremaining: 43.5s\n",
      "232:\tlearn: 0.0014010\ttotal: 13.2s\tremaining: 43.4s\n",
      "233:\tlearn: 0.0014000\ttotal: 13.3s\tremaining: 43.4s\n",
      "234:\tlearn: 0.0013996\ttotal: 13.3s\tremaining: 43.3s\n",
      "235:\tlearn: 0.0013931\ttotal: 13.4s\tremaining: 43.3s\n",
      "236:\tlearn: 0.0013930\ttotal: 13.4s\tremaining: 43.2s\n",
      "237:\tlearn: 0.0013928\ttotal: 13.5s\tremaining: 43.2s\n",
      "238:\tlearn: 0.0013928\ttotal: 13.5s\tremaining: 43.1s\n",
      "239:\tlearn: 0.0013928\ttotal: 13.6s\tremaining: 43s\n",
      "240:\tlearn: 0.0013928\ttotal: 13.7s\tremaining: 43s\n",
      "241:\tlearn: 0.0013927\ttotal: 13.7s\tremaining: 42.9s\n",
      "242:\tlearn: 0.0013921\ttotal: 13.8s\tremaining: 42.9s\n",
      "243:\tlearn: 0.0013920\ttotal: 13.8s\tremaining: 42.8s\n",
      "244:\tlearn: 0.0013920\ttotal: 13.9s\tremaining: 42.8s\n",
      "245:\tlearn: 0.0013872\ttotal: 13.9s\tremaining: 42.7s\n",
      "246:\tlearn: 0.0013870\ttotal: 14s\tremaining: 42.7s\n",
      "247:\tlearn: 0.0013870\ttotal: 14.1s\tremaining: 42.6s\n",
      "248:\tlearn: 0.0013870\ttotal: 14.1s\tremaining: 42.6s\n",
      "249:\tlearn: 0.0013870\ttotal: 14.2s\tremaining: 42.5s\n",
      "250:\tlearn: 0.0013870\ttotal: 14.2s\tremaining: 42.4s\n",
      "251:\tlearn: 0.0013869\ttotal: 14.3s\tremaining: 42.4s\n",
      "252:\tlearn: 0.0013863\ttotal: 14.3s\tremaining: 42.3s\n",
      "253:\tlearn: 0.0013858\ttotal: 14.4s\tremaining: 42.3s\n",
      "254:\tlearn: 0.0013854\ttotal: 14.4s\tremaining: 42.2s\n",
      "255:\tlearn: 0.0013853\ttotal: 14.5s\tremaining: 42.2s\n",
      "256:\tlearn: 0.0013853\ttotal: 14.6s\tremaining: 42.1s\n",
      "257:\tlearn: 0.0013852\ttotal: 14.6s\tremaining: 42s\n",
      "258:\tlearn: 0.0013852\ttotal: 14.7s\tremaining: 42s\n",
      "259:\tlearn: 0.0013852\ttotal: 14.7s\tremaining: 41.9s\n",
      "260:\tlearn: 0.0013847\ttotal: 14.8s\tremaining: 41.9s\n",
      "261:\tlearn: 0.0013847\ttotal: 14.8s\tremaining: 41.8s\n",
      "262:\tlearn: 0.0013846\ttotal: 14.9s\tremaining: 41.8s\n",
      "263:\tlearn: 0.0013846\ttotal: 15s\tremaining: 41.7s\n",
      "264:\tlearn: 0.0013845\ttotal: 15s\tremaining: 41.6s\n",
      "265:\tlearn: 0.0013844\ttotal: 15.1s\tremaining: 41.6s\n",
      "266:\tlearn: 0.0013844\ttotal: 15.1s\tremaining: 41.5s\n",
      "267:\tlearn: 0.0013843\ttotal: 15.2s\tremaining: 41.5s\n",
      "268:\tlearn: 0.0013842\ttotal: 15.2s\tremaining: 41.4s\n",
      "269:\tlearn: 0.0013840\ttotal: 15.3s\tremaining: 41.3s\n",
      "270:\tlearn: 0.0013827\ttotal: 15.3s\tremaining: 41.3s\n",
      "271:\tlearn: 0.0013826\ttotal: 15.4s\tremaining: 41.2s\n",
      "272:\tlearn: 0.0013826\ttotal: 15.5s\tremaining: 41.2s\n",
      "273:\tlearn: 0.0013826\ttotal: 15.5s\tremaining: 41.1s\n",
      "274:\tlearn: 0.0013824\ttotal: 15.6s\tremaining: 41.1s\n",
      "275:\tlearn: 0.0013824\ttotal: 15.6s\tremaining: 41s\n",
      "276:\tlearn: 0.0013824\ttotal: 15.7s\tremaining: 40.9s\n",
      "277:\tlearn: 0.0013824\ttotal: 15.7s\tremaining: 40.9s\n",
      "278:\tlearn: 0.0013823\ttotal: 15.8s\tremaining: 40.8s\n",
      "279:\tlearn: 0.0013823\ttotal: 15.9s\tremaining: 40.8s\n",
      "280:\tlearn: 0.0013823\ttotal: 15.9s\tremaining: 40.7s\n",
      "281:\tlearn: 0.0013631\ttotal: 16s\tremaining: 40.7s\n",
      "282:\tlearn: 0.0013626\ttotal: 16s\tremaining: 40.6s\n",
      "283:\tlearn: 0.0013623\ttotal: 16.1s\tremaining: 40.6s\n",
      "284:\tlearn: 0.0013622\ttotal: 16.1s\tremaining: 40.5s\n",
      "285:\tlearn: 0.0013622\ttotal: 16.2s\tremaining: 40.4s\n",
      "286:\tlearn: 0.0013619\ttotal: 16.3s\tremaining: 40.4s\n",
      "287:\tlearn: 0.0013618\ttotal: 16.3s\tremaining: 40.3s\n",
      "288:\tlearn: 0.0013618\ttotal: 16.4s\tremaining: 40.3s\n",
      "289:\tlearn: 0.0013618\ttotal: 16.4s\tremaining: 40.2s\n",
      "290:\tlearn: 0.0013591\ttotal: 16.5s\tremaining: 40.1s\n",
      "291:\tlearn: 0.0013591\ttotal: 16.5s\tremaining: 40.1s\n",
      "292:\tlearn: 0.0013589\ttotal: 16.6s\tremaining: 40s\n",
      "293:\tlearn: 0.0013587\ttotal: 16.6s\tremaining: 40s\n",
      "294:\tlearn: 0.0013586\ttotal: 16.7s\tremaining: 39.9s\n",
      "295:\tlearn: 0.0013585\ttotal: 16.8s\tremaining: 39.9s\n",
      "296:\tlearn: 0.0013581\ttotal: 16.8s\tremaining: 39.8s\n",
      "297:\tlearn: 0.0013578\ttotal: 16.9s\tremaining: 39.7s\n",
      "298:\tlearn: 0.0013571\ttotal: 16.9s\tremaining: 39.7s\n",
      "299:\tlearn: 0.0013569\ttotal: 17s\tremaining: 39.6s\n",
      "300:\tlearn: 0.0013569\ttotal: 17s\tremaining: 39.6s\n",
      "301:\tlearn: 0.0013569\ttotal: 17.1s\tremaining: 39.5s\n",
      "302:\tlearn: 0.0013568\ttotal: 17.2s\tremaining: 39.5s\n",
      "303:\tlearn: 0.0013568\ttotal: 17.2s\tremaining: 39.4s\n",
      "304:\tlearn: 0.0013566\ttotal: 17.3s\tremaining: 39.3s\n",
      "305:\tlearn: 0.0013566\ttotal: 17.3s\tremaining: 39.3s\n",
      "306:\tlearn: 0.0013565\ttotal: 17.4s\tremaining: 39.2s\n",
      "307:\tlearn: 0.0013565\ttotal: 17.4s\tremaining: 39.2s\n",
      "308:\tlearn: 0.0013564\ttotal: 17.5s\tremaining: 39.1s\n",
      "309:\tlearn: 0.0013563\ttotal: 17.5s\tremaining: 39.1s\n",
      "310:\tlearn: 0.0013561\ttotal: 17.6s\tremaining: 39s\n",
      "311:\tlearn: 0.0013530\ttotal: 17.7s\tremaining: 38.9s\n",
      "312:\tlearn: 0.0013530\ttotal: 17.7s\tremaining: 38.9s\n",
      "313:\tlearn: 0.0013500\ttotal: 17.8s\tremaining: 38.8s\n",
      "314:\tlearn: 0.0013499\ttotal: 17.8s\tremaining: 38.8s\n",
      "315:\tlearn: 0.0013498\ttotal: 17.9s\tremaining: 38.7s\n",
      "316:\tlearn: 0.0013497\ttotal: 17.9s\tremaining: 38.7s\n",
      "317:\tlearn: 0.0013497\ttotal: 18s\tremaining: 38.6s\n",
      "318:\tlearn: 0.0013496\ttotal: 18.1s\tremaining: 38.5s\n",
      "319:\tlearn: 0.0013495\ttotal: 18.1s\tremaining: 38.5s\n",
      "320:\tlearn: 0.0013495\ttotal: 18.2s\tremaining: 38.4s\n",
      "321:\tlearn: 0.0013493\ttotal: 18.2s\tremaining: 38.4s\n",
      "322:\tlearn: 0.0013493\ttotal: 18.3s\tremaining: 38.3s\n",
      "323:\tlearn: 0.0013443\ttotal: 18.3s\tremaining: 38.2s\n",
      "324:\tlearn: 0.0013441\ttotal: 18.4s\tremaining: 38.2s\n",
      "325:\tlearn: 0.0013393\ttotal: 18.4s\tremaining: 38.1s\n",
      "326:\tlearn: 0.0013378\ttotal: 18.5s\tremaining: 38.1s\n",
      "327:\tlearn: 0.0013378\ttotal: 18.6s\tremaining: 38s\n",
      "328:\tlearn: 0.0013377\ttotal: 18.6s\tremaining: 38s\n",
      "329:\tlearn: 0.0013376\ttotal: 18.7s\tremaining: 37.9s\n",
      "330:\tlearn: 0.0013376\ttotal: 18.7s\tremaining: 37.8s\n",
      "331:\tlearn: 0.0013372\ttotal: 18.8s\tremaining: 37.8s\n",
      "332:\tlearn: 0.0013371\ttotal: 18.8s\tremaining: 37.7s\n",
      "333:\tlearn: 0.0013366\ttotal: 18.9s\tremaining: 37.7s\n",
      "334:\tlearn: 0.0013193\ttotal: 18.9s\tremaining: 37.6s\n",
      "335:\tlearn: 0.0013190\ttotal: 19s\tremaining: 37.6s\n",
      "336:\tlearn: 0.0013188\ttotal: 19.1s\tremaining: 37.5s\n",
      "337:\tlearn: 0.0013186\ttotal: 19.1s\tremaining: 37.4s\n",
      "338:\tlearn: 0.0013183\ttotal: 19.2s\tremaining: 37.4s\n",
      "339:\tlearn: 0.0013181\ttotal: 19.2s\tremaining: 37.3s\n",
      "340:\tlearn: 0.0013181\ttotal: 19.3s\tremaining: 37.3s\n",
      "341:\tlearn: 0.0013179\ttotal: 19.3s\tremaining: 37.2s\n",
      "342:\tlearn: 0.0013175\ttotal: 19.4s\tremaining: 37.2s\n",
      "343:\tlearn: 0.0013175\ttotal: 19.5s\tremaining: 37.1s\n",
      "344:\tlearn: 0.0013171\ttotal: 19.5s\tremaining: 37s\n",
      "345:\tlearn: 0.0013171\ttotal: 19.6s\tremaining: 37s\n",
      "346:\tlearn: 0.0013170\ttotal: 19.6s\tremaining: 36.9s\n",
      "347:\tlearn: 0.0013170\ttotal: 19.7s\tremaining: 36.9s\n",
      "348:\tlearn: 0.0013170\ttotal: 19.7s\tremaining: 36.8s\n",
      "349:\tlearn: 0.0013169\ttotal: 19.8s\tremaining: 36.8s\n",
      "350:\tlearn: 0.0013160\ttotal: 19.8s\tremaining: 36.7s\n",
      "351:\tlearn: 0.0013157\ttotal: 19.9s\tremaining: 36.6s\n",
      "352:\tlearn: 0.0013156\ttotal: 20s\tremaining: 36.6s\n",
      "353:\tlearn: 0.0013154\ttotal: 20s\tremaining: 36.5s\n",
      "354:\tlearn: 0.0013153\ttotal: 20.1s\tremaining: 36.5s\n",
      "355:\tlearn: 0.0013148\ttotal: 20.1s\tremaining: 36.4s\n",
      "356:\tlearn: 0.0013148\ttotal: 20.2s\tremaining: 36.4s\n",
      "357:\tlearn: 0.0013148\ttotal: 20.2s\tremaining: 36.3s\n",
      "358:\tlearn: 0.0013141\ttotal: 20.3s\tremaining: 36.2s\n",
      "359:\tlearn: 0.0013047\ttotal: 20.4s\tremaining: 36.2s\n",
      "360:\tlearn: 0.0013047\ttotal: 20.4s\tremaining: 36.1s\n",
      "361:\tlearn: 0.0013044\ttotal: 20.5s\tremaining: 36.1s\n",
      "362:\tlearn: 0.0013044\ttotal: 20.5s\tremaining: 36s\n",
      "363:\tlearn: 0.0013041\ttotal: 20.6s\tremaining: 36s\n",
      "364:\tlearn: 0.0013041\ttotal: 20.6s\tremaining: 35.9s\n",
      "365:\tlearn: 0.0013039\ttotal: 20.7s\tremaining: 35.8s\n",
      "366:\tlearn: 0.0013039\ttotal: 20.8s\tremaining: 35.8s\n",
      "367:\tlearn: 0.0013033\ttotal: 20.8s\tremaining: 35.7s\n",
      "368:\tlearn: 0.0013032\ttotal: 20.9s\tremaining: 35.7s\n",
      "369:\tlearn: 0.0013032\ttotal: 20.9s\tremaining: 35.6s\n",
      "370:\tlearn: 0.0013032\ttotal: 21s\tremaining: 35.6s\n",
      "371:\tlearn: 0.0013031\ttotal: 21s\tremaining: 35.5s\n",
      "372:\tlearn: 0.0013029\ttotal: 21.1s\tremaining: 35.4s\n",
      "373:\tlearn: 0.0013029\ttotal: 21.1s\tremaining: 35.4s\n",
      "374:\tlearn: 0.0013028\ttotal: 21.2s\tremaining: 35.3s\n",
      "375:\tlearn: 0.0013025\ttotal: 21.3s\tremaining: 35.3s\n",
      "376:\tlearn: 0.0013023\ttotal: 21.3s\tremaining: 35.2s\n",
      "377:\tlearn: 0.0013021\ttotal: 21.4s\tremaining: 35.2s\n",
      "378:\tlearn: 0.0013020\ttotal: 21.4s\tremaining: 35.1s\n",
      "379:\tlearn: 0.0013019\ttotal: 21.5s\tremaining: 35s\n",
      "380:\tlearn: 0.0013019\ttotal: 21.5s\tremaining: 35s\n",
      "381:\tlearn: 0.0013015\ttotal: 21.6s\tremaining: 34.9s\n",
      "382:\tlearn: 0.0013015\ttotal: 21.6s\tremaining: 34.9s\n",
      "383:\tlearn: 0.0013011\ttotal: 21.7s\tremaining: 34.8s\n",
      "384:\tlearn: 0.0013009\ttotal: 21.8s\tremaining: 34.8s\n",
      "385:\tlearn: 0.0013005\ttotal: 21.8s\tremaining: 34.7s\n",
      "386:\tlearn: 0.0013005\ttotal: 21.9s\tremaining: 34.6s\n",
      "387:\tlearn: 0.0013005\ttotal: 21.9s\tremaining: 34.6s\n",
      "388:\tlearn: 0.0013005\ttotal: 22s\tremaining: 34.5s\n",
      "389:\tlearn: 0.0013005\ttotal: 22s\tremaining: 34.5s\n",
      "390:\tlearn: 0.0013005\ttotal: 22.1s\tremaining: 34.4s\n",
      "391:\tlearn: 0.0013005\ttotal: 22.2s\tremaining: 34.4s\n",
      "392:\tlearn: 0.0013004\ttotal: 22.2s\tremaining: 34.3s\n",
      "393:\tlearn: 0.0012998\ttotal: 22.3s\tremaining: 34.2s\n",
      "394:\tlearn: 0.0012997\ttotal: 22.3s\tremaining: 34.2s\n",
      "395:\tlearn: 0.0012996\ttotal: 22.4s\tremaining: 34.1s\n",
      "396:\tlearn: 0.0012990\ttotal: 22.4s\tremaining: 34.1s\n",
      "397:\tlearn: 0.0012990\ttotal: 22.5s\tremaining: 34s\n",
      "398:\tlearn: 0.0012990\ttotal: 22.5s\tremaining: 34s\n",
      "399:\tlearn: 0.0012990\ttotal: 22.6s\tremaining: 33.9s\n",
      "400:\tlearn: 0.0012988\ttotal: 22.7s\tremaining: 33.8s\n",
      "401:\tlearn: 0.0012988\ttotal: 22.7s\tremaining: 33.8s\n",
      "402:\tlearn: 0.0012988\ttotal: 22.8s\tremaining: 33.7s\n",
      "403:\tlearn: 0.0012986\ttotal: 22.8s\tremaining: 33.7s\n",
      "404:\tlearn: 0.0012982\ttotal: 22.9s\tremaining: 33.6s\n",
      "405:\tlearn: 0.0012978\ttotal: 22.9s\tremaining: 33.6s\n",
      "406:\tlearn: 0.0012977\ttotal: 23s\tremaining: 33.5s\n",
      "407:\tlearn: 0.0012977\ttotal: 23.1s\tremaining: 33.5s\n",
      "408:\tlearn: 0.0012977\ttotal: 23.1s\tremaining: 33.4s\n",
      "409:\tlearn: 0.0012972\ttotal: 23.2s\tremaining: 33.3s\n",
      "410:\tlearn: 0.0012972\ttotal: 23.2s\tremaining: 33.3s\n",
      "411:\tlearn: 0.0012971\ttotal: 23.3s\tremaining: 33.2s\n",
      "412:\tlearn: 0.0012968\ttotal: 23.3s\tremaining: 33.2s\n",
      "413:\tlearn: 0.0012936\ttotal: 23.4s\tremaining: 33.1s\n",
      "414:\tlearn: 0.0012789\ttotal: 23.5s\tremaining: 33.1s\n",
      "415:\tlearn: 0.0012789\ttotal: 23.5s\tremaining: 33s\n",
      "416:\tlearn: 0.0012789\ttotal: 23.6s\tremaining: 33s\n",
      "417:\tlearn: 0.0012787\ttotal: 23.6s\tremaining: 32.9s\n",
      "418:\tlearn: 0.0012785\ttotal: 23.7s\tremaining: 32.8s\n",
      "419:\tlearn: 0.0012784\ttotal: 23.7s\tremaining: 32.8s\n",
      "420:\tlearn: 0.0012780\ttotal: 23.8s\tremaining: 32.7s\n",
      "421:\tlearn: 0.0012761\ttotal: 23.9s\tremaining: 32.7s\n",
      "422:\tlearn: 0.0012761\ttotal: 23.9s\tremaining: 32.6s\n",
      "423:\tlearn: 0.0012760\ttotal: 24s\tremaining: 32.6s\n",
      "424:\tlearn: 0.0012757\ttotal: 24s\tremaining: 32.5s\n",
      "425:\tlearn: 0.0012756\ttotal: 24.1s\tremaining: 32.4s\n",
      "426:\tlearn: 0.0012756\ttotal: 24.1s\tremaining: 32.4s\n",
      "427:\tlearn: 0.0012754\ttotal: 24.2s\tremaining: 32.3s\n",
      "428:\tlearn: 0.0012753\ttotal: 24.3s\tremaining: 32.3s\n",
      "429:\tlearn: 0.0012753\ttotal: 24.3s\tremaining: 32.2s\n",
      "430:\tlearn: 0.0012752\ttotal: 24.4s\tremaining: 32.2s\n",
      "431:\tlearn: 0.0012749\ttotal: 24.4s\tremaining: 32.1s\n",
      "432:\tlearn: 0.0012749\ttotal: 24.5s\tremaining: 32.1s\n",
      "433:\tlearn: 0.0012741\ttotal: 24.5s\tremaining: 32s\n",
      "434:\tlearn: 0.0012738\ttotal: 24.6s\tremaining: 31.9s\n",
      "435:\tlearn: 0.0012738\ttotal: 24.6s\tremaining: 31.9s\n",
      "436:\tlearn: 0.0012736\ttotal: 24.7s\tremaining: 31.8s\n",
      "437:\tlearn: 0.0012736\ttotal: 24.8s\tremaining: 31.8s\n",
      "438:\tlearn: 0.0012736\ttotal: 24.8s\tremaining: 31.7s\n",
      "439:\tlearn: 0.0012734\ttotal: 24.9s\tremaining: 31.7s\n",
      "440:\tlearn: 0.0012734\ttotal: 24.9s\tremaining: 31.6s\n",
      "441:\tlearn: 0.0012733\ttotal: 25s\tremaining: 31.5s\n",
      "442:\tlearn: 0.0012723\ttotal: 25s\tremaining: 31.5s\n",
      "443:\tlearn: 0.0012722\ttotal: 25.1s\tremaining: 31.4s\n",
      "444:\tlearn: 0.0012722\ttotal: 25.2s\tremaining: 31.4s\n",
      "445:\tlearn: 0.0012722\ttotal: 25.2s\tremaining: 31.3s\n",
      "446:\tlearn: 0.0012720\ttotal: 25.3s\tremaining: 31.3s\n",
      "447:\tlearn: 0.0012717\ttotal: 25.3s\tremaining: 31.2s\n",
      "448:\tlearn: 0.0012672\ttotal: 25.4s\tremaining: 31.1s\n",
      "449:\tlearn: 0.0012670\ttotal: 25.4s\tremaining: 31.1s\n",
      "450:\tlearn: 0.0012523\ttotal: 25.5s\tremaining: 31s\n",
      "451:\tlearn: 0.0012522\ttotal: 25.6s\tremaining: 31s\n",
      "452:\tlearn: 0.0012522\ttotal: 25.6s\tremaining: 30.9s\n",
      "453:\tlearn: 0.0012521\ttotal: 25.7s\tremaining: 30.9s\n",
      "454:\tlearn: 0.0012520\ttotal: 25.7s\tremaining: 30.8s\n",
      "455:\tlearn: 0.0012518\ttotal: 25.8s\tremaining: 30.8s\n",
      "456:\tlearn: 0.0012518\ttotal: 25.8s\tremaining: 30.7s\n",
      "457:\tlearn: 0.0012515\ttotal: 25.9s\tremaining: 30.6s\n",
      "458:\tlearn: 0.0012468\ttotal: 25.9s\tremaining: 30.6s\n",
      "459:\tlearn: 0.0012468\ttotal: 26s\tremaining: 30.5s\n",
      "460:\tlearn: 0.0012468\ttotal: 26.1s\tremaining: 30.5s\n",
      "461:\tlearn: 0.0012465\ttotal: 26.1s\tremaining: 30.4s\n",
      "462:\tlearn: 0.0012462\ttotal: 26.2s\tremaining: 30.4s\n",
      "463:\tlearn: 0.0012462\ttotal: 26.2s\tremaining: 30.3s\n",
      "464:\tlearn: 0.0012462\ttotal: 26.3s\tremaining: 30.2s\n",
      "465:\tlearn: 0.0012452\ttotal: 26.3s\tremaining: 30.2s\n",
      "466:\tlearn: 0.0012452\ttotal: 26.4s\tremaining: 30.1s\n",
      "467:\tlearn: 0.0012451\ttotal: 26.4s\tremaining: 30.1s\n",
      "468:\tlearn: 0.0012449\ttotal: 26.5s\tremaining: 30s\n",
      "469:\tlearn: 0.0012449\ttotal: 26.6s\tremaining: 30s\n",
      "470:\tlearn: 0.0012446\ttotal: 26.6s\tremaining: 29.9s\n",
      "471:\tlearn: 0.0012445\ttotal: 26.7s\tremaining: 29.8s\n",
      "472:\tlearn: 0.0012422\ttotal: 26.7s\tremaining: 29.8s\n",
      "473:\tlearn: 0.0012420\ttotal: 26.8s\tremaining: 29.7s\n",
      "474:\tlearn: 0.0012382\ttotal: 26.8s\tremaining: 29.7s\n",
      "475:\tlearn: 0.0012381\ttotal: 26.9s\tremaining: 29.6s\n",
      "476:\tlearn: 0.0012379\ttotal: 27s\tremaining: 29.6s\n",
      "477:\tlearn: 0.0012379\ttotal: 27s\tremaining: 29.5s\n",
      "478:\tlearn: 0.0012374\ttotal: 27.1s\tremaining: 29.4s\n",
      "479:\tlearn: 0.0012372\ttotal: 27.1s\tremaining: 29.4s\n",
      "480:\tlearn: 0.0012372\ttotal: 27.2s\tremaining: 29.3s\n",
      "481:\tlearn: 0.0012372\ttotal: 27.2s\tremaining: 29.3s\n",
      "482:\tlearn: 0.0012372\ttotal: 27.3s\tremaining: 29.2s\n",
      "483:\tlearn: 0.0012371\ttotal: 27.3s\tremaining: 29.2s\n",
      "484:\tlearn: 0.0012371\ttotal: 27.4s\tremaining: 29.1s\n",
      "485:\tlearn: 0.0012371\ttotal: 27.5s\tremaining: 29s\n",
      "486:\tlearn: 0.0012368\ttotal: 27.5s\tremaining: 29s\n",
      "487:\tlearn: 0.0012279\ttotal: 27.6s\tremaining: 28.9s\n",
      "488:\tlearn: 0.0012276\ttotal: 27.6s\tremaining: 28.9s\n",
      "489:\tlearn: 0.0012273\ttotal: 27.7s\tremaining: 28.8s\n",
      "490:\tlearn: 0.0012271\ttotal: 27.7s\tremaining: 28.8s\n",
      "491:\tlearn: 0.0012268\ttotal: 27.8s\tremaining: 28.7s\n",
      "492:\tlearn: 0.0012266\ttotal: 27.9s\tremaining: 28.6s\n",
      "493:\tlearn: 0.0012265\ttotal: 27.9s\tremaining: 28.6s\n",
      "494:\tlearn: 0.0012265\ttotal: 28s\tremaining: 28.5s\n",
      "495:\tlearn: 0.0012262\ttotal: 28s\tremaining: 28.5s\n",
      "496:\tlearn: 0.0012260\ttotal: 28.1s\tremaining: 28.4s\n",
      "497:\tlearn: 0.0012258\ttotal: 28.1s\tremaining: 28.4s\n",
      "498:\tlearn: 0.0012258\ttotal: 28.2s\tremaining: 28.3s\n",
      "499:\tlearn: 0.0012255\ttotal: 28.2s\tremaining: 28.2s\n",
      "500:\tlearn: 0.0012253\ttotal: 28.3s\tremaining: 28.2s\n",
      "501:\tlearn: 0.0012253\ttotal: 28.4s\tremaining: 28.1s\n",
      "502:\tlearn: 0.0012253\ttotal: 28.4s\tremaining: 28.1s\n",
      "503:\tlearn: 0.0012250\ttotal: 28.5s\tremaining: 28s\n",
      "504:\tlearn: 0.0012250\ttotal: 28.5s\tremaining: 28s\n",
      "505:\tlearn: 0.0012250\ttotal: 28.6s\tremaining: 27.9s\n",
      "506:\tlearn: 0.0012248\ttotal: 28.6s\tremaining: 27.8s\n",
      "507:\tlearn: 0.0012247\ttotal: 28.7s\tremaining: 27.8s\n",
      "508:\tlearn: 0.0012246\ttotal: 28.7s\tremaining: 27.7s\n",
      "509:\tlearn: 0.0012240\ttotal: 28.8s\tremaining: 27.7s\n",
      "510:\tlearn: 0.0012239\ttotal: 28.9s\tremaining: 27.6s\n",
      "511:\tlearn: 0.0012239\ttotal: 28.9s\tremaining: 27.6s\n",
      "512:\tlearn: 0.0012237\ttotal: 29s\tremaining: 27.5s\n",
      "513:\tlearn: 0.0012232\ttotal: 29s\tremaining: 27.5s\n",
      "514:\tlearn: 0.0012227\ttotal: 29.1s\tremaining: 27.4s\n",
      "515:\tlearn: 0.0012221\ttotal: 29.1s\tremaining: 27.3s\n",
      "516:\tlearn: 0.0012157\ttotal: 29.2s\tremaining: 27.3s\n",
      "517:\tlearn: 0.0012151\ttotal: 29.3s\tremaining: 27.2s\n",
      "518:\tlearn: 0.0012147\ttotal: 29.3s\tremaining: 27.2s\n",
      "519:\tlearn: 0.0012147\ttotal: 29.4s\tremaining: 27.1s\n",
      "520:\tlearn: 0.0012146\ttotal: 29.4s\tremaining: 27.1s\n",
      "521:\tlearn: 0.0012146\ttotal: 29.5s\tremaining: 27s\n",
      "522:\tlearn: 0.0012146\ttotal: 29.5s\tremaining: 26.9s\n",
      "523:\tlearn: 0.0012143\ttotal: 29.6s\tremaining: 26.9s\n",
      "524:\tlearn: 0.0012142\ttotal: 29.6s\tremaining: 26.8s\n",
      "525:\tlearn: 0.0012142\ttotal: 29.7s\tremaining: 26.8s\n",
      "526:\tlearn: 0.0012136\ttotal: 29.8s\tremaining: 26.7s\n",
      "527:\tlearn: 0.0012134\ttotal: 29.8s\tremaining: 26.7s\n",
      "528:\tlearn: 0.0012134\ttotal: 29.9s\tremaining: 26.6s\n",
      "529:\tlearn: 0.0012134\ttotal: 29.9s\tremaining: 26.5s\n",
      "530:\tlearn: 0.0012134\ttotal: 30s\tremaining: 26.5s\n",
      "531:\tlearn: 0.0012123\ttotal: 30s\tremaining: 26.4s\n",
      "532:\tlearn: 0.0012121\ttotal: 30.1s\tremaining: 26.4s\n",
      "533:\tlearn: 0.0012121\ttotal: 30.2s\tremaining: 26.3s\n",
      "534:\tlearn: 0.0012118\ttotal: 30.2s\tremaining: 26.3s\n",
      "535:\tlearn: 0.0012118\ttotal: 30.3s\tremaining: 26.2s\n",
      "536:\tlearn: 0.0012117\ttotal: 30.3s\tremaining: 26.1s\n",
      "537:\tlearn: 0.0012117\ttotal: 30.4s\tremaining: 26.1s\n",
      "538:\tlearn: 0.0012117\ttotal: 30.4s\tremaining: 26s\n",
      "539:\tlearn: 0.0012117\ttotal: 30.5s\tremaining: 26s\n",
      "540:\tlearn: 0.0012111\ttotal: 30.6s\tremaining: 25.9s\n",
      "541:\tlearn: 0.0012111\ttotal: 30.6s\tremaining: 25.9s\n",
      "542:\tlearn: 0.0012110\ttotal: 30.7s\tremaining: 25.8s\n",
      "543:\tlearn: 0.0012109\ttotal: 30.7s\tremaining: 25.8s\n",
      "544:\tlearn: 0.0012108\ttotal: 30.8s\tremaining: 25.7s\n",
      "545:\tlearn: 0.0012108\ttotal: 30.8s\tremaining: 25.6s\n",
      "546:\tlearn: 0.0012108\ttotal: 30.9s\tremaining: 25.6s\n",
      "547:\tlearn: 0.0012106\ttotal: 31s\tremaining: 25.5s\n",
      "548:\tlearn: 0.0012106\ttotal: 31s\tremaining: 25.5s\n",
      "549:\tlearn: 0.0012106\ttotal: 31.1s\tremaining: 25.4s\n",
      "550:\tlearn: 0.0012105\ttotal: 31.1s\tremaining: 25.4s\n",
      "551:\tlearn: 0.0012105\ttotal: 31.2s\tremaining: 25.3s\n",
      "552:\tlearn: 0.0012104\ttotal: 31.2s\tremaining: 25.3s\n",
      "553:\tlearn: 0.0012104\ttotal: 31.3s\tremaining: 25.2s\n",
      "554:\tlearn: 0.0012060\ttotal: 31.4s\tremaining: 25.1s\n",
      "555:\tlearn: 0.0012054\ttotal: 31.4s\tremaining: 25.1s\n",
      "556:\tlearn: 0.0012053\ttotal: 31.5s\tremaining: 25s\n",
      "557:\tlearn: 0.0012048\ttotal: 31.5s\tremaining: 25s\n",
      "558:\tlearn: 0.0012046\ttotal: 31.6s\tremaining: 24.9s\n",
      "559:\tlearn: 0.0012042\ttotal: 31.6s\tremaining: 24.9s\n",
      "560:\tlearn: 0.0012041\ttotal: 31.7s\tremaining: 24.8s\n",
      "561:\tlearn: 0.0012041\ttotal: 31.7s\tremaining: 24.7s\n",
      "562:\tlearn: 0.0012041\ttotal: 31.8s\tremaining: 24.7s\n",
      "563:\tlearn: 0.0012040\ttotal: 31.9s\tremaining: 24.6s\n",
      "564:\tlearn: 0.0012038\ttotal: 31.9s\tremaining: 24.6s\n",
      "565:\tlearn: 0.0012038\ttotal: 32s\tremaining: 24.5s\n",
      "566:\tlearn: 0.0012037\ttotal: 32s\tremaining: 24.5s\n",
      "567:\tlearn: 0.0012035\ttotal: 32.1s\tremaining: 24.4s\n",
      "568:\tlearn: 0.0012035\ttotal: 32.1s\tremaining: 24.3s\n",
      "569:\tlearn: 0.0012034\ttotal: 32.2s\tremaining: 24.3s\n",
      "570:\tlearn: 0.0012034\ttotal: 32.3s\tremaining: 24.2s\n",
      "571:\tlearn: 0.0012034\ttotal: 32.3s\tremaining: 24.2s\n",
      "572:\tlearn: 0.0012031\ttotal: 32.4s\tremaining: 24.1s\n",
      "573:\tlearn: 0.0012031\ttotal: 32.4s\tremaining: 24.1s\n",
      "574:\tlearn: 0.0012031\ttotal: 32.5s\tremaining: 24s\n",
      "575:\tlearn: 0.0012031\ttotal: 32.5s\tremaining: 24s\n",
      "576:\tlearn: 0.0012026\ttotal: 32.6s\tremaining: 23.9s\n",
      "577:\tlearn: 0.0012026\ttotal: 32.7s\tremaining: 23.8s\n",
      "578:\tlearn: 0.0012026\ttotal: 32.7s\tremaining: 23.8s\n",
      "579:\tlearn: 0.0012025\ttotal: 32.8s\tremaining: 23.7s\n",
      "580:\tlearn: 0.0012025\ttotal: 32.8s\tremaining: 23.7s\n",
      "581:\tlearn: 0.0012025\ttotal: 32.9s\tremaining: 23.6s\n",
      "582:\tlearn: 0.0012024\ttotal: 32.9s\tremaining: 23.6s\n",
      "583:\tlearn: 0.0012023\ttotal: 33s\tremaining: 23.5s\n",
      "584:\tlearn: 0.0012023\ttotal: 33s\tremaining: 23.4s\n",
      "585:\tlearn: 0.0012023\ttotal: 33.1s\tremaining: 23.4s\n",
      "586:\tlearn: 0.0012021\ttotal: 33.2s\tremaining: 23.3s\n",
      "587:\tlearn: 0.0012020\ttotal: 33.2s\tremaining: 23.3s\n",
      "588:\tlearn: 0.0012020\ttotal: 33.3s\tremaining: 23.2s\n",
      "589:\tlearn: 0.0012019\ttotal: 33.3s\tremaining: 23.2s\n",
      "590:\tlearn: 0.0012019\ttotal: 33.4s\tremaining: 23.1s\n",
      "591:\tlearn: 0.0012019\ttotal: 33.4s\tremaining: 23s\n",
      "592:\tlearn: 0.0012014\ttotal: 33.5s\tremaining: 23s\n",
      "593:\tlearn: 0.0012014\ttotal: 33.6s\tremaining: 22.9s\n",
      "594:\tlearn: 0.0012011\ttotal: 33.6s\tremaining: 22.9s\n",
      "595:\tlearn: 0.0012008\ttotal: 33.7s\tremaining: 22.8s\n",
      "596:\tlearn: 0.0012008\ttotal: 33.7s\tremaining: 22.8s\n",
      "597:\tlearn: 0.0012007\ttotal: 33.8s\tremaining: 22.7s\n",
      "598:\tlearn: 0.0012005\ttotal: 33.8s\tremaining: 22.6s\n",
      "599:\tlearn: 0.0012004\ttotal: 33.9s\tremaining: 22.6s\n",
      "600:\tlearn: 0.0012003\ttotal: 33.9s\tremaining: 22.5s\n",
      "601:\tlearn: 0.0011997\ttotal: 34s\tremaining: 22.5s\n",
      "602:\tlearn: 0.0011994\ttotal: 34.1s\tremaining: 22.4s\n",
      "603:\tlearn: 0.0011993\ttotal: 34.1s\tremaining: 22.4s\n",
      "604:\tlearn: 0.0011900\ttotal: 34.2s\tremaining: 22.3s\n",
      "605:\tlearn: 0.0011893\ttotal: 34.2s\tremaining: 22.3s\n",
      "606:\tlearn: 0.0011881\ttotal: 34.3s\tremaining: 22.2s\n",
      "607:\tlearn: 0.0011875\ttotal: 34.3s\tremaining: 22.1s\n",
      "608:\tlearn: 0.0011874\ttotal: 34.4s\tremaining: 22.1s\n",
      "609:\tlearn: 0.0011873\ttotal: 34.5s\tremaining: 22s\n",
      "610:\tlearn: 0.0011872\ttotal: 34.5s\tremaining: 22s\n",
      "611:\tlearn: 0.0011871\ttotal: 34.6s\tremaining: 21.9s\n",
      "612:\tlearn: 0.0011870\ttotal: 34.6s\tremaining: 21.9s\n",
      "613:\tlearn: 0.0011866\ttotal: 34.7s\tremaining: 21.8s\n",
      "614:\tlearn: 0.0011805\ttotal: 34.7s\tremaining: 21.7s\n",
      "615:\tlearn: 0.0011804\ttotal: 34.8s\tremaining: 21.7s\n",
      "616:\tlearn: 0.0011804\ttotal: 34.8s\tremaining: 21.6s\n",
      "617:\tlearn: 0.0011804\ttotal: 34.9s\tremaining: 21.6s\n",
      "618:\tlearn: 0.0011804\ttotal: 35s\tremaining: 21.5s\n",
      "619:\tlearn: 0.0011804\ttotal: 35s\tremaining: 21.5s\n",
      "620:\tlearn: 0.0011804\ttotal: 35.1s\tremaining: 21.4s\n",
      "621:\tlearn: 0.0011804\ttotal: 35.1s\tremaining: 21.4s\n",
      "622:\tlearn: 0.0011803\ttotal: 35.2s\tremaining: 21.3s\n",
      "623:\tlearn: 0.0011796\ttotal: 35.2s\tremaining: 21.2s\n",
      "624:\tlearn: 0.0011793\ttotal: 35.3s\tremaining: 21.2s\n",
      "625:\tlearn: 0.0011791\ttotal: 35.4s\tremaining: 21.1s\n",
      "626:\tlearn: 0.0011790\ttotal: 35.4s\tremaining: 21.1s\n",
      "627:\tlearn: 0.0011783\ttotal: 35.5s\tremaining: 21s\n",
      "628:\tlearn: 0.0011782\ttotal: 35.5s\tremaining: 21s\n",
      "629:\tlearn: 0.0011781\ttotal: 35.6s\tremaining: 20.9s\n",
      "630:\tlearn: 0.0011777\ttotal: 35.6s\tremaining: 20.8s\n",
      "631:\tlearn: 0.0011776\ttotal: 35.7s\tremaining: 20.8s\n",
      "632:\tlearn: 0.0011774\ttotal: 35.7s\tremaining: 20.7s\n",
      "633:\tlearn: 0.0011774\ttotal: 35.8s\tremaining: 20.7s\n",
      "634:\tlearn: 0.0011773\ttotal: 35.9s\tremaining: 20.6s\n",
      "635:\tlearn: 0.0011773\ttotal: 35.9s\tremaining: 20.6s\n",
      "636:\tlearn: 0.0011773\ttotal: 36s\tremaining: 20.5s\n",
      "637:\tlearn: 0.0011773\ttotal: 36s\tremaining: 20.4s\n",
      "638:\tlearn: 0.0011773\ttotal: 36.1s\tremaining: 20.4s\n",
      "639:\tlearn: 0.0011771\ttotal: 36.1s\tremaining: 20.3s\n",
      "640:\tlearn: 0.0011756\ttotal: 36.2s\tremaining: 20.3s\n",
      "641:\tlearn: 0.0011755\ttotal: 36.3s\tremaining: 20.2s\n",
      "642:\tlearn: 0.0011753\ttotal: 36.3s\tremaining: 20.2s\n",
      "643:\tlearn: 0.0011751\ttotal: 36.4s\tremaining: 20.1s\n",
      "644:\tlearn: 0.0011751\ttotal: 36.4s\tremaining: 20s\n",
      "645:\tlearn: 0.0011751\ttotal: 36.5s\tremaining: 20s\n",
      "646:\tlearn: 0.0011742\ttotal: 36.5s\tremaining: 19.9s\n",
      "647:\tlearn: 0.0011741\ttotal: 36.6s\tremaining: 19.9s\n",
      "648:\tlearn: 0.0011741\ttotal: 36.7s\tremaining: 19.8s\n",
      "649:\tlearn: 0.0011741\ttotal: 36.7s\tremaining: 19.8s\n",
      "650:\tlearn: 0.0011741\ttotal: 36.8s\tremaining: 19.7s\n",
      "651:\tlearn: 0.0011692\ttotal: 36.9s\tremaining: 19.7s\n",
      "652:\tlearn: 0.0011692\ttotal: 37s\tremaining: 19.6s\n",
      "653:\tlearn: 0.0011692\ttotal: 37s\tremaining: 19.6s\n",
      "654:\tlearn: 0.0011692\ttotal: 37.1s\tremaining: 19.5s\n",
      "655:\tlearn: 0.0011692\ttotal: 37.2s\tremaining: 19.5s\n",
      "656:\tlearn: 0.0011688\ttotal: 37.2s\tremaining: 19.4s\n",
      "657:\tlearn: 0.0011688\ttotal: 37.3s\tremaining: 19.4s\n",
      "658:\tlearn: 0.0011688\ttotal: 37.4s\tremaining: 19.3s\n",
      "659:\tlearn: 0.0011613\ttotal: 37.4s\tremaining: 19.3s\n",
      "660:\tlearn: 0.0011613\ttotal: 37.5s\tremaining: 19.2s\n",
      "661:\tlearn: 0.0011612\ttotal: 37.5s\tremaining: 19.2s\n",
      "662:\tlearn: 0.0011611\ttotal: 37.6s\tremaining: 19.1s\n",
      "663:\tlearn: 0.0011611\ttotal: 37.6s\tremaining: 19s\n",
      "664:\tlearn: 0.0011609\ttotal: 37.7s\tremaining: 19s\n",
      "665:\tlearn: 0.0011608\ttotal: 37.8s\tremaining: 18.9s\n",
      "666:\tlearn: 0.0011606\ttotal: 37.9s\tremaining: 18.9s\n",
      "667:\tlearn: 0.0011605\ttotal: 37.9s\tremaining: 18.9s\n",
      "668:\tlearn: 0.0011604\ttotal: 38s\tremaining: 18.8s\n",
      "669:\tlearn: 0.0011603\ttotal: 38.1s\tremaining: 18.8s\n",
      "670:\tlearn: 0.0011601\ttotal: 38.2s\tremaining: 18.7s\n",
      "671:\tlearn: 0.0011601\ttotal: 38.2s\tremaining: 18.7s\n",
      "672:\tlearn: 0.0011598\ttotal: 38.3s\tremaining: 18.6s\n",
      "673:\tlearn: 0.0011598\ttotal: 38.3s\tremaining: 18.5s\n",
      "674:\tlearn: 0.0011596\ttotal: 38.4s\tremaining: 18.5s\n",
      "675:\tlearn: 0.0011595\ttotal: 38.4s\tremaining: 18.4s\n",
      "676:\tlearn: 0.0011595\ttotal: 38.5s\tremaining: 18.4s\n",
      "677:\tlearn: 0.0011430\ttotal: 38.6s\tremaining: 18.3s\n",
      "678:\tlearn: 0.0011428\ttotal: 38.6s\tremaining: 18.3s\n",
      "679:\tlearn: 0.0011428\ttotal: 38.7s\tremaining: 18.2s\n",
      "680:\tlearn: 0.0011426\ttotal: 38.7s\tremaining: 18.1s\n",
      "681:\tlearn: 0.0011424\ttotal: 38.8s\tremaining: 18.1s\n",
      "682:\tlearn: 0.0011422\ttotal: 38.8s\tremaining: 18s\n",
      "683:\tlearn: 0.0011420\ttotal: 38.9s\tremaining: 18s\n",
      "684:\tlearn: 0.0011420\ttotal: 39s\tremaining: 17.9s\n",
      "685:\tlearn: 0.0011420\ttotal: 39s\tremaining: 17.9s\n",
      "686:\tlearn: 0.0011417\ttotal: 39.1s\tremaining: 17.8s\n",
      "687:\tlearn: 0.0011417\ttotal: 39.1s\tremaining: 17.7s\n",
      "688:\tlearn: 0.0011416\ttotal: 39.2s\tremaining: 17.7s\n",
      "689:\tlearn: 0.0011414\ttotal: 39.2s\tremaining: 17.6s\n",
      "690:\tlearn: 0.0011413\ttotal: 39.3s\tremaining: 17.6s\n",
      "691:\tlearn: 0.0011413\ttotal: 39.3s\tremaining: 17.5s\n",
      "692:\tlearn: 0.0011412\ttotal: 39.4s\tremaining: 17.5s\n",
      "693:\tlearn: 0.0011411\ttotal: 39.5s\tremaining: 17.4s\n",
      "694:\tlearn: 0.0011410\ttotal: 39.5s\tremaining: 17.3s\n",
      "695:\tlearn: 0.0011407\ttotal: 39.6s\tremaining: 17.3s\n",
      "696:\tlearn: 0.0011346\ttotal: 39.6s\tremaining: 17.2s\n",
      "697:\tlearn: 0.0011345\ttotal: 39.7s\tremaining: 17.2s\n",
      "698:\tlearn: 0.0011344\ttotal: 39.8s\tremaining: 17.1s\n",
      "699:\tlearn: 0.0011344\ttotal: 39.8s\tremaining: 17.1s\n",
      "700:\tlearn: 0.0011344\ttotal: 39.9s\tremaining: 17s\n",
      "701:\tlearn: 0.0011343\ttotal: 39.9s\tremaining: 16.9s\n",
      "702:\tlearn: 0.0011343\ttotal: 40s\tremaining: 16.9s\n",
      "703:\tlearn: 0.0011343\ttotal: 40s\tremaining: 16.8s\n",
      "704:\tlearn: 0.0011342\ttotal: 40.1s\tremaining: 16.8s\n",
      "705:\tlearn: 0.0011340\ttotal: 40.1s\tremaining: 16.7s\n",
      "706:\tlearn: 0.0011339\ttotal: 40.2s\tremaining: 16.7s\n",
      "707:\tlearn: 0.0011339\ttotal: 40.3s\tremaining: 16.6s\n",
      "708:\tlearn: 0.0011338\ttotal: 40.3s\tremaining: 16.5s\n",
      "709:\tlearn: 0.0011338\ttotal: 40.4s\tremaining: 16.5s\n",
      "710:\tlearn: 0.0011337\ttotal: 40.4s\tremaining: 16.4s\n",
      "711:\tlearn: 0.0011336\ttotal: 40.5s\tremaining: 16.4s\n",
      "712:\tlearn: 0.0011336\ttotal: 40.5s\tremaining: 16.3s\n",
      "713:\tlearn: 0.0011242\ttotal: 40.6s\tremaining: 16.3s\n",
      "714:\tlearn: 0.0011241\ttotal: 40.6s\tremaining: 16.2s\n",
      "715:\tlearn: 0.0011241\ttotal: 40.7s\tremaining: 16.1s\n",
      "716:\tlearn: 0.0011241\ttotal: 40.8s\tremaining: 16.1s\n",
      "717:\tlearn: 0.0011241\ttotal: 40.8s\tremaining: 16s\n",
      "718:\tlearn: 0.0011239\ttotal: 40.9s\tremaining: 16s\n",
      "719:\tlearn: 0.0011231\ttotal: 40.9s\tremaining: 15.9s\n",
      "720:\tlearn: 0.0011227\ttotal: 41s\tremaining: 15.9s\n",
      "721:\tlearn: 0.0011223\ttotal: 41s\tremaining: 15.8s\n",
      "722:\tlearn: 0.0011223\ttotal: 41.1s\tremaining: 15.7s\n",
      "723:\tlearn: 0.0011221\ttotal: 41.1s\tremaining: 15.7s\n",
      "724:\tlearn: 0.0011221\ttotal: 41.2s\tremaining: 15.6s\n",
      "725:\tlearn: 0.0011221\ttotal: 41.3s\tremaining: 15.6s\n",
      "726:\tlearn: 0.0011221\ttotal: 41.3s\tremaining: 15.5s\n",
      "727:\tlearn: 0.0011220\ttotal: 41.4s\tremaining: 15.5s\n",
      "728:\tlearn: 0.0011220\ttotal: 41.5s\tremaining: 15.4s\n",
      "729:\tlearn: 0.0011220\ttotal: 41.5s\tremaining: 15.4s\n",
      "730:\tlearn: 0.0011218\ttotal: 41.6s\tremaining: 15.3s\n",
      "731:\tlearn: 0.0011218\ttotal: 41.7s\tremaining: 15.3s\n",
      "732:\tlearn: 0.0011217\ttotal: 41.8s\tremaining: 15.2s\n",
      "733:\tlearn: 0.0011208\ttotal: 41.8s\tremaining: 15.2s\n",
      "734:\tlearn: 0.0011207\ttotal: 41.9s\tremaining: 15.1s\n",
      "735:\tlearn: 0.0011207\ttotal: 42s\tremaining: 15.1s\n",
      "736:\tlearn: 0.0011207\ttotal: 42.1s\tremaining: 15s\n",
      "737:\tlearn: 0.0011206\ttotal: 42.1s\tremaining: 15s\n",
      "738:\tlearn: 0.0011206\ttotal: 42.2s\tremaining: 14.9s\n",
      "739:\tlearn: 0.0011205\ttotal: 42.3s\tremaining: 14.9s\n",
      "740:\tlearn: 0.0011149\ttotal: 42.3s\tremaining: 14.8s\n",
      "741:\tlearn: 0.0011147\ttotal: 42.4s\tremaining: 14.7s\n",
      "742:\tlearn: 0.0011146\ttotal: 42.5s\tremaining: 14.7s\n",
      "743:\tlearn: 0.0011146\ttotal: 42.6s\tremaining: 14.6s\n",
      "744:\tlearn: 0.0011146\ttotal: 42.6s\tremaining: 14.6s\n",
      "745:\tlearn: 0.0011144\ttotal: 42.7s\tremaining: 14.5s\n",
      "746:\tlearn: 0.0011058\ttotal: 42.8s\tremaining: 14.5s\n",
      "747:\tlearn: 0.0011058\ttotal: 42.8s\tremaining: 14.4s\n",
      "748:\tlearn: 0.0011057\ttotal: 42.9s\tremaining: 14.4s\n",
      "749:\tlearn: 0.0011055\ttotal: 43s\tremaining: 14.3s\n",
      "750:\tlearn: 0.0011054\ttotal: 43.1s\tremaining: 14.3s\n",
      "751:\tlearn: 0.0011049\ttotal: 43.1s\tremaining: 14.2s\n",
      "752:\tlearn: 0.0011048\ttotal: 43.2s\tremaining: 14.2s\n",
      "753:\tlearn: 0.0011048\ttotal: 43.3s\tremaining: 14.1s\n",
      "754:\tlearn: 0.0011040\ttotal: 43.4s\tremaining: 14.1s\n",
      "755:\tlearn: 0.0011040\ttotal: 43.4s\tremaining: 14s\n",
      "756:\tlearn: 0.0011039\ttotal: 43.5s\tremaining: 14s\n",
      "757:\tlearn: 0.0011039\ttotal: 43.6s\tremaining: 13.9s\n",
      "758:\tlearn: 0.0011039\ttotal: 43.6s\tremaining: 13.9s\n",
      "759:\tlearn: 0.0011039\ttotal: 43.7s\tremaining: 13.8s\n",
      "760:\tlearn: 0.0011036\ttotal: 43.8s\tremaining: 13.8s\n",
      "761:\tlearn: 0.0011035\ttotal: 43.9s\tremaining: 13.7s\n",
      "762:\tlearn: 0.0011031\ttotal: 43.9s\tremaining: 13.6s\n",
      "763:\tlearn: 0.0011029\ttotal: 44s\tremaining: 13.6s\n",
      "764:\tlearn: 0.0011028\ttotal: 44.1s\tremaining: 13.5s\n",
      "765:\tlearn: 0.0011028\ttotal: 44.1s\tremaining: 13.5s\n",
      "766:\tlearn: 0.0011028\ttotal: 44.2s\tremaining: 13.4s\n",
      "767:\tlearn: 0.0011027\ttotal: 44.3s\tremaining: 13.4s\n",
      "768:\tlearn: 0.0011019\ttotal: 44.3s\tremaining: 13.3s\n",
      "769:\tlearn: 0.0011019\ttotal: 44.4s\tremaining: 13.3s\n",
      "770:\tlearn: 0.0011014\ttotal: 44.4s\tremaining: 13.2s\n",
      "771:\tlearn: 0.0011014\ttotal: 44.5s\tremaining: 13.1s\n",
      "772:\tlearn: 0.0011012\ttotal: 44.5s\tremaining: 13.1s\n",
      "773:\tlearn: 0.0011010\ttotal: 44.6s\tremaining: 13s\n",
      "774:\tlearn: 0.0011010\ttotal: 44.6s\tremaining: 13s\n",
      "775:\tlearn: 0.0011009\ttotal: 44.7s\tremaining: 12.9s\n",
      "776:\tlearn: 0.0011009\ttotal: 44.8s\tremaining: 12.8s\n",
      "777:\tlearn: 0.0011006\ttotal: 44.8s\tremaining: 12.8s\n",
      "778:\tlearn: 0.0011005\ttotal: 44.9s\tremaining: 12.7s\n",
      "779:\tlearn: 0.0011004\ttotal: 44.9s\tremaining: 12.7s\n",
      "780:\tlearn: 0.0011003\ttotal: 45s\tremaining: 12.6s\n",
      "781:\tlearn: 0.0011000\ttotal: 45s\tremaining: 12.6s\n",
      "782:\tlearn: 0.0010998\ttotal: 45.1s\tremaining: 12.5s\n",
      "783:\tlearn: 0.0010981\ttotal: 45.2s\tremaining: 12.4s\n",
      "784:\tlearn: 0.0010981\ttotal: 45.2s\tremaining: 12.4s\n",
      "785:\tlearn: 0.0010980\ttotal: 45.3s\tremaining: 12.3s\n",
      "786:\tlearn: 0.0010978\ttotal: 45.3s\tremaining: 12.3s\n",
      "787:\tlearn: 0.0010978\ttotal: 45.4s\tremaining: 12.2s\n",
      "788:\tlearn: 0.0010976\ttotal: 45.4s\tremaining: 12.2s\n",
      "789:\tlearn: 0.0010965\ttotal: 45.5s\tremaining: 12.1s\n",
      "790:\tlearn: 0.0010960\ttotal: 45.6s\tremaining: 12s\n",
      "791:\tlearn: 0.0010958\ttotal: 45.6s\tremaining: 12s\n",
      "792:\tlearn: 0.0010957\ttotal: 45.7s\tremaining: 11.9s\n",
      "793:\tlearn: 0.0010957\ttotal: 45.7s\tremaining: 11.9s\n",
      "794:\tlearn: 0.0010956\ttotal: 45.8s\tremaining: 11.8s\n",
      "795:\tlearn: 0.0010949\ttotal: 45.8s\tremaining: 11.7s\n",
      "796:\tlearn: 0.0010949\ttotal: 45.9s\tremaining: 11.7s\n",
      "797:\tlearn: 0.0010949\ttotal: 45.9s\tremaining: 11.6s\n",
      "798:\tlearn: 0.0010948\ttotal: 46s\tremaining: 11.6s\n",
      "799:\tlearn: 0.0010947\ttotal: 46.1s\tremaining: 11.5s\n",
      "800:\tlearn: 0.0010947\ttotal: 46.1s\tremaining: 11.5s\n",
      "801:\tlearn: 0.0010946\ttotal: 46.2s\tremaining: 11.4s\n",
      "802:\tlearn: 0.0010945\ttotal: 46.2s\tremaining: 11.3s\n",
      "803:\tlearn: 0.0010943\ttotal: 46.3s\tremaining: 11.3s\n",
      "804:\tlearn: 0.0010941\ttotal: 46.3s\tremaining: 11.2s\n",
      "805:\tlearn: 0.0010941\ttotal: 46.4s\tremaining: 11.2s\n",
      "806:\tlearn: 0.0010941\ttotal: 46.5s\tremaining: 11.1s\n",
      "807:\tlearn: 0.0010940\ttotal: 46.5s\tremaining: 11.1s\n",
      "808:\tlearn: 0.0010939\ttotal: 46.6s\tremaining: 11s\n",
      "809:\tlearn: 0.0010939\ttotal: 46.6s\tremaining: 10.9s\n",
      "810:\tlearn: 0.0010939\ttotal: 46.7s\tremaining: 10.9s\n",
      "811:\tlearn: 0.0010939\ttotal: 46.7s\tremaining: 10.8s\n",
      "812:\tlearn: 0.0010938\ttotal: 46.8s\tremaining: 10.8s\n",
      "813:\tlearn: 0.0010938\ttotal: 46.8s\tremaining: 10.7s\n",
      "814:\tlearn: 0.0010936\ttotal: 46.9s\tremaining: 10.6s\n",
      "815:\tlearn: 0.0010935\ttotal: 47s\tremaining: 10.6s\n",
      "816:\tlearn: 0.0010933\ttotal: 47s\tremaining: 10.5s\n",
      "817:\tlearn: 0.0010930\ttotal: 47.1s\tremaining: 10.5s\n",
      "818:\tlearn: 0.0010929\ttotal: 47.1s\tremaining: 10.4s\n",
      "819:\tlearn: 0.0010929\ttotal: 47.2s\tremaining: 10.4s\n",
      "820:\tlearn: 0.0010928\ttotal: 47.2s\tremaining: 10.3s\n",
      "821:\tlearn: 0.0010916\ttotal: 47.3s\tremaining: 10.2s\n",
      "822:\tlearn: 0.0010916\ttotal: 47.4s\tremaining: 10.2s\n",
      "823:\tlearn: 0.0010916\ttotal: 47.4s\tremaining: 10.1s\n",
      "824:\tlearn: 0.0010915\ttotal: 47.5s\tremaining: 10.1s\n",
      "825:\tlearn: 0.0010915\ttotal: 47.5s\tremaining: 10s\n",
      "826:\tlearn: 0.0010914\ttotal: 47.6s\tremaining: 9.95s\n",
      "827:\tlearn: 0.0010913\ttotal: 47.6s\tremaining: 9.89s\n",
      "828:\tlearn: 0.0010913\ttotal: 47.7s\tremaining: 9.84s\n",
      "829:\tlearn: 0.0010913\ttotal: 47.8s\tremaining: 9.78s\n",
      "830:\tlearn: 0.0010913\ttotal: 47.8s\tremaining: 9.72s\n",
      "831:\tlearn: 0.0010911\ttotal: 47.9s\tremaining: 9.66s\n",
      "832:\tlearn: 0.0010911\ttotal: 47.9s\tremaining: 9.61s\n",
      "833:\tlearn: 0.0010909\ttotal: 48s\tremaining: 9.55s\n",
      "834:\tlearn: 0.0010909\ttotal: 48s\tremaining: 9.49s\n",
      "835:\tlearn: 0.0010909\ttotal: 48.1s\tremaining: 9.43s\n",
      "836:\tlearn: 0.0010905\ttotal: 48.1s\tremaining: 9.38s\n",
      "837:\tlearn: 0.0010905\ttotal: 48.2s\tremaining: 9.32s\n",
      "838:\tlearn: 0.0010905\ttotal: 48.3s\tremaining: 9.26s\n",
      "839:\tlearn: 0.0010905\ttotal: 48.3s\tremaining: 9.2s\n",
      "840:\tlearn: 0.0010905\ttotal: 48.4s\tremaining: 9.14s\n",
      "841:\tlearn: 0.0010904\ttotal: 48.4s\tremaining: 9.09s\n",
      "842:\tlearn: 0.0010902\ttotal: 48.5s\tremaining: 9.03s\n",
      "843:\tlearn: 0.0010902\ttotal: 48.5s\tremaining: 8.97s\n",
      "844:\tlearn: 0.0010901\ttotal: 48.6s\tremaining: 8.91s\n",
      "845:\tlearn: 0.0010900\ttotal: 48.6s\tremaining: 8.86s\n",
      "846:\tlearn: 0.0010897\ttotal: 48.7s\tremaining: 8.8s\n",
      "847:\tlearn: 0.0010896\ttotal: 48.8s\tremaining: 8.74s\n",
      "848:\tlearn: 0.0010892\ttotal: 48.8s\tremaining: 8.68s\n",
      "849:\tlearn: 0.0010892\ttotal: 48.9s\tremaining: 8.62s\n",
      "850:\tlearn: 0.0010892\ttotal: 48.9s\tremaining: 8.57s\n",
      "851:\tlearn: 0.0010891\ttotal: 49s\tremaining: 8.51s\n",
      "852:\tlearn: 0.0010891\ttotal: 49s\tremaining: 8.45s\n",
      "853:\tlearn: 0.0010890\ttotal: 49.1s\tremaining: 8.39s\n",
      "854:\tlearn: 0.0010890\ttotal: 49.2s\tremaining: 8.34s\n",
      "855:\tlearn: 0.0010870\ttotal: 49.2s\tremaining: 8.28s\n",
      "856:\tlearn: 0.0010870\ttotal: 49.3s\tremaining: 8.22s\n",
      "857:\tlearn: 0.0010869\ttotal: 49.3s\tremaining: 8.16s\n",
      "858:\tlearn: 0.0010869\ttotal: 49.4s\tremaining: 8.11s\n",
      "859:\tlearn: 0.0010869\ttotal: 49.4s\tremaining: 8.05s\n",
      "860:\tlearn: 0.0010867\ttotal: 49.5s\tremaining: 7.99s\n",
      "861:\tlearn: 0.0010866\ttotal: 49.6s\tremaining: 7.93s\n",
      "862:\tlearn: 0.0010866\ttotal: 49.6s\tremaining: 7.88s\n",
      "863:\tlearn: 0.0010866\ttotal: 49.7s\tremaining: 7.82s\n",
      "864:\tlearn: 0.0010861\ttotal: 49.7s\tremaining: 7.76s\n",
      "865:\tlearn: 0.0010861\ttotal: 49.8s\tremaining: 7.7s\n",
      "866:\tlearn: 0.0010861\ttotal: 49.8s\tremaining: 7.64s\n",
      "867:\tlearn: 0.0010859\ttotal: 49.9s\tremaining: 7.59s\n",
      "868:\tlearn: 0.0010859\ttotal: 49.9s\tremaining: 7.53s\n",
      "869:\tlearn: 0.0010859\ttotal: 50s\tremaining: 7.47s\n",
      "870:\tlearn: 0.0010858\ttotal: 50.1s\tremaining: 7.41s\n",
      "871:\tlearn: 0.0010858\ttotal: 50.1s\tremaining: 7.36s\n",
      "872:\tlearn: 0.0010858\ttotal: 50.2s\tremaining: 7.3s\n",
      "873:\tlearn: 0.0010851\ttotal: 50.2s\tremaining: 7.24s\n",
      "874:\tlearn: 0.0010851\ttotal: 50.3s\tremaining: 7.18s\n",
      "875:\tlearn: 0.0010847\ttotal: 50.3s\tremaining: 7.12s\n",
      "876:\tlearn: 0.0010847\ttotal: 50.4s\tremaining: 7.07s\n",
      "877:\tlearn: 0.0010847\ttotal: 50.4s\tremaining: 7.01s\n",
      "878:\tlearn: 0.0010845\ttotal: 50.5s\tremaining: 6.95s\n",
      "879:\tlearn: 0.0010845\ttotal: 50.6s\tremaining: 6.89s\n",
      "880:\tlearn: 0.0010845\ttotal: 50.6s\tremaining: 6.84s\n",
      "881:\tlearn: 0.0010845\ttotal: 50.7s\tremaining: 6.78s\n",
      "882:\tlearn: 0.0010845\ttotal: 50.7s\tremaining: 6.72s\n",
      "883:\tlearn: 0.0010844\ttotal: 50.8s\tremaining: 6.66s\n",
      "884:\tlearn: 0.0010844\ttotal: 50.8s\tremaining: 6.61s\n",
      "885:\tlearn: 0.0010843\ttotal: 50.9s\tremaining: 6.55s\n",
      "886:\tlearn: 0.0010840\ttotal: 51s\tremaining: 6.49s\n",
      "887:\tlearn: 0.0010840\ttotal: 51s\tremaining: 6.43s\n",
      "888:\tlearn: 0.0010839\ttotal: 51.1s\tremaining: 6.38s\n",
      "889:\tlearn: 0.0010838\ttotal: 51.1s\tremaining: 6.32s\n",
      "890:\tlearn: 0.0010829\ttotal: 51.2s\tremaining: 6.26s\n",
      "891:\tlearn: 0.0010829\ttotal: 51.2s\tremaining: 6.2s\n",
      "892:\tlearn: 0.0010828\ttotal: 51.3s\tremaining: 6.15s\n",
      "893:\tlearn: 0.0010828\ttotal: 51.4s\tremaining: 6.09s\n",
      "894:\tlearn: 0.0010827\ttotal: 51.4s\tremaining: 6.03s\n",
      "895:\tlearn: 0.0010827\ttotal: 51.5s\tremaining: 5.97s\n",
      "896:\tlearn: 0.0010818\ttotal: 51.5s\tremaining: 5.92s\n",
      "897:\tlearn: 0.0010813\ttotal: 51.6s\tremaining: 5.86s\n",
      "898:\tlearn: 0.0010813\ttotal: 51.6s\tremaining: 5.8s\n",
      "899:\tlearn: 0.0010808\ttotal: 51.7s\tremaining: 5.74s\n",
      "900:\tlearn: 0.0010808\ttotal: 51.7s\tremaining: 5.69s\n",
      "901:\tlearn: 0.0010806\ttotal: 51.8s\tremaining: 5.63s\n",
      "902:\tlearn: 0.0010804\ttotal: 51.9s\tremaining: 5.57s\n",
      "903:\tlearn: 0.0010803\ttotal: 51.9s\tremaining: 5.51s\n",
      "904:\tlearn: 0.0010803\ttotal: 52s\tremaining: 5.46s\n",
      "905:\tlearn: 0.0010803\ttotal: 52s\tremaining: 5.4s\n",
      "906:\tlearn: 0.0010802\ttotal: 52.1s\tremaining: 5.34s\n",
      "907:\tlearn: 0.0010802\ttotal: 52.1s\tremaining: 5.28s\n",
      "908:\tlearn: 0.0010802\ttotal: 52.2s\tremaining: 5.22s\n",
      "909:\tlearn: 0.0010802\ttotal: 52.3s\tremaining: 5.17s\n",
      "910:\tlearn: 0.0010800\ttotal: 52.3s\tremaining: 5.11s\n",
      "911:\tlearn: 0.0010798\ttotal: 52.4s\tremaining: 5.05s\n",
      "912:\tlearn: 0.0010797\ttotal: 52.4s\tremaining: 5s\n",
      "913:\tlearn: 0.0010797\ttotal: 52.5s\tremaining: 4.94s\n",
      "914:\tlearn: 0.0010786\ttotal: 52.5s\tremaining: 4.88s\n",
      "915:\tlearn: 0.0010784\ttotal: 52.6s\tremaining: 4.82s\n",
      "916:\tlearn: 0.0010781\ttotal: 52.7s\tremaining: 4.76s\n",
      "917:\tlearn: 0.0010780\ttotal: 52.7s\tremaining: 4.71s\n",
      "918:\tlearn: 0.0010780\ttotal: 52.8s\tremaining: 4.65s\n",
      "919:\tlearn: 0.0010778\ttotal: 52.8s\tremaining: 4.59s\n",
      "920:\tlearn: 0.0010778\ttotal: 52.9s\tremaining: 4.54s\n",
      "921:\tlearn: 0.0010776\ttotal: 52.9s\tremaining: 4.48s\n",
      "922:\tlearn: 0.0010776\ttotal: 53s\tremaining: 4.42s\n",
      "923:\tlearn: 0.0010757\ttotal: 53s\tremaining: 4.36s\n",
      "924:\tlearn: 0.0010756\ttotal: 53.1s\tremaining: 4.31s\n",
      "925:\tlearn: 0.0010755\ttotal: 53.2s\tremaining: 4.25s\n",
      "926:\tlearn: 0.0010754\ttotal: 53.2s\tremaining: 4.19s\n",
      "927:\tlearn: 0.0010753\ttotal: 53.3s\tremaining: 4.13s\n",
      "928:\tlearn: 0.0010746\ttotal: 53.3s\tremaining: 4.08s\n",
      "929:\tlearn: 0.0010746\ttotal: 53.4s\tremaining: 4.02s\n",
      "930:\tlearn: 0.0010744\ttotal: 53.4s\tremaining: 3.96s\n",
      "931:\tlearn: 0.0010742\ttotal: 53.5s\tremaining: 3.9s\n",
      "932:\tlearn: 0.0010739\ttotal: 53.6s\tremaining: 3.85s\n",
      "933:\tlearn: 0.0010736\ttotal: 53.6s\tremaining: 3.79s\n",
      "934:\tlearn: 0.0010735\ttotal: 53.7s\tremaining: 3.73s\n",
      "935:\tlearn: 0.0010735\ttotal: 53.7s\tremaining: 3.67s\n",
      "936:\tlearn: 0.0010732\ttotal: 53.8s\tremaining: 3.62s\n",
      "937:\tlearn: 0.0010731\ttotal: 53.8s\tremaining: 3.56s\n",
      "938:\tlearn: 0.0010731\ttotal: 53.9s\tremaining: 3.5s\n",
      "939:\tlearn: 0.0010730\ttotal: 53.9s\tremaining: 3.44s\n",
      "940:\tlearn: 0.0010730\ttotal: 54s\tremaining: 3.39s\n",
      "941:\tlearn: 0.0010730\ttotal: 54.1s\tremaining: 3.33s\n",
      "942:\tlearn: 0.0010730\ttotal: 54.1s\tremaining: 3.27s\n",
      "943:\tlearn: 0.0010730\ttotal: 54.2s\tremaining: 3.21s\n",
      "944:\tlearn: 0.0010729\ttotal: 54.2s\tremaining: 3.16s\n",
      "945:\tlearn: 0.0010729\ttotal: 54.3s\tremaining: 3.1s\n",
      "946:\tlearn: 0.0010727\ttotal: 54.3s\tremaining: 3.04s\n",
      "947:\tlearn: 0.0010727\ttotal: 54.4s\tremaining: 2.98s\n",
      "948:\tlearn: 0.0010725\ttotal: 54.5s\tremaining: 2.93s\n",
      "949:\tlearn: 0.0010724\ttotal: 54.5s\tremaining: 2.87s\n",
      "950:\tlearn: 0.0010723\ttotal: 54.6s\tremaining: 2.81s\n",
      "951:\tlearn: 0.0010720\ttotal: 54.6s\tremaining: 2.75s\n",
      "952:\tlearn: 0.0010719\ttotal: 54.7s\tremaining: 2.7s\n",
      "953:\tlearn: 0.0010717\ttotal: 54.7s\tremaining: 2.64s\n",
      "954:\tlearn: 0.0010717\ttotal: 54.8s\tremaining: 2.58s\n",
      "955:\tlearn: 0.0010714\ttotal: 54.9s\tremaining: 2.52s\n",
      "956:\tlearn: 0.0010714\ttotal: 54.9s\tremaining: 2.47s\n",
      "957:\tlearn: 0.0010705\ttotal: 55s\tremaining: 2.41s\n",
      "958:\tlearn: 0.0010702\ttotal: 55s\tremaining: 2.35s\n",
      "959:\tlearn: 0.0010702\ttotal: 55.1s\tremaining: 2.29s\n",
      "960:\tlearn: 0.0010701\ttotal: 55.1s\tremaining: 2.24s\n",
      "961:\tlearn: 0.0010700\ttotal: 55.2s\tremaining: 2.18s\n",
      "962:\tlearn: 0.0010699\ttotal: 55.2s\tremaining: 2.12s\n",
      "963:\tlearn: 0.0010699\ttotal: 55.3s\tremaining: 2.06s\n",
      "964:\tlearn: 0.0010698\ttotal: 55.4s\tremaining: 2.01s\n",
      "965:\tlearn: 0.0010698\ttotal: 55.4s\tremaining: 1.95s\n",
      "966:\tlearn: 0.0010697\ttotal: 55.5s\tremaining: 1.89s\n",
      "967:\tlearn: 0.0010697\ttotal: 55.5s\tremaining: 1.83s\n",
      "968:\tlearn: 0.0010696\ttotal: 55.6s\tremaining: 1.78s\n",
      "969:\tlearn: 0.0010696\ttotal: 55.6s\tremaining: 1.72s\n",
      "970:\tlearn: 0.0010695\ttotal: 55.7s\tremaining: 1.66s\n",
      "971:\tlearn: 0.0010694\ttotal: 55.7s\tremaining: 1.6s\n",
      "972:\tlearn: 0.0010691\ttotal: 55.8s\tremaining: 1.55s\n",
      "973:\tlearn: 0.0010691\ttotal: 55.9s\tremaining: 1.49s\n",
      "974:\tlearn: 0.0010689\ttotal: 55.9s\tremaining: 1.43s\n",
      "975:\tlearn: 0.0010689\ttotal: 56s\tremaining: 1.38s\n",
      "976:\tlearn: 0.0010688\ttotal: 56s\tremaining: 1.32s\n",
      "977:\tlearn: 0.0010688\ttotal: 56.1s\tremaining: 1.26s\n",
      "978:\tlearn: 0.0010688\ttotal: 56.1s\tremaining: 1.2s\n",
      "979:\tlearn: 0.0010687\ttotal: 56.2s\tremaining: 1.15s\n",
      "980:\tlearn: 0.0010687\ttotal: 56.3s\tremaining: 1.09s\n",
      "981:\tlearn: 0.0010684\ttotal: 56.3s\tremaining: 1.03s\n",
      "982:\tlearn: 0.0010684\ttotal: 56.4s\tremaining: 975ms\n",
      "983:\tlearn: 0.0010684\ttotal: 56.4s\tremaining: 917ms\n",
      "984:\tlearn: 0.0010683\ttotal: 56.5s\tremaining: 860ms\n",
      "985:\tlearn: 0.0010682\ttotal: 56.5s\tremaining: 803ms\n",
      "986:\tlearn: 0.0010682\ttotal: 56.6s\tremaining: 745ms\n",
      "987:\tlearn: 0.0010682\ttotal: 56.6s\tremaining: 688ms\n",
      "988:\tlearn: 0.0010681\ttotal: 56.7s\tremaining: 631ms\n",
      "989:\tlearn: 0.0010680\ttotal: 56.8s\tremaining: 573ms\n",
      "990:\tlearn: 0.0010680\ttotal: 56.8s\tremaining: 516ms\n",
      "991:\tlearn: 0.0010680\ttotal: 56.9s\tremaining: 459ms\n",
      "992:\tlearn: 0.0010678\ttotal: 56.9s\tremaining: 401ms\n",
      "993:\tlearn: 0.0010678\ttotal: 57s\tremaining: 344ms\n",
      "994:\tlearn: 0.0010676\ttotal: 57s\tremaining: 287ms\n",
      "995:\tlearn: 0.0010669\ttotal: 57.1s\tremaining: 229ms\n",
      "996:\tlearn: 0.0010666\ttotal: 57.2s\tremaining: 172ms\n",
      "997:\tlearn: 0.0010665\ttotal: 57.2s\tremaining: 115ms\n",
      "998:\tlearn: 0.0010664\ttotal: 57.3s\tremaining: 57.3ms\n",
      "999:\tlearn: 0.0010664\ttotal: 57.3s\tremaining: 0us\n",
      "0:\tlearn: 0.5821948\ttotal: 77.7ms\tremaining: 1m 17s\n",
      "1:\tlearn: 0.5353658\ttotal: 133ms\tremaining: 1m 6s\n",
      "2:\tlearn: 0.5115827\ttotal: 191ms\tremaining: 1m 3s\n",
      "3:\tlearn: 0.4780885\ttotal: 246ms\tremaining: 1m 1s\n",
      "4:\tlearn: 0.4516427\ttotal: 302ms\tremaining: 1m\n",
      "5:\tlearn: 0.3962342\ttotal: 357ms\tremaining: 59.2s\n",
      "6:\tlearn: 0.3360577\ttotal: 413ms\tremaining: 58.6s\n",
      "7:\tlearn: 0.2726580\ttotal: 470ms\tremaining: 58.2s\n",
      "8:\tlearn: 0.2146171\ttotal: 525ms\tremaining: 57.9s\n",
      "9:\tlearn: 0.1807195\ttotal: 582ms\tremaining: 57.6s\n",
      "10:\tlearn: 0.1446034\ttotal: 639ms\tremaining: 57.5s\n",
      "11:\tlearn: 0.1155023\ttotal: 696ms\tremaining: 57.3s\n",
      "12:\tlearn: 0.1028576\ttotal: 751ms\tremaining: 57s\n",
      "13:\tlearn: 0.0851078\ttotal: 809ms\tremaining: 57s\n",
      "14:\tlearn: 0.0687324\ttotal: 865ms\tremaining: 56.8s\n",
      "15:\tlearn: 0.0622459\ttotal: 920ms\tremaining: 56.6s\n",
      "16:\tlearn: 0.0508839\ttotal: 976ms\tremaining: 56.4s\n",
      "17:\tlearn: 0.0446578\ttotal: 1.03s\tremaining: 56.3s\n",
      "18:\tlearn: 0.0359197\ttotal: 1.09s\tremaining: 56.3s\n",
      "19:\tlearn: 0.0331160\ttotal: 1.15s\tremaining: 56.1s\n",
      "20:\tlearn: 0.0322838\ttotal: 1.2s\tremaining: 56s\n",
      "21:\tlearn: 0.0286302\ttotal: 1.26s\tremaining: 55.9s\n",
      "22:\tlearn: 0.0254396\ttotal: 1.31s\tremaining: 55.8s\n",
      "23:\tlearn: 0.0249077\ttotal: 1.37s\tremaining: 55.7s\n",
      "24:\tlearn: 0.0214674\ttotal: 1.43s\tremaining: 55.7s\n",
      "25:\tlearn: 0.0199160\ttotal: 1.48s\tremaining: 55.6s\n",
      "26:\tlearn: 0.0185855\ttotal: 1.54s\tremaining: 55.5s\n",
      "27:\tlearn: 0.0173237\ttotal: 1.59s\tremaining: 55.4s\n",
      "28:\tlearn: 0.0156529\ttotal: 1.65s\tremaining: 55.3s\n",
      "29:\tlearn: 0.0151756\ttotal: 1.71s\tremaining: 55.2s\n",
      "30:\tlearn: 0.0140180\ttotal: 1.76s\tremaining: 55.1s\n",
      "31:\tlearn: 0.0131170\ttotal: 1.82s\tremaining: 55s\n",
      "32:\tlearn: 0.0122753\ttotal: 1.88s\tremaining: 55s\n",
      "33:\tlearn: 0.0114216\ttotal: 1.93s\tremaining: 54.9s\n",
      "34:\tlearn: 0.0107951\ttotal: 1.99s\tremaining: 54.8s\n",
      "35:\tlearn: 0.0104321\ttotal: 2.04s\tremaining: 54.7s\n",
      "36:\tlearn: 0.0098747\ttotal: 2.1s\tremaining: 54.7s\n",
      "37:\tlearn: 0.0094557\ttotal: 2.16s\tremaining: 54.6s\n",
      "38:\tlearn: 0.0090863\ttotal: 2.21s\tremaining: 54.6s\n",
      "39:\tlearn: 0.0086350\ttotal: 2.27s\tremaining: 54.5s\n",
      "40:\tlearn: 0.0082542\ttotal: 2.33s\tremaining: 54.4s\n",
      "41:\tlearn: 0.0078731\ttotal: 2.38s\tremaining: 54.3s\n",
      "42:\tlearn: 0.0077353\ttotal: 2.44s\tremaining: 54.3s\n",
      "43:\tlearn: 0.0075698\ttotal: 2.49s\tremaining: 54.2s\n",
      "44:\tlearn: 0.0074375\ttotal: 2.55s\tremaining: 54.1s\n",
      "45:\tlearn: 0.0070895\ttotal: 2.61s\tremaining: 54.1s\n",
      "46:\tlearn: 0.0069070\ttotal: 2.66s\tremaining: 54s\n",
      "47:\tlearn: 0.0066022\ttotal: 2.72s\tremaining: 53.9s\n",
      "48:\tlearn: 0.0063822\ttotal: 2.77s\tremaining: 53.8s\n",
      "49:\tlearn: 0.0061120\ttotal: 2.83s\tremaining: 53.8s\n",
      "50:\tlearn: 0.0059040\ttotal: 2.88s\tremaining: 53.7s\n",
      "51:\tlearn: 0.0055620\ttotal: 2.94s\tremaining: 53.6s\n",
      "52:\tlearn: 0.0054415\ttotal: 3s\tremaining: 53.6s\n",
      "53:\tlearn: 0.0052462\ttotal: 3.05s\tremaining: 53.5s\n",
      "54:\tlearn: 0.0050903\ttotal: 3.11s\tremaining: 53.4s\n",
      "55:\tlearn: 0.0049556\ttotal: 3.17s\tremaining: 53.4s\n",
      "56:\tlearn: 0.0048066\ttotal: 3.22s\tremaining: 53.3s\n",
      "57:\tlearn: 0.0045605\ttotal: 3.28s\tremaining: 53.2s\n",
      "58:\tlearn: 0.0044423\ttotal: 3.33s\tremaining: 53.2s\n",
      "59:\tlearn: 0.0042981\ttotal: 3.39s\tremaining: 53.1s\n",
      "60:\tlearn: 0.0041778\ttotal: 3.45s\tremaining: 53s\n",
      "61:\tlearn: 0.0040436\ttotal: 3.5s\tremaining: 53s\n",
      "62:\tlearn: 0.0038757\ttotal: 3.56s\tremaining: 52.9s\n",
      "63:\tlearn: 0.0038070\ttotal: 3.61s\tremaining: 52.9s\n",
      "64:\tlearn: 0.0037481\ttotal: 3.67s\tremaining: 52.8s\n",
      "65:\tlearn: 0.0036487\ttotal: 3.73s\tremaining: 52.7s\n",
      "66:\tlearn: 0.0035611\ttotal: 3.78s\tremaining: 52.7s\n",
      "67:\tlearn: 0.0034895\ttotal: 3.84s\tremaining: 52.6s\n",
      "68:\tlearn: 0.0034090\ttotal: 3.89s\tremaining: 52.5s\n",
      "69:\tlearn: 0.0033484\ttotal: 3.95s\tremaining: 52.5s\n",
      "70:\tlearn: 0.0033484\ttotal: 4s\tremaining: 52.4s\n",
      "71:\tlearn: 0.0033483\ttotal: 4.07s\tremaining: 52.4s\n",
      "72:\tlearn: 0.0033481\ttotal: 4.12s\tremaining: 52.3s\n",
      "73:\tlearn: 0.0033231\ttotal: 4.18s\tremaining: 52.3s\n",
      "74:\tlearn: 0.0032528\ttotal: 4.23s\tremaining: 52.2s\n",
      "75:\tlearn: 0.0031964\ttotal: 4.29s\tremaining: 52.1s\n",
      "76:\tlearn: 0.0031230\ttotal: 4.34s\tremaining: 52.1s\n",
      "77:\tlearn: 0.0030346\ttotal: 4.4s\tremaining: 52s\n",
      "78:\tlearn: 0.0029607\ttotal: 4.46s\tremaining: 52s\n",
      "79:\tlearn: 0.0028762\ttotal: 4.51s\tremaining: 51.9s\n",
      "80:\tlearn: 0.0028306\ttotal: 4.57s\tremaining: 51.8s\n",
      "81:\tlearn: 0.0027654\ttotal: 4.63s\tremaining: 51.8s\n",
      "82:\tlearn: 0.0027311\ttotal: 4.68s\tremaining: 51.7s\n",
      "83:\tlearn: 0.0026640\ttotal: 4.74s\tremaining: 51.7s\n",
      "84:\tlearn: 0.0026039\ttotal: 4.8s\tremaining: 51.6s\n",
      "85:\tlearn: 0.0025558\ttotal: 4.85s\tremaining: 51.6s\n",
      "86:\tlearn: 0.0024989\ttotal: 4.91s\tremaining: 51.5s\n",
      "87:\tlearn: 0.0024470\ttotal: 4.96s\tremaining: 51.5s\n",
      "88:\tlearn: 0.0023899\ttotal: 5.02s\tremaining: 51.4s\n",
      "89:\tlearn: 0.0023897\ttotal: 5.08s\tremaining: 51.3s\n",
      "90:\tlearn: 0.0023876\ttotal: 5.13s\tremaining: 51.3s\n",
      "91:\tlearn: 0.0023871\ttotal: 5.19s\tremaining: 51.2s\n",
      "92:\tlearn: 0.0023685\ttotal: 5.25s\tremaining: 51.2s\n",
      "93:\tlearn: 0.0023489\ttotal: 5.3s\tremaining: 51.1s\n",
      "94:\tlearn: 0.0023450\ttotal: 5.36s\tremaining: 51s\n",
      "95:\tlearn: 0.0023361\ttotal: 5.42s\tremaining: 51s\n",
      "96:\tlearn: 0.0023064\ttotal: 5.47s\tremaining: 50.9s\n",
      "97:\tlearn: 0.0022651\ttotal: 5.53s\tremaining: 50.9s\n",
      "98:\tlearn: 0.0022097\ttotal: 5.58s\tremaining: 50.8s\n",
      "99:\tlearn: 0.0021687\ttotal: 5.64s\tremaining: 50.8s\n",
      "100:\tlearn: 0.0021280\ttotal: 5.7s\tremaining: 50.7s\n",
      "101:\tlearn: 0.0021276\ttotal: 5.75s\tremaining: 50.7s\n",
      "102:\tlearn: 0.0020980\ttotal: 5.81s\tremaining: 50.6s\n",
      "103:\tlearn: 0.0020978\ttotal: 5.87s\tremaining: 50.5s\n",
      "104:\tlearn: 0.0020976\ttotal: 5.92s\tremaining: 50.5s\n",
      "105:\tlearn: 0.0020973\ttotal: 5.98s\tremaining: 50.4s\n",
      "106:\tlearn: 0.0020973\ttotal: 6.03s\tremaining: 50.4s\n",
      "107:\tlearn: 0.0020971\ttotal: 6.09s\tremaining: 50.3s\n",
      "108:\tlearn: 0.0020963\ttotal: 6.15s\tremaining: 50.2s\n",
      "109:\tlearn: 0.0020963\ttotal: 6.2s\tremaining: 50.2s\n",
      "110:\tlearn: 0.0020725\ttotal: 6.26s\tremaining: 50.1s\n",
      "111:\tlearn: 0.0020281\ttotal: 6.31s\tremaining: 50s\n",
      "112:\tlearn: 0.0019727\ttotal: 6.37s\tremaining: 50s\n",
      "113:\tlearn: 0.0019727\ttotal: 6.42s\tremaining: 49.9s\n",
      "114:\tlearn: 0.0019415\ttotal: 6.48s\tremaining: 49.9s\n",
      "115:\tlearn: 0.0019211\ttotal: 6.53s\tremaining: 49.8s\n",
      "116:\tlearn: 0.0019005\ttotal: 6.59s\tremaining: 49.8s\n",
      "117:\tlearn: 0.0019004\ttotal: 6.65s\tremaining: 49.7s\n",
      "118:\tlearn: 0.0019004\ttotal: 6.7s\tremaining: 49.6s\n",
      "119:\tlearn: 0.0019004\ttotal: 6.76s\tremaining: 49.6s\n",
      "120:\tlearn: 0.0019004\ttotal: 6.81s\tremaining: 49.5s\n",
      "121:\tlearn: 0.0019000\ttotal: 6.87s\tremaining: 49.4s\n",
      "122:\tlearn: 0.0018565\ttotal: 6.92s\tremaining: 49.4s\n",
      "123:\tlearn: 0.0018454\ttotal: 6.98s\tremaining: 49.3s\n",
      "124:\tlearn: 0.0018352\ttotal: 7.04s\tremaining: 49.3s\n",
      "125:\tlearn: 0.0018043\ttotal: 7.09s\tremaining: 49.2s\n",
      "126:\tlearn: 0.0017870\ttotal: 7.15s\tremaining: 49.1s\n",
      "127:\tlearn: 0.0017870\ttotal: 7.2s\tremaining: 49.1s\n",
      "128:\tlearn: 0.0017863\ttotal: 7.26s\tremaining: 49s\n",
      "129:\tlearn: 0.0017751\ttotal: 7.32s\tremaining: 49s\n",
      "130:\tlearn: 0.0017735\ttotal: 7.37s\tremaining: 48.9s\n",
      "131:\tlearn: 0.0017731\ttotal: 7.43s\tremaining: 48.8s\n",
      "132:\tlearn: 0.0017728\ttotal: 7.48s\tremaining: 48.8s\n",
      "133:\tlearn: 0.0017727\ttotal: 7.54s\tremaining: 48.7s\n",
      "134:\tlearn: 0.0017726\ttotal: 7.6s\tremaining: 48.7s\n",
      "135:\tlearn: 0.0017547\ttotal: 7.65s\tremaining: 48.6s\n",
      "136:\tlearn: 0.0017544\ttotal: 7.71s\tremaining: 48.6s\n",
      "137:\tlearn: 0.0017463\ttotal: 7.77s\tremaining: 48.5s\n",
      "138:\tlearn: 0.0017461\ttotal: 7.83s\tremaining: 48.5s\n",
      "139:\tlearn: 0.0017456\ttotal: 7.89s\tremaining: 48.4s\n",
      "140:\tlearn: 0.0017453\ttotal: 7.94s\tremaining: 48.4s\n",
      "141:\tlearn: 0.0017397\ttotal: 8s\tremaining: 48.3s\n",
      "142:\tlearn: 0.0017390\ttotal: 8.05s\tremaining: 48.3s\n",
      "143:\tlearn: 0.0017388\ttotal: 8.11s\tremaining: 48.2s\n",
      "144:\tlearn: 0.0017388\ttotal: 8.17s\tremaining: 48.2s\n",
      "145:\tlearn: 0.0017387\ttotal: 8.22s\tremaining: 48.1s\n",
      "146:\tlearn: 0.0017385\ttotal: 8.28s\tremaining: 48s\n",
      "147:\tlearn: 0.0017381\ttotal: 8.33s\tremaining: 48s\n",
      "148:\tlearn: 0.0017376\ttotal: 8.39s\tremaining: 47.9s\n",
      "149:\tlearn: 0.0017376\ttotal: 8.44s\tremaining: 47.9s\n",
      "150:\tlearn: 0.0017372\ttotal: 8.5s\tremaining: 47.8s\n",
      "151:\tlearn: 0.0017372\ttotal: 8.56s\tremaining: 47.7s\n",
      "152:\tlearn: 0.0017305\ttotal: 8.62s\tremaining: 47.7s\n",
      "153:\tlearn: 0.0017300\ttotal: 8.67s\tremaining: 47.6s\n",
      "154:\tlearn: 0.0017197\ttotal: 8.73s\tremaining: 47.6s\n",
      "155:\tlearn: 0.0017196\ttotal: 8.78s\tremaining: 47.5s\n",
      "156:\tlearn: 0.0017189\ttotal: 8.84s\tremaining: 47.5s\n",
      "157:\tlearn: 0.0017188\ttotal: 8.89s\tremaining: 47.4s\n",
      "158:\tlearn: 0.0017188\ttotal: 8.95s\tremaining: 47.3s\n",
      "159:\tlearn: 0.0016992\ttotal: 9.01s\tremaining: 47.3s\n",
      "160:\tlearn: 0.0016681\ttotal: 9.06s\tremaining: 47.2s\n",
      "161:\tlearn: 0.0016636\ttotal: 9.12s\tremaining: 47.2s\n",
      "162:\tlearn: 0.0016628\ttotal: 9.17s\tremaining: 47.1s\n",
      "163:\tlearn: 0.0016626\ttotal: 9.23s\tremaining: 47s\n",
      "164:\tlearn: 0.0016625\ttotal: 9.28s\tremaining: 47s\n",
      "165:\tlearn: 0.0016625\ttotal: 9.34s\tremaining: 46.9s\n",
      "166:\tlearn: 0.0016623\ttotal: 9.39s\tremaining: 46.9s\n",
      "167:\tlearn: 0.0016620\ttotal: 9.45s\tremaining: 46.8s\n",
      "168:\tlearn: 0.0016494\ttotal: 9.51s\tremaining: 46.8s\n",
      "169:\tlearn: 0.0016494\ttotal: 9.56s\tremaining: 46.7s\n",
      "170:\tlearn: 0.0016494\ttotal: 9.62s\tremaining: 46.6s\n",
      "171:\tlearn: 0.0016493\ttotal: 9.67s\tremaining: 46.6s\n",
      "172:\tlearn: 0.0016452\ttotal: 9.73s\tremaining: 46.5s\n",
      "173:\tlearn: 0.0016325\ttotal: 9.79s\tremaining: 46.5s\n",
      "174:\tlearn: 0.0016325\ttotal: 9.84s\tremaining: 46.4s\n",
      "175:\tlearn: 0.0016322\ttotal: 9.9s\tremaining: 46.3s\n",
      "176:\tlearn: 0.0016244\ttotal: 9.95s\tremaining: 46.3s\n",
      "177:\tlearn: 0.0016243\ttotal: 10s\tremaining: 46.2s\n",
      "178:\tlearn: 0.0016238\ttotal: 10.1s\tremaining: 46.2s\n",
      "179:\tlearn: 0.0016237\ttotal: 10.1s\tremaining: 46.1s\n",
      "180:\tlearn: 0.0016184\ttotal: 10.2s\tremaining: 46.1s\n",
      "181:\tlearn: 0.0016181\ttotal: 10.2s\tremaining: 46s\n",
      "182:\tlearn: 0.0016177\ttotal: 10.3s\tremaining: 45.9s\n",
      "183:\tlearn: 0.0016176\ttotal: 10.3s\tremaining: 45.9s\n",
      "184:\tlearn: 0.0016176\ttotal: 10.4s\tremaining: 45.8s\n",
      "185:\tlearn: 0.0016176\ttotal: 10.5s\tremaining: 45.8s\n",
      "186:\tlearn: 0.0016175\ttotal: 10.5s\tremaining: 45.7s\n",
      "187:\tlearn: 0.0016169\ttotal: 10.6s\tremaining: 45.7s\n",
      "188:\tlearn: 0.0016167\ttotal: 10.6s\tremaining: 45.6s\n",
      "189:\tlearn: 0.0016167\ttotal: 10.7s\tremaining: 45.6s\n",
      "190:\tlearn: 0.0016167\ttotal: 10.7s\tremaining: 45.5s\n",
      "191:\tlearn: 0.0016094\ttotal: 10.8s\tremaining: 45.4s\n",
      "192:\tlearn: 0.0016091\ttotal: 10.9s\tremaining: 45.4s\n",
      "193:\tlearn: 0.0016088\ttotal: 10.9s\tremaining: 45.4s\n",
      "194:\tlearn: 0.0016004\ttotal: 11s\tremaining: 45.3s\n",
      "195:\tlearn: 0.0016003\ttotal: 11s\tremaining: 45.3s\n",
      "196:\tlearn: 0.0016003\ttotal: 11.1s\tremaining: 45.2s\n",
      "197:\tlearn: 0.0015997\ttotal: 11.2s\tremaining: 45.2s\n",
      "198:\tlearn: 0.0015996\ttotal: 11.2s\tremaining: 45.1s\n",
      "199:\tlearn: 0.0015993\ttotal: 11.3s\tremaining: 45.1s\n",
      "200:\tlearn: 0.0015993\ttotal: 11.3s\tremaining: 45s\n",
      "201:\tlearn: 0.0015992\ttotal: 11.4s\tremaining: 45s\n",
      "202:\tlearn: 0.0015989\ttotal: 11.4s\tremaining: 44.9s\n",
      "203:\tlearn: 0.0015989\ttotal: 11.5s\tremaining: 44.9s\n",
      "204:\tlearn: 0.0015805\ttotal: 11.6s\tremaining: 44.8s\n",
      "205:\tlearn: 0.0015804\ttotal: 11.6s\tremaining: 44.7s\n",
      "206:\tlearn: 0.0015801\ttotal: 11.7s\tremaining: 44.7s\n",
      "207:\tlearn: 0.0015800\ttotal: 11.7s\tremaining: 44.6s\n",
      "208:\tlearn: 0.0015800\ttotal: 11.8s\tremaining: 44.6s\n",
      "209:\tlearn: 0.0015766\ttotal: 11.8s\tremaining: 44.5s\n",
      "210:\tlearn: 0.0015766\ttotal: 11.9s\tremaining: 44.5s\n",
      "211:\tlearn: 0.0015766\ttotal: 11.9s\tremaining: 44.4s\n",
      "212:\tlearn: 0.0015760\ttotal: 12s\tremaining: 44.4s\n",
      "213:\tlearn: 0.0015553\ttotal: 12.1s\tremaining: 44.3s\n",
      "214:\tlearn: 0.0015548\ttotal: 12.1s\tremaining: 44.3s\n",
      "215:\tlearn: 0.0015366\ttotal: 12.2s\tremaining: 44.2s\n",
      "216:\tlearn: 0.0015365\ttotal: 12.2s\tremaining: 44.2s\n",
      "217:\tlearn: 0.0015364\ttotal: 12.3s\tremaining: 44.1s\n",
      "218:\tlearn: 0.0015362\ttotal: 12.3s\tremaining: 44s\n",
      "219:\tlearn: 0.0015360\ttotal: 12.4s\tremaining: 44s\n",
      "220:\tlearn: 0.0015359\ttotal: 12.5s\tremaining: 43.9s\n",
      "221:\tlearn: 0.0015357\ttotal: 12.5s\tremaining: 43.9s\n",
      "222:\tlearn: 0.0015352\ttotal: 12.6s\tremaining: 43.8s\n",
      "223:\tlearn: 0.0015350\ttotal: 12.6s\tremaining: 43.8s\n",
      "224:\tlearn: 0.0015350\ttotal: 12.7s\tremaining: 43.7s\n",
      "225:\tlearn: 0.0015349\ttotal: 12.7s\tremaining: 43.7s\n",
      "226:\tlearn: 0.0015347\ttotal: 12.8s\tremaining: 43.6s\n",
      "227:\tlearn: 0.0015337\ttotal: 12.9s\tremaining: 43.5s\n",
      "228:\tlearn: 0.0015335\ttotal: 12.9s\tremaining: 43.5s\n",
      "229:\tlearn: 0.0015333\ttotal: 13s\tremaining: 43.4s\n",
      "230:\tlearn: 0.0015332\ttotal: 13s\tremaining: 43.4s\n",
      "231:\tlearn: 0.0015331\ttotal: 13.1s\tremaining: 43.3s\n",
      "232:\tlearn: 0.0015324\ttotal: 13.1s\tremaining: 43.3s\n",
      "233:\tlearn: 0.0015324\ttotal: 13.2s\tremaining: 43.2s\n",
      "234:\tlearn: 0.0015240\ttotal: 13.3s\tremaining: 43.1s\n",
      "235:\tlearn: 0.0015239\ttotal: 13.3s\tremaining: 43.1s\n",
      "236:\tlearn: 0.0015238\ttotal: 13.4s\tremaining: 43s\n",
      "237:\tlearn: 0.0015136\ttotal: 13.4s\tremaining: 43s\n",
      "238:\tlearn: 0.0015114\ttotal: 13.5s\tremaining: 42.9s\n",
      "239:\tlearn: 0.0015114\ttotal: 13.5s\tremaining: 42.9s\n",
      "240:\tlearn: 0.0015113\ttotal: 13.6s\tremaining: 42.9s\n",
      "241:\tlearn: 0.0015113\ttotal: 13.7s\tremaining: 42.8s\n",
      "242:\tlearn: 0.0015111\ttotal: 13.7s\tremaining: 42.7s\n",
      "243:\tlearn: 0.0015110\ttotal: 13.8s\tremaining: 42.7s\n",
      "244:\tlearn: 0.0015106\ttotal: 13.8s\tremaining: 42.6s\n",
      "245:\tlearn: 0.0015102\ttotal: 13.9s\tremaining: 42.6s\n",
      "246:\tlearn: 0.0015094\ttotal: 13.9s\tremaining: 42.5s\n",
      "247:\tlearn: 0.0015094\ttotal: 14s\tremaining: 42.5s\n",
      "248:\tlearn: 0.0015092\ttotal: 14.1s\tremaining: 42.4s\n",
      "249:\tlearn: 0.0015089\ttotal: 14.1s\tremaining: 42.4s\n",
      "250:\tlearn: 0.0015087\ttotal: 14.2s\tremaining: 42.3s\n",
      "251:\tlearn: 0.0015087\ttotal: 14.2s\tremaining: 42.2s\n",
      "252:\tlearn: 0.0015057\ttotal: 14.3s\tremaining: 42.2s\n",
      "253:\tlearn: 0.0014942\ttotal: 14.3s\tremaining: 42.1s\n",
      "254:\tlearn: 0.0014942\ttotal: 14.4s\tremaining: 42.1s\n",
      "255:\tlearn: 0.0014820\ttotal: 14.5s\tremaining: 42s\n",
      "256:\tlearn: 0.0014817\ttotal: 14.5s\tremaining: 42s\n",
      "257:\tlearn: 0.0014817\ttotal: 14.6s\tremaining: 41.9s\n",
      "258:\tlearn: 0.0014816\ttotal: 14.6s\tremaining: 41.9s\n",
      "259:\tlearn: 0.0014815\ttotal: 14.7s\tremaining: 41.8s\n",
      "260:\tlearn: 0.0014814\ttotal: 14.7s\tremaining: 41.7s\n",
      "261:\tlearn: 0.0014813\ttotal: 14.8s\tremaining: 41.7s\n",
      "262:\tlearn: 0.0014813\ttotal: 14.9s\tremaining: 41.6s\n",
      "263:\tlearn: 0.0014810\ttotal: 14.9s\tremaining: 41.6s\n",
      "264:\tlearn: 0.0014810\ttotal: 15s\tremaining: 41.5s\n",
      "265:\tlearn: 0.0014808\ttotal: 15s\tremaining: 41.5s\n",
      "266:\tlearn: 0.0014805\ttotal: 15.1s\tremaining: 41.4s\n",
      "267:\tlearn: 0.0014804\ttotal: 15.1s\tremaining: 41.4s\n",
      "268:\tlearn: 0.0014706\ttotal: 15.2s\tremaining: 41.3s\n",
      "269:\tlearn: 0.0014705\ttotal: 15.3s\tremaining: 41.2s\n",
      "270:\tlearn: 0.0014700\ttotal: 15.3s\tremaining: 41.2s\n",
      "271:\tlearn: 0.0014693\ttotal: 15.4s\tremaining: 41.1s\n",
      "272:\tlearn: 0.0014692\ttotal: 15.4s\tremaining: 41.1s\n",
      "273:\tlearn: 0.0014690\ttotal: 15.5s\tremaining: 41s\n",
      "274:\tlearn: 0.0014690\ttotal: 15.5s\tremaining: 41s\n",
      "275:\tlearn: 0.0014690\ttotal: 15.6s\tremaining: 40.9s\n",
      "276:\tlearn: 0.0014690\ttotal: 15.7s\tremaining: 40.9s\n",
      "277:\tlearn: 0.0014689\ttotal: 15.7s\tremaining: 40.8s\n",
      "278:\tlearn: 0.0014685\ttotal: 15.8s\tremaining: 40.7s\n",
      "279:\tlearn: 0.0014684\ttotal: 15.8s\tremaining: 40.7s\n",
      "280:\tlearn: 0.0014681\ttotal: 15.9s\tremaining: 40.6s\n",
      "281:\tlearn: 0.0014678\ttotal: 15.9s\tremaining: 40.6s\n",
      "282:\tlearn: 0.0014678\ttotal: 16s\tremaining: 40.5s\n",
      "283:\tlearn: 0.0014678\ttotal: 16s\tremaining: 40.5s\n",
      "284:\tlearn: 0.0014678\ttotal: 16.1s\tremaining: 40.4s\n",
      "285:\tlearn: 0.0014678\ttotal: 16.2s\tremaining: 40.3s\n",
      "286:\tlearn: 0.0014676\ttotal: 16.2s\tremaining: 40.3s\n",
      "287:\tlearn: 0.0014661\ttotal: 16.3s\tremaining: 40.2s\n",
      "288:\tlearn: 0.0014658\ttotal: 16.3s\tremaining: 40.2s\n",
      "289:\tlearn: 0.0014656\ttotal: 16.4s\tremaining: 40.1s\n",
      "290:\tlearn: 0.0014646\ttotal: 16.4s\tremaining: 40.1s\n",
      "291:\tlearn: 0.0014641\ttotal: 16.5s\tremaining: 40s\n",
      "292:\tlearn: 0.0014637\ttotal: 16.6s\tremaining: 40s\n",
      "293:\tlearn: 0.0014633\ttotal: 16.6s\tremaining: 39.9s\n",
      "294:\tlearn: 0.0014632\ttotal: 16.7s\tremaining: 39.8s\n",
      "295:\tlearn: 0.0014533\ttotal: 16.7s\tremaining: 39.8s\n",
      "296:\tlearn: 0.0014533\ttotal: 16.8s\tremaining: 39.7s\n",
      "297:\tlearn: 0.0014532\ttotal: 16.8s\tremaining: 39.7s\n",
      "298:\tlearn: 0.0014519\ttotal: 16.9s\tremaining: 39.6s\n",
      "299:\tlearn: 0.0014516\ttotal: 16.9s\tremaining: 39.5s\n",
      "300:\tlearn: 0.0014515\ttotal: 17s\tremaining: 39.5s\n",
      "301:\tlearn: 0.0014515\ttotal: 17.1s\tremaining: 39.4s\n",
      "302:\tlearn: 0.0014513\ttotal: 17.1s\tremaining: 39.4s\n",
      "303:\tlearn: 0.0014513\ttotal: 17.2s\tremaining: 39.3s\n",
      "304:\tlearn: 0.0014510\ttotal: 17.2s\tremaining: 39.3s\n",
      "305:\tlearn: 0.0014510\ttotal: 17.3s\tremaining: 39.2s\n",
      "306:\tlearn: 0.0014507\ttotal: 17.3s\tremaining: 39.1s\n",
      "307:\tlearn: 0.0014507\ttotal: 17.4s\tremaining: 39.1s\n",
      "308:\tlearn: 0.0014505\ttotal: 17.4s\tremaining: 39s\n",
      "309:\tlearn: 0.0014486\ttotal: 17.5s\tremaining: 39s\n",
      "310:\tlearn: 0.0014483\ttotal: 17.6s\tremaining: 38.9s\n",
      "311:\tlearn: 0.0014482\ttotal: 17.6s\tremaining: 38.8s\n",
      "312:\tlearn: 0.0014475\ttotal: 17.7s\tremaining: 38.8s\n",
      "313:\tlearn: 0.0014474\ttotal: 17.7s\tremaining: 38.7s\n",
      "314:\tlearn: 0.0014474\ttotal: 17.8s\tremaining: 38.7s\n",
      "315:\tlearn: 0.0014472\ttotal: 17.8s\tremaining: 38.6s\n",
      "316:\tlearn: 0.0014470\ttotal: 17.9s\tremaining: 38.6s\n",
      "317:\tlearn: 0.0014469\ttotal: 18s\tremaining: 38.5s\n",
      "318:\tlearn: 0.0014463\ttotal: 18s\tremaining: 38.4s\n",
      "319:\tlearn: 0.0014445\ttotal: 18.1s\tremaining: 38.4s\n",
      "320:\tlearn: 0.0014445\ttotal: 18.1s\tremaining: 38.3s\n",
      "321:\tlearn: 0.0014431\ttotal: 18.2s\tremaining: 38.3s\n",
      "322:\tlearn: 0.0014358\ttotal: 18.2s\tremaining: 38.2s\n",
      "323:\tlearn: 0.0014356\ttotal: 18.3s\tremaining: 38.2s\n",
      "324:\tlearn: 0.0014356\ttotal: 18.3s\tremaining: 38.1s\n",
      "325:\tlearn: 0.0014352\ttotal: 18.4s\tremaining: 38s\n",
      "326:\tlearn: 0.0014350\ttotal: 18.5s\tremaining: 38s\n",
      "327:\tlearn: 0.0014350\ttotal: 18.5s\tremaining: 37.9s\n",
      "328:\tlearn: 0.0014348\ttotal: 18.6s\tremaining: 37.9s\n",
      "329:\tlearn: 0.0014348\ttotal: 18.6s\tremaining: 37.8s\n",
      "330:\tlearn: 0.0014347\ttotal: 18.7s\tremaining: 37.8s\n",
      "331:\tlearn: 0.0014346\ttotal: 18.7s\tremaining: 37.7s\n",
      "332:\tlearn: 0.0014277\ttotal: 18.8s\tremaining: 37.6s\n",
      "333:\tlearn: 0.0014274\ttotal: 18.9s\tremaining: 37.6s\n",
      "334:\tlearn: 0.0014265\ttotal: 18.9s\tremaining: 37.5s\n",
      "335:\tlearn: 0.0014263\ttotal: 19s\tremaining: 37.5s\n",
      "336:\tlearn: 0.0014263\ttotal: 19s\tremaining: 37.4s\n",
      "337:\tlearn: 0.0014262\ttotal: 19.1s\tremaining: 37.4s\n",
      "338:\tlearn: 0.0014260\ttotal: 19.1s\tremaining: 37.3s\n",
      "339:\tlearn: 0.0014260\ttotal: 19.2s\tremaining: 37.2s\n",
      "340:\tlearn: 0.0014260\ttotal: 19.2s\tremaining: 37.2s\n",
      "341:\tlearn: 0.0014253\ttotal: 19.3s\tremaining: 37.1s\n",
      "342:\tlearn: 0.0014250\ttotal: 19.4s\tremaining: 37.1s\n",
      "343:\tlearn: 0.0014244\ttotal: 19.4s\tremaining: 37s\n",
      "344:\tlearn: 0.0014243\ttotal: 19.5s\tremaining: 37s\n",
      "345:\tlearn: 0.0014241\ttotal: 19.5s\tremaining: 36.9s\n",
      "346:\tlearn: 0.0014241\ttotal: 19.6s\tremaining: 36.8s\n",
      "347:\tlearn: 0.0014239\ttotal: 19.6s\tremaining: 36.8s\n",
      "348:\tlearn: 0.0014239\ttotal: 19.7s\tremaining: 36.7s\n",
      "349:\tlearn: 0.0014238\ttotal: 19.7s\tremaining: 36.7s\n",
      "350:\tlearn: 0.0014229\ttotal: 19.8s\tremaining: 36.6s\n",
      "351:\tlearn: 0.0014228\ttotal: 19.9s\tremaining: 36.6s\n",
      "352:\tlearn: 0.0014222\ttotal: 19.9s\tremaining: 36.5s\n",
      "353:\tlearn: 0.0014174\ttotal: 20s\tremaining: 36.4s\n",
      "354:\tlearn: 0.0014093\ttotal: 20s\tremaining: 36.4s\n",
      "355:\tlearn: 0.0014085\ttotal: 20.1s\tremaining: 36.3s\n",
      "356:\tlearn: 0.0014084\ttotal: 20.1s\tremaining: 36.3s\n",
      "357:\tlearn: 0.0014080\ttotal: 20.2s\tremaining: 36.2s\n",
      "358:\tlearn: 0.0014074\ttotal: 20.3s\tremaining: 36.2s\n",
      "359:\tlearn: 0.0014074\ttotal: 20.3s\tremaining: 36.1s\n",
      "360:\tlearn: 0.0014073\ttotal: 20.4s\tremaining: 36s\n",
      "361:\tlearn: 0.0014073\ttotal: 20.4s\tremaining: 36s\n",
      "362:\tlearn: 0.0014072\ttotal: 20.5s\tremaining: 35.9s\n",
      "363:\tlearn: 0.0014072\ttotal: 20.5s\tremaining: 35.9s\n",
      "364:\tlearn: 0.0014071\ttotal: 20.6s\tremaining: 35.8s\n",
      "365:\tlearn: 0.0014063\ttotal: 20.6s\tremaining: 35.8s\n",
      "366:\tlearn: 0.0014062\ttotal: 20.7s\tremaining: 35.7s\n",
      "367:\tlearn: 0.0014059\ttotal: 20.8s\tremaining: 35.6s\n",
      "368:\tlearn: 0.0014058\ttotal: 20.8s\tremaining: 35.6s\n",
      "369:\tlearn: 0.0014058\ttotal: 20.9s\tremaining: 35.5s\n",
      "370:\tlearn: 0.0014058\ttotal: 20.9s\tremaining: 35.5s\n",
      "371:\tlearn: 0.0014057\ttotal: 21s\tremaining: 35.4s\n",
      "372:\tlearn: 0.0014057\ttotal: 21s\tremaining: 35.4s\n",
      "373:\tlearn: 0.0014056\ttotal: 21.1s\tremaining: 35.3s\n",
      "374:\tlearn: 0.0014055\ttotal: 21.1s\tremaining: 35.2s\n",
      "375:\tlearn: 0.0014041\ttotal: 21.2s\tremaining: 35.2s\n",
      "376:\tlearn: 0.0014034\ttotal: 21.3s\tremaining: 35.1s\n",
      "377:\tlearn: 0.0014030\ttotal: 21.3s\tremaining: 35.1s\n",
      "378:\tlearn: 0.0014011\ttotal: 21.4s\tremaining: 35s\n",
      "379:\tlearn: 0.0014008\ttotal: 21.4s\tremaining: 35s\n",
      "380:\tlearn: 0.0013895\ttotal: 21.5s\tremaining: 34.9s\n",
      "381:\tlearn: 0.0013894\ttotal: 21.5s\tremaining: 34.8s\n",
      "382:\tlearn: 0.0013894\ttotal: 21.6s\tremaining: 34.8s\n",
      "383:\tlearn: 0.0013890\ttotal: 21.6s\tremaining: 34.7s\n",
      "384:\tlearn: 0.0013889\ttotal: 21.7s\tremaining: 34.7s\n",
      "385:\tlearn: 0.0013889\ttotal: 21.8s\tremaining: 34.6s\n",
      "386:\tlearn: 0.0013880\ttotal: 21.8s\tremaining: 34.6s\n",
      "387:\tlearn: 0.0013880\ttotal: 21.9s\tremaining: 34.5s\n",
      "388:\tlearn: 0.0013877\ttotal: 21.9s\tremaining: 34.4s\n",
      "389:\tlearn: 0.0013872\ttotal: 22s\tremaining: 34.4s\n",
      "390:\tlearn: 0.0013871\ttotal: 22s\tremaining: 34.3s\n",
      "391:\tlearn: 0.0013871\ttotal: 22.1s\tremaining: 34.3s\n",
      "392:\tlearn: 0.0013842\ttotal: 22.2s\tremaining: 34.2s\n",
      "393:\tlearn: 0.0013842\ttotal: 22.2s\tremaining: 34.2s\n",
      "394:\tlearn: 0.0013841\ttotal: 22.3s\tremaining: 34.1s\n",
      "395:\tlearn: 0.0013841\ttotal: 22.3s\tremaining: 34s\n",
      "396:\tlearn: 0.0013738\ttotal: 22.4s\tremaining: 34s\n",
      "397:\tlearn: 0.0013738\ttotal: 22.4s\tremaining: 33.9s\n",
      "398:\tlearn: 0.0013737\ttotal: 22.5s\tremaining: 33.9s\n",
      "399:\tlearn: 0.0013736\ttotal: 22.5s\tremaining: 33.8s\n",
      "400:\tlearn: 0.0013734\ttotal: 22.6s\tremaining: 33.8s\n",
      "401:\tlearn: 0.0013625\ttotal: 22.7s\tremaining: 33.7s\n",
      "402:\tlearn: 0.0013624\ttotal: 22.7s\tremaining: 33.7s\n",
      "403:\tlearn: 0.0013619\ttotal: 22.8s\tremaining: 33.6s\n",
      "404:\tlearn: 0.0013616\ttotal: 22.8s\tremaining: 33.5s\n",
      "405:\tlearn: 0.0013616\ttotal: 22.9s\tremaining: 33.5s\n",
      "406:\tlearn: 0.0013611\ttotal: 22.9s\tremaining: 33.4s\n",
      "407:\tlearn: 0.0013610\ttotal: 23s\tremaining: 33.4s\n",
      "408:\tlearn: 0.0013610\ttotal: 23.1s\tremaining: 33.3s\n",
      "409:\tlearn: 0.0013606\ttotal: 23.1s\tremaining: 33.3s\n",
      "410:\tlearn: 0.0013605\ttotal: 23.2s\tremaining: 33.2s\n",
      "411:\tlearn: 0.0013602\ttotal: 23.2s\tremaining: 33.1s\n",
      "412:\tlearn: 0.0013602\ttotal: 23.3s\tremaining: 33.1s\n",
      "413:\tlearn: 0.0013602\ttotal: 23.3s\tremaining: 33s\n",
      "414:\tlearn: 0.0013601\ttotal: 23.4s\tremaining: 33s\n",
      "415:\tlearn: 0.0013600\ttotal: 23.4s\tremaining: 32.9s\n",
      "416:\tlearn: 0.0013600\ttotal: 23.5s\tremaining: 32.8s\n",
      "417:\tlearn: 0.0013599\ttotal: 23.6s\tremaining: 32.8s\n",
      "418:\tlearn: 0.0013597\ttotal: 23.6s\tremaining: 32.7s\n",
      "419:\tlearn: 0.0013597\ttotal: 23.7s\tremaining: 32.7s\n",
      "420:\tlearn: 0.0013596\ttotal: 23.7s\tremaining: 32.6s\n",
      "421:\tlearn: 0.0013596\ttotal: 23.8s\tremaining: 32.6s\n",
      "422:\tlearn: 0.0013569\ttotal: 23.8s\tremaining: 32.5s\n",
      "423:\tlearn: 0.0013562\ttotal: 23.9s\tremaining: 32.5s\n",
      "424:\tlearn: 0.0013561\ttotal: 23.9s\tremaining: 32.4s\n",
      "425:\tlearn: 0.0013559\ttotal: 24s\tremaining: 32.3s\n",
      "426:\tlearn: 0.0013559\ttotal: 24.1s\tremaining: 32.3s\n",
      "427:\tlearn: 0.0013559\ttotal: 24.1s\tremaining: 32.2s\n",
      "428:\tlearn: 0.0013557\ttotal: 24.2s\tremaining: 32.2s\n",
      "429:\tlearn: 0.0013556\ttotal: 24.2s\tremaining: 32.1s\n",
      "430:\tlearn: 0.0013556\ttotal: 24.3s\tremaining: 32.1s\n",
      "431:\tlearn: 0.0013553\ttotal: 24.3s\tremaining: 32s\n",
      "432:\tlearn: 0.0013553\ttotal: 24.4s\tremaining: 31.9s\n",
      "433:\tlearn: 0.0013551\ttotal: 24.4s\tremaining: 31.9s\n",
      "434:\tlearn: 0.0013549\ttotal: 24.5s\tremaining: 31.8s\n",
      "435:\tlearn: 0.0013547\ttotal: 24.6s\tremaining: 31.8s\n",
      "436:\tlearn: 0.0013545\ttotal: 24.6s\tremaining: 31.7s\n",
      "437:\tlearn: 0.0013545\ttotal: 24.7s\tremaining: 31.6s\n",
      "438:\tlearn: 0.0013545\ttotal: 24.7s\tremaining: 31.6s\n",
      "439:\tlearn: 0.0013545\ttotal: 24.8s\tremaining: 31.5s\n",
      "440:\tlearn: 0.0013545\ttotal: 24.8s\tremaining: 31.5s\n",
      "441:\tlearn: 0.0013500\ttotal: 24.9s\tremaining: 31.4s\n",
      "442:\tlearn: 0.0013496\ttotal: 24.9s\tremaining: 31.4s\n",
      "443:\tlearn: 0.0013496\ttotal: 25s\tremaining: 31.3s\n",
      "444:\tlearn: 0.0013496\ttotal: 25.1s\tremaining: 31.3s\n",
      "445:\tlearn: 0.0013495\ttotal: 25.1s\tremaining: 31.2s\n",
      "446:\tlearn: 0.0013494\ttotal: 25.2s\tremaining: 31.2s\n",
      "447:\tlearn: 0.0013493\ttotal: 25.2s\tremaining: 31.1s\n",
      "448:\tlearn: 0.0013493\ttotal: 25.3s\tremaining: 31s\n",
      "449:\tlearn: 0.0013491\ttotal: 25.4s\tremaining: 31s\n",
      "450:\tlearn: 0.0013489\ttotal: 25.4s\tremaining: 30.9s\n",
      "451:\tlearn: 0.0013489\ttotal: 25.5s\tremaining: 30.9s\n",
      "452:\tlearn: 0.0013489\ttotal: 25.5s\tremaining: 30.8s\n",
      "453:\tlearn: 0.0013489\ttotal: 25.6s\tremaining: 30.8s\n",
      "454:\tlearn: 0.0013487\ttotal: 25.6s\tremaining: 30.7s\n",
      "455:\tlearn: 0.0013487\ttotal: 25.7s\tremaining: 30.6s\n",
      "456:\tlearn: 0.0013487\ttotal: 25.8s\tremaining: 30.6s\n",
      "457:\tlearn: 0.0013483\ttotal: 25.8s\tremaining: 30.6s\n",
      "458:\tlearn: 0.0013483\ttotal: 25.9s\tremaining: 30.5s\n",
      "459:\tlearn: 0.0013475\ttotal: 25.9s\tremaining: 30.4s\n",
      "460:\tlearn: 0.0013462\ttotal: 26s\tremaining: 30.4s\n",
      "461:\tlearn: 0.0013459\ttotal: 26s\tremaining: 30.3s\n",
      "462:\tlearn: 0.0013457\ttotal: 26.1s\tremaining: 30.3s\n",
      "463:\tlearn: 0.0013457\ttotal: 26.2s\tremaining: 30.2s\n",
      "464:\tlearn: 0.0013451\ttotal: 26.2s\tremaining: 30.2s\n",
      "465:\tlearn: 0.0013445\ttotal: 26.3s\tremaining: 30.1s\n",
      "466:\tlearn: 0.0013400\ttotal: 26.3s\tremaining: 30s\n",
      "467:\tlearn: 0.0013400\ttotal: 26.4s\tremaining: 30s\n",
      "468:\tlearn: 0.0013400\ttotal: 26.4s\tremaining: 29.9s\n",
      "469:\tlearn: 0.0013397\ttotal: 26.5s\tremaining: 29.9s\n",
      "470:\tlearn: 0.0013391\ttotal: 26.5s\tremaining: 29.8s\n",
      "471:\tlearn: 0.0013391\ttotal: 26.6s\tremaining: 29.8s\n",
      "472:\tlearn: 0.0013388\ttotal: 26.7s\tremaining: 29.7s\n",
      "473:\tlearn: 0.0013388\ttotal: 26.7s\tremaining: 29.7s\n",
      "474:\tlearn: 0.0013376\ttotal: 26.8s\tremaining: 29.6s\n",
      "475:\tlearn: 0.0013375\ttotal: 26.8s\tremaining: 29.5s\n",
      "476:\tlearn: 0.0013371\ttotal: 26.9s\tremaining: 29.5s\n",
      "477:\tlearn: 0.0013371\ttotal: 26.9s\tremaining: 29.4s\n",
      "478:\tlearn: 0.0013368\ttotal: 27s\tremaining: 29.4s\n",
      "479:\tlearn: 0.0013364\ttotal: 27.1s\tremaining: 29.3s\n",
      "480:\tlearn: 0.0013363\ttotal: 27.1s\tremaining: 29.3s\n",
      "481:\tlearn: 0.0013356\ttotal: 27.2s\tremaining: 29.2s\n",
      "482:\tlearn: 0.0013355\ttotal: 27.2s\tremaining: 29.2s\n",
      "483:\tlearn: 0.0013354\ttotal: 27.3s\tremaining: 29.1s\n",
      "484:\tlearn: 0.0013352\ttotal: 27.4s\tremaining: 29s\n",
      "485:\tlearn: 0.0013352\ttotal: 27.4s\tremaining: 29s\n",
      "486:\tlearn: 0.0013352\ttotal: 27.5s\tremaining: 28.9s\n",
      "487:\tlearn: 0.0013351\ttotal: 27.5s\tremaining: 28.9s\n",
      "488:\tlearn: 0.0013351\ttotal: 27.6s\tremaining: 28.8s\n",
      "489:\tlearn: 0.0013227\ttotal: 27.6s\tremaining: 28.8s\n",
      "490:\tlearn: 0.0013225\ttotal: 27.7s\tremaining: 28.7s\n",
      "491:\tlearn: 0.0013222\ttotal: 27.7s\tremaining: 28.6s\n",
      "492:\tlearn: 0.0013218\ttotal: 27.8s\tremaining: 28.6s\n",
      "493:\tlearn: 0.0013217\ttotal: 27.9s\tremaining: 28.5s\n",
      "494:\tlearn: 0.0013216\ttotal: 27.9s\tremaining: 28.5s\n",
      "495:\tlearn: 0.0013215\ttotal: 28s\tremaining: 28.4s\n",
      "496:\tlearn: 0.0013214\ttotal: 28s\tremaining: 28.4s\n",
      "497:\tlearn: 0.0013214\ttotal: 28.1s\tremaining: 28.3s\n",
      "498:\tlearn: 0.0013214\ttotal: 28.1s\tremaining: 28.2s\n",
      "499:\tlearn: 0.0013211\ttotal: 28.2s\tremaining: 28.2s\n",
      "500:\tlearn: 0.0013210\ttotal: 28.2s\tremaining: 28.1s\n",
      "501:\tlearn: 0.0013210\ttotal: 28.3s\tremaining: 28.1s\n",
      "502:\tlearn: 0.0013210\ttotal: 28.4s\tremaining: 28s\n",
      "503:\tlearn: 0.0013208\ttotal: 28.4s\tremaining: 28s\n",
      "504:\tlearn: 0.0013207\ttotal: 28.5s\tremaining: 27.9s\n",
      "505:\tlearn: 0.0013206\ttotal: 28.5s\tremaining: 27.8s\n",
      "506:\tlearn: 0.0013205\ttotal: 28.6s\tremaining: 27.8s\n",
      "507:\tlearn: 0.0013157\ttotal: 28.6s\tremaining: 27.7s\n",
      "508:\tlearn: 0.0013157\ttotal: 28.7s\tremaining: 27.7s\n",
      "509:\tlearn: 0.0013156\ttotal: 28.7s\tremaining: 27.6s\n",
      "510:\tlearn: 0.0013155\ttotal: 28.8s\tremaining: 27.6s\n",
      "511:\tlearn: 0.0013155\ttotal: 28.9s\tremaining: 27.5s\n",
      "512:\tlearn: 0.0013155\ttotal: 28.9s\tremaining: 27.5s\n",
      "513:\tlearn: 0.0013154\ttotal: 29s\tremaining: 27.4s\n",
      "514:\tlearn: 0.0013154\ttotal: 29s\tremaining: 27.3s\n",
      "515:\tlearn: 0.0013153\ttotal: 29.1s\tremaining: 27.3s\n",
      "516:\tlearn: 0.0013100\ttotal: 29.1s\tremaining: 27.2s\n",
      "517:\tlearn: 0.0013099\ttotal: 29.2s\tremaining: 27.2s\n",
      "518:\tlearn: 0.0013099\ttotal: 29.3s\tremaining: 27.1s\n",
      "519:\tlearn: 0.0013096\ttotal: 29.3s\tremaining: 27.1s\n",
      "520:\tlearn: 0.0013096\ttotal: 29.4s\tremaining: 27s\n",
      "521:\tlearn: 0.0013095\ttotal: 29.4s\tremaining: 26.9s\n",
      "522:\tlearn: 0.0013095\ttotal: 29.5s\tremaining: 26.9s\n",
      "523:\tlearn: 0.0013092\ttotal: 29.5s\tremaining: 26.8s\n",
      "524:\tlearn: 0.0013091\ttotal: 29.6s\tremaining: 26.8s\n",
      "525:\tlearn: 0.0013091\ttotal: 29.7s\tremaining: 26.7s\n",
      "526:\tlearn: 0.0013090\ttotal: 29.7s\tremaining: 26.7s\n",
      "527:\tlearn: 0.0013089\ttotal: 29.8s\tremaining: 26.6s\n",
      "528:\tlearn: 0.0013084\ttotal: 29.8s\tremaining: 26.6s\n",
      "529:\tlearn: 0.0013084\ttotal: 29.9s\tremaining: 26.5s\n",
      "530:\tlearn: 0.0013083\ttotal: 29.9s\tremaining: 26.4s\n",
      "531:\tlearn: 0.0013078\ttotal: 30s\tremaining: 26.4s\n",
      "532:\tlearn: 0.0013078\ttotal: 30.1s\tremaining: 26.3s\n",
      "533:\tlearn: 0.0013078\ttotal: 30.1s\tremaining: 26.3s\n",
      "534:\tlearn: 0.0013076\ttotal: 30.2s\tremaining: 26.2s\n",
      "535:\tlearn: 0.0013076\ttotal: 30.2s\tremaining: 26.2s\n",
      "536:\tlearn: 0.0013075\ttotal: 30.3s\tremaining: 26.1s\n",
      "537:\tlearn: 0.0013074\ttotal: 30.3s\tremaining: 26s\n",
      "538:\tlearn: 0.0013073\ttotal: 30.4s\tremaining: 26s\n",
      "539:\tlearn: 0.0013073\ttotal: 30.4s\tremaining: 25.9s\n",
      "540:\tlearn: 0.0013072\ttotal: 30.5s\tremaining: 25.9s\n",
      "541:\tlearn: 0.0013072\ttotal: 30.6s\tremaining: 25.8s\n",
      "542:\tlearn: 0.0013038\ttotal: 30.6s\tremaining: 25.8s\n",
      "543:\tlearn: 0.0013038\ttotal: 30.7s\tremaining: 25.7s\n",
      "544:\tlearn: 0.0013035\ttotal: 30.7s\tremaining: 25.6s\n",
      "545:\tlearn: 0.0013034\ttotal: 30.8s\tremaining: 25.6s\n",
      "546:\tlearn: 0.0013032\ttotal: 30.8s\tremaining: 25.5s\n",
      "547:\tlearn: 0.0013029\ttotal: 30.9s\tremaining: 25.5s\n",
      "548:\tlearn: 0.0013029\ttotal: 30.9s\tremaining: 25.4s\n",
      "549:\tlearn: 0.0013028\ttotal: 31s\tremaining: 25.4s\n",
      "550:\tlearn: 0.0013024\ttotal: 31.1s\tremaining: 25.3s\n",
      "551:\tlearn: 0.0013024\ttotal: 31.1s\tremaining: 25.3s\n",
      "552:\tlearn: 0.0013024\ttotal: 31.2s\tremaining: 25.2s\n",
      "553:\tlearn: 0.0013024\ttotal: 31.2s\tremaining: 25.1s\n",
      "554:\tlearn: 0.0013022\ttotal: 31.3s\tremaining: 25.1s\n",
      "555:\tlearn: 0.0013019\ttotal: 31.3s\tremaining: 25s\n",
      "556:\tlearn: 0.0013016\ttotal: 31.4s\tremaining: 25s\n",
      "557:\tlearn: 0.0013016\ttotal: 31.4s\tremaining: 24.9s\n",
      "558:\tlearn: 0.0013010\ttotal: 31.5s\tremaining: 24.9s\n",
      "559:\tlearn: 0.0013010\ttotal: 31.6s\tremaining: 24.8s\n",
      "560:\tlearn: 0.0013006\ttotal: 31.6s\tremaining: 24.7s\n",
      "561:\tlearn: 0.0013005\ttotal: 31.7s\tremaining: 24.7s\n",
      "562:\tlearn: 0.0013003\ttotal: 31.7s\tremaining: 24.6s\n",
      "563:\tlearn: 0.0013001\ttotal: 31.8s\tremaining: 24.6s\n",
      "564:\tlearn: 0.0013001\ttotal: 31.8s\tremaining: 24.5s\n",
      "565:\tlearn: 0.0013000\ttotal: 31.9s\tremaining: 24.5s\n",
      "566:\tlearn: 0.0012997\ttotal: 32s\tremaining: 24.4s\n",
      "567:\tlearn: 0.0012996\ttotal: 32s\tremaining: 24.3s\n",
      "568:\tlearn: 0.0012996\ttotal: 32.1s\tremaining: 24.3s\n",
      "569:\tlearn: 0.0012996\ttotal: 32.1s\tremaining: 24.2s\n",
      "570:\tlearn: 0.0012994\ttotal: 32.2s\tremaining: 24.2s\n",
      "571:\tlearn: 0.0012993\ttotal: 32.2s\tremaining: 24.1s\n",
      "572:\tlearn: 0.0012992\ttotal: 32.3s\tremaining: 24.1s\n",
      "573:\tlearn: 0.0012991\ttotal: 32.3s\tremaining: 24s\n",
      "574:\tlearn: 0.0012991\ttotal: 32.4s\tremaining: 23.9s\n",
      "575:\tlearn: 0.0012991\ttotal: 32.5s\tremaining: 23.9s\n",
      "576:\tlearn: 0.0012989\ttotal: 32.5s\tremaining: 23.8s\n",
      "577:\tlearn: 0.0012988\ttotal: 32.6s\tremaining: 23.8s\n",
      "578:\tlearn: 0.0012987\ttotal: 32.6s\tremaining: 23.7s\n",
      "579:\tlearn: 0.0012982\ttotal: 32.7s\tremaining: 23.7s\n",
      "580:\tlearn: 0.0012982\ttotal: 32.7s\tremaining: 23.6s\n",
      "581:\tlearn: 0.0012981\ttotal: 32.8s\tremaining: 23.6s\n",
      "582:\tlearn: 0.0012977\ttotal: 32.8s\tremaining: 23.5s\n",
      "583:\tlearn: 0.0012975\ttotal: 32.9s\tremaining: 23.4s\n",
      "584:\tlearn: 0.0012975\ttotal: 33s\tremaining: 23.4s\n",
      "585:\tlearn: 0.0012973\ttotal: 33s\tremaining: 23.3s\n",
      "586:\tlearn: 0.0012970\ttotal: 33.1s\tremaining: 23.3s\n",
      "587:\tlearn: 0.0012970\ttotal: 33.1s\tremaining: 23.2s\n",
      "588:\tlearn: 0.0012969\ttotal: 33.2s\tremaining: 23.2s\n",
      "589:\tlearn: 0.0012967\ttotal: 33.2s\tremaining: 23.1s\n",
      "590:\tlearn: 0.0012967\ttotal: 33.3s\tremaining: 23s\n",
      "591:\tlearn: 0.0012961\ttotal: 33.4s\tremaining: 23s\n",
      "592:\tlearn: 0.0012961\ttotal: 33.4s\tremaining: 22.9s\n",
      "593:\tlearn: 0.0012959\ttotal: 33.5s\tremaining: 22.9s\n",
      "594:\tlearn: 0.0012959\ttotal: 33.5s\tremaining: 22.8s\n",
      "595:\tlearn: 0.0012949\ttotal: 33.6s\tremaining: 22.8s\n",
      "596:\tlearn: 0.0012945\ttotal: 33.6s\tremaining: 22.7s\n",
      "597:\tlearn: 0.0012938\ttotal: 33.7s\tremaining: 22.6s\n",
      "598:\tlearn: 0.0012936\ttotal: 33.7s\tremaining: 22.6s\n",
      "599:\tlearn: 0.0012935\ttotal: 33.8s\tremaining: 22.5s\n",
      "600:\tlearn: 0.0012930\ttotal: 33.9s\tremaining: 22.5s\n",
      "601:\tlearn: 0.0012928\ttotal: 33.9s\tremaining: 22.4s\n",
      "602:\tlearn: 0.0012926\ttotal: 34s\tremaining: 22.4s\n",
      "603:\tlearn: 0.0012924\ttotal: 34s\tremaining: 22.3s\n",
      "604:\tlearn: 0.0012869\ttotal: 34.1s\tremaining: 22.3s\n",
      "605:\tlearn: 0.0012869\ttotal: 34.1s\tremaining: 22.2s\n",
      "606:\tlearn: 0.0012869\ttotal: 34.2s\tremaining: 22.1s\n",
      "607:\tlearn: 0.0012868\ttotal: 34.3s\tremaining: 22.1s\n",
      "608:\tlearn: 0.0012868\ttotal: 34.3s\tremaining: 22s\n",
      "609:\tlearn: 0.0012866\ttotal: 34.4s\tremaining: 22s\n",
      "610:\tlearn: 0.0012865\ttotal: 34.4s\tremaining: 21.9s\n",
      "611:\tlearn: 0.0012865\ttotal: 34.5s\tremaining: 21.9s\n",
      "612:\tlearn: 0.0012865\ttotal: 34.5s\tremaining: 21.8s\n",
      "613:\tlearn: 0.0012865\ttotal: 34.6s\tremaining: 21.7s\n",
      "614:\tlearn: 0.0012864\ttotal: 34.6s\tremaining: 21.7s\n",
      "615:\tlearn: 0.0012864\ttotal: 34.7s\tremaining: 21.6s\n",
      "616:\tlearn: 0.0012861\ttotal: 34.8s\tremaining: 21.6s\n",
      "617:\tlearn: 0.0012861\ttotal: 34.8s\tremaining: 21.5s\n",
      "618:\tlearn: 0.0012855\ttotal: 34.9s\tremaining: 21.5s\n",
      "619:\tlearn: 0.0012854\ttotal: 34.9s\tremaining: 21.4s\n",
      "620:\tlearn: 0.0012854\ttotal: 35s\tremaining: 21.3s\n",
      "621:\tlearn: 0.0012854\ttotal: 35s\tremaining: 21.3s\n",
      "622:\tlearn: 0.0012854\ttotal: 35.1s\tremaining: 21.2s\n",
      "623:\tlearn: 0.0012853\ttotal: 35.1s\tremaining: 21.2s\n",
      "624:\tlearn: 0.0012827\ttotal: 35.2s\tremaining: 21.1s\n",
      "625:\tlearn: 0.0012825\ttotal: 35.3s\tremaining: 21.1s\n",
      "626:\tlearn: 0.0012825\ttotal: 35.3s\tremaining: 21s\n",
      "627:\tlearn: 0.0012825\ttotal: 35.4s\tremaining: 20.9s\n",
      "628:\tlearn: 0.0012824\ttotal: 35.4s\tremaining: 20.9s\n",
      "629:\tlearn: 0.0012824\ttotal: 35.5s\tremaining: 20.8s\n",
      "630:\tlearn: 0.0012823\ttotal: 35.5s\tremaining: 20.8s\n",
      "631:\tlearn: 0.0012823\ttotal: 35.6s\tremaining: 20.7s\n",
      "632:\tlearn: 0.0012823\ttotal: 35.6s\tremaining: 20.7s\n",
      "633:\tlearn: 0.0012821\ttotal: 35.7s\tremaining: 20.6s\n",
      "634:\tlearn: 0.0012713\ttotal: 35.8s\tremaining: 20.6s\n",
      "635:\tlearn: 0.0012713\ttotal: 35.8s\tremaining: 20.5s\n",
      "636:\tlearn: 0.0012713\ttotal: 35.9s\tremaining: 20.4s\n",
      "637:\tlearn: 0.0012713\ttotal: 35.9s\tremaining: 20.4s\n",
      "638:\tlearn: 0.0012710\ttotal: 36s\tremaining: 20.3s\n",
      "639:\tlearn: 0.0012710\ttotal: 36s\tremaining: 20.3s\n",
      "640:\tlearn: 0.0012710\ttotal: 36.1s\tremaining: 20.2s\n",
      "641:\tlearn: 0.0012709\ttotal: 36.1s\tremaining: 20.2s\n",
      "642:\tlearn: 0.0012709\ttotal: 36.2s\tremaining: 20.1s\n",
      "643:\tlearn: 0.0012708\ttotal: 36.3s\tremaining: 20s\n",
      "644:\tlearn: 0.0012707\ttotal: 36.3s\tremaining: 20s\n",
      "645:\tlearn: 0.0012705\ttotal: 36.4s\tremaining: 19.9s\n",
      "646:\tlearn: 0.0012694\ttotal: 36.4s\tremaining: 19.9s\n",
      "647:\tlearn: 0.0012692\ttotal: 36.5s\tremaining: 19.8s\n",
      "648:\tlearn: 0.0012691\ttotal: 36.5s\tremaining: 19.8s\n",
      "649:\tlearn: 0.0012688\ttotal: 36.6s\tremaining: 19.7s\n",
      "650:\tlearn: 0.0012687\ttotal: 36.7s\tremaining: 19.6s\n",
      "651:\tlearn: 0.0012686\ttotal: 36.7s\tremaining: 19.6s\n",
      "652:\tlearn: 0.0012637\ttotal: 36.8s\tremaining: 19.5s\n",
      "653:\tlearn: 0.0012635\ttotal: 36.8s\tremaining: 19.5s\n",
      "654:\tlearn: 0.0012634\ttotal: 36.9s\tremaining: 19.4s\n",
      "655:\tlearn: 0.0012634\ttotal: 36.9s\tremaining: 19.4s\n",
      "656:\tlearn: 0.0012634\ttotal: 37s\tremaining: 19.3s\n",
      "657:\tlearn: 0.0012633\ttotal: 37s\tremaining: 19.3s\n",
      "658:\tlearn: 0.0012630\ttotal: 37.1s\tremaining: 19.2s\n",
      "659:\tlearn: 0.0012630\ttotal: 37.2s\tremaining: 19.1s\n",
      "660:\tlearn: 0.0012628\ttotal: 37.2s\tremaining: 19.1s\n",
      "661:\tlearn: 0.0012627\ttotal: 37.3s\tremaining: 19s\n",
      "662:\tlearn: 0.0012626\ttotal: 37.3s\tremaining: 19s\n",
      "663:\tlearn: 0.0012625\ttotal: 37.4s\tremaining: 18.9s\n",
      "664:\tlearn: 0.0012612\ttotal: 37.4s\tremaining: 18.9s\n",
      "665:\tlearn: 0.0012612\ttotal: 37.5s\tremaining: 18.8s\n",
      "666:\tlearn: 0.0012598\ttotal: 37.5s\tremaining: 18.7s\n",
      "667:\tlearn: 0.0012596\ttotal: 37.6s\tremaining: 18.7s\n",
      "668:\tlearn: 0.0012595\ttotal: 37.7s\tremaining: 18.6s\n",
      "669:\tlearn: 0.0012595\ttotal: 37.7s\tremaining: 18.6s\n",
      "670:\tlearn: 0.0012591\ttotal: 37.8s\tremaining: 18.5s\n",
      "671:\tlearn: 0.0012591\ttotal: 37.8s\tremaining: 18.5s\n",
      "672:\tlearn: 0.0012590\ttotal: 37.9s\tremaining: 18.4s\n",
      "673:\tlearn: 0.0012590\ttotal: 37.9s\tremaining: 18.4s\n",
      "674:\tlearn: 0.0012589\ttotal: 38s\tremaining: 18.3s\n",
      "675:\tlearn: 0.0012582\ttotal: 38.1s\tremaining: 18.2s\n",
      "676:\tlearn: 0.0012581\ttotal: 38.1s\tremaining: 18.2s\n",
      "677:\tlearn: 0.0012580\ttotal: 38.2s\tremaining: 18.1s\n",
      "678:\tlearn: 0.0012577\ttotal: 38.2s\tremaining: 18.1s\n",
      "679:\tlearn: 0.0012577\ttotal: 38.3s\tremaining: 18s\n",
      "680:\tlearn: 0.0012577\ttotal: 38.3s\tremaining: 18s\n",
      "681:\tlearn: 0.0012576\ttotal: 38.4s\tremaining: 17.9s\n",
      "682:\tlearn: 0.0012575\ttotal: 38.5s\tremaining: 17.8s\n",
      "683:\tlearn: 0.0012575\ttotal: 38.5s\tremaining: 17.8s\n",
      "684:\tlearn: 0.0012575\ttotal: 38.6s\tremaining: 17.7s\n",
      "685:\tlearn: 0.0012574\ttotal: 38.6s\tremaining: 17.7s\n",
      "686:\tlearn: 0.0012573\ttotal: 38.7s\tremaining: 17.6s\n",
      "687:\tlearn: 0.0012572\ttotal: 38.7s\tremaining: 17.6s\n",
      "688:\tlearn: 0.0012572\ttotal: 38.8s\tremaining: 17.5s\n",
      "689:\tlearn: 0.0012562\ttotal: 38.8s\tremaining: 17.5s\n",
      "690:\tlearn: 0.0012562\ttotal: 38.9s\tremaining: 17.4s\n",
      "691:\tlearn: 0.0012560\ttotal: 39s\tremaining: 17.3s\n",
      "692:\tlearn: 0.0012560\ttotal: 39s\tremaining: 17.3s\n",
      "693:\tlearn: 0.0012558\ttotal: 39.1s\tremaining: 17.2s\n",
      "694:\tlearn: 0.0012556\ttotal: 39.1s\tremaining: 17.2s\n",
      "695:\tlearn: 0.0012550\ttotal: 39.2s\tremaining: 17.1s\n",
      "696:\tlearn: 0.0012550\ttotal: 39.2s\tremaining: 17.1s\n",
      "697:\tlearn: 0.0012548\ttotal: 39.3s\tremaining: 17s\n",
      "698:\tlearn: 0.0012546\ttotal: 39.4s\tremaining: 16.9s\n",
      "699:\tlearn: 0.0012534\ttotal: 39.4s\tremaining: 16.9s\n",
      "700:\tlearn: 0.0012533\ttotal: 39.5s\tremaining: 16.8s\n",
      "701:\tlearn: 0.0012532\ttotal: 39.5s\tremaining: 16.8s\n",
      "702:\tlearn: 0.0012531\ttotal: 39.6s\tremaining: 16.7s\n",
      "703:\tlearn: 0.0012531\ttotal: 39.6s\tremaining: 16.7s\n",
      "704:\tlearn: 0.0012531\ttotal: 39.7s\tremaining: 16.6s\n",
      "705:\tlearn: 0.0012514\ttotal: 39.8s\tremaining: 16.6s\n",
      "706:\tlearn: 0.0012511\ttotal: 39.8s\tremaining: 16.5s\n",
      "707:\tlearn: 0.0012511\ttotal: 39.9s\tremaining: 16.4s\n",
      "708:\tlearn: 0.0012510\ttotal: 39.9s\tremaining: 16.4s\n",
      "709:\tlearn: 0.0012509\ttotal: 40s\tremaining: 16.3s\n",
      "710:\tlearn: 0.0012434\ttotal: 40s\tremaining: 16.3s\n",
      "711:\tlearn: 0.0012434\ttotal: 40.1s\tremaining: 16.2s\n",
      "712:\tlearn: 0.0012433\ttotal: 40.1s\tremaining: 16.2s\n",
      "713:\tlearn: 0.0012432\ttotal: 40.2s\tremaining: 16.1s\n",
      "714:\tlearn: 0.0012395\ttotal: 40.3s\tremaining: 16s\n",
      "715:\tlearn: 0.0012393\ttotal: 40.3s\tremaining: 16s\n",
      "716:\tlearn: 0.0012389\ttotal: 40.4s\tremaining: 15.9s\n",
      "717:\tlearn: 0.0012389\ttotal: 40.4s\tremaining: 15.9s\n",
      "718:\tlearn: 0.0012387\ttotal: 40.5s\tremaining: 15.8s\n",
      "719:\tlearn: 0.0012385\ttotal: 40.5s\tremaining: 15.8s\n",
      "720:\tlearn: 0.0012385\ttotal: 40.6s\tremaining: 15.7s\n",
      "721:\tlearn: 0.0012302\ttotal: 40.7s\tremaining: 15.7s\n",
      "722:\tlearn: 0.0012302\ttotal: 40.7s\tremaining: 15.6s\n",
      "723:\tlearn: 0.0012302\ttotal: 40.8s\tremaining: 15.5s\n",
      "724:\tlearn: 0.0012301\ttotal: 40.8s\tremaining: 15.5s\n",
      "725:\tlearn: 0.0012301\ttotal: 40.9s\tremaining: 15.4s\n",
      "726:\tlearn: 0.0012301\ttotal: 41s\tremaining: 15.4s\n",
      "727:\tlearn: 0.0012300\ttotal: 41s\tremaining: 15.3s\n",
      "728:\tlearn: 0.0012300\ttotal: 41.1s\tremaining: 15.3s\n",
      "729:\tlearn: 0.0012300\ttotal: 41.2s\tremaining: 15.2s\n",
      "730:\tlearn: 0.0012300\ttotal: 41.2s\tremaining: 15.2s\n",
      "731:\tlearn: 0.0012299\ttotal: 41.3s\tremaining: 15.1s\n",
      "732:\tlearn: 0.0012299\ttotal: 41.4s\tremaining: 15.1s\n",
      "733:\tlearn: 0.0012298\ttotal: 41.5s\tremaining: 15s\n",
      "734:\tlearn: 0.0012298\ttotal: 41.5s\tremaining: 15s\n",
      "735:\tlearn: 0.0012297\ttotal: 41.6s\tremaining: 14.9s\n",
      "736:\tlearn: 0.0012297\ttotal: 41.7s\tremaining: 14.9s\n",
      "737:\tlearn: 0.0012295\ttotal: 41.8s\tremaining: 14.8s\n",
      "738:\tlearn: 0.0012295\ttotal: 41.8s\tremaining: 14.8s\n",
      "739:\tlearn: 0.0012294\ttotal: 41.9s\tremaining: 14.7s\n",
      "740:\tlearn: 0.0012294\ttotal: 42s\tremaining: 14.7s\n",
      "741:\tlearn: 0.0012291\ttotal: 42.1s\tremaining: 14.6s\n",
      "742:\tlearn: 0.0012284\ttotal: 42.1s\tremaining: 14.6s\n",
      "743:\tlearn: 0.0012281\ttotal: 42.2s\tremaining: 14.5s\n",
      "744:\tlearn: 0.0012280\ttotal: 42.3s\tremaining: 14.5s\n",
      "745:\tlearn: 0.0012232\ttotal: 42.3s\tremaining: 14.4s\n",
      "746:\tlearn: 0.0012229\ttotal: 42.4s\tremaining: 14.4s\n",
      "747:\tlearn: 0.0012227\ttotal: 42.5s\tremaining: 14.3s\n",
      "748:\tlearn: 0.0012226\ttotal: 42.6s\tremaining: 14.3s\n",
      "749:\tlearn: 0.0012223\ttotal: 42.6s\tremaining: 14.2s\n",
      "750:\tlearn: 0.0012223\ttotal: 42.7s\tremaining: 14.2s\n",
      "751:\tlearn: 0.0012223\ttotal: 42.8s\tremaining: 14.1s\n",
      "752:\tlearn: 0.0012222\ttotal: 42.9s\tremaining: 14.1s\n",
      "753:\tlearn: 0.0012222\ttotal: 42.9s\tremaining: 14s\n",
      "754:\tlearn: 0.0012222\ttotal: 43s\tremaining: 14s\n",
      "755:\tlearn: 0.0012220\ttotal: 43.1s\tremaining: 13.9s\n",
      "756:\tlearn: 0.0012219\ttotal: 43.2s\tremaining: 13.9s\n",
      "757:\tlearn: 0.0012217\ttotal: 43.2s\tremaining: 13.8s\n",
      "758:\tlearn: 0.0012201\ttotal: 43.3s\tremaining: 13.7s\n",
      "759:\tlearn: 0.0012201\ttotal: 43.4s\tremaining: 13.7s\n",
      "760:\tlearn: 0.0012201\ttotal: 43.4s\tremaining: 13.6s\n",
      "761:\tlearn: 0.0012200\ttotal: 43.5s\tremaining: 13.6s\n",
      "762:\tlearn: 0.0012200\ttotal: 43.6s\tremaining: 13.5s\n",
      "763:\tlearn: 0.0012196\ttotal: 43.7s\tremaining: 13.5s\n",
      "764:\tlearn: 0.0012195\ttotal: 43.7s\tremaining: 13.4s\n",
      "765:\tlearn: 0.0012195\ttotal: 43.8s\tremaining: 13.4s\n",
      "766:\tlearn: 0.0012191\ttotal: 43.8s\tremaining: 13.3s\n",
      "767:\tlearn: 0.0012190\ttotal: 43.9s\tremaining: 13.3s\n",
      "768:\tlearn: 0.0012190\ttotal: 44s\tremaining: 13.2s\n",
      "769:\tlearn: 0.0012189\ttotal: 44s\tremaining: 13.1s\n",
      "770:\tlearn: 0.0012189\ttotal: 44.1s\tremaining: 13.1s\n",
      "771:\tlearn: 0.0012188\ttotal: 44.1s\tremaining: 13s\n",
      "772:\tlearn: 0.0012188\ttotal: 44.2s\tremaining: 13s\n",
      "773:\tlearn: 0.0012187\ttotal: 44.2s\tremaining: 12.9s\n",
      "774:\tlearn: 0.0012187\ttotal: 44.3s\tremaining: 12.9s\n",
      "775:\tlearn: 0.0012186\ttotal: 44.4s\tremaining: 12.8s\n",
      "776:\tlearn: 0.0012179\ttotal: 44.4s\tremaining: 12.7s\n",
      "777:\tlearn: 0.0012179\ttotal: 44.5s\tremaining: 12.7s\n",
      "778:\tlearn: 0.0012173\ttotal: 44.5s\tremaining: 12.6s\n",
      "779:\tlearn: 0.0012164\ttotal: 44.6s\tremaining: 12.6s\n",
      "780:\tlearn: 0.0012163\ttotal: 44.6s\tremaining: 12.5s\n",
      "781:\tlearn: 0.0012163\ttotal: 44.7s\tremaining: 12.5s\n",
      "782:\tlearn: 0.0012162\ttotal: 44.7s\tremaining: 12.4s\n",
      "783:\tlearn: 0.0012129\ttotal: 44.8s\tremaining: 12.3s\n",
      "784:\tlearn: 0.0012128\ttotal: 44.9s\tremaining: 12.3s\n",
      "785:\tlearn: 0.0012127\ttotal: 44.9s\tremaining: 12.2s\n",
      "786:\tlearn: 0.0012127\ttotal: 45s\tremaining: 12.2s\n",
      "787:\tlearn: 0.0012124\ttotal: 45s\tremaining: 12.1s\n",
      "788:\tlearn: 0.0012124\ttotal: 45.1s\tremaining: 12.1s\n",
      "789:\tlearn: 0.0012123\ttotal: 45.1s\tremaining: 12s\n",
      "790:\tlearn: 0.0012122\ttotal: 45.2s\tremaining: 11.9s\n",
      "791:\tlearn: 0.0012120\ttotal: 45.3s\tremaining: 11.9s\n",
      "792:\tlearn: 0.0012120\ttotal: 45.3s\tremaining: 11.8s\n",
      "793:\tlearn: 0.0012107\ttotal: 45.4s\tremaining: 11.8s\n",
      "794:\tlearn: 0.0012107\ttotal: 45.4s\tremaining: 11.7s\n",
      "795:\tlearn: 0.0012107\ttotal: 45.5s\tremaining: 11.7s\n",
      "796:\tlearn: 0.0012106\ttotal: 45.5s\tremaining: 11.6s\n",
      "797:\tlearn: 0.0012064\ttotal: 45.6s\tremaining: 11.5s\n",
      "798:\tlearn: 0.0012064\ttotal: 45.7s\tremaining: 11.5s\n",
      "799:\tlearn: 0.0012062\ttotal: 45.7s\tremaining: 11.4s\n",
      "800:\tlearn: 0.0012062\ttotal: 45.8s\tremaining: 11.4s\n",
      "801:\tlearn: 0.0012061\ttotal: 45.8s\tremaining: 11.3s\n",
      "802:\tlearn: 0.0012059\ttotal: 45.9s\tremaining: 11.3s\n",
      "803:\tlearn: 0.0012054\ttotal: 45.9s\tremaining: 11.2s\n",
      "804:\tlearn: 0.0012053\ttotal: 46s\tremaining: 11.1s\n",
      "805:\tlearn: 0.0012052\ttotal: 46.1s\tremaining: 11.1s\n",
      "806:\tlearn: 0.0012052\ttotal: 46.1s\tremaining: 11s\n",
      "807:\tlearn: 0.0012049\ttotal: 46.2s\tremaining: 11s\n",
      "808:\tlearn: 0.0012049\ttotal: 46.2s\tremaining: 10.9s\n",
      "809:\tlearn: 0.0012049\ttotal: 46.3s\tremaining: 10.9s\n",
      "810:\tlearn: 0.0012046\ttotal: 46.3s\tremaining: 10.8s\n",
      "811:\tlearn: 0.0012046\ttotal: 46.4s\tremaining: 10.7s\n",
      "812:\tlearn: 0.0011988\ttotal: 46.5s\tremaining: 10.7s\n",
      "813:\tlearn: 0.0011988\ttotal: 46.5s\tremaining: 10.6s\n",
      "814:\tlearn: 0.0011987\ttotal: 46.6s\tremaining: 10.6s\n",
      "815:\tlearn: 0.0011985\ttotal: 46.6s\tremaining: 10.5s\n",
      "816:\tlearn: 0.0011979\ttotal: 46.7s\tremaining: 10.5s\n",
      "817:\tlearn: 0.0011979\ttotal: 46.7s\tremaining: 10.4s\n",
      "818:\tlearn: 0.0011976\ttotal: 46.8s\tremaining: 10.3s\n",
      "819:\tlearn: 0.0011971\ttotal: 46.9s\tremaining: 10.3s\n",
      "820:\tlearn: 0.0011965\ttotal: 46.9s\tremaining: 10.2s\n",
      "821:\tlearn: 0.0011964\ttotal: 47s\tremaining: 10.2s\n",
      "822:\tlearn: 0.0011945\ttotal: 47s\tremaining: 10.1s\n",
      "823:\tlearn: 0.0011939\ttotal: 47.1s\tremaining: 10.1s\n",
      "824:\tlearn: 0.0011938\ttotal: 47.1s\tremaining: 10s\n",
      "825:\tlearn: 0.0011938\ttotal: 47.2s\tremaining: 9.94s\n",
      "826:\tlearn: 0.0011932\ttotal: 47.3s\tremaining: 9.88s\n",
      "827:\tlearn: 0.0011929\ttotal: 47.3s\tremaining: 9.83s\n",
      "828:\tlearn: 0.0011929\ttotal: 47.4s\tremaining: 9.77s\n",
      "829:\tlearn: 0.0011929\ttotal: 47.4s\tremaining: 9.71s\n",
      "830:\tlearn: 0.0011928\ttotal: 47.5s\tremaining: 9.66s\n",
      "831:\tlearn: 0.0011927\ttotal: 47.5s\tremaining: 9.6s\n",
      "832:\tlearn: 0.0011856\ttotal: 47.6s\tremaining: 9.54s\n",
      "833:\tlearn: 0.0011848\ttotal: 47.6s\tremaining: 9.48s\n",
      "834:\tlearn: 0.0011844\ttotal: 47.7s\tremaining: 9.43s\n",
      "835:\tlearn: 0.0011842\ttotal: 47.8s\tremaining: 9.37s\n",
      "836:\tlearn: 0.0011841\ttotal: 47.8s\tremaining: 9.31s\n",
      "837:\tlearn: 0.0011841\ttotal: 47.9s\tremaining: 9.26s\n",
      "838:\tlearn: 0.0011840\ttotal: 47.9s\tremaining: 9.2s\n",
      "839:\tlearn: 0.0011840\ttotal: 48s\tremaining: 9.14s\n",
      "840:\tlearn: 0.0011838\ttotal: 48s\tremaining: 9.08s\n",
      "841:\tlearn: 0.0011836\ttotal: 48.1s\tremaining: 9.03s\n",
      "842:\tlearn: 0.0011836\ttotal: 48.2s\tremaining: 8.97s\n",
      "843:\tlearn: 0.0011835\ttotal: 48.2s\tremaining: 8.91s\n",
      "844:\tlearn: 0.0011835\ttotal: 48.3s\tremaining: 8.85s\n",
      "845:\tlearn: 0.0011832\ttotal: 48.3s\tremaining: 8.8s\n",
      "846:\tlearn: 0.0011830\ttotal: 48.4s\tremaining: 8.74s\n",
      "847:\tlearn: 0.0011830\ttotal: 48.4s\tremaining: 8.68s\n",
      "848:\tlearn: 0.0011828\ttotal: 48.5s\tremaining: 8.63s\n",
      "849:\tlearn: 0.0011828\ttotal: 48.6s\tremaining: 8.57s\n",
      "850:\tlearn: 0.0011741\ttotal: 48.6s\tremaining: 8.51s\n",
      "851:\tlearn: 0.0011741\ttotal: 48.7s\tremaining: 8.45s\n",
      "852:\tlearn: 0.0011740\ttotal: 48.7s\tremaining: 8.4s\n",
      "853:\tlearn: 0.0011740\ttotal: 48.8s\tremaining: 8.34s\n",
      "854:\tlearn: 0.0011739\ttotal: 48.8s\tremaining: 8.28s\n",
      "855:\tlearn: 0.0011739\ttotal: 48.9s\tremaining: 8.22s\n",
      "856:\tlearn: 0.0011738\ttotal: 48.9s\tremaining: 8.17s\n",
      "857:\tlearn: 0.0011735\ttotal: 49s\tremaining: 8.11s\n",
      "858:\tlearn: 0.0011733\ttotal: 49.1s\tremaining: 8.05s\n",
      "859:\tlearn: 0.0011731\ttotal: 49.1s\tremaining: 8s\n",
      "860:\tlearn: 0.0011730\ttotal: 49.2s\tremaining: 7.94s\n",
      "861:\tlearn: 0.0011729\ttotal: 49.2s\tremaining: 7.88s\n",
      "862:\tlearn: 0.0011729\ttotal: 49.3s\tremaining: 7.82s\n",
      "863:\tlearn: 0.0011727\ttotal: 49.3s\tremaining: 7.77s\n",
      "864:\tlearn: 0.0011727\ttotal: 49.4s\tremaining: 7.71s\n",
      "865:\tlearn: 0.0011725\ttotal: 49.5s\tremaining: 7.65s\n",
      "866:\tlearn: 0.0011723\ttotal: 49.5s\tremaining: 7.59s\n",
      "867:\tlearn: 0.0011719\ttotal: 49.6s\tremaining: 7.54s\n",
      "868:\tlearn: 0.0011719\ttotal: 49.6s\tremaining: 7.48s\n",
      "869:\tlearn: 0.0011719\ttotal: 49.7s\tremaining: 7.42s\n",
      "870:\tlearn: 0.0011719\ttotal: 49.7s\tremaining: 7.37s\n",
      "871:\tlearn: 0.0011716\ttotal: 49.8s\tremaining: 7.31s\n",
      "872:\tlearn: 0.0011715\ttotal: 49.8s\tremaining: 7.25s\n",
      "873:\tlearn: 0.0011715\ttotal: 49.9s\tremaining: 7.19s\n",
      "874:\tlearn: 0.0011715\ttotal: 50s\tremaining: 7.14s\n",
      "875:\tlearn: 0.0011714\ttotal: 50s\tremaining: 7.08s\n",
      "876:\tlearn: 0.0011713\ttotal: 50.1s\tremaining: 7.02s\n",
      "877:\tlearn: 0.0011712\ttotal: 50.1s\tremaining: 6.96s\n",
      "878:\tlearn: 0.0011712\ttotal: 50.2s\tremaining: 6.91s\n",
      "879:\tlearn: 0.0011712\ttotal: 50.2s\tremaining: 6.85s\n",
      "880:\tlearn: 0.0011712\ttotal: 50.3s\tremaining: 6.79s\n",
      "881:\tlearn: 0.0011687\ttotal: 50.4s\tremaining: 6.74s\n",
      "882:\tlearn: 0.0011687\ttotal: 50.4s\tremaining: 6.68s\n",
      "883:\tlearn: 0.0011687\ttotal: 50.5s\tremaining: 6.62s\n",
      "884:\tlearn: 0.0011687\ttotal: 50.5s\tremaining: 6.56s\n",
      "885:\tlearn: 0.0011686\ttotal: 50.6s\tremaining: 6.51s\n",
      "886:\tlearn: 0.0011684\ttotal: 50.6s\tremaining: 6.45s\n",
      "887:\tlearn: 0.0011678\ttotal: 50.7s\tremaining: 6.39s\n",
      "888:\tlearn: 0.0011678\ttotal: 50.7s\tremaining: 6.33s\n",
      "889:\tlearn: 0.0011673\ttotal: 50.8s\tremaining: 6.28s\n",
      "890:\tlearn: 0.0011673\ttotal: 50.9s\tremaining: 6.22s\n",
      "891:\tlearn: 0.0011672\ttotal: 50.9s\tremaining: 6.16s\n",
      "892:\tlearn: 0.0011670\ttotal: 51s\tremaining: 6.11s\n",
      "893:\tlearn: 0.0011670\ttotal: 51s\tremaining: 6.05s\n",
      "894:\tlearn: 0.0011667\ttotal: 51.1s\tremaining: 5.99s\n",
      "895:\tlearn: 0.0011611\ttotal: 51.1s\tremaining: 5.93s\n",
      "896:\tlearn: 0.0011598\ttotal: 51.2s\tremaining: 5.88s\n",
      "897:\tlearn: 0.0011598\ttotal: 51.2s\tremaining: 5.82s\n",
      "898:\tlearn: 0.0011598\ttotal: 51.3s\tremaining: 5.76s\n",
      "899:\tlearn: 0.0011596\ttotal: 51.4s\tremaining: 5.71s\n",
      "900:\tlearn: 0.0011596\ttotal: 51.4s\tremaining: 5.65s\n",
      "901:\tlearn: 0.0011595\ttotal: 51.5s\tremaining: 5.59s\n",
      "902:\tlearn: 0.0011595\ttotal: 51.5s\tremaining: 5.54s\n",
      "903:\tlearn: 0.0011593\ttotal: 51.6s\tremaining: 5.48s\n",
      "904:\tlearn: 0.0011593\ttotal: 51.6s\tremaining: 5.42s\n",
      "905:\tlearn: 0.0011592\ttotal: 51.7s\tremaining: 5.36s\n",
      "906:\tlearn: 0.0011592\ttotal: 51.8s\tremaining: 5.31s\n",
      "907:\tlearn: 0.0011587\ttotal: 51.8s\tremaining: 5.25s\n",
      "908:\tlearn: 0.0011587\ttotal: 51.9s\tremaining: 5.19s\n",
      "909:\tlearn: 0.0011587\ttotal: 51.9s\tremaining: 5.14s\n",
      "910:\tlearn: 0.0011586\ttotal: 52s\tremaining: 5.08s\n",
      "911:\tlearn: 0.0011583\ttotal: 52s\tremaining: 5.02s\n",
      "912:\tlearn: 0.0011583\ttotal: 52.1s\tremaining: 4.96s\n",
      "913:\tlearn: 0.0011575\ttotal: 52.2s\tremaining: 4.91s\n",
      "914:\tlearn: 0.0011575\ttotal: 52.2s\tremaining: 4.85s\n",
      "915:\tlearn: 0.0011566\ttotal: 52.3s\tremaining: 4.79s\n",
      "916:\tlearn: 0.0011564\ttotal: 52.3s\tremaining: 4.74s\n",
      "917:\tlearn: 0.0011562\ttotal: 52.4s\tremaining: 4.68s\n",
      "918:\tlearn: 0.0011562\ttotal: 52.4s\tremaining: 4.62s\n",
      "919:\tlearn: 0.0011558\ttotal: 52.5s\tremaining: 4.57s\n",
      "920:\tlearn: 0.0011558\ttotal: 52.6s\tremaining: 4.51s\n",
      "921:\tlearn: 0.0011557\ttotal: 52.6s\tremaining: 4.45s\n",
      "922:\tlearn: 0.0011551\ttotal: 52.7s\tremaining: 4.39s\n",
      "923:\tlearn: 0.0011551\ttotal: 52.7s\tremaining: 4.34s\n",
      "924:\tlearn: 0.0011551\ttotal: 52.8s\tremaining: 4.28s\n",
      "925:\tlearn: 0.0011543\ttotal: 52.8s\tremaining: 4.22s\n",
      "926:\tlearn: 0.0011540\ttotal: 52.9s\tremaining: 4.17s\n",
      "927:\tlearn: 0.0011540\ttotal: 53s\tremaining: 4.11s\n",
      "928:\tlearn: 0.0011540\ttotal: 53s\tremaining: 4.05s\n",
      "929:\tlearn: 0.0011537\ttotal: 53.1s\tremaining: 3.99s\n",
      "930:\tlearn: 0.0011537\ttotal: 53.1s\tremaining: 3.94s\n",
      "931:\tlearn: 0.0011537\ttotal: 53.2s\tremaining: 3.88s\n",
      "932:\tlearn: 0.0011535\ttotal: 53.2s\tremaining: 3.82s\n",
      "933:\tlearn: 0.0011535\ttotal: 53.3s\tremaining: 3.77s\n",
      "934:\tlearn: 0.0011535\ttotal: 53.3s\tremaining: 3.71s\n",
      "935:\tlearn: 0.0011534\ttotal: 53.4s\tremaining: 3.65s\n",
      "936:\tlearn: 0.0011532\ttotal: 53.5s\tremaining: 3.59s\n",
      "937:\tlearn: 0.0011532\ttotal: 53.5s\tremaining: 3.54s\n",
      "938:\tlearn: 0.0011532\ttotal: 53.6s\tremaining: 3.48s\n",
      "939:\tlearn: 0.0011532\ttotal: 53.6s\tremaining: 3.42s\n",
      "940:\tlearn: 0.0011530\ttotal: 53.7s\tremaining: 3.37s\n",
      "941:\tlearn: 0.0011530\ttotal: 53.7s\tremaining: 3.31s\n",
      "942:\tlearn: 0.0011530\ttotal: 53.8s\tremaining: 3.25s\n",
      "943:\tlearn: 0.0011529\ttotal: 53.9s\tremaining: 3.19s\n",
      "944:\tlearn: 0.0011518\ttotal: 53.9s\tremaining: 3.14s\n",
      "945:\tlearn: 0.0011516\ttotal: 54s\tremaining: 3.08s\n",
      "946:\tlearn: 0.0011515\ttotal: 54s\tremaining: 3.02s\n",
      "947:\tlearn: 0.0011515\ttotal: 54.1s\tremaining: 2.97s\n",
      "948:\tlearn: 0.0011514\ttotal: 54.1s\tremaining: 2.91s\n",
      "949:\tlearn: 0.0011513\ttotal: 54.2s\tremaining: 2.85s\n",
      "950:\tlearn: 0.0011512\ttotal: 54.3s\tremaining: 2.79s\n",
      "951:\tlearn: 0.0011512\ttotal: 54.3s\tremaining: 2.74s\n",
      "952:\tlearn: 0.0011509\ttotal: 54.4s\tremaining: 2.68s\n",
      "953:\tlearn: 0.0011505\ttotal: 54.4s\tremaining: 2.62s\n",
      "954:\tlearn: 0.0011505\ttotal: 54.5s\tremaining: 2.57s\n",
      "955:\tlearn: 0.0011504\ttotal: 54.5s\tremaining: 2.51s\n",
      "956:\tlearn: 0.0011470\ttotal: 54.6s\tremaining: 2.45s\n",
      "957:\tlearn: 0.0011469\ttotal: 54.7s\tremaining: 2.4s\n",
      "958:\tlearn: 0.0011469\ttotal: 54.7s\tremaining: 2.34s\n",
      "959:\tlearn: 0.0011469\ttotal: 54.8s\tremaining: 2.28s\n",
      "960:\tlearn: 0.0011467\ttotal: 54.8s\tremaining: 2.23s\n",
      "961:\tlearn: 0.0011467\ttotal: 54.9s\tremaining: 2.17s\n",
      "962:\tlearn: 0.0011467\ttotal: 54.9s\tremaining: 2.11s\n",
      "963:\tlearn: 0.0011467\ttotal: 55s\tremaining: 2.05s\n",
      "964:\tlearn: 0.0011465\ttotal: 55.1s\tremaining: 2s\n",
      "965:\tlearn: 0.0011462\ttotal: 55.1s\tremaining: 1.94s\n",
      "966:\tlearn: 0.0011455\ttotal: 55.2s\tremaining: 1.88s\n",
      "967:\tlearn: 0.0011453\ttotal: 55.2s\tremaining: 1.82s\n",
      "968:\tlearn: 0.0011381\ttotal: 55.3s\tremaining: 1.77s\n",
      "969:\tlearn: 0.0011373\ttotal: 55.3s\tremaining: 1.71s\n",
      "970:\tlearn: 0.0011373\ttotal: 55.4s\tremaining: 1.65s\n",
      "971:\tlearn: 0.0011373\ttotal: 55.5s\tremaining: 1.6s\n",
      "972:\tlearn: 0.0011373\ttotal: 55.5s\tremaining: 1.54s\n",
      "973:\tlearn: 0.0011368\ttotal: 55.6s\tremaining: 1.48s\n",
      "974:\tlearn: 0.0011367\ttotal: 55.6s\tremaining: 1.43s\n",
      "975:\tlearn: 0.0011327\ttotal: 55.7s\tremaining: 1.37s\n",
      "976:\tlearn: 0.0011327\ttotal: 55.7s\tremaining: 1.31s\n",
      "977:\tlearn: 0.0011323\ttotal: 55.8s\tremaining: 1.25s\n",
      "978:\tlearn: 0.0011322\ttotal: 55.9s\tremaining: 1.2s\n",
      "979:\tlearn: 0.0011274\ttotal: 55.9s\tremaining: 1.14s\n",
      "980:\tlearn: 0.0011271\ttotal: 56s\tremaining: 1.08s\n",
      "981:\tlearn: 0.0011269\ttotal: 56s\tremaining: 1.03s\n",
      "982:\tlearn: 0.0011269\ttotal: 56.1s\tremaining: 970ms\n",
      "983:\tlearn: 0.0011268\ttotal: 56.1s\tremaining: 913ms\n",
      "984:\tlearn: 0.0011267\ttotal: 56.2s\tremaining: 856ms\n",
      "985:\tlearn: 0.0011264\ttotal: 56.3s\tremaining: 799ms\n",
      "986:\tlearn: 0.0011264\ttotal: 56.3s\tremaining: 742ms\n",
      "987:\tlearn: 0.0011263\ttotal: 56.4s\tremaining: 685ms\n",
      "988:\tlearn: 0.0011263\ttotal: 56.4s\tremaining: 628ms\n",
      "989:\tlearn: 0.0011262\ttotal: 56.5s\tremaining: 571ms\n",
      "990:\tlearn: 0.0011262\ttotal: 56.5s\tremaining: 514ms\n",
      "991:\tlearn: 0.0011262\ttotal: 56.6s\tremaining: 456ms\n",
      "992:\tlearn: 0.0011255\ttotal: 56.7s\tremaining: 399ms\n",
      "993:\tlearn: 0.0011255\ttotal: 56.7s\tremaining: 342ms\n",
      "994:\tlearn: 0.0011254\ttotal: 56.8s\tremaining: 285ms\n",
      "995:\tlearn: 0.0011251\ttotal: 56.8s\tremaining: 228ms\n",
      "996:\tlearn: 0.0011222\ttotal: 56.9s\tremaining: 171ms\n",
      "997:\tlearn: 0.0011221\ttotal: 56.9s\tremaining: 114ms\n",
      "998:\tlearn: 0.0011220\ttotal: 57s\tremaining: 57.1ms\n",
      "999:\tlearn: 0.0011220\ttotal: 57.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6268668\ttotal: 67.5ms\tremaining: 1m 7s\n",
      "1:\tlearn: 0.5522470\ttotal: 123ms\tremaining: 1m 1s\n",
      "2:\tlearn: 0.5000763\ttotal: 179ms\tremaining: 59.5s\n",
      "3:\tlearn: 0.4612546\ttotal: 235ms\tremaining: 58.5s\n",
      "4:\tlearn: 0.4114447\ttotal: 293ms\tremaining: 58.3s\n",
      "5:\tlearn: 0.3812345\ttotal: 349ms\tremaining: 57.8s\n",
      "6:\tlearn: 0.3247388\ttotal: 405ms\tremaining: 57.5s\n",
      "7:\tlearn: 0.2754985\ttotal: 464ms\tremaining: 57.5s\n",
      "8:\tlearn: 0.2319733\ttotal: 523ms\tremaining: 57.6s\n",
      "9:\tlearn: 0.1842498\ttotal: 581ms\tremaining: 57.5s\n",
      "10:\tlearn: 0.1457037\ttotal: 639ms\tremaining: 57.4s\n",
      "11:\tlearn: 0.1354506\ttotal: 695ms\tremaining: 57.2s\n",
      "12:\tlearn: 0.1131564\ttotal: 751ms\tremaining: 57s\n",
      "13:\tlearn: 0.1074712\ttotal: 806ms\tremaining: 56.8s\n",
      "14:\tlearn: 0.0915493\ttotal: 863ms\tremaining: 56.7s\n",
      "15:\tlearn: 0.0783388\ttotal: 921ms\tremaining: 56.6s\n",
      "16:\tlearn: 0.0631089\ttotal: 977ms\tremaining: 56.5s\n",
      "17:\tlearn: 0.0584883\ttotal: 1.03s\tremaining: 56.4s\n",
      "18:\tlearn: 0.0487508\ttotal: 1.09s\tremaining: 56.4s\n",
      "19:\tlearn: 0.0428146\ttotal: 1.15s\tremaining: 56.3s\n",
      "20:\tlearn: 0.0414702\ttotal: 1.21s\tremaining: 56.2s\n",
      "21:\tlearn: 0.0371936\ttotal: 1.26s\tremaining: 56.1s\n",
      "22:\tlearn: 0.0332927\ttotal: 1.32s\tremaining: 56s\n",
      "23:\tlearn: 0.0327161\ttotal: 1.37s\tremaining: 55.9s\n",
      "24:\tlearn: 0.0321179\ttotal: 1.43s\tremaining: 55.9s\n",
      "25:\tlearn: 0.0273603\ttotal: 1.49s\tremaining: 55.9s\n",
      "26:\tlearn: 0.0247721\ttotal: 1.55s\tremaining: 55.8s\n",
      "27:\tlearn: 0.0229143\ttotal: 1.6s\tremaining: 55.7s\n",
      "28:\tlearn: 0.0211763\ttotal: 1.66s\tremaining: 55.7s\n",
      "29:\tlearn: 0.0198387\ttotal: 1.72s\tremaining: 55.6s\n",
      "30:\tlearn: 0.0195546\ttotal: 1.78s\tremaining: 55.5s\n",
      "31:\tlearn: 0.0191932\ttotal: 1.83s\tremaining: 55.4s\n",
      "32:\tlearn: 0.0189209\ttotal: 1.89s\tremaining: 55.3s\n",
      "33:\tlearn: 0.0182477\ttotal: 1.94s\tremaining: 55.3s\n",
      "34:\tlearn: 0.0174867\ttotal: 2s\tremaining: 55.2s\n",
      "35:\tlearn: 0.0157172\ttotal: 2.06s\tremaining: 55.1s\n",
      "36:\tlearn: 0.0154357\ttotal: 2.12s\tremaining: 55.1s\n",
      "37:\tlearn: 0.0140516\ttotal: 2.17s\tremaining: 55s\n",
      "38:\tlearn: 0.0132266\ttotal: 2.23s\tremaining: 55s\n",
      "39:\tlearn: 0.0131114\ttotal: 2.29s\tremaining: 54.9s\n",
      "40:\tlearn: 0.0130078\ttotal: 2.34s\tremaining: 54.8s\n",
      "41:\tlearn: 0.0126893\ttotal: 2.4s\tremaining: 54.7s\n",
      "42:\tlearn: 0.0121454\ttotal: 2.46s\tremaining: 54.6s\n",
      "43:\tlearn: 0.0116267\ttotal: 2.51s\tremaining: 54.6s\n",
      "44:\tlearn: 0.0111128\ttotal: 2.57s\tremaining: 54.5s\n",
      "45:\tlearn: 0.0106152\ttotal: 2.63s\tremaining: 54.5s\n",
      "46:\tlearn: 0.0100607\ttotal: 2.68s\tremaining: 54.4s\n",
      "47:\tlearn: 0.0097154\ttotal: 2.74s\tremaining: 54.3s\n",
      "48:\tlearn: 0.0094633\ttotal: 2.8s\tremaining: 54.3s\n",
      "49:\tlearn: 0.0089429\ttotal: 2.86s\tremaining: 54.3s\n",
      "50:\tlearn: 0.0084797\ttotal: 2.91s\tremaining: 54.2s\n",
      "51:\tlearn: 0.0081721\ttotal: 2.97s\tremaining: 54.1s\n",
      "52:\tlearn: 0.0077688\ttotal: 3.03s\tremaining: 54.1s\n",
      "53:\tlearn: 0.0072982\ttotal: 3.08s\tremaining: 54s\n",
      "54:\tlearn: 0.0070677\ttotal: 3.14s\tremaining: 54s\n",
      "55:\tlearn: 0.0067922\ttotal: 3.2s\tremaining: 53.9s\n",
      "56:\tlearn: 0.0064837\ttotal: 3.25s\tremaining: 53.9s\n",
      "57:\tlearn: 0.0062150\ttotal: 3.31s\tremaining: 53.8s\n",
      "58:\tlearn: 0.0059256\ttotal: 3.37s\tremaining: 53.7s\n",
      "59:\tlearn: 0.0058017\ttotal: 3.42s\tremaining: 53.7s\n",
      "60:\tlearn: 0.0056202\ttotal: 3.48s\tremaining: 53.6s\n",
      "61:\tlearn: 0.0054379\ttotal: 3.54s\tremaining: 53.6s\n",
      "62:\tlearn: 0.0053050\ttotal: 3.6s\tremaining: 53.5s\n",
      "63:\tlearn: 0.0052310\ttotal: 3.65s\tremaining: 53.5s\n",
      "64:\tlearn: 0.0050873\ttotal: 3.71s\tremaining: 53.4s\n",
      "65:\tlearn: 0.0049914\ttotal: 3.77s\tremaining: 53.3s\n",
      "66:\tlearn: 0.0048179\ttotal: 3.83s\tremaining: 53.3s\n",
      "67:\tlearn: 0.0047299\ttotal: 3.88s\tremaining: 53.2s\n",
      "68:\tlearn: 0.0046277\ttotal: 3.94s\tremaining: 53.2s\n",
      "69:\tlearn: 0.0045226\ttotal: 4s\tremaining: 53.2s\n",
      "70:\tlearn: 0.0043891\ttotal: 4.06s\tremaining: 53.1s\n",
      "71:\tlearn: 0.0042601\ttotal: 4.12s\tremaining: 53s\n",
      "72:\tlearn: 0.0040926\ttotal: 4.17s\tremaining: 53s\n",
      "73:\tlearn: 0.0039870\ttotal: 4.23s\tremaining: 52.9s\n",
      "74:\tlearn: 0.0038937\ttotal: 4.29s\tremaining: 52.9s\n",
      "75:\tlearn: 0.0037910\ttotal: 4.34s\tremaining: 52.8s\n",
      "76:\tlearn: 0.0037054\ttotal: 4.4s\tremaining: 52.7s\n",
      "77:\tlearn: 0.0036214\ttotal: 4.46s\tremaining: 52.7s\n",
      "78:\tlearn: 0.0034961\ttotal: 4.51s\tremaining: 52.6s\n",
      "79:\tlearn: 0.0033965\ttotal: 4.57s\tremaining: 52.6s\n",
      "80:\tlearn: 0.0033744\ttotal: 4.63s\tremaining: 52.5s\n",
      "81:\tlearn: 0.0033273\ttotal: 4.69s\tremaining: 52.5s\n",
      "82:\tlearn: 0.0032772\ttotal: 4.74s\tremaining: 52.4s\n",
      "83:\tlearn: 0.0032772\ttotal: 4.8s\tremaining: 52.3s\n",
      "84:\tlearn: 0.0032771\ttotal: 4.86s\tremaining: 52.3s\n",
      "85:\tlearn: 0.0032013\ttotal: 4.91s\tremaining: 52.2s\n",
      "86:\tlearn: 0.0032013\ttotal: 4.97s\tremaining: 52.2s\n",
      "87:\tlearn: 0.0031834\ttotal: 5.03s\tremaining: 52.1s\n",
      "88:\tlearn: 0.0031159\ttotal: 5.08s\tremaining: 52s\n",
      "89:\tlearn: 0.0030279\ttotal: 5.14s\tremaining: 52s\n",
      "90:\tlearn: 0.0029879\ttotal: 5.2s\tremaining: 51.9s\n",
      "91:\tlearn: 0.0029273\ttotal: 5.25s\tremaining: 51.9s\n",
      "92:\tlearn: 0.0028569\ttotal: 5.31s\tremaining: 51.8s\n",
      "93:\tlearn: 0.0028154\ttotal: 5.37s\tremaining: 51.8s\n",
      "94:\tlearn: 0.0028147\ttotal: 5.43s\tremaining: 51.7s\n",
      "95:\tlearn: 0.0027833\ttotal: 5.48s\tremaining: 51.6s\n",
      "96:\tlearn: 0.0027826\ttotal: 5.54s\tremaining: 51.6s\n",
      "97:\tlearn: 0.0027826\ttotal: 5.59s\tremaining: 51.5s\n",
      "98:\tlearn: 0.0027222\ttotal: 5.65s\tremaining: 51.4s\n",
      "99:\tlearn: 0.0026669\ttotal: 5.71s\tremaining: 51.4s\n",
      "100:\tlearn: 0.0026074\ttotal: 5.76s\tremaining: 51.3s\n",
      "101:\tlearn: 0.0025876\ttotal: 5.82s\tremaining: 51.3s\n",
      "102:\tlearn: 0.0025469\ttotal: 5.88s\tremaining: 51.2s\n",
      "103:\tlearn: 0.0024864\ttotal: 5.94s\tremaining: 51.2s\n",
      "104:\tlearn: 0.0024435\ttotal: 6s\tremaining: 51.1s\n",
      "105:\tlearn: 0.0024435\ttotal: 6.05s\tremaining: 51.1s\n",
      "106:\tlearn: 0.0024431\ttotal: 6.11s\tremaining: 51s\n",
      "107:\tlearn: 0.0024110\ttotal: 6.17s\tremaining: 50.9s\n",
      "108:\tlearn: 0.0023722\ttotal: 6.22s\tremaining: 50.9s\n",
      "109:\tlearn: 0.0023179\ttotal: 6.28s\tremaining: 50.8s\n",
      "110:\tlearn: 0.0022845\ttotal: 6.33s\tremaining: 50.7s\n",
      "111:\tlearn: 0.0022354\ttotal: 6.39s\tremaining: 50.7s\n",
      "112:\tlearn: 0.0022350\ttotal: 6.45s\tremaining: 50.6s\n",
      "113:\tlearn: 0.0022348\ttotal: 6.5s\tremaining: 50.6s\n",
      "114:\tlearn: 0.0022347\ttotal: 6.56s\tremaining: 50.5s\n",
      "115:\tlearn: 0.0022173\ttotal: 6.62s\tremaining: 50.4s\n",
      "116:\tlearn: 0.0021855\ttotal: 6.68s\tremaining: 50.4s\n",
      "117:\tlearn: 0.0021627\ttotal: 6.73s\tremaining: 50.3s\n",
      "118:\tlearn: 0.0021625\ttotal: 6.79s\tremaining: 50.3s\n",
      "119:\tlearn: 0.0021625\ttotal: 6.85s\tremaining: 50.2s\n",
      "120:\tlearn: 0.0021264\ttotal: 6.9s\tremaining: 50.1s\n",
      "121:\tlearn: 0.0021002\ttotal: 6.96s\tremaining: 50.1s\n",
      "122:\tlearn: 0.0021002\ttotal: 7.01s\tremaining: 50s\n",
      "123:\tlearn: 0.0020999\ttotal: 7.07s\tremaining: 50s\n",
      "124:\tlearn: 0.0020993\ttotal: 7.13s\tremaining: 49.9s\n",
      "125:\tlearn: 0.0020991\ttotal: 7.18s\tremaining: 49.8s\n",
      "126:\tlearn: 0.0020991\ttotal: 7.24s\tremaining: 49.8s\n",
      "127:\tlearn: 0.0020991\ttotal: 7.3s\tremaining: 49.7s\n",
      "128:\tlearn: 0.0020987\ttotal: 7.35s\tremaining: 49.6s\n",
      "129:\tlearn: 0.0020985\ttotal: 7.41s\tremaining: 49.6s\n",
      "130:\tlearn: 0.0020985\ttotal: 7.46s\tremaining: 49.5s\n",
      "131:\tlearn: 0.0020974\ttotal: 7.52s\tremaining: 49.4s\n",
      "132:\tlearn: 0.0020973\ttotal: 7.57s\tremaining: 49.4s\n",
      "133:\tlearn: 0.0020969\ttotal: 7.63s\tremaining: 49.3s\n",
      "134:\tlearn: 0.0020969\ttotal: 7.68s\tremaining: 49.2s\n",
      "135:\tlearn: 0.0020969\ttotal: 7.74s\tremaining: 49.2s\n",
      "136:\tlearn: 0.0020856\ttotal: 7.8s\tremaining: 49.1s\n",
      "137:\tlearn: 0.0020543\ttotal: 7.85s\tremaining: 49s\n",
      "138:\tlearn: 0.0020259\ttotal: 7.91s\tremaining: 49s\n",
      "139:\tlearn: 0.0020256\ttotal: 7.96s\tremaining: 48.9s\n",
      "140:\tlearn: 0.0020166\ttotal: 8.02s\tremaining: 48.9s\n",
      "141:\tlearn: 0.0020166\ttotal: 8.07s\tremaining: 48.8s\n",
      "142:\tlearn: 0.0019828\ttotal: 8.13s\tremaining: 48.7s\n",
      "143:\tlearn: 0.0019376\ttotal: 8.19s\tremaining: 48.7s\n",
      "144:\tlearn: 0.0018873\ttotal: 8.24s\tremaining: 48.6s\n",
      "145:\tlearn: 0.0018872\ttotal: 8.3s\tremaining: 48.5s\n",
      "146:\tlearn: 0.0018871\ttotal: 8.35s\tremaining: 48.5s\n",
      "147:\tlearn: 0.0018863\ttotal: 8.41s\tremaining: 48.4s\n",
      "148:\tlearn: 0.0018863\ttotal: 8.46s\tremaining: 48.4s\n",
      "149:\tlearn: 0.0018811\ttotal: 8.52s\tremaining: 48.3s\n",
      "150:\tlearn: 0.0018811\ttotal: 8.58s\tremaining: 48.2s\n",
      "151:\tlearn: 0.0018811\ttotal: 8.64s\tremaining: 48.2s\n",
      "152:\tlearn: 0.0018810\ttotal: 8.69s\tremaining: 48.1s\n",
      "153:\tlearn: 0.0018809\ttotal: 8.75s\tremaining: 48.1s\n",
      "154:\tlearn: 0.0018582\ttotal: 8.8s\tremaining: 48s\n",
      "155:\tlearn: 0.0018570\ttotal: 8.86s\tremaining: 47.9s\n",
      "156:\tlearn: 0.0018568\ttotal: 8.92s\tremaining: 47.9s\n",
      "157:\tlearn: 0.0018567\ttotal: 8.97s\tremaining: 47.8s\n",
      "158:\tlearn: 0.0018549\ttotal: 9.03s\tremaining: 47.8s\n",
      "159:\tlearn: 0.0018539\ttotal: 9.08s\tremaining: 47.7s\n",
      "160:\tlearn: 0.0018469\ttotal: 9.14s\tremaining: 47.6s\n",
      "161:\tlearn: 0.0018468\ttotal: 9.19s\tremaining: 47.6s\n",
      "162:\tlearn: 0.0018464\ttotal: 9.25s\tremaining: 47.5s\n",
      "163:\tlearn: 0.0018463\ttotal: 9.31s\tremaining: 47.4s\n",
      "164:\tlearn: 0.0018463\ttotal: 9.36s\tremaining: 47.4s\n",
      "165:\tlearn: 0.0018461\ttotal: 9.42s\tremaining: 47.3s\n",
      "166:\tlearn: 0.0018461\ttotal: 9.47s\tremaining: 47.3s\n",
      "167:\tlearn: 0.0018456\ttotal: 9.53s\tremaining: 47.2s\n",
      "168:\tlearn: 0.0018456\ttotal: 9.58s\tremaining: 47.1s\n",
      "169:\tlearn: 0.0018454\ttotal: 9.64s\tremaining: 47.1s\n",
      "170:\tlearn: 0.0018453\ttotal: 9.7s\tremaining: 47s\n",
      "171:\tlearn: 0.0018453\ttotal: 9.76s\tremaining: 47s\n",
      "172:\tlearn: 0.0018450\ttotal: 9.81s\tremaining: 46.9s\n",
      "173:\tlearn: 0.0018450\ttotal: 9.87s\tremaining: 46.9s\n",
      "174:\tlearn: 0.0018450\ttotal: 9.93s\tremaining: 46.8s\n",
      "175:\tlearn: 0.0018450\ttotal: 9.98s\tremaining: 46.7s\n",
      "176:\tlearn: 0.0018380\ttotal: 10s\tremaining: 46.7s\n",
      "177:\tlearn: 0.0018302\ttotal: 10.1s\tremaining: 46.6s\n",
      "178:\tlearn: 0.0018236\ttotal: 10.1s\tremaining: 46.5s\n",
      "179:\tlearn: 0.0018097\ttotal: 10.2s\tremaining: 46.5s\n",
      "180:\tlearn: 0.0017851\ttotal: 10.3s\tremaining: 46.4s\n",
      "181:\tlearn: 0.0017657\ttotal: 10.3s\tremaining: 46.4s\n",
      "182:\tlearn: 0.0017655\ttotal: 10.4s\tremaining: 46.3s\n",
      "183:\tlearn: 0.0017655\ttotal: 10.4s\tremaining: 46.3s\n",
      "184:\tlearn: 0.0017652\ttotal: 10.5s\tremaining: 46.2s\n",
      "185:\tlearn: 0.0017651\ttotal: 10.5s\tremaining: 46.1s\n",
      "186:\tlearn: 0.0017651\ttotal: 10.6s\tremaining: 46.1s\n",
      "187:\tlearn: 0.0017651\ttotal: 10.7s\tremaining: 46s\n",
      "188:\tlearn: 0.0017633\ttotal: 10.7s\tremaining: 46s\n",
      "189:\tlearn: 0.0017632\ttotal: 10.8s\tremaining: 45.9s\n",
      "190:\tlearn: 0.0017632\ttotal: 10.8s\tremaining: 45.8s\n",
      "191:\tlearn: 0.0017630\ttotal: 10.9s\tremaining: 45.8s\n",
      "192:\tlearn: 0.0017619\ttotal: 10.9s\tremaining: 45.7s\n",
      "193:\tlearn: 0.0017524\ttotal: 11s\tremaining: 45.7s\n",
      "194:\tlearn: 0.0017522\ttotal: 11.1s\tremaining: 45.6s\n",
      "195:\tlearn: 0.0017520\ttotal: 11.1s\tremaining: 45.6s\n",
      "196:\tlearn: 0.0017519\ttotal: 11.2s\tremaining: 45.5s\n",
      "197:\tlearn: 0.0017507\ttotal: 11.2s\tremaining: 45.4s\n",
      "198:\tlearn: 0.0017506\ttotal: 11.3s\tremaining: 45.4s\n",
      "199:\tlearn: 0.0017504\ttotal: 11.3s\tremaining: 45.3s\n",
      "200:\tlearn: 0.0017504\ttotal: 11.4s\tremaining: 45.3s\n",
      "201:\tlearn: 0.0017499\ttotal: 11.4s\tremaining: 45.2s\n",
      "202:\tlearn: 0.0017499\ttotal: 11.5s\tremaining: 45.2s\n",
      "203:\tlearn: 0.0017443\ttotal: 11.6s\tremaining: 45.1s\n",
      "204:\tlearn: 0.0017442\ttotal: 11.6s\tremaining: 45s\n",
      "205:\tlearn: 0.0017441\ttotal: 11.7s\tremaining: 45s\n",
      "206:\tlearn: 0.0017440\ttotal: 11.7s\tremaining: 44.9s\n",
      "207:\tlearn: 0.0017440\ttotal: 11.8s\tremaining: 44.9s\n",
      "208:\tlearn: 0.0017437\ttotal: 11.8s\tremaining: 44.8s\n",
      "209:\tlearn: 0.0017437\ttotal: 11.9s\tremaining: 44.8s\n",
      "210:\tlearn: 0.0017435\ttotal: 12s\tremaining: 44.7s\n",
      "211:\tlearn: 0.0017432\ttotal: 12s\tremaining: 44.6s\n",
      "212:\tlearn: 0.0017429\ttotal: 12.1s\tremaining: 44.6s\n",
      "213:\tlearn: 0.0017428\ttotal: 12.1s\tremaining: 44.5s\n",
      "214:\tlearn: 0.0017428\ttotal: 12.2s\tremaining: 44.5s\n",
      "215:\tlearn: 0.0017427\ttotal: 12.2s\tremaining: 44.4s\n",
      "216:\tlearn: 0.0017427\ttotal: 12.3s\tremaining: 44.4s\n",
      "217:\tlearn: 0.0017423\ttotal: 12.3s\tremaining: 44.3s\n",
      "218:\tlearn: 0.0017423\ttotal: 12.4s\tremaining: 44.2s\n",
      "219:\tlearn: 0.0017423\ttotal: 12.5s\tremaining: 44.2s\n",
      "220:\tlearn: 0.0017419\ttotal: 12.5s\tremaining: 44.1s\n",
      "221:\tlearn: 0.0017419\ttotal: 12.6s\tremaining: 44.1s\n",
      "222:\tlearn: 0.0017418\ttotal: 12.6s\tremaining: 44s\n",
      "223:\tlearn: 0.0017415\ttotal: 12.7s\tremaining: 44s\n",
      "224:\tlearn: 0.0017414\ttotal: 12.7s\tremaining: 43.9s\n",
      "225:\tlearn: 0.0017412\ttotal: 12.8s\tremaining: 43.8s\n",
      "226:\tlearn: 0.0017412\ttotal: 12.9s\tremaining: 43.8s\n",
      "227:\tlearn: 0.0017411\ttotal: 12.9s\tremaining: 43.7s\n",
      "228:\tlearn: 0.0017411\ttotal: 13s\tremaining: 43.7s\n",
      "229:\tlearn: 0.0017411\ttotal: 13s\tremaining: 43.6s\n",
      "230:\tlearn: 0.0017399\ttotal: 13.1s\tremaining: 43.6s\n",
      "231:\tlearn: 0.0017397\ttotal: 13.1s\tremaining: 43.5s\n",
      "232:\tlearn: 0.0017393\ttotal: 13.2s\tremaining: 43.5s\n",
      "233:\tlearn: 0.0017392\ttotal: 13.3s\tremaining: 43.4s\n",
      "234:\tlearn: 0.0017391\ttotal: 13.3s\tremaining: 43.3s\n",
      "235:\tlearn: 0.0017383\ttotal: 13.4s\tremaining: 43.3s\n",
      "236:\tlearn: 0.0017381\ttotal: 13.4s\tremaining: 43.2s\n",
      "237:\tlearn: 0.0017381\ttotal: 13.5s\tremaining: 43.2s\n",
      "238:\tlearn: 0.0017379\ttotal: 13.5s\tremaining: 43.1s\n",
      "239:\tlearn: 0.0017377\ttotal: 13.6s\tremaining: 43.1s\n",
      "240:\tlearn: 0.0017377\ttotal: 13.7s\tremaining: 43s\n",
      "241:\tlearn: 0.0017375\ttotal: 13.7s\tremaining: 42.9s\n",
      "242:\tlearn: 0.0017372\ttotal: 13.8s\tremaining: 42.9s\n",
      "243:\tlearn: 0.0017368\ttotal: 13.8s\tremaining: 42.8s\n",
      "244:\tlearn: 0.0017368\ttotal: 13.9s\tremaining: 42.8s\n",
      "245:\tlearn: 0.0017313\ttotal: 13.9s\tremaining: 42.7s\n",
      "246:\tlearn: 0.0017313\ttotal: 14s\tremaining: 42.7s\n",
      "247:\tlearn: 0.0017307\ttotal: 14.1s\tremaining: 42.6s\n",
      "248:\tlearn: 0.0017307\ttotal: 14.1s\tremaining: 42.6s\n",
      "249:\tlearn: 0.0017307\ttotal: 14.2s\tremaining: 42.5s\n",
      "250:\tlearn: 0.0017307\ttotal: 14.2s\tremaining: 42.4s\n",
      "251:\tlearn: 0.0017307\ttotal: 14.3s\tremaining: 42.4s\n",
      "252:\tlearn: 0.0017306\ttotal: 14.3s\tremaining: 42.3s\n",
      "253:\tlearn: 0.0017251\ttotal: 14.4s\tremaining: 42.3s\n",
      "254:\tlearn: 0.0017250\ttotal: 14.5s\tremaining: 42.2s\n",
      "255:\tlearn: 0.0017250\ttotal: 14.5s\tremaining: 42.2s\n",
      "256:\tlearn: 0.0017239\ttotal: 14.6s\tremaining: 42.1s\n",
      "257:\tlearn: 0.0017239\ttotal: 14.6s\tremaining: 42.1s\n",
      "258:\tlearn: 0.0017200\ttotal: 14.7s\tremaining: 42s\n",
      "259:\tlearn: 0.0017196\ttotal: 14.7s\tremaining: 41.9s\n",
      "260:\tlearn: 0.0016931\ttotal: 14.8s\tremaining: 41.9s\n",
      "261:\tlearn: 0.0016931\ttotal: 14.8s\tremaining: 41.8s\n",
      "262:\tlearn: 0.0016929\ttotal: 14.9s\tremaining: 41.8s\n",
      "263:\tlearn: 0.0016927\ttotal: 15s\tremaining: 41.7s\n",
      "264:\tlearn: 0.0016925\ttotal: 15s\tremaining: 41.7s\n",
      "265:\tlearn: 0.0016925\ttotal: 15.1s\tremaining: 41.6s\n",
      "266:\tlearn: 0.0016920\ttotal: 15.1s\tremaining: 41.6s\n",
      "267:\tlearn: 0.0016904\ttotal: 15.2s\tremaining: 41.5s\n",
      "268:\tlearn: 0.0016899\ttotal: 15.3s\tremaining: 41.5s\n",
      "269:\tlearn: 0.0016877\ttotal: 15.3s\tremaining: 41.4s\n",
      "270:\tlearn: 0.0016874\ttotal: 15.4s\tremaining: 41.3s\n",
      "271:\tlearn: 0.0016873\ttotal: 15.4s\tremaining: 41.3s\n",
      "272:\tlearn: 0.0016811\ttotal: 15.5s\tremaining: 41.2s\n",
      "273:\tlearn: 0.0016811\ttotal: 15.5s\tremaining: 41.2s\n",
      "274:\tlearn: 0.0016767\ttotal: 15.6s\tremaining: 41.1s\n",
      "275:\tlearn: 0.0016649\ttotal: 15.7s\tremaining: 41.1s\n",
      "276:\tlearn: 0.0016246\ttotal: 15.7s\tremaining: 41s\n",
      "277:\tlearn: 0.0016245\ttotal: 15.8s\tremaining: 40.9s\n",
      "278:\tlearn: 0.0016244\ttotal: 15.8s\tremaining: 40.9s\n",
      "279:\tlearn: 0.0016178\ttotal: 15.9s\tremaining: 40.8s\n",
      "280:\tlearn: 0.0016035\ttotal: 15.9s\tremaining: 40.8s\n",
      "281:\tlearn: 0.0016035\ttotal: 16s\tremaining: 40.7s\n",
      "282:\tlearn: 0.0016023\ttotal: 16.1s\tremaining: 40.7s\n",
      "283:\tlearn: 0.0016022\ttotal: 16.1s\tremaining: 40.6s\n",
      "284:\tlearn: 0.0016016\ttotal: 16.2s\tremaining: 40.6s\n",
      "285:\tlearn: 0.0016007\ttotal: 16.2s\tremaining: 40.5s\n",
      "286:\tlearn: 0.0016000\ttotal: 16.3s\tremaining: 40.5s\n",
      "287:\tlearn: 0.0015813\ttotal: 16.3s\tremaining: 40.4s\n",
      "288:\tlearn: 0.0015743\ttotal: 16.4s\tremaining: 40.3s\n",
      "289:\tlearn: 0.0015743\ttotal: 16.5s\tremaining: 40.3s\n",
      "290:\tlearn: 0.0015720\ttotal: 16.5s\tremaining: 40.2s\n",
      "291:\tlearn: 0.0015714\ttotal: 16.6s\tremaining: 40.2s\n",
      "292:\tlearn: 0.0015711\ttotal: 16.6s\tremaining: 40.1s\n",
      "293:\tlearn: 0.0015708\ttotal: 16.7s\tremaining: 40.1s\n",
      "294:\tlearn: 0.0015515\ttotal: 16.7s\tremaining: 40s\n",
      "295:\tlearn: 0.0015482\ttotal: 16.8s\tremaining: 39.9s\n",
      "296:\tlearn: 0.0015480\ttotal: 16.9s\tremaining: 39.9s\n",
      "297:\tlearn: 0.0015475\ttotal: 16.9s\tremaining: 39.8s\n",
      "298:\tlearn: 0.0015474\ttotal: 17s\tremaining: 39.8s\n",
      "299:\tlearn: 0.0015473\ttotal: 17s\tremaining: 39.7s\n",
      "300:\tlearn: 0.0015473\ttotal: 17.1s\tremaining: 39.7s\n",
      "301:\tlearn: 0.0015473\ttotal: 17.1s\tremaining: 39.6s\n",
      "302:\tlearn: 0.0015472\ttotal: 17.2s\tremaining: 39.5s\n",
      "303:\tlearn: 0.0015454\ttotal: 17.2s\tremaining: 39.5s\n",
      "304:\tlearn: 0.0015445\ttotal: 17.3s\tremaining: 39.4s\n",
      "305:\tlearn: 0.0015426\ttotal: 17.4s\tremaining: 39.4s\n",
      "306:\tlearn: 0.0015420\ttotal: 17.4s\tremaining: 39.3s\n",
      "307:\tlearn: 0.0015418\ttotal: 17.5s\tremaining: 39.2s\n",
      "308:\tlearn: 0.0015416\ttotal: 17.5s\tremaining: 39.2s\n",
      "309:\tlearn: 0.0015415\ttotal: 17.6s\tremaining: 39.1s\n",
      "310:\tlearn: 0.0015414\ttotal: 17.6s\tremaining: 39.1s\n",
      "311:\tlearn: 0.0015400\ttotal: 17.7s\tremaining: 39s\n",
      "312:\tlearn: 0.0015400\ttotal: 17.7s\tremaining: 39s\n",
      "313:\tlearn: 0.0015399\ttotal: 17.8s\tremaining: 38.9s\n",
      "314:\tlearn: 0.0015385\ttotal: 17.9s\tremaining: 38.9s\n",
      "315:\tlearn: 0.0015385\ttotal: 17.9s\tremaining: 38.8s\n",
      "316:\tlearn: 0.0015374\ttotal: 18s\tremaining: 38.8s\n",
      "317:\tlearn: 0.0015352\ttotal: 18s\tremaining: 38.7s\n",
      "318:\tlearn: 0.0015351\ttotal: 18.1s\tremaining: 38.6s\n",
      "319:\tlearn: 0.0015347\ttotal: 18.2s\tremaining: 38.6s\n",
      "320:\tlearn: 0.0015345\ttotal: 18.2s\tremaining: 38.5s\n",
      "321:\tlearn: 0.0015202\ttotal: 18.3s\tremaining: 38.5s\n",
      "322:\tlearn: 0.0015202\ttotal: 18.3s\tremaining: 38.4s\n",
      "323:\tlearn: 0.0015201\ttotal: 18.4s\tremaining: 38.4s\n",
      "324:\tlearn: 0.0015201\ttotal: 18.4s\tremaining: 38.3s\n",
      "325:\tlearn: 0.0015201\ttotal: 18.5s\tremaining: 38.2s\n",
      "326:\tlearn: 0.0015201\ttotal: 18.6s\tremaining: 38.2s\n",
      "327:\tlearn: 0.0015045\ttotal: 18.6s\tremaining: 38.1s\n",
      "328:\tlearn: 0.0015045\ttotal: 18.7s\tremaining: 38.1s\n",
      "329:\tlearn: 0.0015044\ttotal: 18.7s\tremaining: 38s\n",
      "330:\tlearn: 0.0015034\ttotal: 18.8s\tremaining: 38s\n",
      "331:\tlearn: 0.0014797\ttotal: 18.8s\tremaining: 37.9s\n",
      "332:\tlearn: 0.0014794\ttotal: 18.9s\tremaining: 37.9s\n",
      "333:\tlearn: 0.0014793\ttotal: 19s\tremaining: 37.8s\n",
      "334:\tlearn: 0.0014793\ttotal: 19s\tremaining: 37.7s\n",
      "335:\tlearn: 0.0014786\ttotal: 19.1s\tremaining: 37.7s\n",
      "336:\tlearn: 0.0014785\ttotal: 19.1s\tremaining: 37.6s\n",
      "337:\tlearn: 0.0014785\ttotal: 19.2s\tremaining: 37.6s\n",
      "338:\tlearn: 0.0014781\ttotal: 19.3s\tremaining: 37.6s\n",
      "339:\tlearn: 0.0014781\ttotal: 19.4s\tremaining: 37.6s\n",
      "340:\tlearn: 0.0014780\ttotal: 19.4s\tremaining: 37.6s\n",
      "341:\tlearn: 0.0014765\ttotal: 19.5s\tremaining: 37.5s\n",
      "342:\tlearn: 0.0014638\ttotal: 19.6s\tremaining: 37.5s\n",
      "343:\tlearn: 0.0014635\ttotal: 19.7s\tremaining: 37.5s\n",
      "344:\tlearn: 0.0014635\ttotal: 19.7s\tremaining: 37.4s\n",
      "345:\tlearn: 0.0014634\ttotal: 19.8s\tremaining: 37.4s\n",
      "346:\tlearn: 0.0014634\ttotal: 19.8s\tremaining: 37.3s\n",
      "347:\tlearn: 0.0014634\ttotal: 19.9s\tremaining: 37.3s\n",
      "348:\tlearn: 0.0014633\ttotal: 20s\tremaining: 37.2s\n",
      "349:\tlearn: 0.0014633\ttotal: 20s\tremaining: 37.2s\n",
      "350:\tlearn: 0.0014633\ttotal: 20.1s\tremaining: 37.1s\n",
      "351:\tlearn: 0.0014633\ttotal: 20.1s\tremaining: 37.1s\n",
      "352:\tlearn: 0.0014631\ttotal: 20.2s\tremaining: 37s\n",
      "353:\tlearn: 0.0014625\ttotal: 20.3s\tremaining: 37s\n",
      "354:\tlearn: 0.0014624\ttotal: 20.3s\tremaining: 36.9s\n",
      "355:\tlearn: 0.0014623\ttotal: 20.4s\tremaining: 36.9s\n",
      "356:\tlearn: 0.0014622\ttotal: 20.4s\tremaining: 36.8s\n",
      "357:\tlearn: 0.0014622\ttotal: 20.6s\tremaining: 36.9s\n",
      "358:\tlearn: 0.0014622\ttotal: 20.7s\tremaining: 36.9s\n",
      "359:\tlearn: 0.0014622\ttotal: 20.9s\tremaining: 37.1s\n",
      "360:\tlearn: 0.0014621\ttotal: 21s\tremaining: 37.2s\n",
      "361:\tlearn: 0.0014618\ttotal: 21.2s\tremaining: 37.4s\n",
      "362:\tlearn: 0.0014618\ttotal: 21.3s\tremaining: 37.4s\n",
      "363:\tlearn: 0.0014618\ttotal: 21.4s\tremaining: 37.4s\n",
      "364:\tlearn: 0.0014618\ttotal: 21.4s\tremaining: 37.3s\n",
      "365:\tlearn: 0.0014618\ttotal: 21.5s\tremaining: 37.3s\n",
      "366:\tlearn: 0.0014618\ttotal: 21.6s\tremaining: 37.2s\n",
      "367:\tlearn: 0.0014616\ttotal: 21.6s\tremaining: 37.2s\n",
      "368:\tlearn: 0.0014613\ttotal: 21.7s\tremaining: 37.1s\n",
      "369:\tlearn: 0.0014610\ttotal: 21.8s\tremaining: 37s\n",
      "370:\tlearn: 0.0014610\ttotal: 21.8s\tremaining: 37s\n",
      "371:\tlearn: 0.0014610\ttotal: 21.9s\tremaining: 36.9s\n",
      "372:\tlearn: 0.0014609\ttotal: 21.9s\tremaining: 36.9s\n",
      "373:\tlearn: 0.0014609\ttotal: 22s\tremaining: 36.8s\n",
      "374:\tlearn: 0.0014609\ttotal: 22.1s\tremaining: 36.8s\n",
      "375:\tlearn: 0.0014607\ttotal: 22.2s\tremaining: 36.8s\n",
      "376:\tlearn: 0.0014607\ttotal: 22.2s\tremaining: 36.7s\n",
      "377:\tlearn: 0.0014604\ttotal: 22.3s\tremaining: 36.7s\n",
      "378:\tlearn: 0.0014602\ttotal: 22.3s\tremaining: 36.6s\n",
      "379:\tlearn: 0.0014601\ttotal: 22.4s\tremaining: 36.5s\n",
      "380:\tlearn: 0.0014600\ttotal: 22.5s\tremaining: 36.5s\n",
      "381:\tlearn: 0.0014491\ttotal: 22.5s\tremaining: 36.4s\n",
      "382:\tlearn: 0.0014489\ttotal: 22.6s\tremaining: 36.4s\n",
      "383:\tlearn: 0.0014488\ttotal: 22.7s\tremaining: 36.4s\n",
      "384:\tlearn: 0.0014487\ttotal: 22.7s\tremaining: 36.3s\n",
      "385:\tlearn: 0.0014486\ttotal: 22.8s\tremaining: 36.3s\n",
      "386:\tlearn: 0.0014486\ttotal: 22.9s\tremaining: 36.2s\n",
      "387:\tlearn: 0.0014486\ttotal: 22.9s\tremaining: 36.2s\n",
      "388:\tlearn: 0.0014482\ttotal: 23s\tremaining: 36.1s\n",
      "389:\tlearn: 0.0014300\ttotal: 23.1s\tremaining: 36.1s\n",
      "390:\tlearn: 0.0014244\ttotal: 23.1s\tremaining: 36s\n",
      "391:\tlearn: 0.0014240\ttotal: 23.2s\tremaining: 36s\n",
      "392:\tlearn: 0.0014240\ttotal: 23.3s\tremaining: 36s\n",
      "393:\tlearn: 0.0014239\ttotal: 23.3s\tremaining: 35.9s\n",
      "394:\tlearn: 0.0014239\ttotal: 23.4s\tremaining: 35.8s\n",
      "395:\tlearn: 0.0014238\ttotal: 23.5s\tremaining: 35.8s\n",
      "396:\tlearn: 0.0014238\ttotal: 23.5s\tremaining: 35.8s\n",
      "397:\tlearn: 0.0014237\ttotal: 23.6s\tremaining: 35.7s\n",
      "398:\tlearn: 0.0014234\ttotal: 23.7s\tremaining: 35.7s\n",
      "399:\tlearn: 0.0014234\ttotal: 23.8s\tremaining: 35.7s\n",
      "400:\tlearn: 0.0014141\ttotal: 23.9s\tremaining: 35.7s\n",
      "401:\tlearn: 0.0014061\ttotal: 23.9s\tremaining: 35.6s\n",
      "402:\tlearn: 0.0014016\ttotal: 24s\tremaining: 35.6s\n",
      "403:\tlearn: 0.0014012\ttotal: 24.1s\tremaining: 35.5s\n",
      "404:\tlearn: 0.0014008\ttotal: 24.2s\tremaining: 35.5s\n",
      "405:\tlearn: 0.0014007\ttotal: 24.2s\tremaining: 35.5s\n",
      "406:\tlearn: 0.0014004\ttotal: 24.3s\tremaining: 35.4s\n",
      "407:\tlearn: 0.0014004\ttotal: 24.4s\tremaining: 35.4s\n",
      "408:\tlearn: 0.0014004\ttotal: 24.5s\tremaining: 35.4s\n",
      "409:\tlearn: 0.0014004\ttotal: 24.6s\tremaining: 35.3s\n",
      "410:\tlearn: 0.0014001\ttotal: 24.6s\tremaining: 35.3s\n",
      "411:\tlearn: 0.0014000\ttotal: 24.7s\tremaining: 35.3s\n",
      "412:\tlearn: 0.0014000\ttotal: 24.8s\tremaining: 35.2s\n",
      "413:\tlearn: 0.0013997\ttotal: 24.9s\tremaining: 35.2s\n",
      "414:\tlearn: 0.0013995\ttotal: 24.9s\tremaining: 35.1s\n",
      "415:\tlearn: 0.0013995\ttotal: 25s\tremaining: 35.1s\n",
      "416:\tlearn: 0.0013994\ttotal: 25.1s\tremaining: 35s\n",
      "417:\tlearn: 0.0013994\ttotal: 25.1s\tremaining: 35s\n",
      "418:\tlearn: 0.0013930\ttotal: 25.2s\tremaining: 34.9s\n",
      "419:\tlearn: 0.0013928\ttotal: 25.3s\tremaining: 34.9s\n",
      "420:\tlearn: 0.0013928\ttotal: 25.3s\tremaining: 34.8s\n",
      "421:\tlearn: 0.0013777\ttotal: 25.4s\tremaining: 34.8s\n",
      "422:\tlearn: 0.0013772\ttotal: 25.5s\tremaining: 34.7s\n",
      "423:\tlearn: 0.0013771\ttotal: 25.5s\tremaining: 34.7s\n",
      "424:\tlearn: 0.0013771\ttotal: 25.6s\tremaining: 34.6s\n",
      "425:\tlearn: 0.0013770\ttotal: 25.6s\tremaining: 34.6s\n",
      "426:\tlearn: 0.0013766\ttotal: 25.7s\tremaining: 34.5s\n",
      "427:\tlearn: 0.0013766\ttotal: 25.8s\tremaining: 34.5s\n",
      "428:\tlearn: 0.0013766\ttotal: 25.9s\tremaining: 34.4s\n",
      "429:\tlearn: 0.0013758\ttotal: 25.9s\tremaining: 34.4s\n",
      "430:\tlearn: 0.0013757\ttotal: 26s\tremaining: 34.4s\n",
      "431:\tlearn: 0.0013757\ttotal: 26.1s\tremaining: 34.3s\n",
      "432:\tlearn: 0.0013757\ttotal: 26.2s\tremaining: 34.3s\n",
      "433:\tlearn: 0.0013756\ttotal: 26.3s\tremaining: 34.3s\n",
      "434:\tlearn: 0.0013669\ttotal: 26.4s\tremaining: 34.2s\n",
      "435:\tlearn: 0.0013668\ttotal: 26.4s\tremaining: 34.2s\n",
      "436:\tlearn: 0.0013665\ttotal: 26.5s\tremaining: 34.2s\n",
      "437:\tlearn: 0.0013665\ttotal: 26.6s\tremaining: 34.2s\n",
      "438:\tlearn: 0.0013665\ttotal: 26.7s\tremaining: 34.1s\n",
      "439:\tlearn: 0.0013665\ttotal: 26.8s\tremaining: 34.1s\n",
      "440:\tlearn: 0.0013664\ttotal: 26.9s\tremaining: 34.1s\n",
      "441:\tlearn: 0.0013664\ttotal: 27s\tremaining: 34s\n",
      "442:\tlearn: 0.0013661\ttotal: 27s\tremaining: 34s\n",
      "443:\tlearn: 0.0013661\ttotal: 27.1s\tremaining: 33.9s\n",
      "444:\tlearn: 0.0013661\ttotal: 27.2s\tremaining: 33.9s\n",
      "445:\tlearn: 0.0013658\ttotal: 27.2s\tremaining: 33.8s\n",
      "446:\tlearn: 0.0013653\ttotal: 27.3s\tremaining: 33.8s\n",
      "447:\tlearn: 0.0013653\ttotal: 27.4s\tremaining: 33.7s\n",
      "448:\tlearn: 0.0013541\ttotal: 27.4s\tremaining: 33.7s\n",
      "449:\tlearn: 0.0013537\ttotal: 27.5s\tremaining: 33.6s\n",
      "450:\tlearn: 0.0013536\ttotal: 27.6s\tremaining: 33.6s\n",
      "451:\tlearn: 0.0013535\ttotal: 27.7s\tremaining: 33.5s\n",
      "452:\tlearn: 0.0013501\ttotal: 27.7s\tremaining: 33.5s\n",
      "453:\tlearn: 0.0013501\ttotal: 27.8s\tremaining: 33.4s\n",
      "454:\tlearn: 0.0013501\ttotal: 27.9s\tremaining: 33.4s\n",
      "455:\tlearn: 0.0013498\ttotal: 27.9s\tremaining: 33.3s\n",
      "456:\tlearn: 0.0013496\ttotal: 28s\tremaining: 33.3s\n",
      "457:\tlearn: 0.0013494\ttotal: 28.1s\tremaining: 33.2s\n",
      "458:\tlearn: 0.0013494\ttotal: 28.1s\tremaining: 33.1s\n",
      "459:\tlearn: 0.0013494\ttotal: 28.2s\tremaining: 33.1s\n",
      "460:\tlearn: 0.0013494\ttotal: 28.3s\tremaining: 33.1s\n",
      "461:\tlearn: 0.0013489\ttotal: 28.3s\tremaining: 33s\n",
      "462:\tlearn: 0.0013488\ttotal: 28.4s\tremaining: 32.9s\n",
      "463:\tlearn: 0.0013485\ttotal: 28.5s\tremaining: 32.9s\n",
      "464:\tlearn: 0.0013481\ttotal: 28.5s\tremaining: 32.8s\n",
      "465:\tlearn: 0.0013480\ttotal: 28.6s\tremaining: 32.8s\n",
      "466:\tlearn: 0.0013479\ttotal: 28.7s\tremaining: 32.7s\n",
      "467:\tlearn: 0.0013479\ttotal: 28.8s\tremaining: 32.7s\n",
      "468:\tlearn: 0.0013478\ttotal: 28.8s\tremaining: 32.6s\n",
      "469:\tlearn: 0.0013477\ttotal: 28.9s\tremaining: 32.6s\n",
      "470:\tlearn: 0.0013476\ttotal: 29s\tremaining: 32.6s\n",
      "471:\tlearn: 0.0013475\ttotal: 29.1s\tremaining: 32.5s\n",
      "472:\tlearn: 0.0013473\ttotal: 29.1s\tremaining: 32.5s\n",
      "473:\tlearn: 0.0013472\ttotal: 29.2s\tremaining: 32.4s\n",
      "474:\tlearn: 0.0013470\ttotal: 29.3s\tremaining: 32.4s\n",
      "475:\tlearn: 0.0013470\ttotal: 29.3s\tremaining: 32.3s\n",
      "476:\tlearn: 0.0013469\ttotal: 29.4s\tremaining: 32.2s\n",
      "477:\tlearn: 0.0013469\ttotal: 29.5s\tremaining: 32.2s\n",
      "478:\tlearn: 0.0013468\ttotal: 29.5s\tremaining: 32.1s\n",
      "479:\tlearn: 0.0013466\ttotal: 29.6s\tremaining: 32.1s\n",
      "480:\tlearn: 0.0013463\ttotal: 29.7s\tremaining: 32s\n",
      "481:\tlearn: 0.0013462\ttotal: 29.7s\tremaining: 32s\n",
      "482:\tlearn: 0.0013461\ttotal: 29.8s\tremaining: 31.9s\n",
      "483:\tlearn: 0.0013459\ttotal: 29.9s\tremaining: 31.8s\n",
      "484:\tlearn: 0.0013459\ttotal: 29.9s\tremaining: 31.8s\n",
      "485:\tlearn: 0.0013459\ttotal: 30s\tremaining: 31.7s\n",
      "486:\tlearn: 0.0013458\ttotal: 30s\tremaining: 31.6s\n",
      "487:\tlearn: 0.0013457\ttotal: 30.1s\tremaining: 31.6s\n",
      "488:\tlearn: 0.0013454\ttotal: 30.1s\tremaining: 31.5s\n",
      "489:\tlearn: 0.0013452\ttotal: 30.2s\tremaining: 31.4s\n",
      "490:\tlearn: 0.0013450\ttotal: 30.3s\tremaining: 31.4s\n",
      "491:\tlearn: 0.0013447\ttotal: 30.3s\tremaining: 31.3s\n",
      "492:\tlearn: 0.0013444\ttotal: 30.4s\tremaining: 31.2s\n",
      "493:\tlearn: 0.0013444\ttotal: 30.4s\tremaining: 31.2s\n",
      "494:\tlearn: 0.0013406\ttotal: 30.5s\tremaining: 31.1s\n",
      "495:\tlearn: 0.0013406\ttotal: 30.5s\tremaining: 31s\n",
      "496:\tlearn: 0.0013333\ttotal: 30.6s\tremaining: 31s\n",
      "497:\tlearn: 0.0013332\ttotal: 30.7s\tremaining: 30.9s\n",
      "498:\tlearn: 0.0013326\ttotal: 30.7s\tremaining: 30.8s\n",
      "499:\tlearn: 0.0013326\ttotal: 30.8s\tremaining: 30.8s\n",
      "500:\tlearn: 0.0013326\ttotal: 30.8s\tremaining: 30.7s\n",
      "501:\tlearn: 0.0013325\ttotal: 30.9s\tremaining: 30.6s\n",
      "502:\tlearn: 0.0013323\ttotal: 30.9s\tremaining: 30.6s\n",
      "503:\tlearn: 0.0013323\ttotal: 31s\tremaining: 30.5s\n",
      "504:\tlearn: 0.0013322\ttotal: 31.1s\tremaining: 30.4s\n",
      "505:\tlearn: 0.0013321\ttotal: 31.1s\tremaining: 30.4s\n",
      "506:\tlearn: 0.0013320\ttotal: 31.2s\tremaining: 30.3s\n",
      "507:\tlearn: 0.0013320\ttotal: 31.2s\tremaining: 30.2s\n",
      "508:\tlearn: 0.0013319\ttotal: 31.3s\tremaining: 30.2s\n",
      "509:\tlearn: 0.0013318\ttotal: 31.3s\tremaining: 30.1s\n",
      "510:\tlearn: 0.0013317\ttotal: 31.4s\tremaining: 30s\n",
      "511:\tlearn: 0.0013246\ttotal: 31.4s\tremaining: 30s\n",
      "512:\tlearn: 0.0013244\ttotal: 31.5s\tremaining: 29.9s\n",
      "513:\tlearn: 0.0013242\ttotal: 31.6s\tremaining: 29.8s\n",
      "514:\tlearn: 0.0013242\ttotal: 31.6s\tremaining: 29.8s\n",
      "515:\tlearn: 0.0013232\ttotal: 31.7s\tremaining: 29.7s\n",
      "516:\tlearn: 0.0013226\ttotal: 31.7s\tremaining: 29.6s\n",
      "517:\tlearn: 0.0013226\ttotal: 31.8s\tremaining: 29.6s\n",
      "518:\tlearn: 0.0013226\ttotal: 31.8s\tremaining: 29.5s\n",
      "519:\tlearn: 0.0013223\ttotal: 31.9s\tremaining: 29.4s\n",
      "520:\tlearn: 0.0013221\ttotal: 32s\tremaining: 29.4s\n",
      "521:\tlearn: 0.0013219\ttotal: 32s\tremaining: 29.3s\n",
      "522:\tlearn: 0.0013218\ttotal: 32.1s\tremaining: 29.3s\n",
      "523:\tlearn: 0.0013218\ttotal: 32.1s\tremaining: 29.2s\n",
      "524:\tlearn: 0.0013217\ttotal: 32.2s\tremaining: 29.1s\n",
      "525:\tlearn: 0.0013217\ttotal: 32.2s\tremaining: 29.1s\n",
      "526:\tlearn: 0.0013217\ttotal: 32.3s\tremaining: 29s\n",
      "527:\tlearn: 0.0013217\ttotal: 32.4s\tremaining: 28.9s\n",
      "528:\tlearn: 0.0013215\ttotal: 32.4s\tremaining: 28.9s\n",
      "529:\tlearn: 0.0013213\ttotal: 32.5s\tremaining: 28.8s\n",
      "530:\tlearn: 0.0013212\ttotal: 32.5s\tremaining: 28.7s\n",
      "531:\tlearn: 0.0013207\ttotal: 32.6s\tremaining: 28.7s\n",
      "532:\tlearn: 0.0013207\ttotal: 32.6s\tremaining: 28.6s\n",
      "533:\tlearn: 0.0013206\ttotal: 32.7s\tremaining: 28.5s\n",
      "534:\tlearn: 0.0013205\ttotal: 32.8s\tremaining: 28.5s\n",
      "535:\tlearn: 0.0013203\ttotal: 32.8s\tremaining: 28.4s\n",
      "536:\tlearn: 0.0013203\ttotal: 32.9s\tremaining: 28.3s\n",
      "537:\tlearn: 0.0013203\ttotal: 32.9s\tremaining: 28.3s\n",
      "538:\tlearn: 0.0013131\ttotal: 33s\tremaining: 28.2s\n",
      "539:\tlearn: 0.0013129\ttotal: 33s\tremaining: 28.2s\n",
      "540:\tlearn: 0.0013128\ttotal: 33.1s\tremaining: 28.1s\n",
      "541:\tlearn: 0.0013125\ttotal: 33.2s\tremaining: 28s\n",
      "542:\tlearn: 0.0013121\ttotal: 33.2s\tremaining: 28s\n",
      "543:\tlearn: 0.0013118\ttotal: 33.3s\tremaining: 27.9s\n",
      "544:\tlearn: 0.0013118\ttotal: 33.3s\tremaining: 27.8s\n",
      "545:\tlearn: 0.0013115\ttotal: 33.4s\tremaining: 27.8s\n",
      "546:\tlearn: 0.0013113\ttotal: 33.5s\tremaining: 27.7s\n",
      "547:\tlearn: 0.0013113\ttotal: 33.5s\tremaining: 27.6s\n",
      "548:\tlearn: 0.0013113\ttotal: 33.6s\tremaining: 27.6s\n",
      "549:\tlearn: 0.0013113\ttotal: 33.6s\tremaining: 27.5s\n",
      "550:\tlearn: 0.0013112\ttotal: 33.7s\tremaining: 27.4s\n",
      "551:\tlearn: 0.0013112\ttotal: 33.7s\tremaining: 27.4s\n",
      "552:\tlearn: 0.0013112\ttotal: 33.8s\tremaining: 27.3s\n",
      "553:\tlearn: 0.0013110\ttotal: 33.9s\tremaining: 27.3s\n",
      "554:\tlearn: 0.0013108\ttotal: 33.9s\tremaining: 27.2s\n",
      "555:\tlearn: 0.0013108\ttotal: 34s\tremaining: 27.1s\n",
      "556:\tlearn: 0.0013108\ttotal: 34s\tremaining: 27.1s\n",
      "557:\tlearn: 0.0013106\ttotal: 34.1s\tremaining: 27s\n",
      "558:\tlearn: 0.0013101\ttotal: 34.1s\tremaining: 26.9s\n",
      "559:\tlearn: 0.0013099\ttotal: 34.2s\tremaining: 26.9s\n",
      "560:\tlearn: 0.0013098\ttotal: 34.3s\tremaining: 26.8s\n",
      "561:\tlearn: 0.0013098\ttotal: 34.3s\tremaining: 26.8s\n",
      "562:\tlearn: 0.0013098\ttotal: 34.4s\tremaining: 26.7s\n",
      "563:\tlearn: 0.0013096\ttotal: 34.5s\tremaining: 26.6s\n",
      "564:\tlearn: 0.0013096\ttotal: 34.5s\tremaining: 26.6s\n",
      "565:\tlearn: 0.0013094\ttotal: 34.6s\tremaining: 26.5s\n",
      "566:\tlearn: 0.0013093\ttotal: 34.6s\tremaining: 26.4s\n",
      "567:\tlearn: 0.0013092\ttotal: 34.7s\tremaining: 26.4s\n",
      "568:\tlearn: 0.0013090\ttotal: 34.7s\tremaining: 26.3s\n",
      "569:\tlearn: 0.0013089\ttotal: 34.8s\tremaining: 26.3s\n",
      "570:\tlearn: 0.0013089\ttotal: 34.9s\tremaining: 26.2s\n",
      "571:\tlearn: 0.0013089\ttotal: 34.9s\tremaining: 26.1s\n",
      "572:\tlearn: 0.0013089\ttotal: 35s\tremaining: 26.1s\n",
      "573:\tlearn: 0.0013089\ttotal: 35s\tremaining: 26s\n",
      "574:\tlearn: 0.0013088\ttotal: 35.1s\tremaining: 25.9s\n",
      "575:\tlearn: 0.0013083\ttotal: 35.1s\tremaining: 25.9s\n",
      "576:\tlearn: 0.0013083\ttotal: 35.2s\tremaining: 25.8s\n",
      "577:\tlearn: 0.0013074\ttotal: 35.3s\tremaining: 25.7s\n",
      "578:\tlearn: 0.0013071\ttotal: 35.3s\tremaining: 25.7s\n",
      "579:\tlearn: 0.0013071\ttotal: 35.4s\tremaining: 25.6s\n",
      "580:\tlearn: 0.0013069\ttotal: 35.4s\tremaining: 25.6s\n",
      "581:\tlearn: 0.0013068\ttotal: 35.5s\tremaining: 25.5s\n",
      "582:\tlearn: 0.0013067\ttotal: 35.5s\tremaining: 25.4s\n",
      "583:\tlearn: 0.0013067\ttotal: 35.6s\tremaining: 25.4s\n",
      "584:\tlearn: 0.0013064\ttotal: 35.7s\tremaining: 25.3s\n",
      "585:\tlearn: 0.0013061\ttotal: 35.7s\tremaining: 25.2s\n",
      "586:\tlearn: 0.0013056\ttotal: 35.8s\tremaining: 25.2s\n",
      "587:\tlearn: 0.0013056\ttotal: 35.8s\tremaining: 25.1s\n",
      "588:\tlearn: 0.0013048\ttotal: 35.9s\tremaining: 25s\n",
      "589:\tlearn: 0.0013048\ttotal: 36s\tremaining: 25s\n",
      "590:\tlearn: 0.0013047\ttotal: 36s\tremaining: 24.9s\n",
      "591:\tlearn: 0.0013047\ttotal: 36.1s\tremaining: 24.9s\n",
      "592:\tlearn: 0.0013047\ttotal: 36.1s\tremaining: 24.8s\n",
      "593:\tlearn: 0.0013042\ttotal: 36.2s\tremaining: 24.7s\n",
      "594:\tlearn: 0.0013042\ttotal: 36.2s\tremaining: 24.7s\n",
      "595:\tlearn: 0.0013040\ttotal: 36.3s\tremaining: 24.6s\n",
      "596:\tlearn: 0.0013037\ttotal: 36.4s\tremaining: 24.5s\n",
      "597:\tlearn: 0.0013037\ttotal: 36.4s\tremaining: 24.5s\n",
      "598:\tlearn: 0.0013036\ttotal: 36.5s\tremaining: 24.4s\n",
      "599:\tlearn: 0.0013029\ttotal: 36.5s\tremaining: 24.4s\n",
      "600:\tlearn: 0.0013024\ttotal: 36.6s\tremaining: 24.3s\n",
      "601:\tlearn: 0.0013015\ttotal: 36.6s\tremaining: 24.2s\n",
      "602:\tlearn: 0.0013013\ttotal: 36.7s\tremaining: 24.2s\n",
      "603:\tlearn: 0.0013012\ttotal: 36.8s\tremaining: 24.1s\n",
      "604:\tlearn: 0.0013011\ttotal: 36.8s\tremaining: 24s\n",
      "605:\tlearn: 0.0013009\ttotal: 36.9s\tremaining: 24s\n",
      "606:\tlearn: 0.0013009\ttotal: 36.9s\tremaining: 23.9s\n",
      "607:\tlearn: 0.0013008\ttotal: 37s\tremaining: 23.8s\n",
      "608:\tlearn: 0.0013008\ttotal: 37s\tremaining: 23.8s\n",
      "609:\tlearn: 0.0013007\ttotal: 37.1s\tremaining: 23.7s\n",
      "610:\tlearn: 0.0013006\ttotal: 37.1s\tremaining: 23.6s\n",
      "611:\tlearn: 0.0013006\ttotal: 37.2s\tremaining: 23.6s\n",
      "612:\tlearn: 0.0012986\ttotal: 37.3s\tremaining: 23.5s\n",
      "613:\tlearn: 0.0012986\ttotal: 37.3s\tremaining: 23.5s\n",
      "614:\tlearn: 0.0012968\ttotal: 37.4s\tremaining: 23.4s\n",
      "615:\tlearn: 0.0012963\ttotal: 37.4s\tremaining: 23.3s\n",
      "616:\tlearn: 0.0012961\ttotal: 37.5s\tremaining: 23.3s\n",
      "617:\tlearn: 0.0012956\ttotal: 37.5s\tremaining: 23.2s\n",
      "618:\tlearn: 0.0012955\ttotal: 37.6s\tremaining: 23.1s\n",
      "619:\tlearn: 0.0012955\ttotal: 37.7s\tremaining: 23.1s\n",
      "620:\tlearn: 0.0012954\ttotal: 37.7s\tremaining: 23s\n",
      "621:\tlearn: 0.0012950\ttotal: 37.8s\tremaining: 23s\n",
      "622:\tlearn: 0.0012950\ttotal: 37.8s\tremaining: 22.9s\n",
      "623:\tlearn: 0.0012950\ttotal: 37.9s\tremaining: 22.8s\n",
      "624:\tlearn: 0.0012949\ttotal: 37.9s\tremaining: 22.8s\n",
      "625:\tlearn: 0.0012945\ttotal: 38s\tremaining: 22.7s\n",
      "626:\tlearn: 0.0012945\ttotal: 38.1s\tremaining: 22.6s\n",
      "627:\tlearn: 0.0012942\ttotal: 38.1s\tremaining: 22.6s\n",
      "628:\tlearn: 0.0012942\ttotal: 38.2s\tremaining: 22.5s\n",
      "629:\tlearn: 0.0012787\ttotal: 38.2s\tremaining: 22.5s\n",
      "630:\tlearn: 0.0012561\ttotal: 38.3s\tremaining: 22.4s\n",
      "631:\tlearn: 0.0012561\ttotal: 38.3s\tremaining: 22.3s\n",
      "632:\tlearn: 0.0012560\ttotal: 38.4s\tremaining: 22.3s\n",
      "633:\tlearn: 0.0012560\ttotal: 38.5s\tremaining: 22.2s\n",
      "634:\tlearn: 0.0012559\ttotal: 38.5s\tremaining: 22.1s\n",
      "635:\tlearn: 0.0012553\ttotal: 38.6s\tremaining: 22.1s\n",
      "636:\tlearn: 0.0012553\ttotal: 38.6s\tremaining: 22s\n",
      "637:\tlearn: 0.0012553\ttotal: 38.7s\tremaining: 22s\n",
      "638:\tlearn: 0.0012552\ttotal: 38.7s\tremaining: 21.9s\n",
      "639:\tlearn: 0.0012551\ttotal: 38.8s\tremaining: 21.8s\n",
      "640:\tlearn: 0.0012548\ttotal: 38.9s\tremaining: 21.8s\n",
      "641:\tlearn: 0.0012547\ttotal: 38.9s\tremaining: 21.7s\n",
      "642:\tlearn: 0.0012412\ttotal: 39s\tremaining: 21.6s\n",
      "643:\tlearn: 0.0012407\ttotal: 39s\tremaining: 21.6s\n",
      "644:\tlearn: 0.0012407\ttotal: 39.1s\tremaining: 21.5s\n",
      "645:\tlearn: 0.0012402\ttotal: 39.2s\tremaining: 21.5s\n",
      "646:\tlearn: 0.0012402\ttotal: 39.2s\tremaining: 21.4s\n",
      "647:\tlearn: 0.0012402\ttotal: 39.3s\tremaining: 21.3s\n",
      "648:\tlearn: 0.0012400\ttotal: 39.3s\tremaining: 21.3s\n",
      "649:\tlearn: 0.0012320\ttotal: 39.4s\tremaining: 21.2s\n",
      "650:\tlearn: 0.0012319\ttotal: 39.4s\tremaining: 21.1s\n",
      "651:\tlearn: 0.0012315\ttotal: 39.5s\tremaining: 21.1s\n",
      "652:\tlearn: 0.0012315\ttotal: 39.6s\tremaining: 21s\n",
      "653:\tlearn: 0.0012313\ttotal: 39.6s\tremaining: 21s\n",
      "654:\tlearn: 0.0012312\ttotal: 39.7s\tremaining: 20.9s\n",
      "655:\tlearn: 0.0012312\ttotal: 39.7s\tremaining: 20.8s\n",
      "656:\tlearn: 0.0012310\ttotal: 39.8s\tremaining: 20.8s\n",
      "657:\tlearn: 0.0012310\ttotal: 39.9s\tremaining: 20.7s\n",
      "658:\tlearn: 0.0012303\ttotal: 39.9s\tremaining: 20.7s\n",
      "659:\tlearn: 0.0012302\ttotal: 40s\tremaining: 20.6s\n",
      "660:\tlearn: 0.0012302\ttotal: 40s\tremaining: 20.5s\n",
      "661:\tlearn: 0.0012299\ttotal: 40.1s\tremaining: 20.5s\n",
      "662:\tlearn: 0.0012296\ttotal: 40.1s\tremaining: 20.4s\n",
      "663:\tlearn: 0.0012296\ttotal: 40.2s\tremaining: 20.3s\n",
      "664:\tlearn: 0.0012296\ttotal: 40.3s\tremaining: 20.3s\n",
      "665:\tlearn: 0.0012295\ttotal: 40.3s\tremaining: 20.2s\n",
      "666:\tlearn: 0.0012295\ttotal: 40.4s\tremaining: 20.2s\n",
      "667:\tlearn: 0.0012292\ttotal: 40.4s\tremaining: 20.1s\n",
      "668:\tlearn: 0.0012292\ttotal: 40.5s\tremaining: 20s\n",
      "669:\tlearn: 0.0012286\ttotal: 40.5s\tremaining: 20s\n",
      "670:\tlearn: 0.0012284\ttotal: 40.6s\tremaining: 19.9s\n",
      "671:\tlearn: 0.0012281\ttotal: 40.7s\tremaining: 19.8s\n",
      "672:\tlearn: 0.0012278\ttotal: 40.7s\tremaining: 19.8s\n",
      "673:\tlearn: 0.0012276\ttotal: 40.8s\tremaining: 19.7s\n",
      "674:\tlearn: 0.0012273\ttotal: 40.8s\tremaining: 19.7s\n",
      "675:\tlearn: 0.0012272\ttotal: 40.9s\tremaining: 19.6s\n",
      "676:\tlearn: 0.0012270\ttotal: 40.9s\tremaining: 19.5s\n",
      "677:\tlearn: 0.0012269\ttotal: 41s\tremaining: 19.5s\n",
      "678:\tlearn: 0.0012268\ttotal: 41.1s\tremaining: 19.4s\n",
      "679:\tlearn: 0.0012268\ttotal: 41.1s\tremaining: 19.3s\n",
      "680:\tlearn: 0.0012188\ttotal: 41.2s\tremaining: 19.3s\n",
      "681:\tlearn: 0.0012188\ttotal: 41.2s\tremaining: 19.2s\n",
      "682:\tlearn: 0.0012188\ttotal: 41.3s\tremaining: 19.2s\n",
      "683:\tlearn: 0.0012188\ttotal: 41.3s\tremaining: 19.1s\n",
      "684:\tlearn: 0.0012188\ttotal: 41.4s\tremaining: 19s\n",
      "685:\tlearn: 0.0012187\ttotal: 41.5s\tremaining: 19s\n",
      "686:\tlearn: 0.0012186\ttotal: 41.5s\tremaining: 18.9s\n",
      "687:\tlearn: 0.0012185\ttotal: 41.6s\tremaining: 18.9s\n",
      "688:\tlearn: 0.0012183\ttotal: 41.6s\tremaining: 18.8s\n",
      "689:\tlearn: 0.0012180\ttotal: 41.7s\tremaining: 18.7s\n",
      "690:\tlearn: 0.0012058\ttotal: 41.7s\tremaining: 18.7s\n",
      "691:\tlearn: 0.0012057\ttotal: 41.8s\tremaining: 18.6s\n",
      "692:\tlearn: 0.0012056\ttotal: 41.9s\tremaining: 18.5s\n",
      "693:\tlearn: 0.0012055\ttotal: 41.9s\tremaining: 18.5s\n",
      "694:\tlearn: 0.0012053\ttotal: 42s\tremaining: 18.4s\n",
      "695:\tlearn: 0.0012052\ttotal: 42s\tremaining: 18.4s\n",
      "696:\tlearn: 0.0012052\ttotal: 42.1s\tremaining: 18.3s\n",
      "697:\tlearn: 0.0012052\ttotal: 42.1s\tremaining: 18.2s\n",
      "698:\tlearn: 0.0012047\ttotal: 42.2s\tremaining: 18.2s\n",
      "699:\tlearn: 0.0011929\ttotal: 42.3s\tremaining: 18.1s\n",
      "700:\tlearn: 0.0011925\ttotal: 42.3s\tremaining: 18.1s\n",
      "701:\tlearn: 0.0011925\ttotal: 42.4s\tremaining: 18s\n",
      "702:\tlearn: 0.0011925\ttotal: 42.4s\tremaining: 17.9s\n",
      "703:\tlearn: 0.0011924\ttotal: 42.5s\tremaining: 17.9s\n",
      "704:\tlearn: 0.0011924\ttotal: 42.5s\tremaining: 17.8s\n",
      "705:\tlearn: 0.0011924\ttotal: 42.6s\tremaining: 17.7s\n",
      "706:\tlearn: 0.0011923\ttotal: 42.7s\tremaining: 17.7s\n",
      "707:\tlearn: 0.0011922\ttotal: 42.7s\tremaining: 17.6s\n",
      "708:\tlearn: 0.0011922\ttotal: 42.8s\tremaining: 17.6s\n",
      "709:\tlearn: 0.0011921\ttotal: 42.8s\tremaining: 17.5s\n",
      "710:\tlearn: 0.0011921\ttotal: 42.9s\tremaining: 17.4s\n",
      "711:\tlearn: 0.0011921\ttotal: 42.9s\tremaining: 17.4s\n",
      "712:\tlearn: 0.0011921\ttotal: 43s\tremaining: 17.3s\n",
      "713:\tlearn: 0.0011919\ttotal: 43.1s\tremaining: 17.2s\n",
      "714:\tlearn: 0.0011919\ttotal: 43.1s\tremaining: 17.2s\n",
      "715:\tlearn: 0.0011918\ttotal: 43.2s\tremaining: 17.1s\n",
      "716:\tlearn: 0.0011917\ttotal: 43.2s\tremaining: 17.1s\n",
      "717:\tlearn: 0.0011916\ttotal: 43.3s\tremaining: 17s\n",
      "718:\tlearn: 0.0011910\ttotal: 43.3s\tremaining: 16.9s\n",
      "719:\tlearn: 0.0011907\ttotal: 43.4s\tremaining: 16.9s\n",
      "720:\tlearn: 0.0011905\ttotal: 43.5s\tremaining: 16.8s\n",
      "721:\tlearn: 0.0011905\ttotal: 43.5s\tremaining: 16.8s\n",
      "722:\tlearn: 0.0011901\ttotal: 43.6s\tremaining: 16.7s\n",
      "723:\tlearn: 0.0011900\ttotal: 43.6s\tremaining: 16.6s\n",
      "724:\tlearn: 0.0011899\ttotal: 43.7s\tremaining: 16.6s\n",
      "725:\tlearn: 0.0011898\ttotal: 43.7s\tremaining: 16.5s\n",
      "726:\tlearn: 0.0011897\ttotal: 43.8s\tremaining: 16.5s\n",
      "727:\tlearn: 0.0011897\ttotal: 43.9s\tremaining: 16.4s\n",
      "728:\tlearn: 0.0011897\ttotal: 44s\tremaining: 16.3s\n",
      "729:\tlearn: 0.0011897\ttotal: 44s\tremaining: 16.3s\n",
      "730:\tlearn: 0.0011895\ttotal: 44.1s\tremaining: 16.2s\n",
      "731:\tlearn: 0.0011895\ttotal: 44.2s\tremaining: 16.2s\n",
      "732:\tlearn: 0.0011891\ttotal: 44.2s\tremaining: 16.1s\n",
      "733:\tlearn: 0.0011890\ttotal: 44.3s\tremaining: 16.1s\n",
      "734:\tlearn: 0.0011889\ttotal: 44.4s\tremaining: 16s\n",
      "735:\tlearn: 0.0011889\ttotal: 44.5s\tremaining: 16s\n",
      "736:\tlearn: 0.0011879\ttotal: 44.6s\tremaining: 15.9s\n",
      "737:\tlearn: 0.0011879\ttotal: 44.6s\tremaining: 15.8s\n",
      "738:\tlearn: 0.0011855\ttotal: 44.7s\tremaining: 15.8s\n",
      "739:\tlearn: 0.0011855\ttotal: 44.8s\tremaining: 15.7s\n",
      "740:\tlearn: 0.0011854\ttotal: 44.8s\tremaining: 15.7s\n",
      "741:\tlearn: 0.0011854\ttotal: 44.9s\tremaining: 15.6s\n",
      "742:\tlearn: 0.0011852\ttotal: 45s\tremaining: 15.6s\n",
      "743:\tlearn: 0.0011852\ttotal: 45.1s\tremaining: 15.5s\n",
      "744:\tlearn: 0.0011851\ttotal: 45.1s\tremaining: 15.4s\n",
      "745:\tlearn: 0.0011843\ttotal: 45.2s\tremaining: 15.4s\n",
      "746:\tlearn: 0.0011842\ttotal: 45.3s\tremaining: 15.3s\n",
      "747:\tlearn: 0.0011841\ttotal: 45.4s\tremaining: 15.3s\n",
      "748:\tlearn: 0.0011841\ttotal: 45.4s\tremaining: 15.2s\n",
      "749:\tlearn: 0.0011840\ttotal: 45.5s\tremaining: 15.2s\n",
      "750:\tlearn: 0.0011837\ttotal: 45.6s\tremaining: 15.1s\n",
      "751:\tlearn: 0.0011836\ttotal: 45.6s\tremaining: 15.1s\n",
      "752:\tlearn: 0.0011832\ttotal: 45.7s\tremaining: 15s\n",
      "753:\tlearn: 0.0011831\ttotal: 45.8s\tremaining: 14.9s\n",
      "754:\tlearn: 0.0011828\ttotal: 45.9s\tremaining: 14.9s\n",
      "755:\tlearn: 0.0011828\ttotal: 45.9s\tremaining: 14.8s\n",
      "756:\tlearn: 0.0011828\ttotal: 46s\tremaining: 14.8s\n",
      "757:\tlearn: 0.0011828\ttotal: 46.1s\tremaining: 14.7s\n",
      "758:\tlearn: 0.0011828\ttotal: 46.2s\tremaining: 14.7s\n",
      "759:\tlearn: 0.0011827\ttotal: 46.2s\tremaining: 14.6s\n",
      "760:\tlearn: 0.0011827\ttotal: 46.3s\tremaining: 14.5s\n",
      "761:\tlearn: 0.0011826\ttotal: 46.4s\tremaining: 14.5s\n",
      "762:\tlearn: 0.0011826\ttotal: 46.5s\tremaining: 14.4s\n",
      "763:\tlearn: 0.0011824\ttotal: 46.5s\tremaining: 14.4s\n",
      "764:\tlearn: 0.0011822\ttotal: 46.6s\tremaining: 14.3s\n",
      "765:\tlearn: 0.0011819\ttotal: 46.7s\tremaining: 14.3s\n",
      "766:\tlearn: 0.0011815\ttotal: 46.7s\tremaining: 14.2s\n",
      "767:\tlearn: 0.0011815\ttotal: 46.8s\tremaining: 14.1s\n",
      "768:\tlearn: 0.0011813\ttotal: 46.8s\tremaining: 14.1s\n",
      "769:\tlearn: 0.0011812\ttotal: 46.9s\tremaining: 14s\n",
      "770:\tlearn: 0.0011812\ttotal: 47s\tremaining: 13.9s\n",
      "771:\tlearn: 0.0011811\ttotal: 47s\tremaining: 13.9s\n",
      "772:\tlearn: 0.0011799\ttotal: 47.1s\tremaining: 13.8s\n",
      "773:\tlearn: 0.0011799\ttotal: 47.1s\tremaining: 13.8s\n",
      "774:\tlearn: 0.0011794\ttotal: 47.2s\tremaining: 13.7s\n",
      "775:\tlearn: 0.0011794\ttotal: 47.2s\tremaining: 13.6s\n",
      "776:\tlearn: 0.0011794\ttotal: 47.3s\tremaining: 13.6s\n",
      "777:\tlearn: 0.0011793\ttotal: 47.4s\tremaining: 13.5s\n",
      "778:\tlearn: 0.0011792\ttotal: 47.4s\tremaining: 13.4s\n",
      "779:\tlearn: 0.0011791\ttotal: 47.5s\tremaining: 13.4s\n",
      "780:\tlearn: 0.0011789\ttotal: 47.5s\tremaining: 13.3s\n",
      "781:\tlearn: 0.0011789\ttotal: 47.6s\tremaining: 13.3s\n",
      "782:\tlearn: 0.0011789\ttotal: 47.6s\tremaining: 13.2s\n",
      "783:\tlearn: 0.0011788\ttotal: 47.7s\tremaining: 13.1s\n",
      "784:\tlearn: 0.0011785\ttotal: 47.8s\tremaining: 13.1s\n",
      "785:\tlearn: 0.0011783\ttotal: 47.8s\tremaining: 13s\n",
      "786:\tlearn: 0.0011777\ttotal: 47.9s\tremaining: 13s\n",
      "787:\tlearn: 0.0011643\ttotal: 47.9s\tremaining: 12.9s\n",
      "788:\tlearn: 0.0011643\ttotal: 48s\tremaining: 12.8s\n",
      "789:\tlearn: 0.0011643\ttotal: 48s\tremaining: 12.8s\n",
      "790:\tlearn: 0.0011641\ttotal: 48.1s\tremaining: 12.7s\n",
      "791:\tlearn: 0.0011641\ttotal: 48.2s\tremaining: 12.6s\n",
      "792:\tlearn: 0.0011641\ttotal: 48.2s\tremaining: 12.6s\n",
      "793:\tlearn: 0.0011640\ttotal: 48.3s\tremaining: 12.5s\n",
      "794:\tlearn: 0.0011639\ttotal: 48.3s\tremaining: 12.5s\n",
      "795:\tlearn: 0.0011639\ttotal: 48.4s\tremaining: 12.4s\n",
      "796:\tlearn: 0.0011639\ttotal: 48.4s\tremaining: 12.3s\n",
      "797:\tlearn: 0.0011639\ttotal: 48.5s\tremaining: 12.3s\n",
      "798:\tlearn: 0.0011637\ttotal: 48.6s\tremaining: 12.2s\n",
      "799:\tlearn: 0.0011636\ttotal: 48.6s\tremaining: 12.2s\n",
      "800:\tlearn: 0.0011635\ttotal: 48.7s\tremaining: 12.1s\n",
      "801:\tlearn: 0.0011635\ttotal: 48.7s\tremaining: 12s\n",
      "802:\tlearn: 0.0011633\ttotal: 48.8s\tremaining: 12s\n",
      "803:\tlearn: 0.0011631\ttotal: 48.8s\tremaining: 11.9s\n",
      "804:\tlearn: 0.0011630\ttotal: 48.9s\tremaining: 11.8s\n",
      "805:\tlearn: 0.0011629\ttotal: 49s\tremaining: 11.8s\n",
      "806:\tlearn: 0.0011629\ttotal: 49s\tremaining: 11.7s\n",
      "807:\tlearn: 0.0011629\ttotal: 49.1s\tremaining: 11.7s\n",
      "808:\tlearn: 0.0011626\ttotal: 49.1s\tremaining: 11.6s\n",
      "809:\tlearn: 0.0011626\ttotal: 49.2s\tremaining: 11.5s\n",
      "810:\tlearn: 0.0011626\ttotal: 49.3s\tremaining: 11.5s\n",
      "811:\tlearn: 0.0011625\ttotal: 49.3s\tremaining: 11.4s\n",
      "812:\tlearn: 0.0011623\ttotal: 49.4s\tremaining: 11.4s\n",
      "813:\tlearn: 0.0011621\ttotal: 49.4s\tremaining: 11.3s\n",
      "814:\tlearn: 0.0011620\ttotal: 49.5s\tremaining: 11.2s\n",
      "815:\tlearn: 0.0011613\ttotal: 49.5s\tremaining: 11.2s\n",
      "816:\tlearn: 0.0011612\ttotal: 49.6s\tremaining: 11.1s\n",
      "817:\tlearn: 0.0011612\ttotal: 49.7s\tremaining: 11s\n",
      "818:\tlearn: 0.0011609\ttotal: 49.7s\tremaining: 11s\n",
      "819:\tlearn: 0.0011609\ttotal: 49.8s\tremaining: 10.9s\n",
      "820:\tlearn: 0.0011607\ttotal: 49.8s\tremaining: 10.9s\n",
      "821:\tlearn: 0.0011606\ttotal: 49.9s\tremaining: 10.8s\n",
      "822:\tlearn: 0.0011606\ttotal: 49.9s\tremaining: 10.7s\n",
      "823:\tlearn: 0.0011601\ttotal: 50s\tremaining: 10.7s\n",
      "824:\tlearn: 0.0011601\ttotal: 50.1s\tremaining: 10.6s\n",
      "825:\tlearn: 0.0011599\ttotal: 50.1s\tremaining: 10.6s\n",
      "826:\tlearn: 0.0011598\ttotal: 50.3s\tremaining: 10.5s\n",
      "827:\tlearn: 0.0011597\ttotal: 50.3s\tremaining: 10.5s\n",
      "828:\tlearn: 0.0011597\ttotal: 50.4s\tremaining: 10.4s\n",
      "829:\tlearn: 0.0011594\ttotal: 50.4s\tremaining: 10.3s\n",
      "830:\tlearn: 0.0011591\ttotal: 50.5s\tremaining: 10.3s\n",
      "831:\tlearn: 0.0011590\ttotal: 50.5s\tremaining: 10.2s\n",
      "832:\tlearn: 0.0011585\ttotal: 50.6s\tremaining: 10.1s\n",
      "833:\tlearn: 0.0011585\ttotal: 50.7s\tremaining: 10.1s\n",
      "834:\tlearn: 0.0011585\ttotal: 50.7s\tremaining: 10s\n",
      "835:\tlearn: 0.0011583\ttotal: 50.8s\tremaining: 9.96s\n",
      "836:\tlearn: 0.0011583\ttotal: 50.8s\tremaining: 9.9s\n",
      "837:\tlearn: 0.0011573\ttotal: 50.9s\tremaining: 9.84s\n",
      "838:\tlearn: 0.0011573\ttotal: 50.9s\tremaining: 9.78s\n",
      "839:\tlearn: 0.0011573\ttotal: 51s\tremaining: 9.71s\n",
      "840:\tlearn: 0.0011569\ttotal: 51.1s\tremaining: 9.65s\n",
      "841:\tlearn: 0.0011568\ttotal: 51.1s\tremaining: 9.59s\n",
      "842:\tlearn: 0.0011566\ttotal: 51.2s\tremaining: 9.53s\n",
      "843:\tlearn: 0.0011566\ttotal: 51.2s\tremaining: 9.47s\n",
      "844:\tlearn: 0.0011560\ttotal: 51.3s\tremaining: 9.41s\n",
      "845:\tlearn: 0.0011559\ttotal: 51.3s\tremaining: 9.35s\n",
      "846:\tlearn: 0.0011559\ttotal: 51.4s\tremaining: 9.28s\n",
      "847:\tlearn: 0.0011559\ttotal: 51.5s\tremaining: 9.22s\n",
      "848:\tlearn: 0.0011559\ttotal: 51.5s\tremaining: 9.16s\n",
      "849:\tlearn: 0.0011557\ttotal: 51.6s\tremaining: 9.1s\n",
      "850:\tlearn: 0.0011557\ttotal: 51.6s\tremaining: 9.04s\n",
      "851:\tlearn: 0.0011555\ttotal: 51.7s\tremaining: 8.98s\n",
      "852:\tlearn: 0.0011549\ttotal: 51.7s\tremaining: 8.91s\n",
      "853:\tlearn: 0.0011547\ttotal: 51.8s\tremaining: 8.85s\n",
      "854:\tlearn: 0.0011546\ttotal: 51.8s\tremaining: 8.79s\n",
      "855:\tlearn: 0.0011545\ttotal: 51.9s\tremaining: 8.73s\n",
      "856:\tlearn: 0.0011544\ttotal: 52s\tremaining: 8.67s\n",
      "857:\tlearn: 0.0011543\ttotal: 52s\tremaining: 8.61s\n",
      "858:\tlearn: 0.0011542\ttotal: 52.1s\tremaining: 8.55s\n",
      "859:\tlearn: 0.0011539\ttotal: 52.1s\tremaining: 8.49s\n",
      "860:\tlearn: 0.0011539\ttotal: 52.2s\tremaining: 8.43s\n",
      "861:\tlearn: 0.0011536\ttotal: 52.2s\tremaining: 8.36s\n",
      "862:\tlearn: 0.0011527\ttotal: 52.3s\tremaining: 8.3s\n",
      "863:\tlearn: 0.0011525\ttotal: 52.4s\tremaining: 8.24s\n",
      "864:\tlearn: 0.0011521\ttotal: 52.4s\tremaining: 8.18s\n",
      "865:\tlearn: 0.0011521\ttotal: 52.5s\tremaining: 8.12s\n",
      "866:\tlearn: 0.0011520\ttotal: 52.5s\tremaining: 8.06s\n",
      "867:\tlearn: 0.0011520\ttotal: 52.6s\tremaining: 8s\n",
      "868:\tlearn: 0.0011520\ttotal: 52.6s\tremaining: 7.94s\n",
      "869:\tlearn: 0.0011454\ttotal: 52.7s\tremaining: 7.88s\n",
      "870:\tlearn: 0.0011450\ttotal: 52.8s\tremaining: 7.81s\n",
      "871:\tlearn: 0.0011448\ttotal: 52.8s\tremaining: 7.75s\n",
      "872:\tlearn: 0.0011446\ttotal: 52.9s\tremaining: 7.69s\n",
      "873:\tlearn: 0.0011445\ttotal: 52.9s\tremaining: 7.63s\n",
      "874:\tlearn: 0.0011445\ttotal: 53s\tremaining: 7.57s\n",
      "875:\tlearn: 0.0011442\ttotal: 53s\tremaining: 7.51s\n",
      "876:\tlearn: 0.0011441\ttotal: 53.1s\tremaining: 7.45s\n",
      "877:\tlearn: 0.0011441\ttotal: 53.2s\tremaining: 7.39s\n",
      "878:\tlearn: 0.0011441\ttotal: 53.2s\tremaining: 7.33s\n",
      "879:\tlearn: 0.0011440\ttotal: 53.3s\tremaining: 7.26s\n",
      "880:\tlearn: 0.0011440\ttotal: 53.3s\tremaining: 7.2s\n",
      "881:\tlearn: 0.0011440\ttotal: 53.4s\tremaining: 7.14s\n",
      "882:\tlearn: 0.0011439\ttotal: 53.4s\tremaining: 7.08s\n",
      "883:\tlearn: 0.0011427\ttotal: 53.5s\tremaining: 7.02s\n",
      "884:\tlearn: 0.0011427\ttotal: 53.6s\tremaining: 6.96s\n",
      "885:\tlearn: 0.0011426\ttotal: 53.6s\tremaining: 6.9s\n",
      "886:\tlearn: 0.0011424\ttotal: 53.7s\tremaining: 6.84s\n",
      "887:\tlearn: 0.0011423\ttotal: 53.7s\tremaining: 6.78s\n",
      "888:\tlearn: 0.0011409\ttotal: 53.8s\tremaining: 6.71s\n",
      "889:\tlearn: 0.0011407\ttotal: 53.8s\tremaining: 6.65s\n",
      "890:\tlearn: 0.0011404\ttotal: 53.9s\tremaining: 6.59s\n",
      "891:\tlearn: 0.0011403\ttotal: 54s\tremaining: 6.53s\n",
      "892:\tlearn: 0.0011402\ttotal: 54s\tremaining: 6.47s\n",
      "893:\tlearn: 0.0011402\ttotal: 54.1s\tremaining: 6.41s\n",
      "894:\tlearn: 0.0011401\ttotal: 54.1s\tremaining: 6.35s\n",
      "895:\tlearn: 0.0011399\ttotal: 54.2s\tremaining: 6.29s\n",
      "896:\tlearn: 0.0011398\ttotal: 54.2s\tremaining: 6.23s\n",
      "897:\tlearn: 0.0011398\ttotal: 54.3s\tremaining: 6.17s\n",
      "898:\tlearn: 0.0011396\ttotal: 54.4s\tremaining: 6.11s\n",
      "899:\tlearn: 0.0011396\ttotal: 54.4s\tremaining: 6.05s\n",
      "900:\tlearn: 0.0011393\ttotal: 54.5s\tremaining: 5.99s\n",
      "901:\tlearn: 0.0011393\ttotal: 54.5s\tremaining: 5.92s\n",
      "902:\tlearn: 0.0011391\ttotal: 54.6s\tremaining: 5.86s\n",
      "903:\tlearn: 0.0011390\ttotal: 54.7s\tremaining: 5.8s\n",
      "904:\tlearn: 0.0011389\ttotal: 54.7s\tremaining: 5.75s\n",
      "905:\tlearn: 0.0011389\ttotal: 54.8s\tremaining: 5.68s\n",
      "906:\tlearn: 0.0011387\ttotal: 54.9s\tremaining: 5.63s\n",
      "907:\tlearn: 0.0011387\ttotal: 54.9s\tremaining: 5.56s\n",
      "908:\tlearn: 0.0011387\ttotal: 55s\tremaining: 5.5s\n",
      "909:\tlearn: 0.0011386\ttotal: 55s\tremaining: 5.44s\n",
      "910:\tlearn: 0.0011383\ttotal: 55.1s\tremaining: 5.38s\n",
      "911:\tlearn: 0.0011382\ttotal: 55.2s\tremaining: 5.32s\n",
      "912:\tlearn: 0.0011381\ttotal: 55.2s\tremaining: 5.26s\n",
      "913:\tlearn: 0.0011380\ttotal: 55.3s\tremaining: 5.2s\n",
      "914:\tlearn: 0.0011379\ttotal: 55.3s\tremaining: 5.14s\n",
      "915:\tlearn: 0.0011379\ttotal: 55.4s\tremaining: 5.08s\n",
      "916:\tlearn: 0.0011376\ttotal: 55.4s\tremaining: 5.02s\n",
      "917:\tlearn: 0.0011373\ttotal: 55.5s\tremaining: 4.96s\n",
      "918:\tlearn: 0.0011373\ttotal: 55.6s\tremaining: 4.9s\n",
      "919:\tlearn: 0.0011373\ttotal: 55.6s\tremaining: 4.83s\n",
      "920:\tlearn: 0.0011373\ttotal: 55.7s\tremaining: 4.77s\n",
      "921:\tlearn: 0.0011371\ttotal: 55.7s\tremaining: 4.71s\n",
      "922:\tlearn: 0.0011370\ttotal: 55.8s\tremaining: 4.65s\n",
      "923:\tlearn: 0.0011369\ttotal: 55.8s\tremaining: 4.59s\n",
      "924:\tlearn: 0.0011369\ttotal: 55.9s\tremaining: 4.53s\n",
      "925:\tlearn: 0.0011369\ttotal: 56s\tremaining: 4.47s\n",
      "926:\tlearn: 0.0011367\ttotal: 56s\tremaining: 4.41s\n",
      "927:\tlearn: 0.0011366\ttotal: 56.1s\tremaining: 4.35s\n",
      "928:\tlearn: 0.0011366\ttotal: 56.1s\tremaining: 4.29s\n",
      "929:\tlearn: 0.0011364\ttotal: 56.2s\tremaining: 4.23s\n",
      "930:\tlearn: 0.0011363\ttotal: 56.2s\tremaining: 4.17s\n",
      "931:\tlearn: 0.0011363\ttotal: 56.3s\tremaining: 4.11s\n",
      "932:\tlearn: 0.0011361\ttotal: 56.4s\tremaining: 4.05s\n",
      "933:\tlearn: 0.0011360\ttotal: 56.4s\tremaining: 3.99s\n",
      "934:\tlearn: 0.0011360\ttotal: 56.5s\tremaining: 3.92s\n",
      "935:\tlearn: 0.0011360\ttotal: 56.5s\tremaining: 3.87s\n",
      "936:\tlearn: 0.0011357\ttotal: 56.6s\tremaining: 3.8s\n",
      "937:\tlearn: 0.0011355\ttotal: 56.6s\tremaining: 3.74s\n",
      "938:\tlearn: 0.0011349\ttotal: 56.7s\tremaining: 3.68s\n",
      "939:\tlearn: 0.0011344\ttotal: 56.8s\tremaining: 3.62s\n",
      "940:\tlearn: 0.0011288\ttotal: 56.8s\tremaining: 3.56s\n",
      "941:\tlearn: 0.0011288\ttotal: 56.9s\tremaining: 3.5s\n",
      "942:\tlearn: 0.0011287\ttotal: 56.9s\tremaining: 3.44s\n",
      "943:\tlearn: 0.0011287\ttotal: 57s\tremaining: 3.38s\n",
      "944:\tlearn: 0.0011286\ttotal: 57s\tremaining: 3.32s\n",
      "945:\tlearn: 0.0011286\ttotal: 57.1s\tremaining: 3.26s\n",
      "946:\tlearn: 0.0011285\ttotal: 57.2s\tremaining: 3.2s\n",
      "947:\tlearn: 0.0011283\ttotal: 57.2s\tremaining: 3.14s\n",
      "948:\tlearn: 0.0011283\ttotal: 57.3s\tremaining: 3.08s\n",
      "949:\tlearn: 0.0011283\ttotal: 57.3s\tremaining: 3.02s\n",
      "950:\tlearn: 0.0011281\ttotal: 57.4s\tremaining: 2.96s\n",
      "951:\tlearn: 0.0011281\ttotal: 57.4s\tremaining: 2.9s\n",
      "952:\tlearn: 0.0011276\ttotal: 57.5s\tremaining: 2.83s\n",
      "953:\tlearn: 0.0011274\ttotal: 57.5s\tremaining: 2.77s\n",
      "954:\tlearn: 0.0011271\ttotal: 57.6s\tremaining: 2.71s\n",
      "955:\tlearn: 0.0011269\ttotal: 57.7s\tremaining: 2.65s\n",
      "956:\tlearn: 0.0011269\ttotal: 57.7s\tremaining: 2.59s\n",
      "957:\tlearn: 0.0011269\ttotal: 57.8s\tremaining: 2.53s\n",
      "958:\tlearn: 0.0011269\ttotal: 57.8s\tremaining: 2.47s\n",
      "959:\tlearn: 0.0011263\ttotal: 57.9s\tremaining: 2.41s\n",
      "960:\tlearn: 0.0011263\ttotal: 57.9s\tremaining: 2.35s\n",
      "961:\tlearn: 0.0011261\ttotal: 58s\tremaining: 2.29s\n",
      "962:\tlearn: 0.0011260\ttotal: 58.1s\tremaining: 2.23s\n",
      "963:\tlearn: 0.0011260\ttotal: 58.1s\tremaining: 2.17s\n",
      "964:\tlearn: 0.0011260\ttotal: 58.2s\tremaining: 2.11s\n",
      "965:\tlearn: 0.0011257\ttotal: 58.2s\tremaining: 2.05s\n",
      "966:\tlearn: 0.0011253\ttotal: 58.3s\tremaining: 1.99s\n",
      "967:\tlearn: 0.0011253\ttotal: 58.3s\tremaining: 1.93s\n",
      "968:\tlearn: 0.0011252\ttotal: 58.4s\tremaining: 1.87s\n",
      "969:\tlearn: 0.0011252\ttotal: 58.4s\tremaining: 1.81s\n",
      "970:\tlearn: 0.0011249\ttotal: 58.5s\tremaining: 1.75s\n",
      "971:\tlearn: 0.0011245\ttotal: 58.6s\tremaining: 1.69s\n",
      "972:\tlearn: 0.0011243\ttotal: 58.6s\tremaining: 1.63s\n",
      "973:\tlearn: 0.0011221\ttotal: 58.7s\tremaining: 1.57s\n",
      "974:\tlearn: 0.0011216\ttotal: 58.7s\tremaining: 1.5s\n",
      "975:\tlearn: 0.0011214\ttotal: 58.8s\tremaining: 1.45s\n",
      "976:\tlearn: 0.0011214\ttotal: 58.8s\tremaining: 1.39s\n",
      "977:\tlearn: 0.0011213\ttotal: 58.9s\tremaining: 1.32s\n",
      "978:\tlearn: 0.0011212\ttotal: 59s\tremaining: 1.26s\n",
      "979:\tlearn: 0.0011207\ttotal: 59s\tremaining: 1.2s\n",
      "980:\tlearn: 0.0011206\ttotal: 59.1s\tremaining: 1.14s\n",
      "981:\tlearn: 0.0011206\ttotal: 59.1s\tremaining: 1.08s\n",
      "982:\tlearn: 0.0011204\ttotal: 59.2s\tremaining: 1.02s\n",
      "983:\tlearn: 0.0011191\ttotal: 59.2s\tremaining: 963ms\n",
      "984:\tlearn: 0.0011191\ttotal: 59.3s\tremaining: 903ms\n",
      "985:\tlearn: 0.0011191\ttotal: 59.3s\tremaining: 843ms\n",
      "986:\tlearn: 0.0011190\ttotal: 59.4s\tremaining: 782ms\n",
      "987:\tlearn: 0.0011189\ttotal: 59.5s\tremaining: 722ms\n",
      "988:\tlearn: 0.0011189\ttotal: 59.5s\tremaining: 662ms\n",
      "989:\tlearn: 0.0011187\ttotal: 59.6s\tremaining: 602ms\n",
      "990:\tlearn: 0.0011186\ttotal: 59.6s\tremaining: 542ms\n",
      "991:\tlearn: 0.0011182\ttotal: 59.7s\tremaining: 481ms\n",
      "992:\tlearn: 0.0011182\ttotal: 59.7s\tremaining: 421ms\n",
      "993:\tlearn: 0.0011178\ttotal: 59.8s\tremaining: 361ms\n",
      "994:\tlearn: 0.0011176\ttotal: 59.9s\tremaining: 301ms\n",
      "995:\tlearn: 0.0011174\ttotal: 59.9s\tremaining: 241ms\n",
      "996:\tlearn: 0.0011174\ttotal: 60s\tremaining: 180ms\n",
      "997:\tlearn: 0.0011174\ttotal: 1m\tremaining: 120ms\n",
      "998:\tlearn: 0.0011174\ttotal: 1m\tremaining: 60.1ms\n",
      "999:\tlearn: 0.0011173\ttotal: 1m\tremaining: 0us\n",
      "0:\tlearn: 0.5970881\ttotal: 73.1ms\tremaining: 1m 12s\n",
      "1:\tlearn: 0.5484442\ttotal: 130ms\tremaining: 1m 4s\n",
      "2:\tlearn: 0.5075570\ttotal: 186ms\tremaining: 1m 1s\n",
      "3:\tlearn: 0.4754436\ttotal: 243ms\tremaining: 1m\n",
      "4:\tlearn: 0.4191364\ttotal: 306ms\tremaining: 1m\n",
      "5:\tlearn: 0.3420888\ttotal: 361ms\tremaining: 59.8s\n",
      "6:\tlearn: 0.2974931\ttotal: 417ms\tremaining: 59.2s\n",
      "7:\tlearn: 0.2545128\ttotal: 474ms\tremaining: 58.8s\n",
      "8:\tlearn: 0.2112463\ttotal: 532ms\tremaining: 58.6s\n",
      "9:\tlearn: 0.1784149\ttotal: 588ms\tremaining: 58.2s\n",
      "10:\tlearn: 0.1483133\ttotal: 645ms\tremaining: 57.9s\n",
      "11:\tlearn: 0.1167807\ttotal: 701ms\tremaining: 57.8s\n",
      "12:\tlearn: 0.1038222\ttotal: 757ms\tremaining: 57.5s\n",
      "13:\tlearn: 0.0833070\ttotal: 814ms\tremaining: 57.3s\n",
      "14:\tlearn: 0.0679732\ttotal: 871ms\tremaining: 57.2s\n",
      "15:\tlearn: 0.0627177\ttotal: 927ms\tremaining: 57s\n",
      "16:\tlearn: 0.0606318\ttotal: 984ms\tremaining: 56.9s\n",
      "17:\tlearn: 0.0526142\ttotal: 1.04s\tremaining: 56.7s\n",
      "18:\tlearn: 0.0474818\ttotal: 1.09s\tremaining: 56.6s\n",
      "19:\tlearn: 0.0410675\ttotal: 1.15s\tremaining: 56.4s\n",
      "20:\tlearn: 0.0399849\ttotal: 1.21s\tremaining: 56.6s\n",
      "21:\tlearn: 0.0329694\ttotal: 1.27s\tremaining: 56.5s\n",
      "22:\tlearn: 0.0292760\ttotal: 1.32s\tremaining: 56.3s\n",
      "23:\tlearn: 0.0262815\ttotal: 1.38s\tremaining: 56.2s\n",
      "24:\tlearn: 0.0260860\ttotal: 1.44s\tremaining: 56.1s\n",
      "25:\tlearn: 0.0259216\ttotal: 1.5s\tremaining: 56s\n",
      "26:\tlearn: 0.0231920\ttotal: 1.55s\tremaining: 55.9s\n",
      "27:\tlearn: 0.0223250\ttotal: 1.61s\tremaining: 55.9s\n",
      "28:\tlearn: 0.0196685\ttotal: 1.67s\tremaining: 55.8s\n",
      "29:\tlearn: 0.0184740\ttotal: 1.72s\tremaining: 55.7s\n",
      "30:\tlearn: 0.0172325\ttotal: 1.78s\tremaining: 55.6s\n",
      "31:\tlearn: 0.0163642\ttotal: 1.83s\tremaining: 55.5s\n",
      "32:\tlearn: 0.0142974\ttotal: 1.89s\tremaining: 55.5s\n",
      "33:\tlearn: 0.0134899\ttotal: 1.95s\tremaining: 55.4s\n",
      "34:\tlearn: 0.0124361\ttotal: 2.01s\tremaining: 55.3s\n",
      "35:\tlearn: 0.0118066\ttotal: 2.06s\tremaining: 55.2s\n",
      "36:\tlearn: 0.0109261\ttotal: 2.12s\tremaining: 55.2s\n",
      "37:\tlearn: 0.0105589\ttotal: 2.18s\tremaining: 55.1s\n",
      "38:\tlearn: 0.0100286\ttotal: 2.23s\tremaining: 55s\n",
      "39:\tlearn: 0.0097870\ttotal: 2.29s\tremaining: 54.9s\n",
      "40:\tlearn: 0.0093398\ttotal: 2.35s\tremaining: 54.9s\n",
      "41:\tlearn: 0.0089004\ttotal: 2.4s\tremaining: 54.8s\n",
      "42:\tlearn: 0.0085745\ttotal: 2.46s\tremaining: 54.7s\n",
      "43:\tlearn: 0.0080396\ttotal: 2.52s\tremaining: 54.7s\n",
      "44:\tlearn: 0.0076856\ttotal: 2.58s\tremaining: 54.7s\n",
      "45:\tlearn: 0.0074581\ttotal: 2.63s\tremaining: 54.6s\n",
      "46:\tlearn: 0.0070218\ttotal: 2.69s\tremaining: 54.6s\n",
      "47:\tlearn: 0.0067989\ttotal: 2.75s\tremaining: 54.5s\n",
      "48:\tlearn: 0.0066215\ttotal: 2.8s\tremaining: 54.4s\n",
      "49:\tlearn: 0.0064062\ttotal: 2.86s\tremaining: 54.3s\n",
      "50:\tlearn: 0.0061263\ttotal: 2.92s\tremaining: 54.3s\n",
      "51:\tlearn: 0.0059341\ttotal: 2.97s\tremaining: 54.2s\n",
      "52:\tlearn: 0.0058014\ttotal: 3.03s\tremaining: 54.1s\n",
      "53:\tlearn: 0.0055545\ttotal: 3.09s\tremaining: 54.1s\n",
      "54:\tlearn: 0.0052948\ttotal: 3.14s\tremaining: 54s\n",
      "55:\tlearn: 0.0051525\ttotal: 3.2s\tremaining: 53.9s\n",
      "56:\tlearn: 0.0050165\ttotal: 3.26s\tremaining: 53.9s\n",
      "57:\tlearn: 0.0049821\ttotal: 3.31s\tremaining: 53.8s\n",
      "58:\tlearn: 0.0048014\ttotal: 3.37s\tremaining: 53.7s\n",
      "59:\tlearn: 0.0046823\ttotal: 3.43s\tremaining: 53.7s\n",
      "60:\tlearn: 0.0045032\ttotal: 3.48s\tremaining: 53.6s\n",
      "61:\tlearn: 0.0043358\ttotal: 3.54s\tremaining: 53.6s\n",
      "62:\tlearn: 0.0041861\ttotal: 3.6s\tremaining: 53.6s\n",
      "63:\tlearn: 0.0040985\ttotal: 3.66s\tremaining: 53.5s\n",
      "64:\tlearn: 0.0040130\ttotal: 3.72s\tremaining: 53.5s\n",
      "65:\tlearn: 0.0039193\ttotal: 3.77s\tremaining: 53.4s\n",
      "66:\tlearn: 0.0038646\ttotal: 3.83s\tremaining: 53.3s\n",
      "67:\tlearn: 0.0038638\ttotal: 3.89s\tremaining: 53.3s\n",
      "68:\tlearn: 0.0037626\ttotal: 3.95s\tremaining: 53.2s\n",
      "69:\tlearn: 0.0036737\ttotal: 4s\tremaining: 53.2s\n",
      "70:\tlearn: 0.0035822\ttotal: 4.06s\tremaining: 53.1s\n",
      "71:\tlearn: 0.0034423\ttotal: 4.12s\tremaining: 53.1s\n",
      "72:\tlearn: 0.0032924\ttotal: 4.18s\tremaining: 53.1s\n",
      "73:\tlearn: 0.0032453\ttotal: 4.24s\tremaining: 53s\n",
      "74:\tlearn: 0.0031405\ttotal: 4.29s\tremaining: 52.9s\n",
      "75:\tlearn: 0.0031215\ttotal: 4.35s\tremaining: 52.9s\n",
      "76:\tlearn: 0.0030692\ttotal: 4.41s\tremaining: 52.8s\n",
      "77:\tlearn: 0.0029608\ttotal: 4.46s\tremaining: 52.8s\n",
      "78:\tlearn: 0.0028729\ttotal: 4.52s\tremaining: 52.7s\n",
      "79:\tlearn: 0.0028337\ttotal: 4.58s\tremaining: 52.7s\n",
      "80:\tlearn: 0.0028115\ttotal: 4.64s\tremaining: 52.6s\n",
      "81:\tlearn: 0.0027649\ttotal: 4.69s\tremaining: 52.5s\n",
      "82:\tlearn: 0.0027647\ttotal: 4.75s\tremaining: 52.5s\n",
      "83:\tlearn: 0.0027291\ttotal: 4.81s\tremaining: 52.4s\n",
      "84:\tlearn: 0.0026607\ttotal: 4.86s\tremaining: 52.3s\n",
      "85:\tlearn: 0.0025797\ttotal: 4.92s\tremaining: 52.3s\n",
      "86:\tlearn: 0.0025294\ttotal: 4.98s\tremaining: 52.2s\n",
      "87:\tlearn: 0.0025294\ttotal: 5.03s\tremaining: 52.1s\n",
      "88:\tlearn: 0.0025294\ttotal: 5.09s\tremaining: 52.1s\n",
      "89:\tlearn: 0.0025293\ttotal: 5.14s\tremaining: 52s\n",
      "90:\tlearn: 0.0025286\ttotal: 5.2s\tremaining: 52s\n",
      "91:\tlearn: 0.0024806\ttotal: 5.26s\tremaining: 51.9s\n",
      "92:\tlearn: 0.0024805\ttotal: 5.32s\tremaining: 51.8s\n",
      "93:\tlearn: 0.0024586\ttotal: 5.37s\tremaining: 51.8s\n",
      "94:\tlearn: 0.0024440\ttotal: 5.43s\tremaining: 51.7s\n",
      "95:\tlearn: 0.0024182\ttotal: 5.49s\tremaining: 51.7s\n",
      "96:\tlearn: 0.0023644\ttotal: 5.54s\tremaining: 51.6s\n",
      "97:\tlearn: 0.0023640\ttotal: 5.6s\tremaining: 51.5s\n",
      "98:\tlearn: 0.0023467\ttotal: 5.66s\tremaining: 51.5s\n",
      "99:\tlearn: 0.0023083\ttotal: 5.71s\tremaining: 51.4s\n",
      "100:\tlearn: 0.0022806\ttotal: 5.77s\tremaining: 51.4s\n",
      "101:\tlearn: 0.0022805\ttotal: 5.83s\tremaining: 51.3s\n",
      "102:\tlearn: 0.0022713\ttotal: 5.89s\tremaining: 51.3s\n",
      "103:\tlearn: 0.0022711\ttotal: 5.94s\tremaining: 51.2s\n",
      "104:\tlearn: 0.0022350\ttotal: 6s\tremaining: 51.2s\n",
      "105:\tlearn: 0.0022239\ttotal: 6.06s\tremaining: 51.1s\n",
      "106:\tlearn: 0.0021928\ttotal: 6.12s\tremaining: 51s\n",
      "107:\tlearn: 0.0021381\ttotal: 6.18s\tremaining: 51s\n",
      "108:\tlearn: 0.0021003\ttotal: 6.24s\tremaining: 51s\n",
      "109:\tlearn: 0.0021003\ttotal: 6.29s\tremaining: 50.9s\n",
      "110:\tlearn: 0.0020348\ttotal: 6.35s\tremaining: 50.9s\n",
      "111:\tlearn: 0.0020254\ttotal: 6.41s\tremaining: 50.8s\n",
      "112:\tlearn: 0.0020253\ttotal: 6.46s\tremaining: 50.7s\n",
      "113:\tlearn: 0.0020251\ttotal: 6.52s\tremaining: 50.7s\n",
      "114:\tlearn: 0.0020251\ttotal: 6.58s\tremaining: 50.6s\n",
      "115:\tlearn: 0.0020106\ttotal: 6.63s\tremaining: 50.6s\n",
      "116:\tlearn: 0.0020102\ttotal: 6.69s\tremaining: 50.5s\n",
      "117:\tlearn: 0.0019921\ttotal: 6.75s\tremaining: 50.5s\n",
      "118:\tlearn: 0.0019316\ttotal: 6.81s\tremaining: 50.4s\n",
      "119:\tlearn: 0.0019312\ttotal: 6.87s\tremaining: 50.4s\n",
      "120:\tlearn: 0.0019303\ttotal: 6.92s\tremaining: 50.3s\n",
      "121:\tlearn: 0.0019127\ttotal: 6.98s\tremaining: 50.2s\n",
      "122:\tlearn: 0.0018832\ttotal: 7.04s\tremaining: 50.2s\n",
      "123:\tlearn: 0.0018826\ttotal: 7.09s\tremaining: 50.1s\n",
      "124:\tlearn: 0.0018823\ttotal: 7.15s\tremaining: 50.1s\n",
      "125:\tlearn: 0.0018823\ttotal: 7.21s\tremaining: 50s\n",
      "126:\tlearn: 0.0018808\ttotal: 7.27s\tremaining: 50s\n",
      "127:\tlearn: 0.0018685\ttotal: 7.32s\tremaining: 49.9s\n",
      "128:\tlearn: 0.0018556\ttotal: 7.38s\tremaining: 49.8s\n",
      "129:\tlearn: 0.0018400\ttotal: 7.44s\tremaining: 49.8s\n",
      "130:\tlearn: 0.0018399\ttotal: 7.5s\tremaining: 49.7s\n",
      "131:\tlearn: 0.0018395\ttotal: 7.55s\tremaining: 49.7s\n",
      "132:\tlearn: 0.0017835\ttotal: 7.61s\tremaining: 49.6s\n",
      "133:\tlearn: 0.0017834\ttotal: 7.67s\tremaining: 49.6s\n",
      "134:\tlearn: 0.0017489\ttotal: 7.73s\tremaining: 49.5s\n",
      "135:\tlearn: 0.0017489\ttotal: 7.79s\tremaining: 49.5s\n",
      "136:\tlearn: 0.0017488\ttotal: 7.84s\tremaining: 49.4s\n",
      "137:\tlearn: 0.0017463\ttotal: 7.9s\tremaining: 49.4s\n",
      "138:\tlearn: 0.0017374\ttotal: 7.96s\tremaining: 49.3s\n",
      "139:\tlearn: 0.0017374\ttotal: 8.02s\tremaining: 49.3s\n",
      "140:\tlearn: 0.0017372\ttotal: 8.07s\tremaining: 49.2s\n",
      "141:\tlearn: 0.0017371\ttotal: 8.13s\tremaining: 49.1s\n",
      "142:\tlearn: 0.0017237\ttotal: 8.19s\tremaining: 49.1s\n",
      "143:\tlearn: 0.0017226\ttotal: 8.25s\tremaining: 49s\n",
      "144:\tlearn: 0.0017226\ttotal: 8.3s\tremaining: 49s\n",
      "145:\tlearn: 0.0017073\ttotal: 8.36s\tremaining: 48.9s\n",
      "146:\tlearn: 0.0017070\ttotal: 8.41s\tremaining: 48.8s\n",
      "147:\tlearn: 0.0017055\ttotal: 8.47s\tremaining: 48.8s\n",
      "148:\tlearn: 0.0017053\ttotal: 8.53s\tremaining: 48.7s\n",
      "149:\tlearn: 0.0017051\ttotal: 8.59s\tremaining: 48.7s\n",
      "150:\tlearn: 0.0016961\ttotal: 8.64s\tremaining: 48.6s\n",
      "151:\tlearn: 0.0016828\ttotal: 8.7s\tremaining: 48.5s\n",
      "152:\tlearn: 0.0016761\ttotal: 8.76s\tremaining: 48.5s\n",
      "153:\tlearn: 0.0016760\ttotal: 8.81s\tremaining: 48.4s\n",
      "154:\tlearn: 0.0016759\ttotal: 8.87s\tremaining: 48.4s\n",
      "155:\tlearn: 0.0016696\ttotal: 8.93s\tremaining: 48.3s\n",
      "156:\tlearn: 0.0016566\ttotal: 8.98s\tremaining: 48.2s\n",
      "157:\tlearn: 0.0016562\ttotal: 9.04s\tremaining: 48.2s\n",
      "158:\tlearn: 0.0016562\ttotal: 9.1s\tremaining: 48.1s\n",
      "159:\tlearn: 0.0016424\ttotal: 9.15s\tremaining: 48.1s\n",
      "160:\tlearn: 0.0016421\ttotal: 9.21s\tremaining: 48s\n",
      "161:\tlearn: 0.0016421\ttotal: 9.27s\tremaining: 48s\n",
      "162:\tlearn: 0.0016206\ttotal: 9.33s\tremaining: 47.9s\n",
      "163:\tlearn: 0.0016193\ttotal: 9.38s\tremaining: 47.8s\n",
      "164:\tlearn: 0.0016183\ttotal: 9.44s\tremaining: 47.8s\n",
      "165:\tlearn: 0.0016183\ttotal: 9.5s\tremaining: 47.7s\n",
      "166:\tlearn: 0.0016183\ttotal: 9.55s\tremaining: 47.7s\n",
      "167:\tlearn: 0.0016044\ttotal: 9.61s\tremaining: 47.6s\n",
      "168:\tlearn: 0.0016041\ttotal: 9.67s\tremaining: 47.5s\n",
      "169:\tlearn: 0.0015996\ttotal: 9.73s\tremaining: 47.5s\n",
      "170:\tlearn: 0.0015969\ttotal: 9.78s\tremaining: 47.4s\n",
      "171:\tlearn: 0.0015968\ttotal: 9.84s\tremaining: 47.4s\n",
      "172:\tlearn: 0.0015966\ttotal: 9.9s\tremaining: 47.3s\n",
      "173:\tlearn: 0.0015965\ttotal: 9.96s\tremaining: 47.3s\n",
      "174:\tlearn: 0.0015954\ttotal: 10s\tremaining: 47.2s\n",
      "175:\tlearn: 0.0015954\ttotal: 10.1s\tremaining: 47.1s\n",
      "176:\tlearn: 0.0015953\ttotal: 10.1s\tremaining: 47.1s\n",
      "177:\tlearn: 0.0015952\ttotal: 10.2s\tremaining: 47s\n",
      "178:\tlearn: 0.0015696\ttotal: 10.2s\tremaining: 47s\n",
      "179:\tlearn: 0.0015692\ttotal: 10.3s\tremaining: 46.9s\n",
      "180:\tlearn: 0.0015692\ttotal: 10.4s\tremaining: 46.9s\n",
      "181:\tlearn: 0.0015690\ttotal: 10.4s\tremaining: 46.8s\n",
      "182:\tlearn: 0.0015689\ttotal: 10.5s\tremaining: 46.7s\n",
      "183:\tlearn: 0.0015689\ttotal: 10.5s\tremaining: 46.7s\n",
      "184:\tlearn: 0.0015688\ttotal: 10.6s\tremaining: 46.6s\n",
      "185:\tlearn: 0.0015686\ttotal: 10.6s\tremaining: 46.6s\n",
      "186:\tlearn: 0.0015685\ttotal: 10.7s\tremaining: 46.5s\n",
      "187:\tlearn: 0.0015501\ttotal: 10.8s\tremaining: 46.5s\n",
      "188:\tlearn: 0.0015501\ttotal: 10.8s\tremaining: 46.4s\n",
      "189:\tlearn: 0.0015495\ttotal: 10.9s\tremaining: 46.4s\n",
      "190:\tlearn: 0.0015495\ttotal: 10.9s\tremaining: 46.3s\n",
      "191:\tlearn: 0.0015494\ttotal: 11s\tremaining: 46.2s\n",
      "192:\tlearn: 0.0015494\ttotal: 11s\tremaining: 46.2s\n",
      "193:\tlearn: 0.0015492\ttotal: 11.1s\tremaining: 46.1s\n",
      "194:\tlearn: 0.0015489\ttotal: 11.2s\tremaining: 46.1s\n",
      "195:\tlearn: 0.0015487\ttotal: 11.2s\tremaining: 46s\n",
      "196:\tlearn: 0.0015486\ttotal: 11.3s\tremaining: 46s\n",
      "197:\tlearn: 0.0015485\ttotal: 11.3s\tremaining: 45.9s\n",
      "198:\tlearn: 0.0015469\ttotal: 11.4s\tremaining: 45.8s\n",
      "199:\tlearn: 0.0015468\ttotal: 11.4s\tremaining: 45.8s\n",
      "200:\tlearn: 0.0015402\ttotal: 11.5s\tremaining: 45.7s\n",
      "201:\tlearn: 0.0015400\ttotal: 11.6s\tremaining: 45.7s\n",
      "202:\tlearn: 0.0015400\ttotal: 11.6s\tremaining: 45.6s\n",
      "203:\tlearn: 0.0015399\ttotal: 11.7s\tremaining: 45.6s\n",
      "204:\tlearn: 0.0015398\ttotal: 11.7s\tremaining: 45.5s\n",
      "205:\tlearn: 0.0015395\ttotal: 11.8s\tremaining: 45.4s\n",
      "206:\tlearn: 0.0015394\ttotal: 11.8s\tremaining: 45.4s\n",
      "207:\tlearn: 0.0015394\ttotal: 11.9s\tremaining: 45.3s\n",
      "208:\tlearn: 0.0015393\ttotal: 12s\tremaining: 45.3s\n",
      "209:\tlearn: 0.0015393\ttotal: 12s\tremaining: 45.2s\n",
      "210:\tlearn: 0.0015247\ttotal: 12.1s\tremaining: 45.2s\n",
      "211:\tlearn: 0.0015166\ttotal: 12.1s\tremaining: 45.1s\n",
      "212:\tlearn: 0.0015131\ttotal: 12.2s\tremaining: 45.1s\n",
      "213:\tlearn: 0.0015131\ttotal: 12.3s\tremaining: 45s\n",
      "214:\tlearn: 0.0015130\ttotal: 12.3s\tremaining: 44.9s\n",
      "215:\tlearn: 0.0015125\ttotal: 12.4s\tremaining: 44.9s\n",
      "216:\tlearn: 0.0015119\ttotal: 12.4s\tremaining: 44.8s\n",
      "217:\tlearn: 0.0015118\ttotal: 12.5s\tremaining: 44.8s\n",
      "218:\tlearn: 0.0015118\ttotal: 12.5s\tremaining: 44.7s\n",
      "219:\tlearn: 0.0015117\ttotal: 12.6s\tremaining: 44.7s\n",
      "220:\tlearn: 0.0014974\ttotal: 12.7s\tremaining: 44.6s\n",
      "221:\tlearn: 0.0014841\ttotal: 12.7s\tremaining: 44.6s\n",
      "222:\tlearn: 0.0014837\ttotal: 12.8s\tremaining: 44.5s\n",
      "223:\tlearn: 0.0014793\ttotal: 12.8s\tremaining: 44.4s\n",
      "224:\tlearn: 0.0014780\ttotal: 12.9s\tremaining: 44.4s\n",
      "225:\tlearn: 0.0014711\ttotal: 12.9s\tremaining: 44.3s\n",
      "226:\tlearn: 0.0014710\ttotal: 13s\tremaining: 44.3s\n",
      "227:\tlearn: 0.0014710\ttotal: 13.1s\tremaining: 44.2s\n",
      "228:\tlearn: 0.0014710\ttotal: 13.1s\tremaining: 44.1s\n",
      "229:\tlearn: 0.0014704\ttotal: 13.2s\tremaining: 44.1s\n",
      "230:\tlearn: 0.0014704\ttotal: 13.2s\tremaining: 44s\n",
      "231:\tlearn: 0.0014599\ttotal: 13.3s\tremaining: 44s\n",
      "232:\tlearn: 0.0014599\ttotal: 13.3s\tremaining: 43.9s\n",
      "233:\tlearn: 0.0014595\ttotal: 13.4s\tremaining: 43.9s\n",
      "234:\tlearn: 0.0014591\ttotal: 13.5s\tremaining: 43.8s\n",
      "235:\tlearn: 0.0014590\ttotal: 13.5s\tremaining: 43.8s\n",
      "236:\tlearn: 0.0014590\ttotal: 13.6s\tremaining: 43.7s\n",
      "237:\tlearn: 0.0014590\ttotal: 13.6s\tremaining: 43.6s\n",
      "238:\tlearn: 0.0014581\ttotal: 13.7s\tremaining: 43.6s\n",
      "239:\tlearn: 0.0014581\ttotal: 13.7s\tremaining: 43.5s\n",
      "240:\tlearn: 0.0014537\ttotal: 13.8s\tremaining: 43.5s\n",
      "241:\tlearn: 0.0014537\ttotal: 13.9s\tremaining: 43.4s\n",
      "242:\tlearn: 0.0014535\ttotal: 13.9s\tremaining: 43.4s\n",
      "243:\tlearn: 0.0014535\ttotal: 14s\tremaining: 43.3s\n",
      "244:\tlearn: 0.0014534\ttotal: 14s\tremaining: 43.3s\n",
      "245:\tlearn: 0.0014530\ttotal: 14.1s\tremaining: 43.2s\n",
      "246:\tlearn: 0.0014529\ttotal: 14.2s\tremaining: 43.1s\n",
      "247:\tlearn: 0.0014526\ttotal: 14.2s\tremaining: 43.1s\n",
      "248:\tlearn: 0.0014525\ttotal: 14.3s\tremaining: 43s\n",
      "249:\tlearn: 0.0014523\ttotal: 14.3s\tremaining: 43s\n",
      "250:\tlearn: 0.0014523\ttotal: 14.4s\tremaining: 42.9s\n",
      "251:\tlearn: 0.0014522\ttotal: 14.4s\tremaining: 42.8s\n",
      "252:\tlearn: 0.0014520\ttotal: 14.5s\tremaining: 42.8s\n",
      "253:\tlearn: 0.0014516\ttotal: 14.5s\tremaining: 42.7s\n",
      "254:\tlearn: 0.0014515\ttotal: 14.6s\tremaining: 42.7s\n",
      "255:\tlearn: 0.0014448\ttotal: 14.7s\tremaining: 42.6s\n",
      "256:\tlearn: 0.0014244\ttotal: 14.7s\tremaining: 42.6s\n",
      "257:\tlearn: 0.0014243\ttotal: 14.8s\tremaining: 42.5s\n",
      "258:\tlearn: 0.0014243\ttotal: 14.8s\tremaining: 42.5s\n",
      "259:\tlearn: 0.0014237\ttotal: 14.9s\tremaining: 42.4s\n",
      "260:\tlearn: 0.0014237\ttotal: 15s\tremaining: 42.3s\n",
      "261:\tlearn: 0.0014237\ttotal: 15s\tremaining: 42.3s\n",
      "262:\tlearn: 0.0014213\ttotal: 15.1s\tremaining: 42.2s\n",
      "263:\tlearn: 0.0014211\ttotal: 15.1s\tremaining: 42.2s\n",
      "264:\tlearn: 0.0014211\ttotal: 15.2s\tremaining: 42.1s\n",
      "265:\tlearn: 0.0014211\ttotal: 15.2s\tremaining: 42.1s\n",
      "266:\tlearn: 0.0014210\ttotal: 15.3s\tremaining: 42s\n",
      "267:\tlearn: 0.0014210\ttotal: 15.4s\tremaining: 42s\n",
      "268:\tlearn: 0.0014198\ttotal: 15.4s\tremaining: 41.9s\n",
      "269:\tlearn: 0.0014198\ttotal: 15.5s\tremaining: 41.9s\n",
      "270:\tlearn: 0.0014198\ttotal: 15.5s\tremaining: 41.8s\n",
      "271:\tlearn: 0.0014136\ttotal: 15.6s\tremaining: 41.7s\n",
      "272:\tlearn: 0.0014133\ttotal: 15.7s\tremaining: 41.7s\n",
      "273:\tlearn: 0.0014122\ttotal: 15.7s\tremaining: 41.6s\n",
      "274:\tlearn: 0.0013984\ttotal: 15.8s\tremaining: 41.6s\n",
      "275:\tlearn: 0.0013984\ttotal: 15.8s\tremaining: 41.5s\n",
      "276:\tlearn: 0.0013984\ttotal: 15.9s\tremaining: 41.5s\n",
      "277:\tlearn: 0.0013983\ttotal: 15.9s\tremaining: 41.4s\n",
      "278:\tlearn: 0.0013981\ttotal: 16s\tremaining: 41.3s\n",
      "279:\tlearn: 0.0013979\ttotal: 16.1s\tremaining: 41.3s\n",
      "280:\tlearn: 0.0013902\ttotal: 16.1s\tremaining: 41.2s\n",
      "281:\tlearn: 0.0013901\ttotal: 16.2s\tremaining: 41.2s\n",
      "282:\tlearn: 0.0013900\ttotal: 16.2s\tremaining: 41.1s\n",
      "283:\tlearn: 0.0013898\ttotal: 16.3s\tremaining: 41.1s\n",
      "284:\tlearn: 0.0013897\ttotal: 16.3s\tremaining: 41s\n",
      "285:\tlearn: 0.0013896\ttotal: 16.4s\tremaining: 40.9s\n",
      "286:\tlearn: 0.0013895\ttotal: 16.5s\tremaining: 40.9s\n",
      "287:\tlearn: 0.0013892\ttotal: 16.5s\tremaining: 40.8s\n",
      "288:\tlearn: 0.0013891\ttotal: 16.6s\tremaining: 40.8s\n",
      "289:\tlearn: 0.0013888\ttotal: 16.6s\tremaining: 40.7s\n",
      "290:\tlearn: 0.0013887\ttotal: 16.7s\tremaining: 40.6s\n",
      "291:\tlearn: 0.0013886\ttotal: 16.7s\tremaining: 40.6s\n",
      "292:\tlearn: 0.0013885\ttotal: 16.8s\tremaining: 40.5s\n",
      "293:\tlearn: 0.0013861\ttotal: 16.9s\tremaining: 40.5s\n",
      "294:\tlearn: 0.0013816\ttotal: 16.9s\tremaining: 40.4s\n",
      "295:\tlearn: 0.0013720\ttotal: 17s\tremaining: 40.4s\n",
      "296:\tlearn: 0.0013717\ttotal: 17s\tremaining: 40.3s\n",
      "297:\tlearn: 0.0013715\ttotal: 17.1s\tremaining: 40.2s\n",
      "298:\tlearn: 0.0013714\ttotal: 17.2s\tremaining: 40.2s\n",
      "299:\tlearn: 0.0013713\ttotal: 17.2s\tremaining: 40.2s\n",
      "300:\tlearn: 0.0013712\ttotal: 17.3s\tremaining: 40.1s\n",
      "301:\tlearn: 0.0013662\ttotal: 17.3s\tremaining: 40s\n",
      "302:\tlearn: 0.0013661\ttotal: 17.4s\tremaining: 40s\n",
      "303:\tlearn: 0.0013658\ttotal: 17.4s\tremaining: 39.9s\n",
      "304:\tlearn: 0.0013657\ttotal: 17.5s\tremaining: 39.9s\n",
      "305:\tlearn: 0.0013656\ttotal: 17.6s\tremaining: 39.8s\n",
      "306:\tlearn: 0.0013652\ttotal: 17.6s\tremaining: 39.8s\n",
      "307:\tlearn: 0.0013650\ttotal: 17.7s\tremaining: 39.7s\n",
      "308:\tlearn: 0.0013649\ttotal: 17.7s\tremaining: 39.6s\n",
      "309:\tlearn: 0.0013647\ttotal: 17.8s\tremaining: 39.6s\n",
      "310:\tlearn: 0.0013644\ttotal: 17.8s\tremaining: 39.5s\n",
      "311:\tlearn: 0.0013644\ttotal: 17.9s\tremaining: 39.5s\n",
      "312:\tlearn: 0.0013404\ttotal: 18s\tremaining: 39.4s\n",
      "313:\tlearn: 0.0013403\ttotal: 18s\tremaining: 39.4s\n",
      "314:\tlearn: 0.0013401\ttotal: 18.1s\tremaining: 39.3s\n",
      "315:\tlearn: 0.0013398\ttotal: 18.1s\tremaining: 39.2s\n",
      "316:\tlearn: 0.0013391\ttotal: 18.2s\tremaining: 39.2s\n",
      "317:\tlearn: 0.0013388\ttotal: 18.2s\tremaining: 39.1s\n",
      "318:\tlearn: 0.0013388\ttotal: 18.3s\tremaining: 39.1s\n",
      "319:\tlearn: 0.0013388\ttotal: 18.4s\tremaining: 39s\n",
      "320:\tlearn: 0.0013386\ttotal: 18.4s\tremaining: 39s\n",
      "321:\tlearn: 0.0013383\ttotal: 18.5s\tremaining: 38.9s\n",
      "322:\tlearn: 0.0013377\ttotal: 18.5s\tremaining: 38.9s\n",
      "323:\tlearn: 0.0013377\ttotal: 18.6s\tremaining: 38.8s\n",
      "324:\tlearn: 0.0013376\ttotal: 18.6s\tremaining: 38.7s\n",
      "325:\tlearn: 0.0013375\ttotal: 18.7s\tremaining: 38.7s\n",
      "326:\tlearn: 0.0013375\ttotal: 18.8s\tremaining: 38.6s\n",
      "327:\tlearn: 0.0013370\ttotal: 18.8s\tremaining: 38.6s\n",
      "328:\tlearn: 0.0013369\ttotal: 18.9s\tremaining: 38.5s\n",
      "329:\tlearn: 0.0013369\ttotal: 18.9s\tremaining: 38.4s\n",
      "330:\tlearn: 0.0013368\ttotal: 19s\tremaining: 38.4s\n",
      "331:\tlearn: 0.0013368\ttotal: 19s\tremaining: 38.3s\n",
      "332:\tlearn: 0.0013367\ttotal: 19.1s\tremaining: 38.3s\n",
      "333:\tlearn: 0.0013364\ttotal: 19.2s\tremaining: 38.2s\n",
      "334:\tlearn: 0.0013355\ttotal: 19.2s\tremaining: 38.2s\n",
      "335:\tlearn: 0.0013355\ttotal: 19.3s\tremaining: 38.1s\n",
      "336:\tlearn: 0.0013345\ttotal: 19.3s\tremaining: 38s\n",
      "337:\tlearn: 0.0013344\ttotal: 19.4s\tremaining: 38s\n",
      "338:\tlearn: 0.0013342\ttotal: 19.4s\tremaining: 37.9s\n",
      "339:\tlearn: 0.0013341\ttotal: 19.5s\tremaining: 37.9s\n",
      "340:\tlearn: 0.0013327\ttotal: 19.6s\tremaining: 37.8s\n",
      "341:\tlearn: 0.0013326\ttotal: 19.6s\tremaining: 37.8s\n",
      "342:\tlearn: 0.0013326\ttotal: 19.7s\tremaining: 37.7s\n",
      "343:\tlearn: 0.0013326\ttotal: 19.7s\tremaining: 37.6s\n",
      "344:\tlearn: 0.0013326\ttotal: 19.8s\tremaining: 37.6s\n",
      "345:\tlearn: 0.0013325\ttotal: 19.9s\tremaining: 37.5s\n",
      "346:\tlearn: 0.0013324\ttotal: 19.9s\tremaining: 37.5s\n",
      "347:\tlearn: 0.0013324\ttotal: 20s\tremaining: 37.4s\n",
      "348:\tlearn: 0.0013320\ttotal: 20s\tremaining: 37.3s\n",
      "349:\tlearn: 0.0013315\ttotal: 20.1s\tremaining: 37.3s\n",
      "350:\tlearn: 0.0013315\ttotal: 20.1s\tremaining: 37.2s\n",
      "351:\tlearn: 0.0013314\ttotal: 20.2s\tremaining: 37.2s\n",
      "352:\tlearn: 0.0013310\ttotal: 20.2s\tremaining: 37.1s\n",
      "353:\tlearn: 0.0013309\ttotal: 20.3s\tremaining: 37.1s\n",
      "354:\tlearn: 0.0013306\ttotal: 20.4s\tremaining: 37s\n",
      "355:\tlearn: 0.0013305\ttotal: 20.4s\tremaining: 36.9s\n",
      "356:\tlearn: 0.0013305\ttotal: 20.5s\tremaining: 36.9s\n",
      "357:\tlearn: 0.0013305\ttotal: 20.5s\tremaining: 36.8s\n",
      "358:\tlearn: 0.0013251\ttotal: 20.6s\tremaining: 36.8s\n",
      "359:\tlearn: 0.0013251\ttotal: 20.7s\tremaining: 36.7s\n",
      "360:\tlearn: 0.0013248\ttotal: 20.7s\tremaining: 36.7s\n",
      "361:\tlearn: 0.0013239\ttotal: 20.8s\tremaining: 36.6s\n",
      "362:\tlearn: 0.0013239\ttotal: 20.8s\tremaining: 36.5s\n",
      "363:\tlearn: 0.0013239\ttotal: 20.9s\tremaining: 36.5s\n",
      "364:\tlearn: 0.0013239\ttotal: 20.9s\tremaining: 36.4s\n",
      "365:\tlearn: 0.0013238\ttotal: 21s\tremaining: 36.4s\n",
      "366:\tlearn: 0.0013237\ttotal: 21.1s\tremaining: 36.3s\n",
      "367:\tlearn: 0.0013217\ttotal: 21.1s\tremaining: 36.3s\n",
      "368:\tlearn: 0.0013217\ttotal: 21.2s\tremaining: 36.2s\n",
      "369:\tlearn: 0.0013216\ttotal: 21.2s\tremaining: 36.1s\n",
      "370:\tlearn: 0.0013208\ttotal: 21.3s\tremaining: 36.1s\n",
      "371:\tlearn: 0.0013207\ttotal: 21.3s\tremaining: 36s\n",
      "372:\tlearn: 0.0013204\ttotal: 21.4s\tremaining: 36s\n",
      "373:\tlearn: 0.0013204\ttotal: 21.4s\tremaining: 35.9s\n",
      "374:\tlearn: 0.0013198\ttotal: 21.5s\tremaining: 35.8s\n",
      "375:\tlearn: 0.0013198\ttotal: 21.6s\tremaining: 35.8s\n",
      "376:\tlearn: 0.0013194\ttotal: 21.6s\tremaining: 35.7s\n",
      "377:\tlearn: 0.0013193\ttotal: 21.7s\tremaining: 35.7s\n",
      "378:\tlearn: 0.0013193\ttotal: 21.7s\tremaining: 35.6s\n",
      "379:\tlearn: 0.0013167\ttotal: 21.8s\tremaining: 35.6s\n",
      "380:\tlearn: 0.0013164\ttotal: 21.8s\tremaining: 35.5s\n",
      "381:\tlearn: 0.0013163\ttotal: 21.9s\tremaining: 35.4s\n",
      "382:\tlearn: 0.0013147\ttotal: 22s\tremaining: 35.4s\n",
      "383:\tlearn: 0.0013145\ttotal: 22s\tremaining: 35.3s\n",
      "384:\tlearn: 0.0013141\ttotal: 22.1s\tremaining: 35.3s\n",
      "385:\tlearn: 0.0013141\ttotal: 22.1s\tremaining: 35.2s\n",
      "386:\tlearn: 0.0013141\ttotal: 22.2s\tremaining: 35.2s\n",
      "387:\tlearn: 0.0013140\ttotal: 22.3s\tremaining: 35.1s\n",
      "388:\tlearn: 0.0013127\ttotal: 22.3s\tremaining: 35.1s\n",
      "389:\tlearn: 0.0013127\ttotal: 22.4s\tremaining: 35s\n",
      "390:\tlearn: 0.0013123\ttotal: 22.4s\tremaining: 34.9s\n",
      "391:\tlearn: 0.0013123\ttotal: 22.5s\tremaining: 34.9s\n",
      "392:\tlearn: 0.0013123\ttotal: 22.5s\tremaining: 34.8s\n",
      "393:\tlearn: 0.0013091\ttotal: 22.6s\tremaining: 34.8s\n",
      "394:\tlearn: 0.0013091\ttotal: 22.7s\tremaining: 34.7s\n",
      "395:\tlearn: 0.0013091\ttotal: 22.7s\tremaining: 34.6s\n",
      "396:\tlearn: 0.0013089\ttotal: 22.8s\tremaining: 34.6s\n",
      "397:\tlearn: 0.0013085\ttotal: 22.8s\tremaining: 34.5s\n",
      "398:\tlearn: 0.0013083\ttotal: 22.9s\tremaining: 34.5s\n",
      "399:\tlearn: 0.0013082\ttotal: 22.9s\tremaining: 34.4s\n",
      "400:\tlearn: 0.0013081\ttotal: 23s\tremaining: 34.4s\n",
      "401:\tlearn: 0.0013076\ttotal: 23.1s\tremaining: 34.3s\n",
      "402:\tlearn: 0.0013075\ttotal: 23.1s\tremaining: 34.2s\n",
      "403:\tlearn: 0.0013075\ttotal: 23.2s\tremaining: 34.2s\n",
      "404:\tlearn: 0.0013074\ttotal: 23.2s\tremaining: 34.1s\n",
      "405:\tlearn: 0.0013074\ttotal: 23.3s\tremaining: 34.1s\n",
      "406:\tlearn: 0.0013068\ttotal: 23.3s\tremaining: 34s\n",
      "407:\tlearn: 0.0013067\ttotal: 23.4s\tremaining: 34s\n",
      "408:\tlearn: 0.0013066\ttotal: 23.5s\tremaining: 33.9s\n",
      "409:\tlearn: 0.0013060\ttotal: 23.5s\tremaining: 33.8s\n",
      "410:\tlearn: 0.0013058\ttotal: 23.6s\tremaining: 33.8s\n",
      "411:\tlearn: 0.0013057\ttotal: 23.6s\tremaining: 33.7s\n",
      "412:\tlearn: 0.0013046\ttotal: 23.7s\tremaining: 33.7s\n",
      "413:\tlearn: 0.0013046\ttotal: 23.7s\tremaining: 33.6s\n",
      "414:\tlearn: 0.0013041\ttotal: 23.8s\tremaining: 33.5s\n",
      "415:\tlearn: 0.0013039\ttotal: 23.9s\tremaining: 33.5s\n",
      "416:\tlearn: 0.0013038\ttotal: 23.9s\tremaining: 33.4s\n",
      "417:\tlearn: 0.0013038\ttotal: 24s\tremaining: 33.4s\n",
      "418:\tlearn: 0.0013036\ttotal: 24s\tremaining: 33.3s\n",
      "419:\tlearn: 0.0013029\ttotal: 24.1s\tremaining: 33.3s\n",
      "420:\tlearn: 0.0013027\ttotal: 24.1s\tremaining: 33.2s\n",
      "421:\tlearn: 0.0013026\ttotal: 24.2s\tremaining: 33.1s\n",
      "422:\tlearn: 0.0013026\ttotal: 24.3s\tremaining: 33.1s\n",
      "423:\tlearn: 0.0013026\ttotal: 24.3s\tremaining: 33s\n",
      "424:\tlearn: 0.0013023\ttotal: 24.4s\tremaining: 33s\n",
      "425:\tlearn: 0.0013023\ttotal: 24.4s\tremaining: 32.9s\n",
      "426:\tlearn: 0.0013021\ttotal: 24.5s\tremaining: 32.9s\n",
      "427:\tlearn: 0.0013018\ttotal: 24.5s\tremaining: 32.8s\n",
      "428:\tlearn: 0.0013017\ttotal: 24.6s\tremaining: 32.7s\n",
      "429:\tlearn: 0.0013016\ttotal: 24.7s\tremaining: 32.7s\n",
      "430:\tlearn: 0.0013015\ttotal: 24.7s\tremaining: 32.6s\n",
      "431:\tlearn: 0.0013014\ttotal: 24.8s\tremaining: 32.6s\n",
      "432:\tlearn: 0.0013013\ttotal: 24.8s\tremaining: 32.5s\n",
      "433:\tlearn: 0.0013012\ttotal: 24.9s\tremaining: 32.5s\n",
      "434:\tlearn: 0.0013012\ttotal: 24.9s\tremaining: 32.4s\n",
      "435:\tlearn: 0.0013012\ttotal: 25s\tremaining: 32.3s\n",
      "436:\tlearn: 0.0013011\ttotal: 25.1s\tremaining: 32.3s\n",
      "437:\tlearn: 0.0013007\ttotal: 25.1s\tremaining: 32.2s\n",
      "438:\tlearn: 0.0013006\ttotal: 25.2s\tremaining: 32.2s\n",
      "439:\tlearn: 0.0013006\ttotal: 25.2s\tremaining: 32.1s\n",
      "440:\tlearn: 0.0013005\ttotal: 25.3s\tremaining: 32.1s\n",
      "441:\tlearn: 0.0013000\ttotal: 25.3s\tremaining: 32s\n",
      "442:\tlearn: 0.0012998\ttotal: 25.4s\tremaining: 31.9s\n",
      "443:\tlearn: 0.0012996\ttotal: 25.5s\tremaining: 31.9s\n",
      "444:\tlearn: 0.0012993\ttotal: 25.5s\tremaining: 31.8s\n",
      "445:\tlearn: 0.0012993\ttotal: 25.6s\tremaining: 31.8s\n",
      "446:\tlearn: 0.0012992\ttotal: 25.6s\tremaining: 31.7s\n",
      "447:\tlearn: 0.0012989\ttotal: 25.7s\tremaining: 31.7s\n",
      "448:\tlearn: 0.0012985\ttotal: 25.7s\tremaining: 31.6s\n",
      "449:\tlearn: 0.0012965\ttotal: 25.8s\tremaining: 31.5s\n",
      "450:\tlearn: 0.0012965\ttotal: 25.9s\tremaining: 31.5s\n",
      "451:\tlearn: 0.0012964\ttotal: 25.9s\tremaining: 31.4s\n",
      "452:\tlearn: 0.0012964\ttotal: 26s\tremaining: 31.4s\n",
      "453:\tlearn: 0.0012914\ttotal: 26s\tremaining: 31.3s\n",
      "454:\tlearn: 0.0012912\ttotal: 26.1s\tremaining: 31.3s\n",
      "455:\tlearn: 0.0012911\ttotal: 26.1s\tremaining: 31.2s\n",
      "456:\tlearn: 0.0012882\ttotal: 26.2s\tremaining: 31.1s\n",
      "457:\tlearn: 0.0012882\ttotal: 26.3s\tremaining: 31.1s\n",
      "458:\tlearn: 0.0012881\ttotal: 26.3s\tremaining: 31s\n",
      "459:\tlearn: 0.0012846\ttotal: 26.4s\tremaining: 31s\n",
      "460:\tlearn: 0.0012846\ttotal: 26.4s\tremaining: 30.9s\n",
      "461:\tlearn: 0.0012845\ttotal: 26.5s\tremaining: 30.8s\n",
      "462:\tlearn: 0.0012843\ttotal: 26.5s\tremaining: 30.8s\n",
      "463:\tlearn: 0.0012789\ttotal: 26.6s\tremaining: 30.7s\n",
      "464:\tlearn: 0.0012700\ttotal: 26.7s\tremaining: 30.7s\n",
      "465:\tlearn: 0.0012700\ttotal: 26.7s\tremaining: 30.6s\n",
      "466:\tlearn: 0.0012699\ttotal: 26.8s\tremaining: 30.6s\n",
      "467:\tlearn: 0.0012699\ttotal: 26.8s\tremaining: 30.5s\n",
      "468:\tlearn: 0.0012699\ttotal: 26.9s\tremaining: 30.4s\n",
      "469:\tlearn: 0.0012699\ttotal: 27s\tremaining: 30.4s\n",
      "470:\tlearn: 0.0012698\ttotal: 27s\tremaining: 30.3s\n",
      "471:\tlearn: 0.0012697\ttotal: 27.1s\tremaining: 30.3s\n",
      "472:\tlearn: 0.0012697\ttotal: 27.1s\tremaining: 30.2s\n",
      "473:\tlearn: 0.0012697\ttotal: 27.2s\tremaining: 30.2s\n",
      "474:\tlearn: 0.0012694\ttotal: 27.2s\tremaining: 30.1s\n",
      "475:\tlearn: 0.0012692\ttotal: 27.3s\tremaining: 30s\n",
      "476:\tlearn: 0.0012692\ttotal: 27.4s\tremaining: 30s\n",
      "477:\tlearn: 0.0012691\ttotal: 27.4s\tremaining: 29.9s\n",
      "478:\tlearn: 0.0012690\ttotal: 27.5s\tremaining: 29.9s\n",
      "479:\tlearn: 0.0012689\ttotal: 27.5s\tremaining: 29.8s\n",
      "480:\tlearn: 0.0012689\ttotal: 27.6s\tremaining: 29.8s\n",
      "481:\tlearn: 0.0012689\ttotal: 27.6s\tremaining: 29.7s\n",
      "482:\tlearn: 0.0012686\ttotal: 27.7s\tremaining: 29.6s\n",
      "483:\tlearn: 0.0012675\ttotal: 27.8s\tremaining: 29.6s\n",
      "484:\tlearn: 0.0012674\ttotal: 27.8s\tremaining: 29.5s\n",
      "485:\tlearn: 0.0012673\ttotal: 27.9s\tremaining: 29.5s\n",
      "486:\tlearn: 0.0012673\ttotal: 27.9s\tremaining: 29.4s\n",
      "487:\tlearn: 0.0012672\ttotal: 28s\tremaining: 29.4s\n",
      "488:\tlearn: 0.0012671\ttotal: 28s\tremaining: 29.3s\n",
      "489:\tlearn: 0.0012670\ttotal: 28.1s\tremaining: 29.3s\n",
      "490:\tlearn: 0.0012670\ttotal: 28.2s\tremaining: 29.2s\n",
      "491:\tlearn: 0.0012670\ttotal: 28.2s\tremaining: 29.1s\n",
      "492:\tlearn: 0.0012670\ttotal: 28.3s\tremaining: 29.1s\n",
      "493:\tlearn: 0.0012669\ttotal: 28.3s\tremaining: 29s\n",
      "494:\tlearn: 0.0012665\ttotal: 28.4s\tremaining: 29s\n",
      "495:\tlearn: 0.0012538\ttotal: 28.4s\tremaining: 28.9s\n",
      "496:\tlearn: 0.0012524\ttotal: 28.5s\tremaining: 28.8s\n",
      "497:\tlearn: 0.0012520\ttotal: 28.6s\tremaining: 28.8s\n",
      "498:\tlearn: 0.0012514\ttotal: 28.6s\tremaining: 28.7s\n",
      "499:\tlearn: 0.0012434\ttotal: 28.7s\tremaining: 28.7s\n",
      "500:\tlearn: 0.0012433\ttotal: 28.7s\tremaining: 28.6s\n",
      "501:\tlearn: 0.0012432\ttotal: 28.8s\tremaining: 28.6s\n",
      "502:\tlearn: 0.0012432\ttotal: 28.8s\tremaining: 28.5s\n",
      "503:\tlearn: 0.0012431\ttotal: 28.9s\tremaining: 28.4s\n",
      "504:\tlearn: 0.0012431\ttotal: 29s\tremaining: 28.4s\n",
      "505:\tlearn: 0.0012431\ttotal: 29s\tremaining: 28.3s\n",
      "506:\tlearn: 0.0012431\ttotal: 29.1s\tremaining: 28.3s\n",
      "507:\tlearn: 0.0012431\ttotal: 29.1s\tremaining: 28.2s\n",
      "508:\tlearn: 0.0012430\ttotal: 29.2s\tremaining: 28.1s\n",
      "509:\tlearn: 0.0012430\ttotal: 29.2s\tremaining: 28.1s\n",
      "510:\tlearn: 0.0012428\ttotal: 29.3s\tremaining: 28s\n",
      "511:\tlearn: 0.0012428\ttotal: 29.4s\tremaining: 28s\n",
      "512:\tlearn: 0.0012425\ttotal: 29.4s\tremaining: 27.9s\n",
      "513:\tlearn: 0.0012425\ttotal: 29.5s\tremaining: 27.9s\n",
      "514:\tlearn: 0.0012422\ttotal: 29.5s\tremaining: 27.8s\n",
      "515:\tlearn: 0.0012422\ttotal: 29.6s\tremaining: 27.8s\n",
      "516:\tlearn: 0.0012421\ttotal: 29.6s\tremaining: 27.7s\n",
      "517:\tlearn: 0.0012420\ttotal: 29.7s\tremaining: 27.6s\n",
      "518:\tlearn: 0.0012420\ttotal: 29.8s\tremaining: 27.6s\n",
      "519:\tlearn: 0.0012420\ttotal: 29.8s\tremaining: 27.5s\n",
      "520:\tlearn: 0.0012416\ttotal: 29.9s\tremaining: 27.5s\n",
      "521:\tlearn: 0.0012416\ttotal: 29.9s\tremaining: 27.4s\n",
      "522:\tlearn: 0.0012415\ttotal: 30s\tremaining: 27.4s\n",
      "523:\tlearn: 0.0012412\ttotal: 30s\tremaining: 27.3s\n",
      "524:\tlearn: 0.0012410\ttotal: 30.1s\tremaining: 27.2s\n",
      "525:\tlearn: 0.0012410\ttotal: 30.2s\tremaining: 27.2s\n",
      "526:\tlearn: 0.0012410\ttotal: 30.2s\tremaining: 27.1s\n",
      "527:\tlearn: 0.0012407\ttotal: 30.3s\tremaining: 27.1s\n",
      "528:\tlearn: 0.0012407\ttotal: 30.3s\tremaining: 27s\n",
      "529:\tlearn: 0.0012407\ttotal: 30.4s\tremaining: 27s\n",
      "530:\tlearn: 0.0012405\ttotal: 30.5s\tremaining: 26.9s\n",
      "531:\tlearn: 0.0012362\ttotal: 30.5s\tremaining: 26.8s\n",
      "532:\tlearn: 0.0012357\ttotal: 30.6s\tremaining: 26.8s\n",
      "533:\tlearn: 0.0012356\ttotal: 30.6s\tremaining: 26.7s\n",
      "534:\tlearn: 0.0012355\ttotal: 30.7s\tremaining: 26.7s\n",
      "535:\tlearn: 0.0012350\ttotal: 30.7s\tremaining: 26.6s\n",
      "536:\tlearn: 0.0012350\ttotal: 30.8s\tremaining: 26.5s\n",
      "537:\tlearn: 0.0012349\ttotal: 30.8s\tremaining: 26.5s\n",
      "538:\tlearn: 0.0012347\ttotal: 30.9s\tremaining: 26.4s\n",
      "539:\tlearn: 0.0012346\ttotal: 31s\tremaining: 26.4s\n",
      "540:\tlearn: 0.0012342\ttotal: 31s\tremaining: 26.3s\n",
      "541:\tlearn: 0.0012341\ttotal: 31.1s\tremaining: 26.3s\n",
      "542:\tlearn: 0.0012339\ttotal: 31.1s\tremaining: 26.2s\n",
      "543:\tlearn: 0.0012338\ttotal: 31.2s\tremaining: 26.1s\n",
      "544:\tlearn: 0.0012338\ttotal: 31.2s\tremaining: 26.1s\n",
      "545:\tlearn: 0.0012338\ttotal: 31.3s\tremaining: 26s\n",
      "546:\tlearn: 0.0012312\ttotal: 31.4s\tremaining: 26s\n",
      "547:\tlearn: 0.0012311\ttotal: 31.4s\tremaining: 25.9s\n",
      "548:\tlearn: 0.0012311\ttotal: 31.5s\tremaining: 25.9s\n",
      "549:\tlearn: 0.0012310\ttotal: 31.5s\tremaining: 25.8s\n",
      "550:\tlearn: 0.0012310\ttotal: 31.6s\tremaining: 25.7s\n",
      "551:\tlearn: 0.0012308\ttotal: 31.6s\tremaining: 25.7s\n",
      "552:\tlearn: 0.0012304\ttotal: 31.7s\tremaining: 25.6s\n",
      "553:\tlearn: 0.0012303\ttotal: 31.8s\tremaining: 25.6s\n",
      "554:\tlearn: 0.0012301\ttotal: 31.8s\tremaining: 25.5s\n",
      "555:\tlearn: 0.0012301\ttotal: 31.9s\tremaining: 25.5s\n",
      "556:\tlearn: 0.0012281\ttotal: 31.9s\tremaining: 25.4s\n",
      "557:\tlearn: 0.0012281\ttotal: 32s\tremaining: 25.3s\n",
      "558:\tlearn: 0.0012280\ttotal: 32s\tremaining: 25.3s\n",
      "559:\tlearn: 0.0012280\ttotal: 32.1s\tremaining: 25.2s\n",
      "560:\tlearn: 0.0012277\ttotal: 32.2s\tremaining: 25.2s\n",
      "561:\tlearn: 0.0012277\ttotal: 32.2s\tremaining: 25.1s\n",
      "562:\tlearn: 0.0012273\ttotal: 32.3s\tremaining: 25.1s\n",
      "563:\tlearn: 0.0012273\ttotal: 32.3s\tremaining: 25s\n",
      "564:\tlearn: 0.0012257\ttotal: 32.4s\tremaining: 24.9s\n",
      "565:\tlearn: 0.0012254\ttotal: 32.4s\tremaining: 24.9s\n",
      "566:\tlearn: 0.0012253\ttotal: 32.5s\tremaining: 24.8s\n",
      "567:\tlearn: 0.0012253\ttotal: 32.6s\tremaining: 24.8s\n",
      "568:\tlearn: 0.0012253\ttotal: 32.6s\tremaining: 24.7s\n",
      "569:\tlearn: 0.0012251\ttotal: 32.7s\tremaining: 24.7s\n",
      "570:\tlearn: 0.0012251\ttotal: 32.7s\tremaining: 24.6s\n",
      "571:\tlearn: 0.0012251\ttotal: 32.8s\tremaining: 24.5s\n",
      "572:\tlearn: 0.0012251\ttotal: 32.8s\tremaining: 24.5s\n",
      "573:\tlearn: 0.0012251\ttotal: 32.9s\tremaining: 24.4s\n",
      "574:\tlearn: 0.0012251\ttotal: 33s\tremaining: 24.4s\n",
      "575:\tlearn: 0.0012247\ttotal: 33s\tremaining: 24.3s\n",
      "576:\tlearn: 0.0012245\ttotal: 33.1s\tremaining: 24.3s\n",
      "577:\tlearn: 0.0012245\ttotal: 33.1s\tremaining: 24.2s\n",
      "578:\tlearn: 0.0012244\ttotal: 33.2s\tremaining: 24.1s\n",
      "579:\tlearn: 0.0012243\ttotal: 33.3s\tremaining: 24.1s\n",
      "580:\tlearn: 0.0012242\ttotal: 33.3s\tremaining: 24s\n",
      "581:\tlearn: 0.0012242\ttotal: 33.4s\tremaining: 24s\n",
      "582:\tlearn: 0.0012241\ttotal: 33.4s\tremaining: 23.9s\n",
      "583:\tlearn: 0.0012237\ttotal: 33.5s\tremaining: 23.9s\n",
      "584:\tlearn: 0.0012236\ttotal: 33.5s\tremaining: 23.8s\n",
      "585:\tlearn: 0.0012236\ttotal: 33.6s\tremaining: 23.7s\n",
      "586:\tlearn: 0.0012236\ttotal: 33.7s\tremaining: 23.7s\n",
      "587:\tlearn: 0.0012234\ttotal: 33.7s\tremaining: 23.6s\n",
      "588:\tlearn: 0.0012226\ttotal: 33.8s\tremaining: 23.6s\n",
      "589:\tlearn: 0.0012225\ttotal: 33.8s\tremaining: 23.5s\n",
      "590:\tlearn: 0.0012221\ttotal: 33.9s\tremaining: 23.5s\n",
      "591:\tlearn: 0.0012208\ttotal: 33.9s\tremaining: 23.4s\n",
      "592:\tlearn: 0.0012208\ttotal: 34s\tremaining: 23.3s\n",
      "593:\tlearn: 0.0012204\ttotal: 34.1s\tremaining: 23.3s\n",
      "594:\tlearn: 0.0012201\ttotal: 34.1s\tremaining: 23.2s\n",
      "595:\tlearn: 0.0012196\ttotal: 34.2s\tremaining: 23.2s\n",
      "596:\tlearn: 0.0012193\ttotal: 34.2s\tremaining: 23.1s\n",
      "597:\tlearn: 0.0012191\ttotal: 34.3s\tremaining: 23.1s\n",
      "598:\tlearn: 0.0012187\ttotal: 34.4s\tremaining: 23s\n",
      "599:\tlearn: 0.0012187\ttotal: 34.4s\tremaining: 22.9s\n",
      "600:\tlearn: 0.0012186\ttotal: 34.5s\tremaining: 22.9s\n",
      "601:\tlearn: 0.0012175\ttotal: 34.5s\tremaining: 22.8s\n",
      "602:\tlearn: 0.0012173\ttotal: 34.6s\tremaining: 22.8s\n",
      "603:\tlearn: 0.0012171\ttotal: 34.6s\tremaining: 22.7s\n",
      "604:\tlearn: 0.0012171\ttotal: 34.7s\tremaining: 22.7s\n",
      "605:\tlearn: 0.0012171\ttotal: 34.8s\tremaining: 22.6s\n",
      "606:\tlearn: 0.0012171\ttotal: 34.8s\tremaining: 22.5s\n",
      "607:\tlearn: 0.0012169\ttotal: 34.9s\tremaining: 22.5s\n",
      "608:\tlearn: 0.0012169\ttotal: 34.9s\tremaining: 22.4s\n",
      "609:\tlearn: 0.0012169\ttotal: 35s\tremaining: 22.4s\n",
      "610:\tlearn: 0.0012169\ttotal: 35s\tremaining: 22.3s\n",
      "611:\tlearn: 0.0012169\ttotal: 35.1s\tremaining: 22.3s\n",
      "612:\tlearn: 0.0012169\ttotal: 35.2s\tremaining: 22.2s\n",
      "613:\tlearn: 0.0012167\ttotal: 35.2s\tremaining: 22.1s\n",
      "614:\tlearn: 0.0012166\ttotal: 35.3s\tremaining: 22.1s\n",
      "615:\tlearn: 0.0012163\ttotal: 35.3s\tremaining: 22s\n",
      "616:\tlearn: 0.0012161\ttotal: 35.4s\tremaining: 22s\n",
      "617:\tlearn: 0.0012159\ttotal: 35.4s\tremaining: 21.9s\n",
      "618:\tlearn: 0.0012159\ttotal: 35.5s\tremaining: 21.8s\n",
      "619:\tlearn: 0.0012153\ttotal: 35.6s\tremaining: 21.8s\n",
      "620:\tlearn: 0.0012153\ttotal: 35.6s\tremaining: 21.7s\n",
      "621:\tlearn: 0.0012150\ttotal: 35.7s\tremaining: 21.7s\n",
      "622:\tlearn: 0.0012125\ttotal: 35.7s\tremaining: 21.6s\n",
      "623:\tlearn: 0.0012125\ttotal: 35.8s\tremaining: 21.6s\n",
      "624:\tlearn: 0.0012125\ttotal: 35.8s\tremaining: 21.5s\n",
      "625:\tlearn: 0.0012124\ttotal: 35.9s\tremaining: 21.4s\n",
      "626:\tlearn: 0.0012123\ttotal: 36s\tremaining: 21.4s\n",
      "627:\tlearn: 0.0012122\ttotal: 36s\tremaining: 21.3s\n",
      "628:\tlearn: 0.0012121\ttotal: 36.1s\tremaining: 21.3s\n",
      "629:\tlearn: 0.0012121\ttotal: 36.1s\tremaining: 21.2s\n",
      "630:\tlearn: 0.0012119\ttotal: 36.2s\tremaining: 21.2s\n",
      "631:\tlearn: 0.0012115\ttotal: 36.2s\tremaining: 21.1s\n",
      "632:\tlearn: 0.0012115\ttotal: 36.3s\tremaining: 21s\n",
      "633:\tlearn: 0.0012112\ttotal: 36.4s\tremaining: 21s\n",
      "634:\tlearn: 0.0012112\ttotal: 36.4s\tremaining: 20.9s\n",
      "635:\tlearn: 0.0012111\ttotal: 36.5s\tremaining: 20.9s\n",
      "636:\tlearn: 0.0012110\ttotal: 36.5s\tremaining: 20.8s\n",
      "637:\tlearn: 0.0012109\ttotal: 36.6s\tremaining: 20.8s\n",
      "638:\tlearn: 0.0012109\ttotal: 36.6s\tremaining: 20.7s\n",
      "639:\tlearn: 0.0012109\ttotal: 36.7s\tremaining: 20.6s\n",
      "640:\tlearn: 0.0012109\ttotal: 36.8s\tremaining: 20.6s\n",
      "641:\tlearn: 0.0012108\ttotal: 36.8s\tremaining: 20.5s\n",
      "642:\tlearn: 0.0012105\ttotal: 36.9s\tremaining: 20.5s\n",
      "643:\tlearn: 0.0012103\ttotal: 36.9s\tremaining: 20.4s\n",
      "644:\tlearn: 0.0012102\ttotal: 37s\tremaining: 20.4s\n",
      "645:\tlearn: 0.0012094\ttotal: 37s\tremaining: 20.3s\n",
      "646:\tlearn: 0.0012093\ttotal: 37.1s\tremaining: 20.2s\n",
      "647:\tlearn: 0.0012091\ttotal: 37.2s\tremaining: 20.2s\n",
      "648:\tlearn: 0.0012091\ttotal: 37.2s\tremaining: 20.1s\n",
      "649:\tlearn: 0.0012090\ttotal: 37.3s\tremaining: 20.1s\n",
      "650:\tlearn: 0.0012090\ttotal: 37.3s\tremaining: 20s\n",
      "651:\tlearn: 0.0012088\ttotal: 37.4s\tremaining: 20s\n",
      "652:\tlearn: 0.0012088\ttotal: 37.4s\tremaining: 19.9s\n",
      "653:\tlearn: 0.0012034\ttotal: 37.5s\tremaining: 19.8s\n",
      "654:\tlearn: 0.0012032\ttotal: 37.6s\tremaining: 19.8s\n",
      "655:\tlearn: 0.0012032\ttotal: 37.6s\tremaining: 19.7s\n",
      "656:\tlearn: 0.0012030\ttotal: 37.7s\tremaining: 19.7s\n",
      "657:\tlearn: 0.0012029\ttotal: 37.7s\tremaining: 19.6s\n",
      "658:\tlearn: 0.0012028\ttotal: 37.8s\tremaining: 19.5s\n",
      "659:\tlearn: 0.0012028\ttotal: 37.8s\tremaining: 19.5s\n",
      "660:\tlearn: 0.0012026\ttotal: 37.9s\tremaining: 19.4s\n",
      "661:\tlearn: 0.0012023\ttotal: 38s\tremaining: 19.4s\n",
      "662:\tlearn: 0.0012023\ttotal: 38s\tremaining: 19.3s\n",
      "663:\tlearn: 0.0012020\ttotal: 38.1s\tremaining: 19.3s\n",
      "664:\tlearn: 0.0012019\ttotal: 38.1s\tremaining: 19.2s\n",
      "665:\tlearn: 0.0012017\ttotal: 38.2s\tremaining: 19.1s\n",
      "666:\tlearn: 0.0012016\ttotal: 38.2s\tremaining: 19.1s\n",
      "667:\tlearn: 0.0012015\ttotal: 38.3s\tremaining: 19s\n",
      "668:\tlearn: 0.0011944\ttotal: 38.4s\tremaining: 19s\n",
      "669:\tlearn: 0.0011942\ttotal: 38.4s\tremaining: 18.9s\n",
      "670:\tlearn: 0.0011942\ttotal: 38.5s\tremaining: 18.9s\n",
      "671:\tlearn: 0.0011942\ttotal: 38.5s\tremaining: 18.8s\n",
      "672:\tlearn: 0.0011938\ttotal: 38.6s\tremaining: 18.7s\n",
      "673:\tlearn: 0.0011929\ttotal: 38.6s\tremaining: 18.7s\n",
      "674:\tlearn: 0.0011929\ttotal: 38.7s\tremaining: 18.6s\n",
      "675:\tlearn: 0.0011929\ttotal: 38.8s\tremaining: 18.6s\n",
      "676:\tlearn: 0.0011928\ttotal: 38.8s\tremaining: 18.5s\n",
      "677:\tlearn: 0.0011927\ttotal: 38.9s\tremaining: 18.5s\n",
      "678:\tlearn: 0.0011923\ttotal: 38.9s\tremaining: 18.4s\n",
      "679:\tlearn: 0.0011923\ttotal: 39s\tremaining: 18.3s\n",
      "680:\tlearn: 0.0011921\ttotal: 39s\tremaining: 18.3s\n",
      "681:\tlearn: 0.0011921\ttotal: 39.1s\tremaining: 18.2s\n",
      "682:\tlearn: 0.0011921\ttotal: 39.2s\tremaining: 18.2s\n",
      "683:\tlearn: 0.0011921\ttotal: 39.2s\tremaining: 18.1s\n",
      "684:\tlearn: 0.0011920\ttotal: 39.3s\tremaining: 18.1s\n",
      "685:\tlearn: 0.0011920\ttotal: 39.3s\tremaining: 18s\n",
      "686:\tlearn: 0.0011918\ttotal: 39.4s\tremaining: 17.9s\n",
      "687:\tlearn: 0.0011917\ttotal: 39.4s\tremaining: 17.9s\n",
      "688:\tlearn: 0.0011914\ttotal: 39.5s\tremaining: 17.8s\n",
      "689:\tlearn: 0.0011909\ttotal: 39.6s\tremaining: 17.8s\n",
      "690:\tlearn: 0.0011908\ttotal: 39.6s\tremaining: 17.7s\n",
      "691:\tlearn: 0.0011907\ttotal: 39.7s\tremaining: 17.7s\n",
      "692:\tlearn: 0.0011907\ttotal: 39.7s\tremaining: 17.6s\n",
      "693:\tlearn: 0.0011905\ttotal: 39.8s\tremaining: 17.5s\n",
      "694:\tlearn: 0.0011904\ttotal: 39.8s\tremaining: 17.5s\n",
      "695:\tlearn: 0.0011904\ttotal: 39.9s\tremaining: 17.4s\n",
      "696:\tlearn: 0.0011903\ttotal: 40s\tremaining: 17.4s\n",
      "697:\tlearn: 0.0011902\ttotal: 40s\tremaining: 17.3s\n",
      "698:\tlearn: 0.0011900\ttotal: 40.1s\tremaining: 17.3s\n",
      "699:\tlearn: 0.0011898\ttotal: 40.1s\tremaining: 17.2s\n",
      "700:\tlearn: 0.0011897\ttotal: 40.2s\tremaining: 17.1s\n",
      "701:\tlearn: 0.0011892\ttotal: 40.2s\tremaining: 17.1s\n",
      "702:\tlearn: 0.0011892\ttotal: 40.3s\tremaining: 17s\n",
      "703:\tlearn: 0.0011891\ttotal: 40.4s\tremaining: 17s\n",
      "704:\tlearn: 0.0011891\ttotal: 40.4s\tremaining: 16.9s\n",
      "705:\tlearn: 0.0011889\ttotal: 40.5s\tremaining: 16.9s\n",
      "706:\tlearn: 0.0011884\ttotal: 40.5s\tremaining: 16.8s\n",
      "707:\tlearn: 0.0011883\ttotal: 40.6s\tremaining: 16.7s\n",
      "708:\tlearn: 0.0011882\ttotal: 40.6s\tremaining: 16.7s\n",
      "709:\tlearn: 0.0011882\ttotal: 40.7s\tremaining: 16.6s\n",
      "710:\tlearn: 0.0011880\ttotal: 40.8s\tremaining: 16.6s\n",
      "711:\tlearn: 0.0011880\ttotal: 40.8s\tremaining: 16.5s\n",
      "712:\tlearn: 0.0011880\ttotal: 40.9s\tremaining: 16.5s\n",
      "713:\tlearn: 0.0011879\ttotal: 40.9s\tremaining: 16.4s\n",
      "714:\tlearn: 0.0011879\ttotal: 41s\tremaining: 16.3s\n",
      "715:\tlearn: 0.0011877\ttotal: 41s\tremaining: 16.3s\n",
      "716:\tlearn: 0.0011876\ttotal: 41.1s\tremaining: 16.2s\n",
      "717:\tlearn: 0.0011872\ttotal: 41.2s\tremaining: 16.2s\n",
      "718:\tlearn: 0.0011872\ttotal: 41.2s\tremaining: 16.1s\n",
      "719:\tlearn: 0.0011872\ttotal: 41.3s\tremaining: 16.1s\n",
      "720:\tlearn: 0.0011791\ttotal: 41.3s\tremaining: 16s\n",
      "721:\tlearn: 0.0011791\ttotal: 41.4s\tremaining: 15.9s\n",
      "722:\tlearn: 0.0011789\ttotal: 41.5s\tremaining: 15.9s\n",
      "723:\tlearn: 0.0011784\ttotal: 41.5s\tremaining: 15.8s\n",
      "724:\tlearn: 0.0011784\ttotal: 41.6s\tremaining: 15.8s\n",
      "725:\tlearn: 0.0011784\ttotal: 41.6s\tremaining: 15.7s\n",
      "726:\tlearn: 0.0011779\ttotal: 41.7s\tremaining: 15.7s\n",
      "727:\tlearn: 0.0011778\ttotal: 41.8s\tremaining: 15.6s\n",
      "728:\tlearn: 0.0011777\ttotal: 41.8s\tremaining: 15.6s\n",
      "729:\tlearn: 0.0011777\ttotal: 41.9s\tremaining: 15.5s\n",
      "730:\tlearn: 0.0011777\ttotal: 42s\tremaining: 15.4s\n",
      "731:\tlearn: 0.0011777\ttotal: 42.1s\tremaining: 15.4s\n",
      "732:\tlearn: 0.0011776\ttotal: 42.1s\tremaining: 15.3s\n",
      "733:\tlearn: 0.0011775\ttotal: 42.2s\tremaining: 15.3s\n",
      "734:\tlearn: 0.0011771\ttotal: 42.3s\tremaining: 15.2s\n",
      "735:\tlearn: 0.0011768\ttotal: 42.3s\tremaining: 15.2s\n",
      "736:\tlearn: 0.0011767\ttotal: 42.4s\tremaining: 15.1s\n",
      "737:\tlearn: 0.0011766\ttotal: 42.5s\tremaining: 15.1s\n",
      "738:\tlearn: 0.0011766\ttotal: 42.6s\tremaining: 15s\n",
      "739:\tlearn: 0.0011765\ttotal: 42.6s\tremaining: 15s\n",
      "740:\tlearn: 0.0011764\ttotal: 42.7s\tremaining: 14.9s\n",
      "741:\tlearn: 0.0011764\ttotal: 42.8s\tremaining: 14.9s\n",
      "742:\tlearn: 0.0011764\ttotal: 42.9s\tremaining: 14.8s\n",
      "743:\tlearn: 0.0011764\ttotal: 42.9s\tremaining: 14.8s\n",
      "744:\tlearn: 0.0011763\ttotal: 43s\tremaining: 14.7s\n",
      "745:\tlearn: 0.0011762\ttotal: 43.1s\tremaining: 14.7s\n",
      "746:\tlearn: 0.0011760\ttotal: 43.2s\tremaining: 14.6s\n",
      "747:\tlearn: 0.0011759\ttotal: 43.2s\tremaining: 14.6s\n",
      "748:\tlearn: 0.0011758\ttotal: 43.3s\tremaining: 14.5s\n",
      "749:\tlearn: 0.0011758\ttotal: 43.4s\tremaining: 14.5s\n",
      "750:\tlearn: 0.0011758\ttotal: 43.5s\tremaining: 14.4s\n",
      "751:\tlearn: 0.0011756\ttotal: 43.5s\tremaining: 14.4s\n",
      "752:\tlearn: 0.0011756\ttotal: 43.6s\tremaining: 14.3s\n",
      "753:\tlearn: 0.0011755\ttotal: 43.7s\tremaining: 14.3s\n",
      "754:\tlearn: 0.0011753\ttotal: 43.8s\tremaining: 14.2s\n",
      "755:\tlearn: 0.0011752\ttotal: 43.8s\tremaining: 14.1s\n",
      "756:\tlearn: 0.0011752\ttotal: 43.9s\tremaining: 14.1s\n",
      "757:\tlearn: 0.0011752\ttotal: 44s\tremaining: 14s\n",
      "758:\tlearn: 0.0011749\ttotal: 44s\tremaining: 14s\n",
      "759:\tlearn: 0.0011749\ttotal: 44.1s\tremaining: 13.9s\n",
      "760:\tlearn: 0.0011748\ttotal: 44.2s\tremaining: 13.9s\n",
      "761:\tlearn: 0.0011743\ttotal: 44.3s\tremaining: 13.8s\n",
      "762:\tlearn: 0.0011743\ttotal: 44.3s\tremaining: 13.8s\n",
      "763:\tlearn: 0.0011743\ttotal: 44.4s\tremaining: 13.7s\n",
      "764:\tlearn: 0.0011742\ttotal: 44.5s\tremaining: 13.7s\n",
      "765:\tlearn: 0.0011741\ttotal: 44.5s\tremaining: 13.6s\n",
      "766:\tlearn: 0.0011739\ttotal: 44.6s\tremaining: 13.5s\n",
      "767:\tlearn: 0.0011737\ttotal: 44.7s\tremaining: 13.5s\n",
      "768:\tlearn: 0.0011734\ttotal: 44.7s\tremaining: 13.4s\n",
      "769:\tlearn: 0.0011732\ttotal: 44.8s\tremaining: 13.4s\n",
      "770:\tlearn: 0.0011731\ttotal: 44.8s\tremaining: 13.3s\n",
      "771:\tlearn: 0.0011727\ttotal: 44.9s\tremaining: 13.3s\n",
      "772:\tlearn: 0.0011726\ttotal: 45s\tremaining: 13.2s\n",
      "773:\tlearn: 0.0011726\ttotal: 45s\tremaining: 13.1s\n",
      "774:\tlearn: 0.0011705\ttotal: 45.1s\tremaining: 13.1s\n",
      "775:\tlearn: 0.0011704\ttotal: 45.1s\tremaining: 13s\n",
      "776:\tlearn: 0.0011704\ttotal: 45.2s\tremaining: 13s\n",
      "777:\tlearn: 0.0011704\ttotal: 45.2s\tremaining: 12.9s\n",
      "778:\tlearn: 0.0011704\ttotal: 45.3s\tremaining: 12.8s\n",
      "779:\tlearn: 0.0011703\ttotal: 45.4s\tremaining: 12.8s\n",
      "780:\tlearn: 0.0011703\ttotal: 45.4s\tremaining: 12.7s\n",
      "781:\tlearn: 0.0011690\ttotal: 45.5s\tremaining: 12.7s\n",
      "782:\tlearn: 0.0011649\ttotal: 45.5s\tremaining: 12.6s\n",
      "783:\tlearn: 0.0011649\ttotal: 45.6s\tremaining: 12.6s\n",
      "784:\tlearn: 0.0011643\ttotal: 45.6s\tremaining: 12.5s\n",
      "785:\tlearn: 0.0011640\ttotal: 45.7s\tremaining: 12.4s\n",
      "786:\tlearn: 0.0011640\ttotal: 45.8s\tremaining: 12.4s\n",
      "787:\tlearn: 0.0011639\ttotal: 45.8s\tremaining: 12.3s\n",
      "788:\tlearn: 0.0011638\ttotal: 45.9s\tremaining: 12.3s\n",
      "789:\tlearn: 0.0011634\ttotal: 45.9s\tremaining: 12.2s\n",
      "790:\tlearn: 0.0011633\ttotal: 46s\tremaining: 12.2s\n",
      "791:\tlearn: 0.0011633\ttotal: 46s\tremaining: 12.1s\n",
      "792:\tlearn: 0.0011633\ttotal: 46.1s\tremaining: 12s\n",
      "793:\tlearn: 0.0011633\ttotal: 46.2s\tremaining: 12s\n",
      "794:\tlearn: 0.0011633\ttotal: 46.2s\tremaining: 11.9s\n",
      "795:\tlearn: 0.0011629\ttotal: 46.3s\tremaining: 11.9s\n",
      "796:\tlearn: 0.0011629\ttotal: 46.3s\tremaining: 11.8s\n",
      "797:\tlearn: 0.0011565\ttotal: 46.4s\tremaining: 11.7s\n",
      "798:\tlearn: 0.0011563\ttotal: 46.4s\tremaining: 11.7s\n",
      "799:\tlearn: 0.0011563\ttotal: 46.5s\tremaining: 11.6s\n",
      "800:\tlearn: 0.0011563\ttotal: 46.6s\tremaining: 11.6s\n",
      "801:\tlearn: 0.0011559\ttotal: 46.6s\tremaining: 11.5s\n",
      "802:\tlearn: 0.0011554\ttotal: 46.7s\tremaining: 11.5s\n",
      "803:\tlearn: 0.0011554\ttotal: 46.7s\tremaining: 11.4s\n",
      "804:\tlearn: 0.0011554\ttotal: 46.8s\tremaining: 11.3s\n",
      "805:\tlearn: 0.0011552\ttotal: 46.9s\tremaining: 11.3s\n",
      "806:\tlearn: 0.0011552\ttotal: 46.9s\tremaining: 11.2s\n",
      "807:\tlearn: 0.0011552\ttotal: 47s\tremaining: 11.2s\n",
      "808:\tlearn: 0.0011551\ttotal: 47s\tremaining: 11.1s\n",
      "809:\tlearn: 0.0011551\ttotal: 47.1s\tremaining: 11s\n",
      "810:\tlearn: 0.0011551\ttotal: 47.2s\tremaining: 11s\n",
      "811:\tlearn: 0.0011519\ttotal: 47.2s\tremaining: 10.9s\n",
      "812:\tlearn: 0.0011515\ttotal: 47.3s\tremaining: 10.9s\n",
      "813:\tlearn: 0.0011515\ttotal: 47.3s\tremaining: 10.8s\n",
      "814:\tlearn: 0.0011514\ttotal: 47.4s\tremaining: 10.8s\n",
      "815:\tlearn: 0.0011507\ttotal: 47.4s\tremaining: 10.7s\n",
      "816:\tlearn: 0.0011502\ttotal: 47.5s\tremaining: 10.6s\n",
      "817:\tlearn: 0.0011502\ttotal: 47.6s\tremaining: 10.6s\n",
      "818:\tlearn: 0.0011502\ttotal: 47.6s\tremaining: 10.5s\n",
      "819:\tlearn: 0.0011499\ttotal: 47.7s\tremaining: 10.5s\n",
      "820:\tlearn: 0.0011499\ttotal: 47.7s\tremaining: 10.4s\n",
      "821:\tlearn: 0.0011496\ttotal: 47.8s\tremaining: 10.3s\n",
      "822:\tlearn: 0.0011495\ttotal: 47.8s\tremaining: 10.3s\n",
      "823:\tlearn: 0.0011495\ttotal: 47.9s\tremaining: 10.2s\n",
      "824:\tlearn: 0.0011490\ttotal: 48s\tremaining: 10.2s\n",
      "825:\tlearn: 0.0011488\ttotal: 48s\tremaining: 10.1s\n",
      "826:\tlearn: 0.0011488\ttotal: 48.1s\tremaining: 10.1s\n",
      "827:\tlearn: 0.0011486\ttotal: 48.1s\tremaining: 10s\n",
      "828:\tlearn: 0.0011485\ttotal: 48.2s\tremaining: 9.94s\n",
      "829:\tlearn: 0.0011484\ttotal: 48.2s\tremaining: 9.88s\n",
      "830:\tlearn: 0.0011484\ttotal: 48.3s\tremaining: 9.82s\n",
      "831:\tlearn: 0.0011483\ttotal: 48.4s\tremaining: 9.76s\n",
      "832:\tlearn: 0.0011483\ttotal: 48.4s\tremaining: 9.71s\n",
      "833:\tlearn: 0.0011482\ttotal: 48.5s\tremaining: 9.65s\n",
      "834:\tlearn: 0.0011482\ttotal: 48.5s\tremaining: 9.59s\n",
      "835:\tlearn: 0.0011481\ttotal: 48.6s\tremaining: 9.53s\n",
      "836:\tlearn: 0.0011481\ttotal: 48.6s\tremaining: 9.47s\n",
      "837:\tlearn: 0.0011481\ttotal: 48.7s\tremaining: 9.41s\n",
      "838:\tlearn: 0.0011480\ttotal: 48.8s\tremaining: 9.36s\n",
      "839:\tlearn: 0.0011475\ttotal: 48.8s\tremaining: 9.3s\n",
      "840:\tlearn: 0.0011470\ttotal: 48.9s\tremaining: 9.24s\n",
      "841:\tlearn: 0.0011470\ttotal: 48.9s\tremaining: 9.18s\n",
      "842:\tlearn: 0.0011469\ttotal: 49s\tremaining: 9.12s\n",
      "843:\tlearn: 0.0011469\ttotal: 49s\tremaining: 9.06s\n",
      "844:\tlearn: 0.0011469\ttotal: 49.1s\tremaining: 9.01s\n",
      "845:\tlearn: 0.0011468\ttotal: 49.2s\tremaining: 8.95s\n",
      "846:\tlearn: 0.0011467\ttotal: 49.2s\tremaining: 8.89s\n",
      "847:\tlearn: 0.0011467\ttotal: 49.3s\tremaining: 8.83s\n",
      "848:\tlearn: 0.0011467\ttotal: 49.3s\tremaining: 8.77s\n",
      "849:\tlearn: 0.0011466\ttotal: 49.4s\tremaining: 8.72s\n",
      "850:\tlearn: 0.0011466\ttotal: 49.4s\tremaining: 8.66s\n",
      "851:\tlearn: 0.0011465\ttotal: 49.5s\tremaining: 8.6s\n",
      "852:\tlearn: 0.0011464\ttotal: 49.6s\tremaining: 8.54s\n",
      "853:\tlearn: 0.0011459\ttotal: 49.6s\tremaining: 8.48s\n",
      "854:\tlearn: 0.0011458\ttotal: 49.7s\tremaining: 8.43s\n",
      "855:\tlearn: 0.0011458\ttotal: 49.7s\tremaining: 8.37s\n",
      "856:\tlearn: 0.0011458\ttotal: 49.8s\tremaining: 8.31s\n",
      "857:\tlearn: 0.0011457\ttotal: 49.9s\tremaining: 8.25s\n",
      "858:\tlearn: 0.0011456\ttotal: 49.9s\tremaining: 8.19s\n",
      "859:\tlearn: 0.0011455\ttotal: 50s\tremaining: 8.13s\n",
      "860:\tlearn: 0.0011452\ttotal: 50s\tremaining: 8.08s\n",
      "861:\tlearn: 0.0011450\ttotal: 50.1s\tremaining: 8.02s\n",
      "862:\tlearn: 0.0011449\ttotal: 50.1s\tremaining: 7.96s\n",
      "863:\tlearn: 0.0011449\ttotal: 50.2s\tremaining: 7.9s\n",
      "864:\tlearn: 0.0011448\ttotal: 50.3s\tremaining: 7.84s\n",
      "865:\tlearn: 0.0011446\ttotal: 50.3s\tremaining: 7.79s\n",
      "866:\tlearn: 0.0011388\ttotal: 50.4s\tremaining: 7.73s\n",
      "867:\tlearn: 0.0011388\ttotal: 50.4s\tremaining: 7.67s\n",
      "868:\tlearn: 0.0011385\ttotal: 50.5s\tremaining: 7.61s\n",
      "869:\tlearn: 0.0011370\ttotal: 50.5s\tremaining: 7.55s\n",
      "870:\tlearn: 0.0011368\ttotal: 50.6s\tremaining: 7.5s\n",
      "871:\tlearn: 0.0011368\ttotal: 50.7s\tremaining: 7.44s\n",
      "872:\tlearn: 0.0011353\ttotal: 50.7s\tremaining: 7.38s\n",
      "873:\tlearn: 0.0011350\ttotal: 50.8s\tremaining: 7.32s\n",
      "874:\tlearn: 0.0011350\ttotal: 50.8s\tremaining: 7.26s\n",
      "875:\tlearn: 0.0011301\ttotal: 50.9s\tremaining: 7.2s\n",
      "876:\tlearn: 0.0011301\ttotal: 51s\tremaining: 7.15s\n",
      "877:\tlearn: 0.0011299\ttotal: 51s\tremaining: 7.09s\n",
      "878:\tlearn: 0.0011298\ttotal: 51.1s\tremaining: 7.03s\n",
      "879:\tlearn: 0.0011298\ttotal: 51.1s\tremaining: 6.97s\n",
      "880:\tlearn: 0.0011298\ttotal: 51.2s\tremaining: 6.91s\n",
      "881:\tlearn: 0.0011296\ttotal: 51.2s\tremaining: 6.86s\n",
      "882:\tlearn: 0.0011296\ttotal: 51.3s\tremaining: 6.8s\n",
      "883:\tlearn: 0.0011296\ttotal: 51.4s\tremaining: 6.74s\n",
      "884:\tlearn: 0.0011295\ttotal: 51.4s\tremaining: 6.68s\n",
      "885:\tlearn: 0.0011277\ttotal: 51.5s\tremaining: 6.62s\n",
      "886:\tlearn: 0.0011277\ttotal: 51.5s\tremaining: 6.56s\n",
      "887:\tlearn: 0.0011276\ttotal: 51.6s\tremaining: 6.51s\n",
      "888:\tlearn: 0.0011272\ttotal: 51.6s\tremaining: 6.45s\n",
      "889:\tlearn: 0.0011271\ttotal: 51.7s\tremaining: 6.39s\n",
      "890:\tlearn: 0.0011264\ttotal: 51.8s\tremaining: 6.33s\n",
      "891:\tlearn: 0.0011264\ttotal: 51.8s\tremaining: 6.27s\n",
      "892:\tlearn: 0.0011264\ttotal: 51.9s\tremaining: 6.21s\n",
      "893:\tlearn: 0.0011261\ttotal: 51.9s\tremaining: 6.16s\n",
      "894:\tlearn: 0.0011260\ttotal: 52s\tremaining: 6.1s\n",
      "895:\tlearn: 0.0011258\ttotal: 52s\tremaining: 6.04s\n",
      "896:\tlearn: 0.0011258\ttotal: 52.1s\tremaining: 5.98s\n",
      "897:\tlearn: 0.0011258\ttotal: 52.2s\tremaining: 5.92s\n",
      "898:\tlearn: 0.0011250\ttotal: 52.2s\tremaining: 5.87s\n",
      "899:\tlearn: 0.0011250\ttotal: 52.3s\tremaining: 5.81s\n",
      "900:\tlearn: 0.0011249\ttotal: 52.3s\tremaining: 5.75s\n",
      "901:\tlearn: 0.0011249\ttotal: 52.4s\tremaining: 5.69s\n",
      "902:\tlearn: 0.0011247\ttotal: 52.4s\tremaining: 5.63s\n",
      "903:\tlearn: 0.0011245\ttotal: 52.5s\tremaining: 5.58s\n",
      "904:\tlearn: 0.0011243\ttotal: 52.6s\tremaining: 5.52s\n",
      "905:\tlearn: 0.0011243\ttotal: 52.6s\tremaining: 5.46s\n",
      "906:\tlearn: 0.0011241\ttotal: 52.7s\tremaining: 5.4s\n",
      "907:\tlearn: 0.0011238\ttotal: 52.7s\tremaining: 5.34s\n",
      "908:\tlearn: 0.0011238\ttotal: 52.8s\tremaining: 5.28s\n",
      "909:\tlearn: 0.0011238\ttotal: 52.8s\tremaining: 5.23s\n",
      "910:\tlearn: 0.0011238\ttotal: 52.9s\tremaining: 5.17s\n",
      "911:\tlearn: 0.0011237\ttotal: 53s\tremaining: 5.11s\n",
      "912:\tlearn: 0.0011237\ttotal: 53s\tremaining: 5.05s\n",
      "913:\tlearn: 0.0011236\ttotal: 53.1s\tremaining: 4.99s\n",
      "914:\tlearn: 0.0011235\ttotal: 53.1s\tremaining: 4.93s\n",
      "915:\tlearn: 0.0011235\ttotal: 53.2s\tremaining: 4.88s\n",
      "916:\tlearn: 0.0011235\ttotal: 53.2s\tremaining: 4.82s\n",
      "917:\tlearn: 0.0011234\ttotal: 53.3s\tremaining: 4.76s\n",
      "918:\tlearn: 0.0011230\ttotal: 53.4s\tremaining: 4.7s\n",
      "919:\tlearn: 0.0011229\ttotal: 53.4s\tremaining: 4.64s\n",
      "920:\tlearn: 0.0011229\ttotal: 53.5s\tremaining: 4.59s\n",
      "921:\tlearn: 0.0011226\ttotal: 53.5s\tremaining: 4.53s\n",
      "922:\tlearn: 0.0011217\ttotal: 53.6s\tremaining: 4.47s\n",
      "923:\tlearn: 0.0011216\ttotal: 53.7s\tremaining: 4.41s\n",
      "924:\tlearn: 0.0011216\ttotal: 53.7s\tremaining: 4.35s\n",
      "925:\tlearn: 0.0011216\ttotal: 53.8s\tremaining: 4.3s\n",
      "926:\tlearn: 0.0011213\ttotal: 53.8s\tremaining: 4.24s\n",
      "927:\tlearn: 0.0011212\ttotal: 53.9s\tremaining: 4.18s\n",
      "928:\tlearn: 0.0011199\ttotal: 53.9s\tremaining: 4.12s\n",
      "929:\tlearn: 0.0011198\ttotal: 54s\tremaining: 4.06s\n",
      "930:\tlearn: 0.0011198\ttotal: 54.1s\tremaining: 4.01s\n",
      "931:\tlearn: 0.0011197\ttotal: 54.1s\tremaining: 3.95s\n",
      "932:\tlearn: 0.0011196\ttotal: 54.2s\tremaining: 3.89s\n",
      "933:\tlearn: 0.0011195\ttotal: 54.2s\tremaining: 3.83s\n",
      "934:\tlearn: 0.0011192\ttotal: 54.3s\tremaining: 3.77s\n",
      "935:\tlearn: 0.0011192\ttotal: 54.3s\tremaining: 3.71s\n",
      "936:\tlearn: 0.0011192\ttotal: 54.4s\tremaining: 3.66s\n",
      "937:\tlearn: 0.0011192\ttotal: 54.5s\tremaining: 3.6s\n",
      "938:\tlearn: 0.0011188\ttotal: 54.5s\tremaining: 3.54s\n",
      "939:\tlearn: 0.0011187\ttotal: 54.6s\tremaining: 3.48s\n",
      "940:\tlearn: 0.0011183\ttotal: 54.6s\tremaining: 3.42s\n",
      "941:\tlearn: 0.0011181\ttotal: 54.7s\tremaining: 3.37s\n",
      "942:\tlearn: 0.0011180\ttotal: 54.7s\tremaining: 3.31s\n",
      "943:\tlearn: 0.0011180\ttotal: 54.8s\tremaining: 3.25s\n",
      "944:\tlearn: 0.0011180\ttotal: 54.9s\tremaining: 3.19s\n",
      "945:\tlearn: 0.0011180\ttotal: 54.9s\tremaining: 3.13s\n",
      "946:\tlearn: 0.0011179\ttotal: 55s\tremaining: 3.08s\n",
      "947:\tlearn: 0.0011178\ttotal: 55s\tremaining: 3.02s\n",
      "948:\tlearn: 0.0011178\ttotal: 55.1s\tremaining: 2.96s\n",
      "949:\tlearn: 0.0011176\ttotal: 55.1s\tremaining: 2.9s\n",
      "950:\tlearn: 0.0011176\ttotal: 55.2s\tremaining: 2.84s\n",
      "951:\tlearn: 0.0011176\ttotal: 55.3s\tremaining: 2.79s\n",
      "952:\tlearn: 0.0011176\ttotal: 55.3s\tremaining: 2.73s\n",
      "953:\tlearn: 0.0011176\ttotal: 55.4s\tremaining: 2.67s\n",
      "954:\tlearn: 0.0011175\ttotal: 55.4s\tremaining: 2.61s\n",
      "955:\tlearn: 0.0011175\ttotal: 55.5s\tremaining: 2.55s\n",
      "956:\tlearn: 0.0011174\ttotal: 55.5s\tremaining: 2.5s\n",
      "957:\tlearn: 0.0011157\ttotal: 55.6s\tremaining: 2.44s\n",
      "958:\tlearn: 0.0011149\ttotal: 55.6s\tremaining: 2.38s\n",
      "959:\tlearn: 0.0011149\ttotal: 55.7s\tremaining: 2.32s\n",
      "960:\tlearn: 0.0011149\ttotal: 55.8s\tremaining: 2.26s\n",
      "961:\tlearn: 0.0011149\ttotal: 55.8s\tremaining: 2.2s\n",
      "962:\tlearn: 0.0011149\ttotal: 55.9s\tremaining: 2.15s\n",
      "963:\tlearn: 0.0011148\ttotal: 55.9s\tremaining: 2.09s\n",
      "964:\tlearn: 0.0011148\ttotal: 56s\tremaining: 2.03s\n",
      "965:\tlearn: 0.0011148\ttotal: 56.1s\tremaining: 1.97s\n",
      "966:\tlearn: 0.0011147\ttotal: 56.1s\tremaining: 1.92s\n",
      "967:\tlearn: 0.0011144\ttotal: 56.2s\tremaining: 1.86s\n",
      "968:\tlearn: 0.0011144\ttotal: 56.2s\tremaining: 1.8s\n",
      "969:\tlearn: 0.0011142\ttotal: 56.3s\tremaining: 1.74s\n",
      "970:\tlearn: 0.0011140\ttotal: 56.4s\tremaining: 1.68s\n",
      "971:\tlearn: 0.0011139\ttotal: 56.4s\tremaining: 1.62s\n",
      "972:\tlearn: 0.0011136\ttotal: 56.5s\tremaining: 1.57s\n",
      "973:\tlearn: 0.0011133\ttotal: 56.5s\tremaining: 1.51s\n",
      "974:\tlearn: 0.0011099\ttotal: 56.6s\tremaining: 1.45s\n",
      "975:\tlearn: 0.0011098\ttotal: 56.6s\tremaining: 1.39s\n",
      "976:\tlearn: 0.0011095\ttotal: 56.7s\tremaining: 1.33s\n",
      "977:\tlearn: 0.0011095\ttotal: 56.8s\tremaining: 1.28s\n",
      "978:\tlearn: 0.0011089\ttotal: 56.8s\tremaining: 1.22s\n",
      "979:\tlearn: 0.0011088\ttotal: 56.9s\tremaining: 1.16s\n",
      "980:\tlearn: 0.0011088\ttotal: 56.9s\tremaining: 1.1s\n",
      "981:\tlearn: 0.0011087\ttotal: 57s\tremaining: 1.04s\n",
      "982:\tlearn: 0.0011086\ttotal: 57s\tremaining: 986ms\n",
      "983:\tlearn: 0.0011086\ttotal: 57.1s\tremaining: 928ms\n",
      "984:\tlearn: 0.0011083\ttotal: 57.1s\tremaining: 870ms\n",
      "985:\tlearn: 0.0011080\ttotal: 57.2s\tremaining: 812ms\n",
      "986:\tlearn: 0.0011080\ttotal: 57.3s\tremaining: 754ms\n",
      "987:\tlearn: 0.0011079\ttotal: 57.3s\tremaining: 696ms\n",
      "988:\tlearn: 0.0011076\ttotal: 57.4s\tremaining: 638ms\n",
      "989:\tlearn: 0.0011075\ttotal: 57.4s\tremaining: 580ms\n",
      "990:\tlearn: 0.0011075\ttotal: 57.5s\tremaining: 522ms\n",
      "991:\tlearn: 0.0011074\ttotal: 57.5s\tremaining: 464ms\n",
      "992:\tlearn: 0.0011044\ttotal: 57.6s\tremaining: 406ms\n",
      "993:\tlearn: 0.0011044\ttotal: 57.7s\tremaining: 348ms\n",
      "994:\tlearn: 0.0011044\ttotal: 57.7s\tremaining: 290ms\n",
      "995:\tlearn: 0.0011043\ttotal: 57.8s\tremaining: 232ms\n",
      "996:\tlearn: 0.0011043\ttotal: 57.8s\tremaining: 174ms\n",
      "997:\tlearn: 0.0011042\ttotal: 57.9s\tremaining: 116ms\n",
      "998:\tlearn: 0.0011037\ttotal: 57.9s\tremaining: 58ms\n",
      "999:\tlearn: 0.0011037\ttotal: 58s\tremaining: 0us\n",
      "0:\tlearn: 0.6160237\ttotal: 67.2ms\tremaining: 1m 7s\n",
      "1:\tlearn: 0.5646111\ttotal: 123ms\tremaining: 1m 1s\n",
      "2:\tlearn: 0.5237824\ttotal: 181ms\tremaining: 1m\n",
      "3:\tlearn: 0.4800398\ttotal: 237ms\tremaining: 58.9s\n",
      "4:\tlearn: 0.4504662\ttotal: 293ms\tremaining: 58.4s\n",
      "5:\tlearn: 0.4097385\ttotal: 350ms\tremaining: 57.9s\n",
      "6:\tlearn: 0.3568213\ttotal: 407ms\tremaining: 57.7s\n",
      "7:\tlearn: 0.3045706\ttotal: 466ms\tremaining: 57.8s\n",
      "8:\tlearn: 0.2395721\ttotal: 523ms\tremaining: 57.5s\n",
      "9:\tlearn: 0.1838924\ttotal: 580ms\tremaining: 57.4s\n",
      "10:\tlearn: 0.1502084\ttotal: 638ms\tremaining: 57.4s\n",
      "11:\tlearn: 0.1312068\ttotal: 694ms\tremaining: 57.1s\n",
      "12:\tlearn: 0.1223513\ttotal: 751ms\tremaining: 57s\n",
      "13:\tlearn: 0.0999434\ttotal: 808ms\tremaining: 56.9s\n",
      "14:\tlearn: 0.0817089\ttotal: 865ms\tremaining: 56.8s\n",
      "15:\tlearn: 0.0778495\ttotal: 921ms\tremaining: 56.7s\n",
      "16:\tlearn: 0.0758035\ttotal: 978ms\tremaining: 56.5s\n",
      "17:\tlearn: 0.0613083\ttotal: 1.03s\tremaining: 56.5s\n",
      "18:\tlearn: 0.0553575\ttotal: 1.09s\tremaining: 56.4s\n",
      "19:\tlearn: 0.0498638\ttotal: 1.15s\tremaining: 56.3s\n",
      "20:\tlearn: 0.0485405\ttotal: 1.21s\tremaining: 56.2s\n",
      "21:\tlearn: 0.0440222\ttotal: 1.26s\tremaining: 56.1s\n",
      "22:\tlearn: 0.0376606\ttotal: 1.32s\tremaining: 56.1s\n",
      "23:\tlearn: 0.0340909\ttotal: 1.38s\tremaining: 56s\n",
      "24:\tlearn: 0.0298879\ttotal: 1.44s\tremaining: 56s\n",
      "25:\tlearn: 0.0273180\ttotal: 1.5s\tremaining: 56.1s\n",
      "26:\tlearn: 0.0249336\ttotal: 1.55s\tremaining: 56s\n",
      "27:\tlearn: 0.0223541\ttotal: 1.61s\tremaining: 55.9s\n",
      "28:\tlearn: 0.0200467\ttotal: 1.67s\tremaining: 55.8s\n",
      "29:\tlearn: 0.0179394\ttotal: 1.73s\tremaining: 56s\n",
      "30:\tlearn: 0.0168633\ttotal: 1.79s\tremaining: 56s\n",
      "31:\tlearn: 0.0152693\ttotal: 1.85s\tremaining: 55.9s\n",
      "32:\tlearn: 0.0137304\ttotal: 1.91s\tremaining: 55.9s\n",
      "33:\tlearn: 0.0127654\ttotal: 1.96s\tremaining: 55.8s\n",
      "34:\tlearn: 0.0122427\ttotal: 2.02s\tremaining: 55.7s\n",
      "35:\tlearn: 0.0114409\ttotal: 2.08s\tremaining: 55.6s\n",
      "36:\tlearn: 0.0110841\ttotal: 2.13s\tremaining: 55.6s\n",
      "37:\tlearn: 0.0109585\ttotal: 2.19s\tremaining: 55.5s\n",
      "38:\tlearn: 0.0100431\ttotal: 2.25s\tremaining: 55.4s\n",
      "39:\tlearn: 0.0099372\ttotal: 2.3s\tremaining: 55.3s\n",
      "40:\tlearn: 0.0095522\ttotal: 2.36s\tremaining: 55.2s\n",
      "41:\tlearn: 0.0091655\ttotal: 2.42s\tremaining: 55.2s\n",
      "42:\tlearn: 0.0087667\ttotal: 2.48s\tremaining: 55.1s\n",
      "43:\tlearn: 0.0082871\ttotal: 2.53s\tremaining: 55.1s\n",
      "44:\tlearn: 0.0077709\ttotal: 2.59s\tremaining: 55s\n",
      "45:\tlearn: 0.0075641\ttotal: 2.65s\tremaining: 54.9s\n",
      "46:\tlearn: 0.0072323\ttotal: 2.71s\tremaining: 55s\n",
      "47:\tlearn: 0.0068914\ttotal: 2.77s\tremaining: 54.9s\n",
      "48:\tlearn: 0.0065444\ttotal: 2.83s\tremaining: 54.9s\n",
      "49:\tlearn: 0.0061949\ttotal: 2.88s\tremaining: 54.8s\n",
      "50:\tlearn: 0.0060584\ttotal: 2.94s\tremaining: 54.7s\n",
      "51:\tlearn: 0.0059228\ttotal: 3s\tremaining: 54.7s\n",
      "52:\tlearn: 0.0056861\ttotal: 3.06s\tremaining: 54.7s\n",
      "53:\tlearn: 0.0055119\ttotal: 3.12s\tremaining: 54.7s\n",
      "54:\tlearn: 0.0053333\ttotal: 3.18s\tremaining: 54.6s\n",
      "55:\tlearn: 0.0051429\ttotal: 3.23s\tremaining: 54.5s\n",
      "56:\tlearn: 0.0049260\ttotal: 3.29s\tremaining: 54.5s\n",
      "57:\tlearn: 0.0048011\ttotal: 3.35s\tremaining: 54.4s\n",
      "58:\tlearn: 0.0047110\ttotal: 3.4s\tremaining: 54.3s\n",
      "59:\tlearn: 0.0045716\ttotal: 3.46s\tremaining: 54.3s\n",
      "60:\tlearn: 0.0043902\ttotal: 3.52s\tremaining: 54.2s\n",
      "61:\tlearn: 0.0043094\ttotal: 3.58s\tremaining: 54.1s\n",
      "62:\tlearn: 0.0041847\ttotal: 3.64s\tremaining: 54.1s\n",
      "63:\tlearn: 0.0041364\ttotal: 3.69s\tremaining: 54s\n",
      "64:\tlearn: 0.0040548\ttotal: 3.75s\tremaining: 54s\n",
      "65:\tlearn: 0.0040104\ttotal: 3.81s\tremaining: 53.9s\n",
      "66:\tlearn: 0.0039415\ttotal: 3.87s\tremaining: 53.8s\n",
      "67:\tlearn: 0.0038610\ttotal: 3.92s\tremaining: 53.8s\n",
      "68:\tlearn: 0.0037770\ttotal: 3.98s\tremaining: 53.7s\n",
      "69:\tlearn: 0.0036933\ttotal: 4.03s\tremaining: 53.6s\n",
      "70:\tlearn: 0.0036099\ttotal: 4.09s\tremaining: 53.6s\n",
      "71:\tlearn: 0.0035651\ttotal: 4.15s\tremaining: 53.5s\n",
      "72:\tlearn: 0.0034688\ttotal: 4.21s\tremaining: 53.4s\n",
      "73:\tlearn: 0.0034543\ttotal: 4.26s\tremaining: 53.4s\n",
      "74:\tlearn: 0.0033527\ttotal: 4.32s\tremaining: 53.3s\n",
      "75:\tlearn: 0.0033006\ttotal: 4.38s\tremaining: 53.3s\n",
      "76:\tlearn: 0.0032265\ttotal: 4.44s\tremaining: 53.2s\n",
      "77:\tlearn: 0.0031642\ttotal: 4.5s\tremaining: 53.1s\n",
      "78:\tlearn: 0.0031062\ttotal: 4.55s\tremaining: 53.1s\n",
      "79:\tlearn: 0.0030137\ttotal: 4.61s\tremaining: 53s\n",
      "80:\tlearn: 0.0030136\ttotal: 4.67s\tremaining: 52.9s\n",
      "81:\tlearn: 0.0029806\ttotal: 4.72s\tremaining: 52.9s\n",
      "82:\tlearn: 0.0028772\ttotal: 4.78s\tremaining: 52.8s\n",
      "83:\tlearn: 0.0027985\ttotal: 4.83s\tremaining: 52.7s\n",
      "84:\tlearn: 0.0027495\ttotal: 4.89s\tremaining: 52.7s\n",
      "85:\tlearn: 0.0026857\ttotal: 4.95s\tremaining: 52.6s\n",
      "86:\tlearn: 0.0026324\ttotal: 5.01s\tremaining: 52.6s\n",
      "87:\tlearn: 0.0025814\ttotal: 5.07s\tremaining: 52.5s\n",
      "88:\tlearn: 0.0025630\ttotal: 5.13s\tremaining: 52.5s\n",
      "89:\tlearn: 0.0025455\ttotal: 5.19s\tremaining: 52.5s\n",
      "90:\tlearn: 0.0025079\ttotal: 5.25s\tremaining: 52.4s\n",
      "91:\tlearn: 0.0024781\ttotal: 5.3s\tremaining: 52.3s\n",
      "92:\tlearn: 0.0024604\ttotal: 5.36s\tremaining: 52.3s\n",
      "93:\tlearn: 0.0024442\ttotal: 5.42s\tremaining: 52.2s\n",
      "94:\tlearn: 0.0023999\ttotal: 5.47s\tremaining: 52.1s\n",
      "95:\tlearn: 0.0023703\ttotal: 5.53s\tremaining: 52.1s\n",
      "96:\tlearn: 0.0023314\ttotal: 5.59s\tremaining: 52s\n",
      "97:\tlearn: 0.0023018\ttotal: 5.65s\tremaining: 52s\n",
      "98:\tlearn: 0.0022838\ttotal: 5.7s\tremaining: 51.9s\n",
      "99:\tlearn: 0.0022372\ttotal: 5.76s\tremaining: 51.9s\n",
      "100:\tlearn: 0.0021980\ttotal: 5.82s\tremaining: 51.8s\n",
      "101:\tlearn: 0.0021978\ttotal: 5.88s\tremaining: 51.7s\n",
      "102:\tlearn: 0.0021853\ttotal: 5.93s\tremaining: 51.7s\n",
      "103:\tlearn: 0.0021534\ttotal: 5.99s\tremaining: 51.6s\n",
      "104:\tlearn: 0.0021322\ttotal: 6.04s\tremaining: 51.5s\n",
      "105:\tlearn: 0.0021019\ttotal: 6.1s\tremaining: 51.5s\n",
      "106:\tlearn: 0.0021017\ttotal: 6.16s\tremaining: 51.4s\n",
      "107:\tlearn: 0.0021016\ttotal: 6.22s\tremaining: 51.3s\n",
      "108:\tlearn: 0.0021015\ttotal: 6.27s\tremaining: 51.3s\n",
      "109:\tlearn: 0.0020463\ttotal: 6.33s\tremaining: 51.2s\n",
      "110:\tlearn: 0.0020107\ttotal: 6.39s\tremaining: 51.2s\n",
      "111:\tlearn: 0.0019776\ttotal: 6.45s\tremaining: 51.1s\n",
      "112:\tlearn: 0.0019466\ttotal: 6.5s\tremaining: 51s\n",
      "113:\tlearn: 0.0019464\ttotal: 6.56s\tremaining: 51s\n",
      "114:\tlearn: 0.0019448\ttotal: 6.62s\tremaining: 50.9s\n",
      "115:\tlearn: 0.0019447\ttotal: 6.67s\tremaining: 50.9s\n",
      "116:\tlearn: 0.0019446\ttotal: 6.73s\tremaining: 50.8s\n",
      "117:\tlearn: 0.0019340\ttotal: 6.79s\tremaining: 50.7s\n",
      "118:\tlearn: 0.0019149\ttotal: 6.84s\tremaining: 50.7s\n",
      "119:\tlearn: 0.0019149\ttotal: 6.9s\tremaining: 50.6s\n",
      "120:\tlearn: 0.0019128\ttotal: 6.96s\tremaining: 50.6s\n",
      "121:\tlearn: 0.0019125\ttotal: 7.02s\tremaining: 50.5s\n",
      "122:\tlearn: 0.0019125\ttotal: 7.07s\tremaining: 50.4s\n",
      "123:\tlearn: 0.0019125\ttotal: 7.13s\tremaining: 50.4s\n",
      "124:\tlearn: 0.0019124\ttotal: 7.19s\tremaining: 50.3s\n",
      "125:\tlearn: 0.0018907\ttotal: 7.24s\tremaining: 50.2s\n",
      "126:\tlearn: 0.0018899\ttotal: 7.3s\tremaining: 50.2s\n",
      "127:\tlearn: 0.0018898\ttotal: 7.35s\tremaining: 50.1s\n",
      "128:\tlearn: 0.0018876\ttotal: 7.41s\tremaining: 50s\n",
      "129:\tlearn: 0.0018760\ttotal: 7.47s\tremaining: 50s\n",
      "130:\tlearn: 0.0018697\ttotal: 7.52s\tremaining: 49.9s\n",
      "131:\tlearn: 0.0018696\ttotal: 7.58s\tremaining: 49.8s\n",
      "132:\tlearn: 0.0018536\ttotal: 7.63s\tremaining: 49.8s\n",
      "133:\tlearn: 0.0018221\ttotal: 7.69s\tremaining: 49.7s\n",
      "134:\tlearn: 0.0017990\ttotal: 7.75s\tremaining: 49.6s\n",
      "135:\tlearn: 0.0017988\ttotal: 7.81s\tremaining: 49.6s\n",
      "136:\tlearn: 0.0017988\ttotal: 7.87s\tremaining: 49.5s\n",
      "137:\tlearn: 0.0017988\ttotal: 7.92s\tremaining: 49.5s\n",
      "138:\tlearn: 0.0017902\ttotal: 7.98s\tremaining: 49.4s\n",
      "139:\tlearn: 0.0017900\ttotal: 8.04s\tremaining: 49.4s\n",
      "140:\tlearn: 0.0017701\ttotal: 8.09s\tremaining: 49.3s\n",
      "141:\tlearn: 0.0017698\ttotal: 8.15s\tremaining: 49.3s\n",
      "142:\tlearn: 0.0017694\ttotal: 8.21s\tremaining: 49.2s\n",
      "143:\tlearn: 0.0017680\ttotal: 8.27s\tremaining: 49.2s\n",
      "144:\tlearn: 0.0017680\ttotal: 8.32s\tremaining: 49.1s\n",
      "145:\tlearn: 0.0017609\ttotal: 8.38s\tremaining: 49s\n",
      "146:\tlearn: 0.0017433\ttotal: 8.44s\tremaining: 49s\n",
      "147:\tlearn: 0.0017420\ttotal: 8.5s\tremaining: 48.9s\n",
      "148:\tlearn: 0.0017412\ttotal: 8.55s\tremaining: 48.9s\n",
      "149:\tlearn: 0.0017391\ttotal: 8.61s\tremaining: 48.8s\n",
      "150:\tlearn: 0.0017389\ttotal: 8.67s\tremaining: 48.7s\n",
      "151:\tlearn: 0.0017384\ttotal: 8.72s\tremaining: 48.7s\n",
      "152:\tlearn: 0.0017379\ttotal: 8.78s\tremaining: 48.6s\n",
      "153:\tlearn: 0.0017155\ttotal: 8.84s\tremaining: 48.5s\n",
      "154:\tlearn: 0.0017115\ttotal: 8.89s\tremaining: 48.5s\n",
      "155:\tlearn: 0.0017114\ttotal: 8.95s\tremaining: 48.4s\n",
      "156:\tlearn: 0.0017114\ttotal: 9.01s\tremaining: 48.4s\n",
      "157:\tlearn: 0.0017055\ttotal: 9.07s\tremaining: 48.3s\n",
      "158:\tlearn: 0.0017053\ttotal: 9.13s\tremaining: 48.3s\n",
      "159:\tlearn: 0.0017018\ttotal: 9.18s\tremaining: 48.2s\n",
      "160:\tlearn: 0.0017018\ttotal: 9.24s\tremaining: 48.2s\n",
      "161:\tlearn: 0.0017017\ttotal: 9.3s\tremaining: 48.1s\n",
      "162:\tlearn: 0.0017016\ttotal: 9.36s\tremaining: 48.1s\n",
      "163:\tlearn: 0.0017009\ttotal: 9.41s\tremaining: 48s\n",
      "164:\tlearn: 0.0017006\ttotal: 9.47s\tremaining: 47.9s\n",
      "165:\tlearn: 0.0017006\ttotal: 9.53s\tremaining: 47.9s\n",
      "166:\tlearn: 0.0017004\ttotal: 9.59s\tremaining: 47.8s\n",
      "167:\tlearn: 0.0017004\ttotal: 9.64s\tremaining: 47.8s\n",
      "168:\tlearn: 0.0017004\ttotal: 9.7s\tremaining: 47.7s\n",
      "169:\tlearn: 0.0017001\ttotal: 9.76s\tremaining: 47.6s\n",
      "170:\tlearn: 0.0016680\ttotal: 9.81s\tremaining: 47.6s\n",
      "171:\tlearn: 0.0016492\ttotal: 9.87s\tremaining: 47.5s\n",
      "172:\tlearn: 0.0016265\ttotal: 9.93s\tremaining: 47.5s\n",
      "173:\tlearn: 0.0016204\ttotal: 9.99s\tremaining: 47.4s\n",
      "174:\tlearn: 0.0016202\ttotal: 10s\tremaining: 47.4s\n",
      "175:\tlearn: 0.0016202\ttotal: 10.1s\tremaining: 47.3s\n",
      "176:\tlearn: 0.0016130\ttotal: 10.2s\tremaining: 47.3s\n",
      "177:\tlearn: 0.0016130\ttotal: 10.2s\tremaining: 47.2s\n",
      "178:\tlearn: 0.0016128\ttotal: 10.3s\tremaining: 47.1s\n",
      "179:\tlearn: 0.0016128\ttotal: 10.3s\tremaining: 47.1s\n",
      "180:\tlearn: 0.0015968\ttotal: 10.4s\tremaining: 47s\n",
      "181:\tlearn: 0.0015967\ttotal: 10.4s\tremaining: 47s\n",
      "182:\tlearn: 0.0015949\ttotal: 10.5s\tremaining: 46.9s\n",
      "183:\tlearn: 0.0015864\ttotal: 10.6s\tremaining: 46.8s\n",
      "184:\tlearn: 0.0015860\ttotal: 10.6s\tremaining: 46.8s\n",
      "185:\tlearn: 0.0015722\ttotal: 10.7s\tremaining: 46.8s\n",
      "186:\tlearn: 0.0015721\ttotal: 10.7s\tremaining: 46.7s\n",
      "187:\tlearn: 0.0015720\ttotal: 10.8s\tremaining: 46.7s\n",
      "188:\tlearn: 0.0015716\ttotal: 10.9s\tremaining: 46.6s\n",
      "189:\tlearn: 0.0015714\ttotal: 10.9s\tremaining: 46.6s\n",
      "190:\tlearn: 0.0015711\ttotal: 11s\tremaining: 46.6s\n",
      "191:\tlearn: 0.0015710\ttotal: 11.1s\tremaining: 46.7s\n",
      "192:\tlearn: 0.0015709\ttotal: 11.2s\tremaining: 46.7s\n",
      "193:\tlearn: 0.0015701\ttotal: 11.2s\tremaining: 46.7s\n",
      "194:\tlearn: 0.0015700\ttotal: 11.3s\tremaining: 46.7s\n",
      "195:\tlearn: 0.0015700\ttotal: 11.4s\tremaining: 46.6s\n",
      "196:\tlearn: 0.0015699\ttotal: 11.4s\tremaining: 46.6s\n",
      "197:\tlearn: 0.0015698\ttotal: 11.5s\tremaining: 46.5s\n",
      "198:\tlearn: 0.0015698\ttotal: 11.5s\tremaining: 46.5s\n",
      "199:\tlearn: 0.0015697\ttotal: 11.6s\tremaining: 46.4s\n",
      "200:\tlearn: 0.0015694\ttotal: 11.7s\tremaining: 46.3s\n",
      "201:\tlearn: 0.0015693\ttotal: 11.7s\tremaining: 46.3s\n",
      "202:\tlearn: 0.0015693\ttotal: 11.8s\tremaining: 46.2s\n",
      "203:\tlearn: 0.0015690\ttotal: 11.8s\tremaining: 46.1s\n",
      "204:\tlearn: 0.0015688\ttotal: 11.9s\tremaining: 46.1s\n",
      "205:\tlearn: 0.0015682\ttotal: 11.9s\tremaining: 46s\n",
      "206:\tlearn: 0.0015682\ttotal: 12s\tremaining: 46s\n",
      "207:\tlearn: 0.0015584\ttotal: 12.1s\tremaining: 45.9s\n",
      "208:\tlearn: 0.0015584\ttotal: 12.1s\tremaining: 45.9s\n",
      "209:\tlearn: 0.0015584\ttotal: 12.2s\tremaining: 45.8s\n",
      "210:\tlearn: 0.0015580\ttotal: 12.2s\tremaining: 45.7s\n",
      "211:\tlearn: 0.0015579\ttotal: 12.3s\tremaining: 45.7s\n",
      "212:\tlearn: 0.0015574\ttotal: 12.3s\tremaining: 45.6s\n",
      "213:\tlearn: 0.0015574\ttotal: 12.4s\tremaining: 45.5s\n",
      "214:\tlearn: 0.0015573\ttotal: 12.5s\tremaining: 45.5s\n",
      "215:\tlearn: 0.0015570\ttotal: 12.5s\tremaining: 45.4s\n",
      "216:\tlearn: 0.0015562\ttotal: 12.6s\tremaining: 45.4s\n",
      "217:\tlearn: 0.0015561\ttotal: 12.6s\tremaining: 45.3s\n",
      "218:\tlearn: 0.0015556\ttotal: 12.7s\tremaining: 45.3s\n",
      "219:\tlearn: 0.0015555\ttotal: 12.7s\tremaining: 45.2s\n",
      "220:\tlearn: 0.0015538\ttotal: 12.8s\tremaining: 45.1s\n",
      "221:\tlearn: 0.0015534\ttotal: 12.9s\tremaining: 45.1s\n",
      "222:\tlearn: 0.0015531\ttotal: 12.9s\tremaining: 45s\n",
      "223:\tlearn: 0.0015530\ttotal: 13s\tremaining: 44.9s\n",
      "224:\tlearn: 0.0015530\ttotal: 13s\tremaining: 44.9s\n",
      "225:\tlearn: 0.0015328\ttotal: 13.1s\tremaining: 44.8s\n",
      "226:\tlearn: 0.0015328\ttotal: 13.1s\tremaining: 44.8s\n",
      "227:\tlearn: 0.0015327\ttotal: 13.2s\tremaining: 44.7s\n",
      "228:\tlearn: 0.0015320\ttotal: 13.3s\tremaining: 44.6s\n",
      "229:\tlearn: 0.0015319\ttotal: 13.3s\tremaining: 44.6s\n",
      "230:\tlearn: 0.0015319\ttotal: 13.4s\tremaining: 44.5s\n",
      "231:\tlearn: 0.0015275\ttotal: 13.4s\tremaining: 44.5s\n",
      "232:\tlearn: 0.0015271\ttotal: 13.5s\tremaining: 44.4s\n",
      "233:\tlearn: 0.0015271\ttotal: 13.5s\tremaining: 44.3s\n",
      "234:\tlearn: 0.0015270\ttotal: 13.6s\tremaining: 44.3s\n",
      "235:\tlearn: 0.0015221\ttotal: 13.7s\tremaining: 44.2s\n",
      "236:\tlearn: 0.0015221\ttotal: 13.7s\tremaining: 44.1s\n",
      "237:\tlearn: 0.0015215\ttotal: 13.8s\tremaining: 44.1s\n",
      "238:\tlearn: 0.0015215\ttotal: 13.8s\tremaining: 44s\n",
      "239:\tlearn: 0.0015211\ttotal: 13.9s\tremaining: 44s\n",
      "240:\tlearn: 0.0015211\ttotal: 13.9s\tremaining: 43.9s\n",
      "241:\tlearn: 0.0015211\ttotal: 14s\tremaining: 43.8s\n",
      "242:\tlearn: 0.0015210\ttotal: 14.1s\tremaining: 43.8s\n",
      "243:\tlearn: 0.0015209\ttotal: 14.1s\tremaining: 43.7s\n",
      "244:\tlearn: 0.0015208\ttotal: 14.2s\tremaining: 43.7s\n",
      "245:\tlearn: 0.0015208\ttotal: 14.2s\tremaining: 43.6s\n",
      "246:\tlearn: 0.0015206\ttotal: 14.3s\tremaining: 43.5s\n",
      "247:\tlearn: 0.0015124\ttotal: 14.3s\tremaining: 43.5s\n",
      "248:\tlearn: 0.0015121\ttotal: 14.4s\tremaining: 43.4s\n",
      "249:\tlearn: 0.0015106\ttotal: 14.4s\tremaining: 43.3s\n",
      "250:\tlearn: 0.0015104\ttotal: 14.5s\tremaining: 43.3s\n",
      "251:\tlearn: 0.0015103\ttotal: 14.6s\tremaining: 43.2s\n",
      "252:\tlearn: 0.0015068\ttotal: 14.6s\tremaining: 43.2s\n",
      "253:\tlearn: 0.0015066\ttotal: 14.7s\tremaining: 43.1s\n",
      "254:\tlearn: 0.0015065\ttotal: 14.7s\tremaining: 43s\n",
      "255:\tlearn: 0.0015065\ttotal: 14.8s\tremaining: 43s\n",
      "256:\tlearn: 0.0015064\ttotal: 14.8s\tremaining: 42.9s\n",
      "257:\tlearn: 0.0015064\ttotal: 14.9s\tremaining: 42.9s\n",
      "258:\tlearn: 0.0015062\ttotal: 15s\tremaining: 42.8s\n",
      "259:\tlearn: 0.0015058\ttotal: 15s\tremaining: 42.7s\n",
      "260:\tlearn: 0.0015058\ttotal: 15.1s\tremaining: 42.7s\n",
      "261:\tlearn: 0.0014947\ttotal: 15.1s\tremaining: 42.6s\n",
      "262:\tlearn: 0.0014865\ttotal: 15.2s\tremaining: 42.6s\n",
      "263:\tlearn: 0.0014865\ttotal: 15.2s\tremaining: 42.5s\n",
      "264:\tlearn: 0.0014864\ttotal: 15.3s\tremaining: 42.4s\n",
      "265:\tlearn: 0.0014860\ttotal: 15.4s\tremaining: 42.4s\n",
      "266:\tlearn: 0.0014857\ttotal: 15.4s\tremaining: 42.3s\n",
      "267:\tlearn: 0.0014856\ttotal: 15.5s\tremaining: 42.3s\n",
      "268:\tlearn: 0.0014855\ttotal: 15.5s\tremaining: 42.2s\n",
      "269:\tlearn: 0.0014851\ttotal: 15.6s\tremaining: 42.1s\n",
      "270:\tlearn: 0.0014850\ttotal: 15.6s\tremaining: 42.1s\n",
      "271:\tlearn: 0.0014848\ttotal: 15.7s\tremaining: 42s\n",
      "272:\tlearn: 0.0014848\ttotal: 15.8s\tremaining: 41.9s\n",
      "273:\tlearn: 0.0014847\ttotal: 15.8s\tremaining: 41.9s\n",
      "274:\tlearn: 0.0014847\ttotal: 15.9s\tremaining: 41.8s\n",
      "275:\tlearn: 0.0014841\ttotal: 15.9s\tremaining: 41.8s\n",
      "276:\tlearn: 0.0014841\ttotal: 16s\tremaining: 41.7s\n",
      "277:\tlearn: 0.0014841\ttotal: 16s\tremaining: 41.6s\n",
      "278:\tlearn: 0.0014841\ttotal: 16.1s\tremaining: 41.6s\n",
      "279:\tlearn: 0.0014839\ttotal: 16.1s\tremaining: 41.5s\n",
      "280:\tlearn: 0.0014729\ttotal: 16.2s\tremaining: 41.5s\n",
      "281:\tlearn: 0.0014725\ttotal: 16.3s\tremaining: 41.4s\n",
      "282:\tlearn: 0.0014725\ttotal: 16.3s\tremaining: 41.4s\n",
      "283:\tlearn: 0.0014724\ttotal: 16.4s\tremaining: 41.3s\n",
      "284:\tlearn: 0.0014699\ttotal: 16.4s\tremaining: 41.2s\n",
      "285:\tlearn: 0.0014695\ttotal: 16.5s\tremaining: 41.2s\n",
      "286:\tlearn: 0.0014546\ttotal: 16.5s\tremaining: 41.1s\n",
      "287:\tlearn: 0.0014404\ttotal: 16.6s\tremaining: 41.1s\n",
      "288:\tlearn: 0.0014403\ttotal: 16.7s\tremaining: 41s\n",
      "289:\tlearn: 0.0014403\ttotal: 16.7s\tremaining: 40.9s\n",
      "290:\tlearn: 0.0014402\ttotal: 16.8s\tremaining: 40.9s\n",
      "291:\tlearn: 0.0014399\ttotal: 16.8s\tremaining: 40.8s\n",
      "292:\tlearn: 0.0014399\ttotal: 16.9s\tremaining: 40.8s\n",
      "293:\tlearn: 0.0014397\ttotal: 17s\tremaining: 40.7s\n",
      "294:\tlearn: 0.0014233\ttotal: 17s\tremaining: 40.7s\n",
      "295:\tlearn: 0.0014233\ttotal: 17.1s\tremaining: 40.6s\n",
      "296:\tlearn: 0.0014232\ttotal: 17.1s\tremaining: 40.5s\n",
      "297:\tlearn: 0.0014231\ttotal: 17.2s\tremaining: 40.5s\n",
      "298:\tlearn: 0.0014182\ttotal: 17.2s\tremaining: 40.4s\n",
      "299:\tlearn: 0.0014179\ttotal: 17.3s\tremaining: 40.4s\n",
      "300:\tlearn: 0.0014178\ttotal: 17.4s\tremaining: 40.3s\n",
      "301:\tlearn: 0.0014178\ttotal: 17.4s\tremaining: 40.2s\n",
      "302:\tlearn: 0.0014162\ttotal: 17.5s\tremaining: 40.2s\n",
      "303:\tlearn: 0.0014161\ttotal: 17.5s\tremaining: 40.1s\n",
      "304:\tlearn: 0.0014154\ttotal: 17.6s\tremaining: 40.1s\n",
      "305:\tlearn: 0.0014154\ttotal: 17.6s\tremaining: 40s\n",
      "306:\tlearn: 0.0014153\ttotal: 17.7s\tremaining: 39.9s\n",
      "307:\tlearn: 0.0014151\ttotal: 17.8s\tremaining: 39.9s\n",
      "308:\tlearn: 0.0014151\ttotal: 17.8s\tremaining: 39.8s\n",
      "309:\tlearn: 0.0014094\ttotal: 17.9s\tremaining: 39.8s\n",
      "310:\tlearn: 0.0014017\ttotal: 17.9s\tremaining: 39.7s\n",
      "311:\tlearn: 0.0014016\ttotal: 18s\tremaining: 39.7s\n",
      "312:\tlearn: 0.0014014\ttotal: 18s\tremaining: 39.6s\n",
      "313:\tlearn: 0.0014014\ttotal: 18.1s\tremaining: 39.5s\n",
      "314:\tlearn: 0.0014012\ttotal: 18.2s\tremaining: 39.5s\n",
      "315:\tlearn: 0.0014012\ttotal: 18.2s\tremaining: 39.4s\n",
      "316:\tlearn: 0.0014011\ttotal: 18.3s\tremaining: 39.4s\n",
      "317:\tlearn: 0.0014011\ttotal: 18.3s\tremaining: 39.3s\n",
      "318:\tlearn: 0.0014010\ttotal: 18.4s\tremaining: 39.2s\n",
      "319:\tlearn: 0.0014010\ttotal: 18.5s\tremaining: 39.2s\n",
      "320:\tlearn: 0.0014009\ttotal: 18.5s\tremaining: 39.1s\n",
      "321:\tlearn: 0.0014008\ttotal: 18.6s\tremaining: 39.1s\n",
      "322:\tlearn: 0.0014007\ttotal: 18.6s\tremaining: 39s\n",
      "323:\tlearn: 0.0014006\ttotal: 18.7s\tremaining: 39s\n",
      "324:\tlearn: 0.0014002\ttotal: 18.7s\tremaining: 38.9s\n",
      "325:\tlearn: 0.0014001\ttotal: 18.8s\tremaining: 38.9s\n",
      "326:\tlearn: 0.0013994\ttotal: 18.9s\tremaining: 38.8s\n",
      "327:\tlearn: 0.0013850\ttotal: 18.9s\tremaining: 38.8s\n",
      "328:\tlearn: 0.0013850\ttotal: 19s\tremaining: 38.7s\n",
      "329:\tlearn: 0.0013849\ttotal: 19s\tremaining: 38.6s\n",
      "330:\tlearn: 0.0013847\ttotal: 19.1s\tremaining: 38.6s\n",
      "331:\tlearn: 0.0013847\ttotal: 19.2s\tremaining: 38.5s\n",
      "332:\tlearn: 0.0013842\ttotal: 19.2s\tremaining: 38.5s\n",
      "333:\tlearn: 0.0013842\ttotal: 19.3s\tremaining: 38.4s\n",
      "334:\tlearn: 0.0013841\ttotal: 19.3s\tremaining: 38.4s\n",
      "335:\tlearn: 0.0013838\ttotal: 19.4s\tremaining: 38.3s\n",
      "336:\tlearn: 0.0013823\ttotal: 19.4s\tremaining: 38.3s\n",
      "337:\tlearn: 0.0013823\ttotal: 19.5s\tremaining: 38.2s\n",
      "338:\tlearn: 0.0013822\ttotal: 19.6s\tremaining: 38.1s\n",
      "339:\tlearn: 0.0013822\ttotal: 19.6s\tremaining: 38.1s\n",
      "340:\tlearn: 0.0013798\ttotal: 19.7s\tremaining: 38s\n",
      "341:\tlearn: 0.0013582\ttotal: 19.7s\tremaining: 38s\n",
      "342:\tlearn: 0.0013579\ttotal: 19.8s\tremaining: 37.9s\n",
      "343:\tlearn: 0.0013579\ttotal: 19.8s\tremaining: 37.8s\n",
      "344:\tlearn: 0.0013579\ttotal: 19.9s\tremaining: 37.8s\n",
      "345:\tlearn: 0.0013471\ttotal: 20s\tremaining: 37.7s\n",
      "346:\tlearn: 0.0013470\ttotal: 20s\tremaining: 37.7s\n",
      "347:\tlearn: 0.0013470\ttotal: 20.1s\tremaining: 37.6s\n",
      "348:\tlearn: 0.0013467\ttotal: 20.1s\tremaining: 37.5s\n",
      "349:\tlearn: 0.0013306\ttotal: 20.2s\tremaining: 37.5s\n",
      "350:\tlearn: 0.0013304\ttotal: 20.2s\tremaining: 37.4s\n",
      "351:\tlearn: 0.0013304\ttotal: 20.3s\tremaining: 37.4s\n",
      "352:\tlearn: 0.0013304\ttotal: 20.4s\tremaining: 37.3s\n",
      "353:\tlearn: 0.0013298\ttotal: 20.4s\tremaining: 37.2s\n",
      "354:\tlearn: 0.0013298\ttotal: 20.5s\tremaining: 37.2s\n",
      "355:\tlearn: 0.0013294\ttotal: 20.5s\tremaining: 37.1s\n",
      "356:\tlearn: 0.0013258\ttotal: 20.6s\tremaining: 37.1s\n",
      "357:\tlearn: 0.0013255\ttotal: 20.6s\tremaining: 37s\n",
      "358:\tlearn: 0.0013254\ttotal: 20.7s\tremaining: 37s\n",
      "359:\tlearn: 0.0013149\ttotal: 20.8s\tremaining: 36.9s\n",
      "360:\tlearn: 0.0013146\ttotal: 20.8s\tremaining: 36.8s\n",
      "361:\tlearn: 0.0013143\ttotal: 20.9s\tremaining: 36.8s\n",
      "362:\tlearn: 0.0013143\ttotal: 20.9s\tremaining: 36.7s\n",
      "363:\tlearn: 0.0013142\ttotal: 21s\tremaining: 36.7s\n",
      "364:\tlearn: 0.0013037\ttotal: 21.1s\tremaining: 36.6s\n",
      "365:\tlearn: 0.0013036\ttotal: 21.1s\tremaining: 36.6s\n",
      "366:\tlearn: 0.0013036\ttotal: 21.2s\tremaining: 36.5s\n",
      "367:\tlearn: 0.0013036\ttotal: 21.2s\tremaining: 36.5s\n",
      "368:\tlearn: 0.0013036\ttotal: 21.3s\tremaining: 36.4s\n",
      "369:\tlearn: 0.0012981\ttotal: 21.3s\tremaining: 36.3s\n",
      "370:\tlearn: 0.0012979\ttotal: 21.4s\tremaining: 36.3s\n",
      "371:\tlearn: 0.0012978\ttotal: 21.5s\tremaining: 36.2s\n",
      "372:\tlearn: 0.0012976\ttotal: 21.5s\tremaining: 36.2s\n",
      "373:\tlearn: 0.0012976\ttotal: 21.6s\tremaining: 36.1s\n",
      "374:\tlearn: 0.0012975\ttotal: 21.6s\tremaining: 36.1s\n",
      "375:\tlearn: 0.0012975\ttotal: 21.7s\tremaining: 36s\n",
      "376:\tlearn: 0.0012972\ttotal: 21.7s\tremaining: 35.9s\n",
      "377:\tlearn: 0.0012970\ttotal: 21.8s\tremaining: 35.9s\n",
      "378:\tlearn: 0.0012875\ttotal: 21.9s\tremaining: 35.8s\n",
      "379:\tlearn: 0.0012863\ttotal: 21.9s\tremaining: 35.8s\n",
      "380:\tlearn: 0.0012863\ttotal: 22s\tremaining: 35.7s\n",
      "381:\tlearn: 0.0012863\ttotal: 22s\tremaining: 35.7s\n",
      "382:\tlearn: 0.0012863\ttotal: 22.1s\tremaining: 35.6s\n",
      "383:\tlearn: 0.0012861\ttotal: 22.2s\tremaining: 35.5s\n",
      "384:\tlearn: 0.0012860\ttotal: 22.2s\tremaining: 35.5s\n",
      "385:\tlearn: 0.0012856\ttotal: 22.3s\tremaining: 35.4s\n",
      "386:\tlearn: 0.0012856\ttotal: 22.3s\tremaining: 35.4s\n",
      "387:\tlearn: 0.0012855\ttotal: 22.4s\tremaining: 35.3s\n",
      "388:\tlearn: 0.0012854\ttotal: 22.5s\tremaining: 35.3s\n",
      "389:\tlearn: 0.0012854\ttotal: 22.5s\tremaining: 35.2s\n",
      "390:\tlearn: 0.0012854\ttotal: 22.6s\tremaining: 35.2s\n",
      "391:\tlearn: 0.0012849\ttotal: 22.6s\tremaining: 35.1s\n",
      "392:\tlearn: 0.0012763\ttotal: 22.7s\tremaining: 35s\n",
      "393:\tlearn: 0.0012746\ttotal: 22.7s\tremaining: 35s\n",
      "394:\tlearn: 0.0012744\ttotal: 22.8s\tremaining: 34.9s\n",
      "395:\tlearn: 0.0012669\ttotal: 22.8s\tremaining: 34.9s\n",
      "396:\tlearn: 0.0012667\ttotal: 22.9s\tremaining: 34.8s\n",
      "397:\tlearn: 0.0012663\ttotal: 23s\tremaining: 34.7s\n",
      "398:\tlearn: 0.0012661\ttotal: 23s\tremaining: 34.7s\n",
      "399:\tlearn: 0.0012661\ttotal: 23.1s\tremaining: 34.6s\n",
      "400:\tlearn: 0.0012658\ttotal: 23.1s\tremaining: 34.6s\n",
      "401:\tlearn: 0.0012658\ttotal: 23.2s\tremaining: 34.5s\n",
      "402:\tlearn: 0.0012652\ttotal: 23.2s\tremaining: 34.4s\n",
      "403:\tlearn: 0.0012652\ttotal: 23.3s\tremaining: 34.4s\n",
      "404:\tlearn: 0.0012651\ttotal: 23.4s\tremaining: 34.3s\n",
      "405:\tlearn: 0.0012651\ttotal: 23.4s\tremaining: 34.3s\n",
      "406:\tlearn: 0.0012651\ttotal: 23.5s\tremaining: 34.2s\n",
      "407:\tlearn: 0.0012650\ttotal: 23.5s\tremaining: 34.1s\n",
      "408:\tlearn: 0.0012650\ttotal: 23.6s\tremaining: 34.1s\n",
      "409:\tlearn: 0.0012650\ttotal: 23.6s\tremaining: 34s\n",
      "410:\tlearn: 0.0012649\ttotal: 23.7s\tremaining: 34s\n",
      "411:\tlearn: 0.0012647\ttotal: 23.8s\tremaining: 33.9s\n",
      "412:\tlearn: 0.0012646\ttotal: 23.8s\tremaining: 33.9s\n",
      "413:\tlearn: 0.0012643\ttotal: 23.9s\tremaining: 33.8s\n",
      "414:\tlearn: 0.0012642\ttotal: 23.9s\tremaining: 33.7s\n",
      "415:\tlearn: 0.0012639\ttotal: 24s\tremaining: 33.7s\n",
      "416:\tlearn: 0.0012633\ttotal: 24s\tremaining: 33.6s\n",
      "417:\tlearn: 0.0012596\ttotal: 24.1s\tremaining: 33.6s\n",
      "418:\tlearn: 0.0012595\ttotal: 24.2s\tremaining: 33.5s\n",
      "419:\tlearn: 0.0012593\ttotal: 24.2s\tremaining: 33.4s\n",
      "420:\tlearn: 0.0012593\ttotal: 24.3s\tremaining: 33.4s\n",
      "421:\tlearn: 0.0012592\ttotal: 24.3s\tremaining: 33.3s\n",
      "422:\tlearn: 0.0012590\ttotal: 24.4s\tremaining: 33.3s\n",
      "423:\tlearn: 0.0012590\ttotal: 24.4s\tremaining: 33.2s\n",
      "424:\tlearn: 0.0012587\ttotal: 24.5s\tremaining: 33.2s\n",
      "425:\tlearn: 0.0012585\ttotal: 24.6s\tremaining: 33.1s\n",
      "426:\tlearn: 0.0012585\ttotal: 24.6s\tremaining: 33s\n",
      "427:\tlearn: 0.0012584\ttotal: 24.7s\tremaining: 33s\n",
      "428:\tlearn: 0.0012584\ttotal: 24.7s\tremaining: 32.9s\n",
      "429:\tlearn: 0.0012584\ttotal: 24.8s\tremaining: 32.9s\n",
      "430:\tlearn: 0.0012579\ttotal: 24.8s\tremaining: 32.8s\n",
      "431:\tlearn: 0.0012577\ttotal: 24.9s\tremaining: 32.7s\n",
      "432:\tlearn: 0.0012576\ttotal: 25s\tremaining: 32.7s\n",
      "433:\tlearn: 0.0012576\ttotal: 25s\tremaining: 32.6s\n",
      "434:\tlearn: 0.0012568\ttotal: 25.1s\tremaining: 32.6s\n",
      "435:\tlearn: 0.0012560\ttotal: 25.1s\tremaining: 32.5s\n",
      "436:\tlearn: 0.0012559\ttotal: 25.2s\tremaining: 32.5s\n",
      "437:\tlearn: 0.0012559\ttotal: 25.3s\tremaining: 32.4s\n",
      "438:\tlearn: 0.0012558\ttotal: 25.3s\tremaining: 32.3s\n",
      "439:\tlearn: 0.0012557\ttotal: 25.4s\tremaining: 32.3s\n",
      "440:\tlearn: 0.0012550\ttotal: 25.4s\tremaining: 32.2s\n",
      "441:\tlearn: 0.0012549\ttotal: 25.5s\tremaining: 32.2s\n",
      "442:\tlearn: 0.0012548\ttotal: 25.5s\tremaining: 32.1s\n",
      "443:\tlearn: 0.0012548\ttotal: 25.6s\tremaining: 32s\n",
      "444:\tlearn: 0.0012547\ttotal: 25.6s\tremaining: 32s\n",
      "445:\tlearn: 0.0012545\ttotal: 25.7s\tremaining: 31.9s\n",
      "446:\tlearn: 0.0012545\ttotal: 25.8s\tremaining: 31.9s\n",
      "447:\tlearn: 0.0012491\ttotal: 25.8s\tremaining: 31.8s\n",
      "448:\tlearn: 0.0012491\ttotal: 25.9s\tremaining: 31.8s\n",
      "449:\tlearn: 0.0012491\ttotal: 25.9s\tremaining: 31.7s\n",
      "450:\tlearn: 0.0012489\ttotal: 26s\tremaining: 31.6s\n",
      "451:\tlearn: 0.0012488\ttotal: 26s\tremaining: 31.6s\n",
      "452:\tlearn: 0.0012488\ttotal: 26.1s\tremaining: 31.5s\n",
      "453:\tlearn: 0.0012488\ttotal: 26.2s\tremaining: 31.5s\n",
      "454:\tlearn: 0.0012487\ttotal: 26.2s\tremaining: 31.4s\n",
      "455:\tlearn: 0.0012486\ttotal: 26.3s\tremaining: 31.3s\n",
      "456:\tlearn: 0.0012485\ttotal: 26.3s\tremaining: 31.3s\n",
      "457:\tlearn: 0.0012483\ttotal: 26.4s\tremaining: 31.2s\n",
      "458:\tlearn: 0.0012469\ttotal: 26.4s\tremaining: 31.2s\n",
      "459:\tlearn: 0.0012468\ttotal: 26.5s\tremaining: 31.1s\n",
      "460:\tlearn: 0.0012462\ttotal: 26.6s\tremaining: 31.1s\n",
      "461:\tlearn: 0.0012462\ttotal: 26.6s\tremaining: 31s\n",
      "462:\tlearn: 0.0012457\ttotal: 26.7s\tremaining: 31s\n",
      "463:\tlearn: 0.0012457\ttotal: 26.7s\tremaining: 30.9s\n",
      "464:\tlearn: 0.0012457\ttotal: 26.8s\tremaining: 30.8s\n",
      "465:\tlearn: 0.0012457\ttotal: 26.9s\tremaining: 30.8s\n",
      "466:\tlearn: 0.0012456\ttotal: 26.9s\tremaining: 30.7s\n",
      "467:\tlearn: 0.0012455\ttotal: 27s\tremaining: 30.7s\n",
      "468:\tlearn: 0.0012455\ttotal: 27s\tremaining: 30.6s\n",
      "469:\tlearn: 0.0012454\ttotal: 27.1s\tremaining: 30.5s\n",
      "470:\tlearn: 0.0012454\ttotal: 27.1s\tremaining: 30.5s\n",
      "471:\tlearn: 0.0012454\ttotal: 27.2s\tremaining: 30.4s\n",
      "472:\tlearn: 0.0012437\ttotal: 27.3s\tremaining: 30.4s\n",
      "473:\tlearn: 0.0012435\ttotal: 27.3s\tremaining: 30.3s\n",
      "474:\tlearn: 0.0012434\ttotal: 27.4s\tremaining: 30.3s\n",
      "475:\tlearn: 0.0012434\ttotal: 27.4s\tremaining: 30.2s\n",
      "476:\tlearn: 0.0012430\ttotal: 27.5s\tremaining: 30.1s\n",
      "477:\tlearn: 0.0012430\ttotal: 27.5s\tremaining: 30.1s\n",
      "478:\tlearn: 0.0012429\ttotal: 27.6s\tremaining: 30s\n",
      "479:\tlearn: 0.0012428\ttotal: 27.7s\tremaining: 30s\n",
      "480:\tlearn: 0.0012427\ttotal: 27.7s\tremaining: 29.9s\n",
      "481:\tlearn: 0.0012427\ttotal: 27.8s\tremaining: 29.9s\n",
      "482:\tlearn: 0.0012427\ttotal: 27.8s\tremaining: 29.8s\n",
      "483:\tlearn: 0.0012425\ttotal: 27.9s\tremaining: 29.7s\n",
      "484:\tlearn: 0.0012425\ttotal: 28s\tremaining: 29.7s\n",
      "485:\tlearn: 0.0012424\ttotal: 28s\tremaining: 29.6s\n",
      "486:\tlearn: 0.0012423\ttotal: 28.1s\tremaining: 29.6s\n",
      "487:\tlearn: 0.0012418\ttotal: 28.1s\tremaining: 29.5s\n",
      "488:\tlearn: 0.0012414\ttotal: 28.2s\tremaining: 29.4s\n",
      "489:\tlearn: 0.0012412\ttotal: 28.2s\tremaining: 29.4s\n",
      "490:\tlearn: 0.0012411\ttotal: 28.3s\tremaining: 29.3s\n",
      "491:\tlearn: 0.0012386\ttotal: 28.3s\tremaining: 29.3s\n",
      "492:\tlearn: 0.0012386\ttotal: 28.4s\tremaining: 29.2s\n",
      "493:\tlearn: 0.0012385\ttotal: 28.5s\tremaining: 29.2s\n",
      "494:\tlearn: 0.0012385\ttotal: 28.5s\tremaining: 29.1s\n",
      "495:\tlearn: 0.0012383\ttotal: 28.6s\tremaining: 29.1s\n",
      "496:\tlearn: 0.0012383\ttotal: 28.6s\tremaining: 29s\n",
      "497:\tlearn: 0.0012380\ttotal: 28.7s\tremaining: 28.9s\n",
      "498:\tlearn: 0.0012285\ttotal: 28.8s\tremaining: 28.9s\n",
      "499:\tlearn: 0.0012282\ttotal: 28.8s\tremaining: 28.8s\n",
      "500:\tlearn: 0.0012282\ttotal: 28.9s\tremaining: 28.8s\n",
      "501:\tlearn: 0.0012227\ttotal: 28.9s\tremaining: 28.7s\n",
      "502:\tlearn: 0.0012224\ttotal: 29s\tremaining: 28.6s\n",
      "503:\tlearn: 0.0012224\ttotal: 29s\tremaining: 28.6s\n",
      "504:\tlearn: 0.0012222\ttotal: 29.1s\tremaining: 28.5s\n",
      "505:\tlearn: 0.0012222\ttotal: 29.2s\tremaining: 28.5s\n",
      "506:\tlearn: 0.0012222\ttotal: 29.2s\tremaining: 28.4s\n",
      "507:\tlearn: 0.0012220\ttotal: 29.3s\tremaining: 28.4s\n",
      "508:\tlearn: 0.0012217\ttotal: 29.3s\tremaining: 28.3s\n",
      "509:\tlearn: 0.0012217\ttotal: 29.4s\tremaining: 28.2s\n",
      "510:\tlearn: 0.0012217\ttotal: 29.5s\tremaining: 28.2s\n",
      "511:\tlearn: 0.0012216\ttotal: 29.5s\tremaining: 28.1s\n",
      "512:\tlearn: 0.0012216\ttotal: 29.6s\tremaining: 28.1s\n",
      "513:\tlearn: 0.0012215\ttotal: 29.6s\tremaining: 28s\n",
      "514:\tlearn: 0.0012215\ttotal: 29.7s\tremaining: 27.9s\n",
      "515:\tlearn: 0.0012214\ttotal: 29.7s\tremaining: 27.9s\n",
      "516:\tlearn: 0.0012204\ttotal: 29.8s\tremaining: 27.8s\n",
      "517:\tlearn: 0.0012204\ttotal: 29.9s\tremaining: 27.8s\n",
      "518:\tlearn: 0.0012203\ttotal: 29.9s\tremaining: 27.7s\n",
      "519:\tlearn: 0.0012203\ttotal: 30s\tremaining: 27.7s\n",
      "520:\tlearn: 0.0012202\ttotal: 30s\tremaining: 27.6s\n",
      "521:\tlearn: 0.0012201\ttotal: 30.1s\tremaining: 27.5s\n",
      "522:\tlearn: 0.0012201\ttotal: 30.1s\tremaining: 27.5s\n",
      "523:\tlearn: 0.0012201\ttotal: 30.2s\tremaining: 27.4s\n",
      "524:\tlearn: 0.0012198\ttotal: 30.3s\tremaining: 27.4s\n",
      "525:\tlearn: 0.0012198\ttotal: 30.3s\tremaining: 27.3s\n",
      "526:\tlearn: 0.0012197\ttotal: 30.4s\tremaining: 27.3s\n",
      "527:\tlearn: 0.0012197\ttotal: 30.4s\tremaining: 27.2s\n",
      "528:\tlearn: 0.0012196\ttotal: 30.5s\tremaining: 27.1s\n",
      "529:\tlearn: 0.0012194\ttotal: 30.5s\tremaining: 27.1s\n",
      "530:\tlearn: 0.0012194\ttotal: 30.6s\tremaining: 27s\n",
      "531:\tlearn: 0.0012193\ttotal: 30.7s\tremaining: 27s\n",
      "532:\tlearn: 0.0012186\ttotal: 30.7s\tremaining: 26.9s\n",
      "533:\tlearn: 0.0012085\ttotal: 30.8s\tremaining: 26.9s\n",
      "534:\tlearn: 0.0012084\ttotal: 30.8s\tremaining: 26.8s\n",
      "535:\tlearn: 0.0012083\ttotal: 30.9s\tremaining: 26.7s\n",
      "536:\tlearn: 0.0012082\ttotal: 30.9s\tremaining: 26.7s\n",
      "537:\tlearn: 0.0012068\ttotal: 31s\tremaining: 26.6s\n",
      "538:\tlearn: 0.0012064\ttotal: 31.1s\tremaining: 26.6s\n",
      "539:\tlearn: 0.0012063\ttotal: 31.1s\tremaining: 26.5s\n",
      "540:\tlearn: 0.0012063\ttotal: 31.2s\tremaining: 26.4s\n",
      "541:\tlearn: 0.0012062\ttotal: 31.2s\tremaining: 26.4s\n",
      "542:\tlearn: 0.0012062\ttotal: 31.3s\tremaining: 26.3s\n",
      "543:\tlearn: 0.0012054\ttotal: 31.3s\tremaining: 26.3s\n",
      "544:\tlearn: 0.0012051\ttotal: 31.4s\tremaining: 26.2s\n",
      "545:\tlearn: 0.0012051\ttotal: 31.5s\tremaining: 26.2s\n",
      "546:\tlearn: 0.0012041\ttotal: 31.5s\tremaining: 26.1s\n",
      "547:\tlearn: 0.0012036\ttotal: 31.6s\tremaining: 26s\n",
      "548:\tlearn: 0.0012036\ttotal: 31.6s\tremaining: 26s\n",
      "549:\tlearn: 0.0012034\ttotal: 31.7s\tremaining: 25.9s\n",
      "550:\tlearn: 0.0012030\ttotal: 31.7s\tremaining: 25.9s\n",
      "551:\tlearn: 0.0012029\ttotal: 31.8s\tremaining: 25.8s\n",
      "552:\tlearn: 0.0012028\ttotal: 31.9s\tremaining: 25.8s\n",
      "553:\tlearn: 0.0012028\ttotal: 31.9s\tremaining: 25.7s\n",
      "554:\tlearn: 0.0012028\ttotal: 32s\tremaining: 25.6s\n",
      "555:\tlearn: 0.0012028\ttotal: 32s\tremaining: 25.6s\n",
      "556:\tlearn: 0.0012009\ttotal: 32.1s\tremaining: 25.5s\n",
      "557:\tlearn: 0.0012009\ttotal: 32.1s\tremaining: 25.5s\n",
      "558:\tlearn: 0.0012004\ttotal: 32.2s\tremaining: 25.4s\n",
      "559:\tlearn: 0.0012004\ttotal: 32.3s\tremaining: 25.3s\n",
      "560:\tlearn: 0.0011999\ttotal: 32.3s\tremaining: 25.3s\n",
      "561:\tlearn: 0.0011998\ttotal: 32.4s\tremaining: 25.2s\n",
      "562:\tlearn: 0.0011986\ttotal: 32.4s\tremaining: 25.2s\n",
      "563:\tlearn: 0.0011984\ttotal: 32.5s\tremaining: 25.1s\n",
      "564:\tlearn: 0.0011984\ttotal: 32.6s\tremaining: 25.1s\n",
      "565:\tlearn: 0.0011983\ttotal: 32.6s\tremaining: 25s\n",
      "566:\tlearn: 0.0011979\ttotal: 32.7s\tremaining: 24.9s\n",
      "567:\tlearn: 0.0011979\ttotal: 32.7s\tremaining: 24.9s\n",
      "568:\tlearn: 0.0011978\ttotal: 32.8s\tremaining: 24.8s\n",
      "569:\tlearn: 0.0011977\ttotal: 32.8s\tremaining: 24.8s\n",
      "570:\tlearn: 0.0011976\ttotal: 32.9s\tremaining: 24.7s\n",
      "571:\tlearn: 0.0011933\ttotal: 33s\tremaining: 24.7s\n",
      "572:\tlearn: 0.0011932\ttotal: 33s\tremaining: 24.6s\n",
      "573:\tlearn: 0.0011932\ttotal: 33.1s\tremaining: 24.5s\n",
      "574:\tlearn: 0.0011931\ttotal: 33.1s\tremaining: 24.5s\n",
      "575:\tlearn: 0.0011918\ttotal: 33.2s\tremaining: 24.4s\n",
      "576:\tlearn: 0.0011917\ttotal: 33.2s\tremaining: 24.4s\n",
      "577:\tlearn: 0.0011900\ttotal: 33.3s\tremaining: 24.3s\n",
      "578:\tlearn: 0.0011896\ttotal: 33.4s\tremaining: 24.3s\n",
      "579:\tlearn: 0.0011895\ttotal: 33.4s\tremaining: 24.2s\n",
      "580:\tlearn: 0.0011895\ttotal: 33.5s\tremaining: 24.1s\n",
      "581:\tlearn: 0.0011894\ttotal: 33.5s\tremaining: 24.1s\n",
      "582:\tlearn: 0.0011884\ttotal: 33.6s\tremaining: 24s\n",
      "583:\tlearn: 0.0011883\ttotal: 33.6s\tremaining: 24s\n",
      "584:\tlearn: 0.0011882\ttotal: 33.7s\tremaining: 23.9s\n",
      "585:\tlearn: 0.0011871\ttotal: 33.8s\tremaining: 23.8s\n",
      "586:\tlearn: 0.0011871\ttotal: 33.8s\tremaining: 23.8s\n",
      "587:\tlearn: 0.0011870\ttotal: 33.9s\tremaining: 23.7s\n",
      "588:\tlearn: 0.0011869\ttotal: 33.9s\tremaining: 23.7s\n",
      "589:\tlearn: 0.0011869\ttotal: 34s\tremaining: 23.6s\n",
      "590:\tlearn: 0.0011852\ttotal: 34s\tremaining: 23.6s\n",
      "591:\tlearn: 0.0011849\ttotal: 34.1s\tremaining: 23.5s\n",
      "592:\tlearn: 0.0011763\ttotal: 34.2s\tremaining: 23.4s\n",
      "593:\tlearn: 0.0011763\ttotal: 34.2s\tremaining: 23.4s\n",
      "594:\tlearn: 0.0011763\ttotal: 34.3s\tremaining: 23.3s\n",
      "595:\tlearn: 0.0011763\ttotal: 34.3s\tremaining: 23.3s\n",
      "596:\tlearn: 0.0011720\ttotal: 34.4s\tremaining: 23.2s\n",
      "597:\tlearn: 0.0011720\ttotal: 34.4s\tremaining: 23.2s\n",
      "598:\tlearn: 0.0011719\ttotal: 34.5s\tremaining: 23.1s\n",
      "599:\tlearn: 0.0011719\ttotal: 34.6s\tremaining: 23s\n",
      "600:\tlearn: 0.0011719\ttotal: 34.6s\tremaining: 23s\n",
      "601:\tlearn: 0.0011717\ttotal: 34.7s\tremaining: 22.9s\n",
      "602:\tlearn: 0.0011717\ttotal: 34.7s\tremaining: 22.9s\n",
      "603:\tlearn: 0.0011676\ttotal: 34.8s\tremaining: 22.8s\n",
      "604:\tlearn: 0.0011676\ttotal: 34.8s\tremaining: 22.8s\n",
      "605:\tlearn: 0.0011675\ttotal: 34.9s\tremaining: 22.7s\n",
      "606:\tlearn: 0.0011674\ttotal: 35s\tremaining: 22.6s\n",
      "607:\tlearn: 0.0011674\ttotal: 35s\tremaining: 22.6s\n",
      "608:\tlearn: 0.0011672\ttotal: 35.1s\tremaining: 22.5s\n",
      "609:\tlearn: 0.0011670\ttotal: 35.1s\tremaining: 22.5s\n",
      "610:\tlearn: 0.0011669\ttotal: 35.2s\tremaining: 22.4s\n",
      "611:\tlearn: 0.0011666\ttotal: 35.2s\tremaining: 22.3s\n",
      "612:\tlearn: 0.0011665\ttotal: 35.3s\tremaining: 22.3s\n",
      "613:\tlearn: 0.0011663\ttotal: 35.4s\tremaining: 22.2s\n",
      "614:\tlearn: 0.0011662\ttotal: 35.4s\tremaining: 22.2s\n",
      "615:\tlearn: 0.0011661\ttotal: 35.5s\tremaining: 22.1s\n",
      "616:\tlearn: 0.0011661\ttotal: 35.5s\tremaining: 22.1s\n",
      "617:\tlearn: 0.0011651\ttotal: 35.6s\tremaining: 22s\n",
      "618:\tlearn: 0.0011649\ttotal: 35.6s\tremaining: 21.9s\n",
      "619:\tlearn: 0.0011647\ttotal: 35.7s\tremaining: 21.9s\n",
      "620:\tlearn: 0.0011647\ttotal: 35.8s\tremaining: 21.8s\n",
      "621:\tlearn: 0.0011647\ttotal: 35.8s\tremaining: 21.8s\n",
      "622:\tlearn: 0.0011646\ttotal: 35.9s\tremaining: 21.7s\n",
      "623:\tlearn: 0.0011644\ttotal: 35.9s\tremaining: 21.7s\n",
      "624:\tlearn: 0.0011638\ttotal: 36s\tremaining: 21.6s\n",
      "625:\tlearn: 0.0011622\ttotal: 36s\tremaining: 21.5s\n",
      "626:\tlearn: 0.0011620\ttotal: 36.1s\tremaining: 21.5s\n",
      "627:\tlearn: 0.0011619\ttotal: 36.2s\tremaining: 21.4s\n",
      "628:\tlearn: 0.0011618\ttotal: 36.2s\tremaining: 21.4s\n",
      "629:\tlearn: 0.0011618\ttotal: 36.3s\tremaining: 21.3s\n",
      "630:\tlearn: 0.0011614\ttotal: 36.3s\tremaining: 21.3s\n",
      "631:\tlearn: 0.0011614\ttotal: 36.4s\tremaining: 21.2s\n",
      "632:\tlearn: 0.0011588\ttotal: 36.5s\tremaining: 21.1s\n",
      "633:\tlearn: 0.0011573\ttotal: 36.5s\tremaining: 21.1s\n",
      "634:\tlearn: 0.0011573\ttotal: 36.6s\tremaining: 21s\n",
      "635:\tlearn: 0.0011572\ttotal: 36.6s\tremaining: 21s\n",
      "636:\tlearn: 0.0011571\ttotal: 36.7s\tremaining: 20.9s\n",
      "637:\tlearn: 0.0011571\ttotal: 36.8s\tremaining: 20.9s\n",
      "638:\tlearn: 0.0011571\ttotal: 36.8s\tremaining: 20.8s\n",
      "639:\tlearn: 0.0011569\ttotal: 36.9s\tremaining: 20.7s\n",
      "640:\tlearn: 0.0011564\ttotal: 36.9s\tremaining: 20.7s\n",
      "641:\tlearn: 0.0011562\ttotal: 37s\tremaining: 20.6s\n",
      "642:\tlearn: 0.0011562\ttotal: 37s\tremaining: 20.6s\n",
      "643:\tlearn: 0.0011561\ttotal: 37.1s\tremaining: 20.5s\n",
      "644:\tlearn: 0.0011561\ttotal: 37.2s\tremaining: 20.5s\n",
      "645:\tlearn: 0.0011561\ttotal: 37.2s\tremaining: 20.4s\n",
      "646:\tlearn: 0.0011552\ttotal: 37.3s\tremaining: 20.3s\n",
      "647:\tlearn: 0.0011551\ttotal: 37.3s\tremaining: 20.3s\n",
      "648:\tlearn: 0.0011551\ttotal: 37.4s\tremaining: 20.2s\n",
      "649:\tlearn: 0.0011551\ttotal: 37.5s\tremaining: 20.2s\n",
      "650:\tlearn: 0.0011551\ttotal: 37.5s\tremaining: 20.1s\n",
      "651:\tlearn: 0.0011551\ttotal: 37.6s\tremaining: 20s\n",
      "652:\tlearn: 0.0011551\ttotal: 37.6s\tremaining: 20s\n",
      "653:\tlearn: 0.0011550\ttotal: 37.7s\tremaining: 19.9s\n",
      "654:\tlearn: 0.0011549\ttotal: 37.7s\tremaining: 19.9s\n",
      "655:\tlearn: 0.0011549\ttotal: 37.8s\tremaining: 19.8s\n",
      "656:\tlearn: 0.0011548\ttotal: 37.9s\tremaining: 19.8s\n",
      "657:\tlearn: 0.0011547\ttotal: 37.9s\tremaining: 19.7s\n",
      "658:\tlearn: 0.0011545\ttotal: 38s\tremaining: 19.6s\n",
      "659:\tlearn: 0.0011544\ttotal: 38s\tremaining: 19.6s\n",
      "660:\tlearn: 0.0011542\ttotal: 38.1s\tremaining: 19.5s\n",
      "661:\tlearn: 0.0011542\ttotal: 38.1s\tremaining: 19.5s\n",
      "662:\tlearn: 0.0011540\ttotal: 38.2s\tremaining: 19.4s\n",
      "663:\tlearn: 0.0011538\ttotal: 38.3s\tremaining: 19.4s\n",
      "664:\tlearn: 0.0011537\ttotal: 38.3s\tremaining: 19.3s\n",
      "665:\tlearn: 0.0011536\ttotal: 38.4s\tremaining: 19.2s\n",
      "666:\tlearn: 0.0011536\ttotal: 38.4s\tremaining: 19.2s\n",
      "667:\tlearn: 0.0011534\ttotal: 38.5s\tremaining: 19.1s\n",
      "668:\tlearn: 0.0011532\ttotal: 38.5s\tremaining: 19.1s\n",
      "669:\tlearn: 0.0011531\ttotal: 38.6s\tremaining: 19s\n",
      "670:\tlearn: 0.0011530\ttotal: 38.7s\tremaining: 19s\n",
      "671:\tlearn: 0.0011528\ttotal: 38.7s\tremaining: 18.9s\n",
      "672:\tlearn: 0.0011526\ttotal: 38.8s\tremaining: 18.8s\n",
      "673:\tlearn: 0.0011526\ttotal: 38.8s\tremaining: 18.8s\n",
      "674:\tlearn: 0.0011526\ttotal: 38.9s\tremaining: 18.7s\n",
      "675:\tlearn: 0.0011526\ttotal: 38.9s\tremaining: 18.7s\n",
      "676:\tlearn: 0.0011526\ttotal: 39s\tremaining: 18.6s\n",
      "677:\tlearn: 0.0011524\ttotal: 39.1s\tremaining: 18.6s\n",
      "678:\tlearn: 0.0011523\ttotal: 39.1s\tremaining: 18.5s\n",
      "679:\tlearn: 0.0011522\ttotal: 39.2s\tremaining: 18.4s\n",
      "680:\tlearn: 0.0011521\ttotal: 39.2s\tremaining: 18.4s\n",
      "681:\tlearn: 0.0011521\ttotal: 39.3s\tremaining: 18.3s\n",
      "682:\tlearn: 0.0011519\ttotal: 39.3s\tremaining: 18.3s\n",
      "683:\tlearn: 0.0011519\ttotal: 39.4s\tremaining: 18.2s\n",
      "684:\tlearn: 0.0011518\ttotal: 39.5s\tremaining: 18.1s\n",
      "685:\tlearn: 0.0011518\ttotal: 39.5s\tremaining: 18.1s\n",
      "686:\tlearn: 0.0011518\ttotal: 39.6s\tremaining: 18s\n",
      "687:\tlearn: 0.0011517\ttotal: 39.7s\tremaining: 18s\n",
      "688:\tlearn: 0.0011515\ttotal: 39.7s\tremaining: 17.9s\n",
      "689:\tlearn: 0.0011514\ttotal: 39.8s\tremaining: 17.9s\n",
      "690:\tlearn: 0.0011513\ttotal: 39.9s\tremaining: 17.8s\n",
      "691:\tlearn: 0.0011511\ttotal: 40s\tremaining: 17.8s\n",
      "692:\tlearn: 0.0011506\ttotal: 40s\tremaining: 17.7s\n",
      "693:\tlearn: 0.0011505\ttotal: 40.1s\tremaining: 17.7s\n",
      "694:\tlearn: 0.0011505\ttotal: 40.2s\tremaining: 17.6s\n",
      "695:\tlearn: 0.0011503\ttotal: 40.2s\tremaining: 17.6s\n",
      "696:\tlearn: 0.0011502\ttotal: 40.3s\tremaining: 17.5s\n",
      "697:\tlearn: 0.0011501\ttotal: 40.3s\tremaining: 17.5s\n",
      "698:\tlearn: 0.0011454\ttotal: 40.4s\tremaining: 17.4s\n",
      "699:\tlearn: 0.0011453\ttotal: 40.5s\tremaining: 17.3s\n",
      "700:\tlearn: 0.0011453\ttotal: 40.5s\tremaining: 17.3s\n",
      "701:\tlearn: 0.0011453\ttotal: 40.6s\tremaining: 17.2s\n",
      "702:\tlearn: 0.0011442\ttotal: 40.6s\tremaining: 17.2s\n",
      "703:\tlearn: 0.0011442\ttotal: 40.7s\tremaining: 17.1s\n",
      "704:\tlearn: 0.0011442\ttotal: 40.7s\tremaining: 17s\n",
      "705:\tlearn: 0.0011441\ttotal: 40.8s\tremaining: 17s\n",
      "706:\tlearn: 0.0011439\ttotal: 40.9s\tremaining: 16.9s\n",
      "707:\tlearn: 0.0011439\ttotal: 40.9s\tremaining: 16.9s\n",
      "708:\tlearn: 0.0011439\ttotal: 41s\tremaining: 16.8s\n",
      "709:\tlearn: 0.0011439\ttotal: 41s\tremaining: 16.8s\n",
      "710:\tlearn: 0.0011435\ttotal: 41.1s\tremaining: 16.7s\n",
      "711:\tlearn: 0.0011431\ttotal: 41.1s\tremaining: 16.6s\n",
      "712:\tlearn: 0.0011428\ttotal: 41.2s\tremaining: 16.6s\n",
      "713:\tlearn: 0.0011427\ttotal: 41.3s\tremaining: 16.5s\n",
      "714:\tlearn: 0.0011425\ttotal: 41.3s\tremaining: 16.5s\n",
      "715:\tlearn: 0.0011425\ttotal: 41.4s\tremaining: 16.4s\n",
      "716:\tlearn: 0.0011425\ttotal: 41.4s\tremaining: 16.4s\n",
      "717:\tlearn: 0.0011425\ttotal: 41.5s\tremaining: 16.3s\n",
      "718:\tlearn: 0.0011424\ttotal: 41.5s\tremaining: 16.2s\n",
      "719:\tlearn: 0.0011294\ttotal: 41.6s\tremaining: 16.2s\n",
      "720:\tlearn: 0.0011294\ttotal: 41.7s\tremaining: 16.1s\n",
      "721:\tlearn: 0.0011293\ttotal: 41.7s\tremaining: 16.1s\n",
      "722:\tlearn: 0.0011292\ttotal: 41.8s\tremaining: 16s\n",
      "723:\tlearn: 0.0011292\ttotal: 41.8s\tremaining: 15.9s\n",
      "724:\tlearn: 0.0011292\ttotal: 41.9s\tremaining: 15.9s\n",
      "725:\tlearn: 0.0011290\ttotal: 42s\tremaining: 15.8s\n",
      "726:\tlearn: 0.0011289\ttotal: 42s\tremaining: 15.8s\n",
      "727:\tlearn: 0.0011287\ttotal: 42.1s\tremaining: 15.7s\n",
      "728:\tlearn: 0.0011286\ttotal: 42.2s\tremaining: 15.7s\n",
      "729:\tlearn: 0.0011285\ttotal: 42.2s\tremaining: 15.6s\n",
      "730:\tlearn: 0.0011283\ttotal: 42.3s\tremaining: 15.6s\n",
      "731:\tlearn: 0.0011283\ttotal: 42.4s\tremaining: 15.5s\n",
      "732:\tlearn: 0.0011283\ttotal: 42.5s\tremaining: 15.5s\n",
      "733:\tlearn: 0.0011283\ttotal: 42.5s\tremaining: 15.4s\n",
      "734:\tlearn: 0.0011283\ttotal: 42.6s\tremaining: 15.4s\n",
      "735:\tlearn: 0.0011283\ttotal: 42.7s\tremaining: 15.3s\n",
      "736:\tlearn: 0.0011282\ttotal: 42.8s\tremaining: 15.3s\n",
      "737:\tlearn: 0.0011282\ttotal: 42.8s\tremaining: 15.2s\n",
      "738:\tlearn: 0.0011281\ttotal: 42.9s\tremaining: 15.2s\n",
      "739:\tlearn: 0.0011281\ttotal: 43s\tremaining: 15.1s\n",
      "740:\tlearn: 0.0011281\ttotal: 43.1s\tremaining: 15s\n",
      "741:\tlearn: 0.0011270\ttotal: 43.1s\tremaining: 15s\n",
      "742:\tlearn: 0.0011264\ttotal: 43.2s\tremaining: 14.9s\n",
      "743:\tlearn: 0.0011264\ttotal: 43.3s\tremaining: 14.9s\n",
      "744:\tlearn: 0.0011264\ttotal: 43.4s\tremaining: 14.8s\n",
      "745:\tlearn: 0.0011264\ttotal: 43.4s\tremaining: 14.8s\n",
      "746:\tlearn: 0.0011263\ttotal: 43.5s\tremaining: 14.7s\n",
      "747:\tlearn: 0.0011247\ttotal: 43.6s\tremaining: 14.7s\n",
      "748:\tlearn: 0.0011114\ttotal: 43.6s\tremaining: 14.6s\n",
      "749:\tlearn: 0.0011074\ttotal: 43.7s\tremaining: 14.6s\n",
      "750:\tlearn: 0.0011065\ttotal: 43.8s\tremaining: 14.5s\n",
      "751:\tlearn: 0.0011065\ttotal: 43.9s\tremaining: 14.5s\n",
      "752:\tlearn: 0.0011064\ttotal: 43.9s\tremaining: 14.4s\n",
      "753:\tlearn: 0.0011059\ttotal: 44s\tremaining: 14.4s\n",
      "754:\tlearn: 0.0011057\ttotal: 44.1s\tremaining: 14.3s\n",
      "755:\tlearn: 0.0011054\ttotal: 44.2s\tremaining: 14.3s\n",
      "756:\tlearn: 0.0011054\ttotal: 44.2s\tremaining: 14.2s\n",
      "757:\tlearn: 0.0011054\ttotal: 44.3s\tremaining: 14.1s\n",
      "758:\tlearn: 0.0011052\ttotal: 44.4s\tremaining: 14.1s\n",
      "759:\tlearn: 0.0011050\ttotal: 44.5s\tremaining: 14s\n",
      "760:\tlearn: 0.0011048\ttotal: 44.5s\tremaining: 14s\n",
      "761:\tlearn: 0.0011044\ttotal: 44.6s\tremaining: 13.9s\n",
      "762:\tlearn: 0.0011042\ttotal: 44.7s\tremaining: 13.9s\n",
      "763:\tlearn: 0.0011041\ttotal: 44.7s\tremaining: 13.8s\n",
      "764:\tlearn: 0.0011041\ttotal: 44.8s\tremaining: 13.8s\n",
      "765:\tlearn: 0.0011033\ttotal: 44.9s\tremaining: 13.7s\n",
      "766:\tlearn: 0.0011031\ttotal: 44.9s\tremaining: 13.7s\n",
      "767:\tlearn: 0.0011027\ttotal: 45s\tremaining: 13.6s\n",
      "768:\tlearn: 0.0011026\ttotal: 45.1s\tremaining: 13.5s\n",
      "769:\tlearn: 0.0011024\ttotal: 45.1s\tremaining: 13.5s\n",
      "770:\tlearn: 0.0011018\ttotal: 45.2s\tremaining: 13.4s\n",
      "771:\tlearn: 0.0011018\ttotal: 45.2s\tremaining: 13.4s\n",
      "772:\tlearn: 0.0011016\ttotal: 45.3s\tremaining: 13.3s\n",
      "773:\tlearn: 0.0011012\ttotal: 45.3s\tremaining: 13.2s\n",
      "774:\tlearn: 0.0011011\ttotal: 45.4s\tremaining: 13.2s\n",
      "775:\tlearn: 0.0011011\ttotal: 45.5s\tremaining: 13.1s\n",
      "776:\tlearn: 0.0011011\ttotal: 45.5s\tremaining: 13.1s\n",
      "777:\tlearn: 0.0011004\ttotal: 45.6s\tremaining: 13s\n",
      "778:\tlearn: 0.0011004\ttotal: 45.6s\tremaining: 12.9s\n",
      "779:\tlearn: 0.0011004\ttotal: 45.7s\tremaining: 12.9s\n",
      "780:\tlearn: 0.0011003\ttotal: 45.7s\tremaining: 12.8s\n",
      "781:\tlearn: 0.0011001\ttotal: 45.8s\tremaining: 12.8s\n",
      "782:\tlearn: 0.0010996\ttotal: 45.9s\tremaining: 12.7s\n",
      "783:\tlearn: 0.0010996\ttotal: 45.9s\tremaining: 12.6s\n",
      "784:\tlearn: 0.0010995\ttotal: 46s\tremaining: 12.6s\n",
      "785:\tlearn: 0.0010991\ttotal: 46s\tremaining: 12.5s\n",
      "786:\tlearn: 0.0010988\ttotal: 46.1s\tremaining: 12.5s\n",
      "787:\tlearn: 0.0010987\ttotal: 46.1s\tremaining: 12.4s\n",
      "788:\tlearn: 0.0010987\ttotal: 46.2s\tremaining: 12.4s\n",
      "789:\tlearn: 0.0010986\ttotal: 46.3s\tremaining: 12.3s\n",
      "790:\tlearn: 0.0010985\ttotal: 46.3s\tremaining: 12.2s\n",
      "791:\tlearn: 0.0010985\ttotal: 46.4s\tremaining: 12.2s\n",
      "792:\tlearn: 0.0010974\ttotal: 46.4s\tremaining: 12.1s\n",
      "793:\tlearn: 0.0010967\ttotal: 46.5s\tremaining: 12.1s\n",
      "794:\tlearn: 0.0010967\ttotal: 46.5s\tremaining: 12s\n",
      "795:\tlearn: 0.0010935\ttotal: 46.6s\tremaining: 11.9s\n",
      "796:\tlearn: 0.0010934\ttotal: 46.7s\tremaining: 11.9s\n",
      "797:\tlearn: 0.0010934\ttotal: 46.7s\tremaining: 11.8s\n",
      "798:\tlearn: 0.0010927\ttotal: 46.8s\tremaining: 11.8s\n",
      "799:\tlearn: 0.0010927\ttotal: 46.8s\tremaining: 11.7s\n",
      "800:\tlearn: 0.0010927\ttotal: 46.9s\tremaining: 11.6s\n",
      "801:\tlearn: 0.0010927\ttotal: 46.9s\tremaining: 11.6s\n",
      "802:\tlearn: 0.0010926\ttotal: 47s\tremaining: 11.5s\n",
      "803:\tlearn: 0.0010925\ttotal: 47.1s\tremaining: 11.5s\n",
      "804:\tlearn: 0.0010922\ttotal: 47.1s\tremaining: 11.4s\n",
      "805:\tlearn: 0.0010919\ttotal: 47.2s\tremaining: 11.4s\n",
      "806:\tlearn: 0.0010919\ttotal: 47.2s\tremaining: 11.3s\n",
      "807:\tlearn: 0.0010919\ttotal: 47.3s\tremaining: 11.2s\n",
      "808:\tlearn: 0.0010919\ttotal: 47.3s\tremaining: 11.2s\n",
      "809:\tlearn: 0.0010914\ttotal: 47.4s\tremaining: 11.1s\n",
      "810:\tlearn: 0.0010913\ttotal: 47.5s\tremaining: 11.1s\n",
      "811:\tlearn: 0.0010911\ttotal: 47.5s\tremaining: 11s\n",
      "812:\tlearn: 0.0010910\ttotal: 47.6s\tremaining: 10.9s\n",
      "813:\tlearn: 0.0010909\ttotal: 47.6s\tremaining: 10.9s\n",
      "814:\tlearn: 0.0010906\ttotal: 47.7s\tremaining: 10.8s\n",
      "815:\tlearn: 0.0010900\ttotal: 47.7s\tremaining: 10.8s\n",
      "816:\tlearn: 0.0010900\ttotal: 47.8s\tremaining: 10.7s\n",
      "817:\tlearn: 0.0010897\ttotal: 47.9s\tremaining: 10.6s\n",
      "818:\tlearn: 0.0010897\ttotal: 47.9s\tremaining: 10.6s\n",
      "819:\tlearn: 0.0010895\ttotal: 48s\tremaining: 10.5s\n",
      "820:\tlearn: 0.0010895\ttotal: 48s\tremaining: 10.5s\n",
      "821:\tlearn: 0.0010895\ttotal: 48.1s\tremaining: 10.4s\n",
      "822:\tlearn: 0.0010891\ttotal: 48.2s\tremaining: 10.4s\n",
      "823:\tlearn: 0.0010889\ttotal: 48.2s\tremaining: 10.3s\n",
      "824:\tlearn: 0.0010889\ttotal: 48.3s\tremaining: 10.2s\n",
      "825:\tlearn: 0.0010889\ttotal: 48.3s\tremaining: 10.2s\n",
      "826:\tlearn: 0.0010888\ttotal: 48.4s\tremaining: 10.1s\n",
      "827:\tlearn: 0.0010888\ttotal: 48.4s\tremaining: 10.1s\n",
      "828:\tlearn: 0.0010888\ttotal: 48.5s\tremaining: 10s\n",
      "829:\tlearn: 0.0010887\ttotal: 48.5s\tremaining: 9.94s\n",
      "830:\tlearn: 0.0010887\ttotal: 48.6s\tremaining: 9.88s\n",
      "831:\tlearn: 0.0010885\ttotal: 48.7s\tremaining: 9.83s\n",
      "832:\tlearn: 0.0010884\ttotal: 48.7s\tremaining: 9.77s\n",
      "833:\tlearn: 0.0010883\ttotal: 48.8s\tremaining: 9.71s\n",
      "834:\tlearn: 0.0010883\ttotal: 48.8s\tremaining: 9.65s\n",
      "835:\tlearn: 0.0010882\ttotal: 48.9s\tremaining: 9.59s\n",
      "836:\tlearn: 0.0010880\ttotal: 49s\tremaining: 9.53s\n",
      "837:\tlearn: 0.0010806\ttotal: 49s\tremaining: 9.48s\n",
      "838:\tlearn: 0.0010806\ttotal: 49.1s\tremaining: 9.42s\n",
      "839:\tlearn: 0.0010806\ttotal: 49.1s\tremaining: 9.36s\n",
      "840:\tlearn: 0.0010806\ttotal: 49.2s\tremaining: 9.3s\n",
      "841:\tlearn: 0.0010805\ttotal: 49.2s\tremaining: 9.24s\n",
      "842:\tlearn: 0.0010805\ttotal: 49.3s\tremaining: 9.18s\n",
      "843:\tlearn: 0.0010805\ttotal: 49.4s\tremaining: 9.12s\n",
      "844:\tlearn: 0.0010801\ttotal: 49.4s\tremaining: 9.06s\n",
      "845:\tlearn: 0.0010801\ttotal: 49.5s\tremaining: 9.01s\n",
      "846:\tlearn: 0.0010799\ttotal: 49.5s\tremaining: 8.95s\n",
      "847:\tlearn: 0.0010798\ttotal: 49.6s\tremaining: 8.89s\n",
      "848:\tlearn: 0.0010797\ttotal: 49.6s\tremaining: 8.83s\n",
      "849:\tlearn: 0.0010797\ttotal: 49.7s\tremaining: 8.77s\n",
      "850:\tlearn: 0.0010796\ttotal: 49.8s\tremaining: 8.71s\n",
      "851:\tlearn: 0.0010795\ttotal: 49.8s\tremaining: 8.65s\n",
      "852:\tlearn: 0.0010795\ttotal: 49.9s\tremaining: 8.6s\n",
      "853:\tlearn: 0.0010791\ttotal: 49.9s\tremaining: 8.54s\n",
      "854:\tlearn: 0.0010785\ttotal: 50s\tremaining: 8.48s\n",
      "855:\tlearn: 0.0010782\ttotal: 50.1s\tremaining: 8.42s\n",
      "856:\tlearn: 0.0010780\ttotal: 50.1s\tremaining: 8.36s\n",
      "857:\tlearn: 0.0010780\ttotal: 50.2s\tremaining: 8.3s\n",
      "858:\tlearn: 0.0010777\ttotal: 50.2s\tremaining: 8.24s\n",
      "859:\tlearn: 0.0010776\ttotal: 50.3s\tremaining: 8.19s\n",
      "860:\tlearn: 0.0010773\ttotal: 50.3s\tremaining: 8.13s\n",
      "861:\tlearn: 0.0010771\ttotal: 50.4s\tremaining: 8.07s\n",
      "862:\tlearn: 0.0010770\ttotal: 50.5s\tremaining: 8.01s\n",
      "863:\tlearn: 0.0010769\ttotal: 50.5s\tremaining: 7.95s\n",
      "864:\tlearn: 0.0010769\ttotal: 50.6s\tremaining: 7.89s\n",
      "865:\tlearn: 0.0010761\ttotal: 50.6s\tremaining: 7.83s\n",
      "866:\tlearn: 0.0010754\ttotal: 50.7s\tremaining: 7.77s\n",
      "867:\tlearn: 0.0010753\ttotal: 50.7s\tremaining: 7.71s\n",
      "868:\tlearn: 0.0010753\ttotal: 50.8s\tremaining: 7.66s\n",
      "869:\tlearn: 0.0010751\ttotal: 50.8s\tremaining: 7.6s\n",
      "870:\tlearn: 0.0010750\ttotal: 50.9s\tremaining: 7.54s\n",
      "871:\tlearn: 0.0010748\ttotal: 51s\tremaining: 7.48s\n",
      "872:\tlearn: 0.0010748\ttotal: 51s\tremaining: 7.42s\n",
      "873:\tlearn: 0.0010700\ttotal: 51.1s\tremaining: 7.36s\n",
      "874:\tlearn: 0.0010692\ttotal: 51.1s\tremaining: 7.3s\n",
      "875:\tlearn: 0.0010692\ttotal: 51.2s\tremaining: 7.25s\n",
      "876:\tlearn: 0.0010690\ttotal: 51.3s\tremaining: 7.19s\n",
      "877:\tlearn: 0.0010687\ttotal: 51.3s\tremaining: 7.13s\n",
      "878:\tlearn: 0.0010678\ttotal: 51.4s\tremaining: 7.07s\n",
      "879:\tlearn: 0.0010677\ttotal: 51.4s\tremaining: 7.01s\n",
      "880:\tlearn: 0.0010676\ttotal: 51.5s\tremaining: 6.95s\n",
      "881:\tlearn: 0.0010676\ttotal: 51.5s\tremaining: 6.89s\n",
      "882:\tlearn: 0.0010674\ttotal: 51.6s\tremaining: 6.84s\n",
      "883:\tlearn: 0.0010674\ttotal: 51.7s\tremaining: 6.78s\n",
      "884:\tlearn: 0.0010674\ttotal: 51.7s\tremaining: 6.72s\n",
      "885:\tlearn: 0.0010673\ttotal: 51.8s\tremaining: 6.66s\n",
      "886:\tlearn: 0.0010673\ttotal: 51.8s\tremaining: 6.6s\n",
      "887:\tlearn: 0.0010672\ttotal: 51.9s\tremaining: 6.54s\n",
      "888:\tlearn: 0.0010662\ttotal: 51.9s\tremaining: 6.49s\n",
      "889:\tlearn: 0.0010662\ttotal: 52s\tremaining: 6.43s\n",
      "890:\tlearn: 0.0010661\ttotal: 52.1s\tremaining: 6.37s\n",
      "891:\tlearn: 0.0010660\ttotal: 52.1s\tremaining: 6.31s\n",
      "892:\tlearn: 0.0010660\ttotal: 52.2s\tremaining: 6.25s\n",
      "893:\tlearn: 0.0010660\ttotal: 52.2s\tremaining: 6.19s\n",
      "894:\tlearn: 0.0010660\ttotal: 52.3s\tremaining: 6.13s\n",
      "895:\tlearn: 0.0010660\ttotal: 52.3s\tremaining: 6.07s\n",
      "896:\tlearn: 0.0010659\ttotal: 52.4s\tremaining: 6.02s\n",
      "897:\tlearn: 0.0010659\ttotal: 52.5s\tremaining: 5.96s\n",
      "898:\tlearn: 0.0010659\ttotal: 52.5s\tremaining: 5.9s\n",
      "899:\tlearn: 0.0010659\ttotal: 52.6s\tremaining: 5.84s\n",
      "900:\tlearn: 0.0010656\ttotal: 52.6s\tremaining: 5.78s\n",
      "901:\tlearn: 0.0010655\ttotal: 52.7s\tremaining: 5.72s\n",
      "902:\tlearn: 0.0010653\ttotal: 52.7s\tremaining: 5.67s\n",
      "903:\tlearn: 0.0010646\ttotal: 52.8s\tremaining: 5.61s\n",
      "904:\tlearn: 0.0010643\ttotal: 52.9s\tremaining: 5.55s\n",
      "905:\tlearn: 0.0010642\ttotal: 52.9s\tremaining: 5.49s\n",
      "906:\tlearn: 0.0010641\ttotal: 53s\tremaining: 5.43s\n",
      "907:\tlearn: 0.0010639\ttotal: 53s\tremaining: 5.37s\n",
      "908:\tlearn: 0.0010638\ttotal: 53.1s\tremaining: 5.31s\n",
      "909:\tlearn: 0.0010638\ttotal: 53.1s\tremaining: 5.26s\n",
      "910:\tlearn: 0.0010637\ttotal: 53.2s\tremaining: 5.2s\n",
      "911:\tlearn: 0.0010635\ttotal: 53.3s\tremaining: 5.14s\n",
      "912:\tlearn: 0.0010635\ttotal: 53.3s\tremaining: 5.08s\n",
      "913:\tlearn: 0.0010634\ttotal: 53.4s\tremaining: 5.02s\n",
      "914:\tlearn: 0.0010631\ttotal: 53.4s\tremaining: 4.96s\n",
      "915:\tlearn: 0.0010626\ttotal: 53.5s\tremaining: 4.9s\n",
      "916:\tlearn: 0.0010625\ttotal: 53.5s\tremaining: 4.85s\n",
      "917:\tlearn: 0.0010625\ttotal: 53.6s\tremaining: 4.79s\n",
      "918:\tlearn: 0.0010622\ttotal: 53.7s\tremaining: 4.73s\n",
      "919:\tlearn: 0.0010621\ttotal: 53.7s\tremaining: 4.67s\n",
      "920:\tlearn: 0.0010620\ttotal: 53.8s\tremaining: 4.61s\n",
      "921:\tlearn: 0.0010619\ttotal: 53.8s\tremaining: 4.55s\n",
      "922:\tlearn: 0.0010615\ttotal: 53.9s\tremaining: 4.5s\n",
      "923:\tlearn: 0.0010614\ttotal: 53.9s\tremaining: 4.44s\n",
      "924:\tlearn: 0.0010610\ttotal: 54s\tremaining: 4.38s\n",
      "925:\tlearn: 0.0010609\ttotal: 54.1s\tremaining: 4.32s\n",
      "926:\tlearn: 0.0010607\ttotal: 54.1s\tremaining: 4.26s\n",
      "927:\tlearn: 0.0010605\ttotal: 54.2s\tremaining: 4.2s\n",
      "928:\tlearn: 0.0010605\ttotal: 54.2s\tremaining: 4.14s\n",
      "929:\tlearn: 0.0010604\ttotal: 54.3s\tremaining: 4.08s\n",
      "930:\tlearn: 0.0010603\ttotal: 54.3s\tremaining: 4.03s\n",
      "931:\tlearn: 0.0010599\ttotal: 54.4s\tremaining: 3.97s\n",
      "932:\tlearn: 0.0010598\ttotal: 54.5s\tremaining: 3.91s\n",
      "933:\tlearn: 0.0010596\ttotal: 54.5s\tremaining: 3.85s\n",
      "934:\tlearn: 0.0010596\ttotal: 54.6s\tremaining: 3.79s\n",
      "935:\tlearn: 0.0010593\ttotal: 54.6s\tremaining: 3.73s\n",
      "936:\tlearn: 0.0010592\ttotal: 54.7s\tremaining: 3.68s\n",
      "937:\tlearn: 0.0010592\ttotal: 54.7s\tremaining: 3.62s\n",
      "938:\tlearn: 0.0010592\ttotal: 54.8s\tremaining: 3.56s\n",
      "939:\tlearn: 0.0010590\ttotal: 54.9s\tremaining: 3.5s\n",
      "940:\tlearn: 0.0010590\ttotal: 54.9s\tremaining: 3.44s\n",
      "941:\tlearn: 0.0010538\ttotal: 55s\tremaining: 3.38s\n",
      "942:\tlearn: 0.0010537\ttotal: 55s\tremaining: 3.33s\n",
      "943:\tlearn: 0.0010533\ttotal: 55.1s\tremaining: 3.27s\n",
      "944:\tlearn: 0.0010528\ttotal: 55.2s\tremaining: 3.21s\n",
      "945:\tlearn: 0.0010528\ttotal: 55.2s\tremaining: 3.15s\n",
      "946:\tlearn: 0.0010527\ttotal: 55.3s\tremaining: 3.09s\n",
      "947:\tlearn: 0.0010526\ttotal: 55.3s\tremaining: 3.03s\n",
      "948:\tlearn: 0.0010524\ttotal: 55.4s\tremaining: 2.98s\n",
      "949:\tlearn: 0.0010521\ttotal: 55.4s\tremaining: 2.92s\n",
      "950:\tlearn: 0.0010518\ttotal: 55.5s\tremaining: 2.86s\n",
      "951:\tlearn: 0.0010517\ttotal: 55.6s\tremaining: 2.8s\n",
      "952:\tlearn: 0.0010517\ttotal: 55.6s\tremaining: 2.74s\n",
      "953:\tlearn: 0.0010517\ttotal: 55.7s\tremaining: 2.68s\n",
      "954:\tlearn: 0.0010516\ttotal: 55.7s\tremaining: 2.63s\n",
      "955:\tlearn: 0.0010508\ttotal: 55.8s\tremaining: 2.57s\n",
      "956:\tlearn: 0.0010507\ttotal: 55.9s\tremaining: 2.51s\n",
      "957:\tlearn: 0.0010506\ttotal: 55.9s\tremaining: 2.45s\n",
      "958:\tlearn: 0.0010504\ttotal: 56s\tremaining: 2.39s\n",
      "959:\tlearn: 0.0010450\ttotal: 56s\tremaining: 2.33s\n",
      "960:\tlearn: 0.0010446\ttotal: 56.1s\tremaining: 2.28s\n",
      "961:\tlearn: 0.0010444\ttotal: 56.2s\tremaining: 2.22s\n",
      "962:\tlearn: 0.0010444\ttotal: 56.2s\tremaining: 2.16s\n",
      "963:\tlearn: 0.0010444\ttotal: 56.3s\tremaining: 2.1s\n",
      "964:\tlearn: 0.0010441\ttotal: 56.3s\tremaining: 2.04s\n",
      "965:\tlearn: 0.0010440\ttotal: 56.4s\tremaining: 1.98s\n",
      "966:\tlearn: 0.0010440\ttotal: 56.4s\tremaining: 1.93s\n",
      "967:\tlearn: 0.0010436\ttotal: 56.5s\tremaining: 1.87s\n",
      "968:\tlearn: 0.0010434\ttotal: 56.6s\tremaining: 1.81s\n",
      "969:\tlearn: 0.0010433\ttotal: 56.6s\tremaining: 1.75s\n",
      "970:\tlearn: 0.0010433\ttotal: 56.7s\tremaining: 1.69s\n",
      "971:\tlearn: 0.0010431\ttotal: 56.7s\tremaining: 1.63s\n",
      "972:\tlearn: 0.0010430\ttotal: 56.8s\tremaining: 1.57s\n",
      "973:\tlearn: 0.0010430\ttotal: 56.9s\tremaining: 1.52s\n",
      "974:\tlearn: 0.0010430\ttotal: 56.9s\tremaining: 1.46s\n",
      "975:\tlearn: 0.0010426\ttotal: 57s\tremaining: 1.4s\n",
      "976:\tlearn: 0.0010426\ttotal: 57s\tremaining: 1.34s\n",
      "977:\tlearn: 0.0010411\ttotal: 57.1s\tremaining: 1.28s\n",
      "978:\tlearn: 0.0010411\ttotal: 57.1s\tremaining: 1.23s\n",
      "979:\tlearn: 0.0010410\ttotal: 57.2s\tremaining: 1.17s\n",
      "980:\tlearn: 0.0010410\ttotal: 57.3s\tremaining: 1.11s\n",
      "981:\tlearn: 0.0010410\ttotal: 57.3s\tremaining: 1.05s\n",
      "982:\tlearn: 0.0010408\ttotal: 57.4s\tremaining: 992ms\n",
      "983:\tlearn: 0.0010408\ttotal: 57.4s\tremaining: 934ms\n",
      "984:\tlearn: 0.0010408\ttotal: 57.5s\tremaining: 875ms\n",
      "985:\tlearn: 0.0010405\ttotal: 57.5s\tremaining: 817ms\n",
      "986:\tlearn: 0.0010404\ttotal: 57.6s\tremaining: 759ms\n",
      "987:\tlearn: 0.0010404\ttotal: 57.7s\tremaining: 700ms\n",
      "988:\tlearn: 0.0010403\ttotal: 57.7s\tremaining: 642ms\n",
      "989:\tlearn: 0.0010402\ttotal: 57.8s\tremaining: 584ms\n",
      "990:\tlearn: 0.0010400\ttotal: 57.8s\tremaining: 525ms\n",
      "991:\tlearn: 0.0010386\ttotal: 57.9s\tremaining: 467ms\n",
      "992:\tlearn: 0.0010367\ttotal: 58s\tremaining: 409ms\n",
      "993:\tlearn: 0.0010366\ttotal: 58s\tremaining: 350ms\n",
      "994:\tlearn: 0.0010366\ttotal: 58.1s\tremaining: 292ms\n",
      "995:\tlearn: 0.0010364\ttotal: 58.1s\tremaining: 233ms\n",
      "996:\tlearn: 0.0010363\ttotal: 58.2s\tremaining: 175ms\n",
      "997:\tlearn: 0.0010363\ttotal: 58.3s\tremaining: 117ms\n",
      "998:\tlearn: 0.0010362\ttotal: 58.3s\tremaining: 58.4ms\n",
      "999:\tlearn: 0.0010362\ttotal: 58.4s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "cgb_forecast_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1', 'updrs_2', 'updrs_3'], [updrs1_yr_df, updrs2_yr_df, updrs3_yr_df]):\n",
    "    model = prepare_catboost_model(cboost_forecast_hyperparams_df, updrs)\n",
    "    print(f'UPDRS: {updrs}')\n",
    "    print(f'Hyperparameters: {model.get_params()}')\n",
    "    print('\\n')\n",
    "    auc, acc, prec, recall = cross_fold_validation(df, model, updrs)\n",
    "    cgb_forecast_results[updrs] = {\"auc\":auc,\n",
    "                        \"acc\":acc,\n",
    "                        \"prec\":prec,\n",
    "                        \"recall\":recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updrs_1': {'auc': 0.6875180353730299,\n",
       "  'acc': 0.7291737868499222,\n",
       "  'prec': 0.7309163059163059,\n",
       "  'recall': 0.4883101242882365},\n",
       " 'updrs_2': {'auc': 0.6656831081016579,\n",
       "  'acc': 0.7504308191808191,\n",
       "  'prec': 0.643524896156475,\n",
       "  'recall': 0.44420327498588363},\n",
       " 'updrs_3': {'auc': 0.661142439158262,\n",
       "  'acc': 0.6960405051769257,\n",
       "  'prec': 0.6438998501498501,\n",
       "  'recall': 0.503075681492109}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgb_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def create_folds(df, target):\n",
    "    # calculate the number of bins by Sturge's rule\n",
    "    num_bins = int(np.floor(1 + np.log2(len(df))))\n",
    "    df.loc[:, \"bins\"] = pd.cut(df[f'{updrs}_cat'], bins=num_bins, labels=False)\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "        \n",
    "    # initiate the kfold class from sklearn\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "        \n",
    "    # create a kfold column\n",
    "    df['kfold'] = -1\n",
    "\n",
    "    # fill the kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=df['bins'].values)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "            \n",
    "    # drop the bins column\n",
    "    df = df.drop('bins', axis=1)\n",
    "    max_kfold = df['kfold'].max()\n",
    "    \n",
    "    print(f'{max_kfold + 1} Kfolds created for {target}_cat')\n",
    "    return df, max_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost(train_df, test_df, updrs):\n",
    "    features = train_df.drop([f'{updrs}_cat', 'kfold'], axis=1)\n",
    "    target = train_df[f'{updrs}_cat']\n",
    "    X_val = val_df.drop([f'{updrs}_cat','kfold'] axis=1)\n",
    "    y_val = val_df[f'{updrs}_cat']\n",
    "\n",
    "    # Define the CatBoost classifier\n",
    "    model = CatBoostClassifier(iterations=1000, eval_metric='AUC', random_seed=42)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model.fit(features, \n",
    "              target,\n",
    "                #early_stopping_rounds=50, \n",
    "                verbose=100)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict_proba(test_df)[:, 1]\n",
    "\n",
    "    # Evaluate AUC on the test data\n",
    "    auc = roc_auc_score(test_df[f'{updrs}_cat'], predictions)\n",
    "    print(\"AUC on Test Data:\", auc)\n",
    "\n",
    "    # Save the best iteration of the model\n",
    "    model.save_model(f'../models/catboost_{updrs}_model.cbm', format='cbm', pool=None)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Kfolds created for updrs_1_cat\n",
      "Learning rate set to 0.023058\n",
      "0:\ttest: 0.4894349\tbest: 0.4894349 (0)\ttotal: 279ms\tremaining: 4m 38s\n",
      "100:\ttest: 0.6599509\tbest: 0.6633907 (98)\ttotal: 20.3s\tremaining: 3m\n",
      "200:\ttest: 0.6914005\tbest: 0.6914005 (169)\ttotal: 38.9s\tremaining: 2m 34s\n",
      "300:\ttest: 0.7027027\tbest: 0.7071253 (287)\ttotal: 57.5s\tremaining: 2m 13s\n",
      "400:\ttest: 0.7115479\tbest: 0.7154791 (386)\ttotal: 1m 16s\tremaining: 1m 53s\n",
      "500:\ttest: 0.7159705\tbest: 0.7169533 (477)\ttotal: 1m 34s\tremaining: 1m 34s\n",
      "600:\ttest: 0.7130221\tbest: 0.7169533 (477)\ttotal: 1m 53s\tremaining: 1m 15s\n",
      "700:\ttest: 0.7135135\tbest: 0.7169533 (477)\ttotal: 2m 11s\tremaining: 56.3s\n",
      "800:\ttest: 0.7233415\tbest: 0.7243243 (785)\ttotal: 2m 30s\tremaining: 37.4s\n",
      "900:\ttest: 0.7233415\tbest: 0.7248157 (808)\ttotal: 2m 49s\tremaining: 18.6s\n",
      "999:\ttest: 0.7267813\tbest: 0.7272727 (960)\ttotal: 3m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7272727273\n",
      "bestIteration = 960\n",
      "\n",
      "Shrink model to first 961 iterations.\n",
      "AUC on Test Data: 0.7326781326781326\n"
     ]
    }
   ],
   "source": [
    "# train and save the best catboost model for each updrs\n",
    "cboost_results = dict()\n",
    "\n",
    "for updrs, df in zip(['updrs_1'], [updrs1_yr_df]):\n",
    "    model = prepare_catboost_model(cboost_forecast_hyperparams_df, updrs)\n",
    "    temp_df, test_kfold = create_folds(df, updrs)\n",
    "    val_kfold = test_kfold - 1 \n",
    "    temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "    X_train = temp_df[(temp_df['kfold'] != test_kfold)&(temp_df['kfold']!=val_kfold)].reset_index(drop=True)\n",
    "    y_train = X_train[f'{updrs}_cat']\n",
    "    X_test = temp_df[temp_df['kfold'] == test_kfold].reset_index(drop=True)\n",
    "    y_test = X_test[f'{updrs}_cat']\n",
    "    val_df = temp_df[temp_df['kfold'] == val_kfold].reset_index(drop=True)\n",
    "    \n",
    "    best_model = train_catboost(X_train, val_df, X_test, updrs)\n",
    "    \n",
    "    #preds = best_model.predict(X_test.drop(columns = [f'{updrs}_cat', 'kfold']))\n",
    "    #test_auc = roc_auc_score(y_test, preds)\n",
    "    #preds = best_model.predict(X_train.drop(columns=[f'{updrs}_cat', 'kfold']))\n",
    "    #train_auc = roc_auc_score(y_train, preds)\n",
    "    \n",
    "    #cboost_results[updrs] = {'test_auc':test_auc, 'train_auc':train_auc}\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6525798525798526, 1.0)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the results of the model\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, test_preds)\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_auc = roc_auc_score(y_train, train_preds)\n",
    "test_auc, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Kfolds created for updrs_2_cat\n",
      "Learning rate set to 0.023058\n",
      "0:\ttest: 0.5623974\tbest: 0.5623974 (0)\ttotal: 206ms\tremaining: 3m 26s\n",
      "100:\ttest: 0.7695676\tbest: 0.7766831 (95)\ttotal: 18.2s\tremaining: 2m 42s\n",
      "200:\ttest: 0.7996716\tbest: 0.8111658 (176)\ttotal: 36.1s\tremaining: 2m 23s\n",
      "300:\ttest: 0.8292282\tbest: 0.8314176 (299)\ttotal: 54.1s\tremaining: 2m 5s\n",
      "400:\ttest: 0.8445539\tbest: 0.8467433 (381)\ttotal: 1m 12s\tremaining: 1m 48s\n",
      "500:\ttest: 0.8494800\tbest: 0.8549535 (464)\ttotal: 1m 30s\tremaining: 1m 29s\n",
      "600:\ttest: 0.8560482\tbest: 0.8565955 (581)\ttotal: 1m 48s\tremaining: 1m 11s\n",
      "700:\ttest: 0.8555008\tbest: 0.8582375 (674)\ttotal: 2m 6s\tremaining: 54s\n",
      "800:\ttest: 0.8587849\tbest: 0.8587849 (738)\ttotal: 2m 26s\tremaining: 36.4s\n",
      "900:\ttest: 0.8576902\tbest: 0.8593322 (804)\ttotal: 2m 45s\tremaining: 18.2s\n",
      "999:\ttest: 0.8587849\tbest: 0.8604269 (946)\ttotal: 3m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8604269294\n",
      "bestIteration = 946\n",
      "\n",
      "Shrink model to first 947 iterations.\n",
      "AUC on Test Data: 0.8292282430213465\n"
     ]
    }
   ],
   "source": [
    "for updrs, df in zip(['updrs_2'], [updrs2_yr_df]):\n",
    "    model = prepare_catboost_model(cboost_forecast_hyperparams_df, updrs)\n",
    "    temp_df, test_kfold = create_folds(df, updrs)\n",
    "    val_kfold = test_kfold - 1 \n",
    "    temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "    X_train = temp_df[(temp_df['kfold'] != test_kfold)&(temp_df['kfold']!=val_kfold)].reset_index(drop=True)\n",
    "    y_train = X_train[f'{updrs}_cat']\n",
    "    X_test = temp_df[temp_df['kfold'] == test_kfold].reset_index(drop=True)\n",
    "    y_test = X_test[f'{updrs}_cat']\n",
    "    val_df = temp_df[temp_df['kfold'] == val_kfold].reset_index(drop=True)\n",
    "    \n",
    "    best_model = train_catboost(X_train, val_df, X_test, updrs)\n",
    "    \n",
    "    #preds = best_model.predict(X_test.drop(columns = [f'{updrs}_cat', 'kfold']))\n",
    "    #test_auc = roc_auc_score(y_test, preds)\n",
    "    #preds = best_model.predict(X_train.drop(columns=[f'{updrs}_cat', 'kfold']))\n",
    "    #train_auc = roc_auc_score(y_train, preds)\n",
    "    \n",
    "    #cboost_results[updrs] = {'test_auc':test_auc, 'train_auc':train_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6737821565407772, 1.0)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the results of the model\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, test_preds)\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_auc = roc_auc_score(y_train, train_preds)\n",
    "test_auc, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Kfolds created for updrs_3_cat\n",
      "Learning rate set to 0.023037\n",
      "0:\ttest: 0.5795918\tbest: 0.5795918 (0)\ttotal: 212ms\tremaining: 3m 32s\n",
      "100:\ttest: 0.6540816\tbest: 0.6709184 (92)\ttotal: 19s\tremaining: 2m 49s\n",
      "200:\ttest: 0.6943878\tbest: 0.6989796 (191)\ttotal: 40.1s\tremaining: 2m 39s\n",
      "300:\ttest: 0.7306122\tbest: 0.7311224 (294)\ttotal: 1m 1s\tremaining: 2m 23s\n",
      "400:\ttest: 0.7372449\tbest: 0.7403061 (359)\ttotal: 1m 22s\tremaining: 2m 3s\n",
      "500:\ttest: 0.7494898\tbest: 0.7510204 (493)\ttotal: 1m 39s\tremaining: 1m 39s\n",
      "600:\ttest: 0.7530612\tbest: 0.7556122 (572)\ttotal: 1m 56s\tremaining: 1m 17s\n",
      "700:\ttest: 0.7551020\tbest: 0.7571429 (689)\ttotal: 2m 13s\tremaining: 57s\n",
      "800:\ttest: 0.7586735\tbest: 0.7602041 (780)\ttotal: 2m 30s\tremaining: 37.4s\n",
      "900:\ttest: 0.7576531\tbest: 0.7612245 (822)\ttotal: 2m 47s\tremaining: 18.4s\n",
      "999:\ttest: 0.7596939\tbest: 0.7612245 (822)\ttotal: 3m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7612244898\n",
      "bestIteration = 822\n",
      "\n",
      "Shrink model to first 823 iterations.\n",
      "AUC on Test Data: 0.7775510204081633\n"
     ]
    }
   ],
   "source": [
    "for updrs, df in zip(['updrs_3'], [updrs3_yr_df]):\n",
    "    model = prepare_catboost_model(cboost_forecast_hyperparams_df, updrs)\n",
    "    temp_df, test_kfold = create_folds(df, updrs)\n",
    "    val_kfold = test_kfold - 1 \n",
    "    temp_df = temp_df.drop(columns=['visit_id', 'patient_id', f'{updrs}'])\n",
    "    X_train = temp_df[(temp_df['kfold'] != test_kfold)&(temp_df['kfold']!=val_kfold)].reset_index(drop=True)\n",
    "    y_train = X_train[f'{updrs}_cat']\n",
    "    X_test = temp_df[temp_df['kfold'] == test_kfold].reset_index(drop=True)\n",
    "    y_test = X_test[f'{updrs}_cat']\n",
    "    val_df = temp_df[temp_df['kfold'] == val_kfold].reset_index(drop=True)\n",
    "    \n",
    "    best_model = train_catboost(X_train, val_df, X_test, updrs)\n",
    "    \n",
    "    #preds = best_model.predict(X_test.drop(columns = [f'{updrs}_cat', 'kfold']))\n",
    "    #test_auc = roc_auc_score(y_test, preds)\n",
    "    #preds = best_model.predict(X_train.drop(columns=[f'{updrs}_cat', 'kfold']))\n",
    "    #train_auc = roc_auc_score(y_train, preds)\n",
    "    \n",
    "    #cboost_results[updrs] = {'test_auc':test_auc, 'train_auc':train_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6303571428571428, 1.0)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the results of the model\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, test_preds)\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_auc = roc_auc_score(y_train, train_preds)\n",
    "test_auc, train_auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# run cross fold validation on the updrs 1 data\n",
    "updrs1_results = cross_fold_validation(updrs1_df, model, 'updrs_1')\n",
    "\n",
    "model = LogisticRegression()\n",
    "updrs2_results = cross_fold_validation(updrs2_df, model, 'updrs_2')\n",
    "\n",
    "model = LogisticRegression()\n",
    "updrs3_results = cross_fold_validation(updrs3_df, model, 'updrs_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.511721383287093,\n",
       " 0.780847703040674,\n",
       " 0.32282051282051283,\n",
       " 0.062211088895139766)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5258909849605227,\n",
       " 0.8464349962704578,\n",
       " 0.42333333333333323,\n",
       " 0.07048988874546684)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5240749211448716,\n",
       " 0.8194894035589735,\n",
       " 0.3485714285714286,\n",
       " 0.07900396151669495)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs3_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "c:\\Users\\Dagart\\anaconda3\\envs\\easypy37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# run cross fold validation on the updrs 1 data\n",
    "updrs1_forecast_results = cross_fold_validation(updrs1_yr_df, model, 'updrs_1')\n",
    "\n",
    "model = LogisticRegression()\n",
    "updrs2_forecast_results = cross_fold_validation(updrs2_yr_df, model, 'updrs_2')\n",
    "\n",
    "model = LogisticRegression()\n",
    "updrs3_forecast_results = cross_fold_validation(updrs3_yr_df, model, 'updrs_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5634874999789138,\n",
       " 0.5994172665293609,\n",
       " 0.4912421630094044,\n",
       " 0.3920253253358589)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs1_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.582742150911908, 0.6805340492840493, 0.4779128959276018, 0.3289821193299454)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs2_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5758538282007636,\n",
       " 0.6218648310387985,\n",
       " 0.5047414576826341,\n",
       " 0.3744481451117032)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updrs3_forecast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
